[
    {
        "link": "https://geeksforgeeks.org/huffman-coding-in-python",
        "document": "Huffman Coding is one of the most popular lossless data compression techniques. This article aims at diving deep into the Huffman Coding and its implementation in Python.\n\nHuffman Coding is an approach used in lossless data compression with the primary objective of delivering reduced transit size without any loss of meaningful data content. There is a key rule that lies in Huffman coding: use shorter codes for frequent letters and longer ones for uncommon letters. Through the employment of the binary tree, called the Huffman tree, the frequency of each character is illustrated, where each leaf node represents the character and its frequency. Shorter codes which are arrived at by traveling from the tree root to the leaf node representing the character, are the ones that are applied.\n\nFirst of all, we define a class to instantiate Huffman nodes. Every node includes the details such as character frequency, rank of its left and right children.\n\nNow, we design a function to construct our Huffman tree. We apply priority queue (heap) to link the nodes according to the lowest frequencies, and when the only one node is left there, it roots the Huffman tree.Ôªø\n\nFor that, next we move across the Huffman tree to produce the Huffman codes for each character. Raw binary code is built from the left (with no preceding bit) to the right (bit is one), until occurring upon the character's leaf.\n\nBelow is the implementation of above approach:\n\nThe Huffman Coding is an effective algorithm for data compression because it saved both storage space and transmission time. Through the effective assignment of symbol codes of variable length to the symbols by following the rule of higher frequency to lower code, Huffman coding optimizes the compression ratio and maintain the soundness of the data."
    },
    {
        "link": "https://geeksforgeeks.org/huffman-coding-greedy-algo-3",
        "document": "Huffman coding is a lossless data compression algorithm. The idea is to assign variable-length codes to input characters, lengths of the assigned codes are based on the frequencies of corresponding characters. \n\nThe variable-length codes assigned to input characters are Prefix Codes, means the codes (bit sequences) are assigned in such a way that the code assigned to one character is not the prefix of code assigned to any other character. This is how Huffman Coding makes sure that there is no ambiguity when decoding the generated bitstream. \n\nLet us understand prefix codes with a counter example. Let there be four characters a, b, c and d, and their corresponding variable length codes be 00, 01, 0 and 1. This coding leads to ambiguity because code assigned to c is the prefix of codes assigned to a and b. If the compressed bit stream is 0001, the de-compressed output may be ‚Äúcccd‚Äù or ‚Äúccb‚Äù or ‚Äúacd‚Äù or ‚Äúab‚Äù.\n\nSee this for applications of Huffman Coding. \n\nThere are mainly two major parts in Huffman Coding\n‚Ä¢ Traverse the Huffman Tree and assign codes to characters.\n\nThe method which is used to construct optimal prefix code is called Huffman coding.\n\nThis algorithm builds a tree in bottom up manner. We can denote this tree by\n\nLet, |c| be number of leaves\n\n|c| -1 are number of operations required to merge the nodes. Q be the priority queue which can be used while constructing binary heap.\n\nSteps to build Huffman Tree\n\nInput is an array of unique characters along with their frequency of occurrences and output is Huffman Tree.\n‚Ä¢ Create a leaf node for each unique character and build a min heap of all leaf nodes (Min Heap is used as a priority queue. The value of frequency field is used to compare two nodes in min heap. Initially, the least frequent character is at root)\n‚Ä¢ Extract two nodes with the minimum frequency from the min heap.\n‚Ä¢ Create a new internal node with a frequency equal to the sum of the two nodes frequencies. Make the first extracted node as its left child and the other extracted node as its right child. Add this node to the min heap.\n‚Ä¢ Repeat steps#2 and #3 until the heap contains only one node. The remaining node is the root node and the tree is complete.\n\nLet us understand the algorithm with an example:\n\nStep 1. Build a min heap that contains 6 nodes where each node represents root of a tree with single node.\n\nStep 2 Extract two minimum frequency nodes from min heap. Add a new internal node with frequency 5 + 9 = 14. \n\n\n\nNow min heap contains 5 nodes where 4 nodes are roots of trees with single element each, and one heap node is root of tree with 3 elements\n\nStep 3: Extract two minimum frequency nodes from heap. Add a new internal node with frequency 12 + 13 = 25\n\n\n\nNow min heap contains 4 nodes where 2 nodes are roots of trees with single element each, and two heap nodes are root of tree with more than one nodes\n\nStep 4: Extract two minimum frequency nodes. Add a new internal node with frequency 14 + 16 = 30\n\n\n\nStep 5: Extract two minimum frequency nodes. Add a new internal node with frequency 25 + 30 = 55\n\n\n\nStep 6: Extract two minimum frequency nodes. Add a new internal node with frequency 45 + 55 = 100\n\n\n\nNow min heap contains only one node.\n\nSince the heap contains only one node, the algorithm stops here.\n\nSteps to print codes from Huffman Tree:\n\nTraverse the tree formed starting from the root. Maintain an auxiliary array. While moving to the left child, write 0 to the array. While moving to the right child, write 1 to the array. Print the array when a leaf node is encountered.\n\n\n\nThe codes are as follows:\n\nBelow is the implementation of above approach:\n\nTime complexity: O(nlogn) where n is the number of unique characters. If there are n nodes, extractMin() is called 2*(n ‚Äì 1) times. extractMin() takes O(logn) time as it calls minHeapify(). So, the overall complexity is O(nlogn).\n\nIf the input array is sorted, there exists a linear time algorithm. We will soon be discussing this in our next post.\n‚Ä¢ They are used for transmitting fax and text.\n‚Ä¢ They are used by conventional compression formats like PKZIP, GZIP, etc.\n‚Ä¢ Multimedia codecs like JPEG, PNG, and MP3 use Huffman encoding(to be more precise the prefix codes).\n\nIt is useful in cases where there is a series of frequently occurring characters.\n\nReference:\n\nhttp://en.wikipedia.org/wiki/Huffman_coding\n\nThis article is compiled by Aashish Barnwal and reviewed by GeeksforGeeks team."
    },
    {
        "link": "https://favtutor.com/blogs/huffman-coding",
        "document": "We uphold a strict editorial policy that emphasizes factual accuracy, relevance, and impartiality. Our content is crafted by top technical writers with deep knowledge in the fields of computer science and data science, ensuring each piece is meticulously reviewed by a team of seasoned editors to guarantee compliance with the highest standards in educational content creation and publishing.\n\nHuffman coding is a type of greedy algorithm developed by David A. Huffman during the late 19th century. It is one of the most used algorithms for various purposes all over the technical domain. In this article, we will study Huffman coding, example, algorithm, and its implementation using python.\n\nHuffman coding is a greedy algorithm frequently used for lossless data compression. The basic principle of Huffman coding is to compress and encode the text or the data depending on the frequency of the characters in the text.\n\nThe idea of this algorithm is to assign variable-length codes to input characters of text based on the frequencies of the corresponding character. So, the most frequent character gets the smallest code, and the least frequent character is assigned the largest code.\n\nHere, the codes assigned to the characters are termed prefix codes which means that the code assigned to one character is not the prefix of the code assigned to any other character. Using this technique, Huffman coding ensures that there is no ambiguity when decoding the generated bitstream.\n\nYes, Huffman coding is a greedy algorithm. It works by selecting the lowest two frequency symbols/subtrees and merging them together at every step until all symbols or subtrees are merged into a single binary tree. This method ensures that the final binary tree minimizes the total number of bits required to represent the symbols in the input.\n\nAs a result, Huffman coding is regarded as a classic example of a greedy algorithm.\n\nLet us understand how Huffman coding works with the example below:\n\nConsider the following input text:\n\nAs the above text is of 11 characters, each character requires 8 bits. Therefore, a total of 11x8=88 bits are required to send this input text.\n\nUsing Huffman coding, we will compress the text to a smaller size by creating a Huffman coding tree using the character frequencies and generating the code for each character.\n\nRemember that we encode the text while sending it, and later, it is necessary to decode it. Hence, the decoding of the text is done using the same tree generated by the Huffman technique.\n\nLet us see how to encode the above text using the Huffman coding algorithm:\n\nLooking at the text, the frequencies of the characters will be as shown in the below image.\n\nNow, we will sort the frequencies string of the characters in increasing order. Consider these characters are stored in the priority queue as shown in the below image.\n\nNow, we will create the Huffman tree using this priority queue. Here, we will create an empty node ‚Äòa‚Äô. Later, we will assign the minimum frequency of the queue as the left child of node ‚Äòa‚Äô and the second minimum frequency as the right child of node ‚Äòa‚Äô.\n\nThe value of node ‚Äòa‚Äô will be the sum of both minimum frequencies and add it to the priority queue as shown in the below image.\n\nRepeat the same process until the complete Huffman tree is formed.\n\nNow, assign 0 to the left edges and 1 to the right edges of the Huffman coding tree as shown below.\n\nRemember that for sending the above text, we will send the tree along with the compressed code for easy decoding. Therefore, the total size is given in the table below:\n\nWithout using the Huffman coding algorithm, the size of the text was 88 bits. Whereas after encoding the text, the size is reduced to 24 + 11 + 16 = 51 bits.\n\nHow to decode the code?\n\nFor decoding the above code, you can traverse the given Huffman tree and find the characters according to the code. For example, if you wish to decode 01, we traverse from the root node as shown in the below image.\n\nHere is the complete algorithm for huffman coding:\n\nHere is the full code to implement Huffman coding in Python:\n\nThe time complexity of Huffman coding is O(n logn), where n is the number of unique characters. It is because the encoding of the text is dependent on the frequency of the characters.\n\nWhat are the two types of encodings in Huffman coding?\n\nIn Huffman coding, there are two types of encodings:\n‚Ä¢ Fixed-length encoding: Each symbol, irrespective of frequency, is assigned a fixed number of bits in this type of encoding. This method is not ideal for data compression and is rarely used.\n‚Ä¢ Variable-length encoding: This type of encoding assigns a variable number of bits to each symbol based on its frequency in the input. Shorter codes are assigned to more frequent symbols, while longer codes are assigned to less frequent symbols. This method is ideal for data compression and is widely used.\n\nWhat is Huffman coding used for?\n\nHere are some of the practical use cases of this algorithm:\n‚Ä¢ Huffman coding is used for conventional compression formats like GZIP, etc\n‚Ä¢ It is used for text and fax transmission\n‚Ä¢ It is used in statistical coding\n‚Ä¢ Huffman coding is used by multimedia codecs like JPEG, PNG, MP3, etc\n\nAre there any drawbacks?\n\nThe major disadvantage of Huffman coding is that it requires two passes over the input data to build the Huffman tree, which can make it slower than other compression algorithms that only require one pass. Furthermore, in some cases, the compression achieved by Huffman coding may not be as good as that achieved by other compression algorithms.\n\nHuffman coding is one of the greedy algorithms widely used by programmers all over the world. It is one of the best ways to compress the data lose it and transfer data over the network efficiently. It is highly recommended to understand the working of Huffman coding and use it to compress your data efficiently."
    },
    {
        "link": "https://w3schools.com/dsa/dsa_ref_huffman_coding.php",
        "document": "Huffman Coding is an algorithm used for lossless data compression.\n\nHuffman Coding is also used as a component in many different compression algorithms. It is used as a component in lossless compressions such as zip, gzip, and png, and even as part of lossy compression algorithms like mp3 and jpeg.\n\nUse the animation below to see how a text can be compressed using Huffman Coding.\n\nThe animation shows how the letters in a text are normally stored using UTF-8, and how Huffman Coding makes it possible to store the same text with fewer bits.\n\nHuffman Coding uses a variable length of bits to represent each piece of data, with a shorter bit representation for the pieces of data that occurs more often.\n\nFurthermore, Huffman Coding ensures that no code is the prefix of another code, which makes the compressed data easy to decode.\n\nData compression is when the original data size is reduced, but the information is mostly, or fully, kept. Sound or music files are for example usually stored in a compressed format, roughly just 10% of the original data size, but with most of the information kept. Lossless means that even after the data is compressed, all the information is still there. This means that for example a compressed text still has all the same letters and characters as the original. Lossy is the other variant of data compression, where some of the original information is lost, or sacrificed, so that the data can be compressed even more. Music, images, and video is normally stored and streamed with lossy compression like mp3, jpeg, and mp4.\n\nTo get a better understanding of how Huffman Coding works, let's create a Huffman code manually, using the same text as in the animation: 'lossless'.\n\nA text is normally stored in the computer using UTF-8, which means that each letter is stored using 8 bits for normal latin letters, like we have in 'lossless'. Other letters or symbols such as '‚Ç¨' or 'ü¶Ñ' are stored using more bits.\n\nTo compress the text 'lossless' using Huffman Coding, we start by counting each letter.\n\nAs you can see in the nodes above, 's' occurs 4 times, 'l' occurs 2 times, and 'o' and 'e' occurs just 1 time each.\n\nWe start building the tree with the least occurring letters 'o' and 'e', and their parent node gets count '2', because the counts for letter 'o' and 'e' are summarized.\n\nThe next nodes that get a new parent node, are the nodes with the lowest count: 'l', and the parent node of 'o' and 'e'.\n\nNow, the last node 's' must be added to the binary tree. Letter node 's' and the parent node with count '4' get a new parent node with count '8'.\n\nFollowing the edges from the root node, we can now determine the Huffman code for each letter in the word 'lossless'.\n\nThe Huffman code for each letter can now be found under each letter node in the image above. A good thing about Huffman coding is that the most used data pieces get the shortest code, so just '0' is the code for the letter 's'.\n\nAs mentioned earlier, such normal latin letters are usually stored with UTF-8, which means they take up 8 bits each. So for example the letter 'o' is stored as '01101111' with UTF-8, but it is stored as '110' with our Huffman code for the word 'lossless'.\n\nTo summarize, we have now compressed the word 'lossless' from its UTF-8 code\n\nusing Huffman Coding, which is a huge improvement.\n\nBut if data is stored with Huffman Coding as , or the code is sent to us, how can it be decoded so that we see what information the Huffman code contains?\n\nFurthermore, the binary code is really , without the spaces, and with variable bit lengths for each piece of data, so how can the computer understand where the binary code for each piece of data starts and ends?\n\nJust like with code stored as UTF-8, which our computers can already decode to the correct letters, the computer needs to know which bits represent which piece of data in the Huffman code.\n\nSo along with a Huffman code, there must also be a conversion table with information about what the Huffman binary code is for each piece of data, so that it can be decoded.\n\nSo, for this Huffman code:\n\nAre you able to decode the Huffman code?\n\nWe start with the first bit:\n\nThere is no letter in the table with just as the Huffman code, so we continue and include the next bit as well.\n\nWe can see from the table that is 'b', so now we have the first letter. We check the next bit:\n\nWe find that is 'a', so now we have the two first letters 'ba' stored in the Huffman code.\n\nWe continue looking up Huffman codes in the table:\n\nThe Huffman code is now decoded, and the word is 'banana'!\n\nAn interesting and very useful part of the Huffman coding algorithm is that it ensures that there is no code that is the prefix of another code.\n\nThis property, that no code is the prefix of another code, makes it possible to decode. And it is especially important in Huffman Coding because of the variable bit lengths.\n\nThe correct word for creating Huffman code based on data or text is \"encoding\", and the opposite would be \"decoding\", when the original data or text is recreated based on the code.\n\nThe code example below takes a word, or any text really, and compress it using Huffman Coding.\n\nIn addition to encode data using Huffman coding, we should also have a way to decode it, to recreate the original information.\n\nThe implementation below is basically the same as the previous code example, but with an additional function for decoding the Huffman code.\n\nThe function takes the Huffman code, and the Python dictionary (a hashmap) with the characters and their corresponding binary codes. The Function then reverse the mapping, and checks the Huffman code bit-by-bit to recreate the original text.\n\nYou have now seen how a text can be compressed using Huffman coding, and how a Huffman code can be decoded to recreate the original text."
    },
    {
        "link": "https://github.com/arnab132/Huffman-Coding-Python",
        "document": "Huffman Coding (HC) is a technique of Compressing data to reduce its size without losing any of the details. David Huffman first developed it.\n\nHC is generally useful to compress the data in which there are frequently occurring characters.\n\nCreate a Priority Queue Q consisting of each unique character. Sort then in ascending order of their frequencies. for all the unique characters: Create a newNode extract minimum value from Q and assign it to leftChild of newNode extract minimum value from Q and assign it to rightChild of newNode calculate the sum of these two minimum values and assign it to the value of newNode insert this newNode into the tree\n\n#How Huffman Coding works? Suppose the string below is to be sent over a network.\n\nEach character occupies 8 bits. There are a total of 15 characters in the above string. Thus, a total of 8 * 15 = 120 bits are required to send this string.\n\nUsing the Huffman Coding technique, we can compress the string to a smaller size.\n\nHuffman coding first creates a tree using the frequencies of the character and then generates code for each character.\n\nOnce the data is encoded, it has to be decoded. Decoding is done using the same tree.\n\nHuffman Coding prevents any ambiguity in the decoding process using the concept of prefix code ie. a code associated with a character should not be present in the prefix of any other code. The tree created above helps in maintaining the property.\n\n#Huffman coding is done with the help of the following steps:\n\n1.Calculate the frequency of each character in the string.\n\n2.Sort the characters in increasing order of the frequency. These are stored in a priority queue Q.\n\nCharacters sorted according to the frequency\n\n4.Create an empty node z. Assign the minimum frequency to the left child of z and assign the second minimum frequency to the right child of z. Set the value of the z as the sum of the above two minimum frequencies.\n\nGetting the sum of the Least numbers\n\n5.Remove these two minimum frequencies from Q and add the sum into the list of frequencies (* denote the internal nodes in the figure above). 6.Insert node z into the tree. 7.Repeat steps 3 to 5 for all the characters.\n\nRepeat steps 3 to 5 for all the characters.\n\nRepeat steps 3 to 5 for all the characters.\n\n8.For each non-leaf node, assign 0 to the left edge and 1 to the right edge.\n\nAssign 0 to the left edge and 1 to the right edge\n\nFor sending the above string over a network, we have to send the tree as well as the above compressed-code. The total size is given by the table below.\n\nWithout encoding, the total size of the string was 120 bits. After encoding the size is reduced to 32 + 15 + 28 = 75.\n\nDecoding the code For decoding the code, we can take the code and traverse through the tree to find the character.\n\nLet 101 is to be decoded, we can traverse from the root as in the figure below."
    },
    {
        "link": "https://opendsa-server.cs.vt.edu/ODSA/Books/CS3/html/Huffman.html",
        "document": "One can often gain an improvement in space requirements in exchange for a penalty in running time. There are many situations where this is a desirable tradeoff. A typical example is storing files on disk. If the files are not actively used, the owner might wish to compress them to save space. Later, they can be uncompressed for use, which costs some time, but only once.\n\nWe often represent a set of items in a computer program by assigning a unique code to each item. For example, the standard ASCII coding scheme assigns a unique eight-bit value to each character. It takes a certain minimum number of bits to provide enough unique codes so that we have a different one for each character. For example, it takes \\(\\left\\lceil log\\ 128\\right\\rceil\\) or seven bits to provide the 128 unique codes needed to represent the 128 symbols of the ASCII character set.\n\nThe requirement for \\(\\left \\lceil log\\ n \\right\\rceil\\) bits to represent \\(n\\) unique code values assumes that all codes will be the same length, as are ASCII codes. These are called fixed-length codes. If all characters were used equally often, then a fixed-length coding scheme is the most space efficient method. However, you are probably aware that not all characters are used equally often in many applications. For example, the various letters in an English language document have greatly different frequencies of use.\n\nTable 7.18.1 shows the relative frequencies of the letters of the alphabet. From this table we can see that the letter ‚ÄòE‚Äô appears about 60 times more often than the letter ‚ÄòZ‚Äô. In normal ASCII, the words ‚ÄúDEED‚Äù and ‚ÄúMUCK‚Äù require the same amount of space (four bytes). It would seem that words such as ‚ÄúDEED‚Äù, which are composed of relatively common letters, should be storable in less space than words such as ‚ÄúMUCK‚Äù, which are composed of relatively uncommon letters.\n\nIf some characters are used more frequently than others, is it possible to take advantage of this fact and somehow assign them shorter codes? The price could be that other characters require longer codes, but this might be worthwhile if such characters appear rarely enough. This concept is at the heart of file compression techniques in common use today. The next section presents one such approach to assigning variable-length codes, called Huffman coding. While it is not commonly used in its simplest form for file compression (there are better methods), Huffman coding gives the flavor of such coding schemes. One motivation for studying Huffman coding is because it provides our first opportunity to see a type of tree structure referred to as a search trie.\n\nHuffman coding assigns codes to characters such that the length of the code depends on the relative frequency or weight of the corresponding character. Thus, it is a variable-length code. If the estimated frequencies for letters match the actual frequency found in an encoded message, then the length of that message will typically be less than if a fixed-length code had been used. The Huffman code for each letter is derived from a full binary tree called the Huffman coding tree, or simply the Huffman tree. Each leaf of the Huffman tree corresponds to a letter, and we define the weight of the leaf node to be the weight (frequency) of its associated letter. The goal is to build a tree with the minimum external path weight. Define the weighted path length of a leaf to be its weight times its depth. The binary tree with minimum external path weight is the one with the minimum sum of weighted path lengths for the given set of leaves. A letter with high weight should have low depth, so that it will count the least against the total path length. As a result, another letter might be pushed deeper in the tree if it has less weight. The process of building the Huffman tree for \\(n\\) letters is quite simple. First, create a collection of \\(n\\) initial Huffman trees, each of which is a single leaf node containing one of the letters. Put the \\(n\\) partial trees onto a priority queue organized by weight (frequency). Next, remove the first two trees (the ones with lowest weight) from the priority queue. Join these two trees together to create a new tree whose root has the two trees as children, and whose weight is the sum of the weights of the two trees. Put this new tree back into the priority queue. This process is repeated until all of the partial Huffman trees have been combined into one. The relative frequencies for eight selected letters. The following slideshow illustrates the Huffman tree construction process for the eight letters of Table 7.18.2. Here is the implementation for Huffman tree nodes. This implementation is similar to a typical class hierarchy for implementing full binary trees. There is an abstract base class, named , and two subclasses, named and . This implementation reflects the fact that leaf and internal nodes contain distinctly different information. Here is the implementation for the Huffman Tree class. // Weight of tree is weight of root Here is the implementation for the tree-building process. takes as input , the min-heap of partial Huffman trees, which initially are single leaf nodes as shown in Step 1 of the slideshow above. The body of function consists mainly of a loop. On each iteration of the loop, the first two partial trees are taken off the heap and placed in variables and . A tree is created ( ) such that the left and right subtrees are and , respectively. Finally, is returned to . Once the Huffman tree has been constructed, it is an easy matter to assign codes to individual letters. Beginning at the root, we assign either a ‚Äò0‚Äô or a ‚Äò1‚Äô to each edge in the tree. ‚Äò0‚Äô is assigned to edges connecting a node with its left child, and ‚Äò1‚Äô to edges connecting a node with its right child. This process is illustrated by the following slideshow. Now that we see how the edges associate with bits in the code, it is a simple matter to generate the codes for each letter (since each letter corresponds to a leaf node in the tree). Now that we have a code for each letter, encoding a text message is done by replacing each letter of the message with its binary code. A lookup table can be used for this purpose.\n\nA set of codes is said to meet the prefix property if no code in the set is the prefix of another. The prefix property guarantees that there will be no ambiguity in how a bit string is decoded. In other words, once we reach the last bit of a code during the decoding process, we know which letter it is the code for. Huffman codes certainly have the prefix property because any prefix for a code would correspond to an internal node, while all codes correspond to leaf nodes. When we decode a character using the Huffman coding tree, we follow a path through the tree dictated by the bits in the code string. Each ‚Äò0‚Äô bit indicates a left branch while each ‚Äò1‚Äô bit indicates a right branch. The following slideshow shows an example for how to decode a message by traversing the tree appropriately.\n\nIn theory, Huffman coding is an optimal coding method whenever the true frequencies are known, and the frequency of a letter is independent of the context of that letter in the message. In practice, the frequencies of letters in an English text document do change depending on context. For example, while E is the most commonly used letter of the alphabet in English documents, T is more common as the first letter of a word. This is why most commercial compression utilities do not use Huffman coding as their primary coding method, but instead use techniques that take advantage of the context for the letters. Another factor that affects the compression efficiency of Huffman coding is the relative frequencies of the letters. Some frequency patterns will save no space as compared to fixed-length codes; others can result in great compression. In general, Huffman coding does better when there is large variation in the frequencies of letters. In the particular case of the frequencies shown in Table 7.18.1, we can determine the expected savings from Huffman coding if the actual frequencies of a coded message match the expected frequencies. Because the sum of the frequencies is 306 and E has frequency 120, we expect it to appear 120 times in a message containing 306 letters. An actual message might or might not meet this expectation. Letters D, L, and U have code lengths of three, and together are expected to appear 121 times in 306 letters. Letter C has a code length of four, and is expected to appear 32 times in 306 letters. Letter M has a code length of five, and is expected to appear 24 times in 306 letters. Finally, letters K and Z have code lengths of six, and together are expected to appear only 9 times in 306 letters. The average expected cost per character is simply the sum of the cost for each character (\\(c_i\\)) times the probability of its occurring (\\(p_i\\)), or \\(c_1 p_1 + c_2 p_2 + \\cdots + c_n p_n.\\) This can be reorganized as \\(\\frac{c_1 f_1 + c_2 f_2 + \\cdots + c_n f_n}{f_T}\\), where \\(f_i\\) is the (relative) frequency of letter \\(i\\) and \\(f_T\\) is the total for all letter frequencies. For this set of frequencies, the expected cost per letter is \\([(1 \\times 120) + (3 \\times 121) + (4 \\times 32) + (5 \\times 24) + (6 \\times 9)]/306 = 785/306 \\approx 2.57.\\) A fixed-length code for these eight characters would require \\(\\log 8 = 3\\) bits per letter as opposed to about 2.57 bits per letter for Huffman coding. Thus, Huffman coding is expected to save about 14% for this set of letters. Huffman coding for all ASCII symbols should do better than this example. The letters of Table 7.18.1 are atypical in that there are too many common letters compared to the number of rare letters. Huffman coding for all 26 letters would yield an expected cost of 4.29 bits per letter. The equivalent fixed-length code would require about five bits. This is somewhat unfair to fixed-length coding because there is actually room for 32 codes in five bits, but only 26 letters. More generally, Huffman coding of a typical text file will save around 40% over ASCII coding if we charge ASCII coding at eight bits per character. Huffman coding for a binary file (such as a compiled executable) would have a very different set of distribution frequencies and so would have a different space savings. Most commercial compression programs use two or three coding schemes to adjust to different types of files. In decoding example, ‚ÄúDEED‚Äù was coded in 8 bits, a saving of 33% over the twelve bits required from a fixed-length coding. However, ‚ÄúMUCK‚Äù would require 18 bits, more space than required by the corresponding fixed-length coding. The problem is that ‚ÄúMUCK‚Äù is composed of letters that are not expected to occur often. If the message does not match the expected frequencies of the letters, than the length of the encoding will not be as expected either. You can use the following visualization to create a huffman tree for your own set of letters and frequencies."
    },
    {
        "link": "https://ics.uci.edu/~dhirschb/pubs/LenLimHuff.pdf",
        "document": ""
    },
    {
        "link": "https://planetmath.org/huffmancoding",
        "document": "is a method of lossless data compression, and a form of entropy encoding. The basic idea is to map an alphabet to a representation for that alphabet, composed of strings of variable size, so that symbols that have a higher probability of occurring have a smaller representation than those that occur less often.\n\nThe key to Huffman coding is Huffman‚Äôs algorithm , which constructs an extended binary tree of minimum weighted path length from a list of weights. For this problem, our list of weights consists of the probabilities of symbol occurrence. From this tree (which we will call a Huffman tree for convenience), the mapping to our variable-sized representations can be defined.\n\nThe mapping is obtained by the path from the root of the Huffman tree to the leaf associated with a symbol‚Äôs weight. The method can be arbitrary, but typically a value of 0 is associated with an edge to any left child and a value of 1 with an edge to any right child (or vice-versa). By concatenating the labels associated with the edges that make up the path from the root to a leaf, we get a binary string. Thus the mapping is defined.\n\nIn order to recover the symbols that make up a string from its representation after encoding, an inverse mapping must be possible. It is important that this mapping is unambiguous. We can show that all possible strings formed by concatenating any number of path labels in a Huffman tree are indeed unambiguous, due to the fact that it is a complete binary tree . That is, given a string composed of Huffman codes, there is exactly one possible way to decompose it into the individual codes.\n\nAmbiguity occurs if there is any path to some symbol whose label is a prefix of the label of a path to some other symbol. In the Huffman tree, every symbol is a leaf. Thus it is impossible for the label of a path to a leaf to be a prefix of any other path label, and so the mapping defined by Huffman coding has an inverse and decoding is possible.\n\nFor a simple example, we will take a short phrase and derive our probabilities from a frequency count of letters within that phrase. The resulting encoding should be good for compressing this phrase, but of course will be inappropriate for other phrases with a different letter distribution. We will use the phrase ‚Äúmath for the people by the people‚Äù. The frequency count of characters in this phrase are as follows (let denote the spaces). We will simply let the frequency counts be the weights. If we pair each symbol with its weight, and pass this list of weights to Huffman‚Äôs algorithm, we will get something like the following tree (edge labels have been added). From this tree we obtain the following mapping. If we were to use a fixed-sized encoding, our original string would have to be 132 bits in length. This is because there are 13 symbols, requiring 4 bits of representation, and the length of our string is 33. The weighted path length of this Huffman tree is 113. Since these weights came directly from the frequency count of our string, the number of bits required to represent our string using this encoding is the same as the weight of 113. Thus the Huffman encoded string is the length of the fixed-sized encoding. Arithmetic encoding can in most cases obtain even greater compression, although it is not quite as simple to implement."
    },
    {
        "link": "https://cgi.cse.unsw.edu.au/~cs2011/HWs/hw2",
        "document": ""
    },
    {
        "link": "https://ics.uci.edu/~dan/pubs/LenLimHuff.pdf",
        "document": ""
    }
]