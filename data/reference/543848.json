[
    {
        "link": "https://tensorflow.org/api_docs/python/tf/keras/layers/LSTM",
        "document": "Used in the notebooks\n\nBased on available runtime hardware and constraints, this layer will choose different implementations (cuDNN-based or backend-native) to maximize the performance. If a GPU is available and all the arguments to the layer meet the requirement of the cuDNN kernel (see below for details), the layer will use a fast cuDNN implementation when using the TensorFlow backend. The requirements to use the cuDNN implementation are:\n‚Ä¢ Inputs, if use masking, are strictly right-padded.\n‚Ä¢ Eager execution is enabled in the outermost context.\n\nThis method is the reverse of , capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by )."
    },
    {
        "link": "https://keras.io/api/layers/recurrent_layers/lstm",
        "document": "Based on available runtime hardware and constraints, this layer will choose different implementations (cuDNN-based or backend-native) to maximize the performance. If a GPU is available and all the arguments to the layer meet the requirement of the cuDNN kernel (see below for details), the layer will use a fast cuDNN implementation when using the TensorFlow backend. The requirements to use the cuDNN implementation are:\n‚Ä¢ Inputs, if use masking, are strictly right-padded.\n‚Ä¢ Eager execution is enabled in the outermost context.\n‚Ä¢ activation: Activation function to use. Default: hyperbolic tangent ( ). If you pass , no activation is applied (ie. \"linear\" activation: ).\n‚Ä¢ recurrent_activation: Activation function to use for the recurrent step. Default: sigmoid ( ). If you pass , no activation is applied (ie. \"linear\" activation: ).\n‚Ä¢ use_bias: Boolean, (default ), whether the layer should use a bias vector.\n‚Ä¢ kernel_initializer: Initializer for the weights matrix, used for the linear transformation of the inputs. Default: .\n‚Ä¢ recurrent_initializer: Initializer for the weights matrix, used for the linear transformation of the recurrent state. Default: .\n‚Ä¢ unit_forget_bias: Boolean (default ). If , add 1 to the bias of the forget gate at initialization. Setting it to will also force . This is recommended in Jozefowicz et al.\n‚Ä¢ activity_regularizer: Regularizer function applied to the output of the layer (its \"activation\"). Default: .\n‚Ä¢ dropout: Float between 0 and 1. Fraction of the units to drop for the linear transformation of the inputs. Default: 0.\n‚Ä¢ recurrent_dropout: Float between 0 and 1. Fraction of the units to drop for the linear transformation of the recurrent state. Default: 0.\n‚Ä¢ return_sequences: Boolean. Whether to return the last output in the output sequence, or the full sequence. Default: .\n‚Ä¢ return_state: Boolean. Whether to return the last state in addition to the output. Default: .\n‚Ä¢ go_backwards: Boolean (default: ). If , process the input sequence backwards and return the reversed sequence.\n‚Ä¢ stateful: Boolean (default: ). If , the last state for each sample at index i in a batch will be used as initial state for the sample of index i in the following batch.\n‚Ä¢ unroll: Boolean (default False). If , the network will be unrolled, else a symbolic loop will be used. Unrolling can speed-up a RNN, although it tends to be more memory-intensive. Unrolling is only suitable for short sequences.\n‚Ä¢ use_cudnn: Whether to use a cuDNN-backed implementation. will attempt to use cuDNN when feasible, and will fallback to the default implementation if not.\n‚Ä¢ mask: Binary tensor of shape indicating whether a given timestep should be masked (optional). An individual entry indicates that the corresponding timestep should be utilized, while a entry indicates that the corresponding timestep should be ignored. Defaults to .\n‚Ä¢ training: Python boolean indicating whether the layer should behave in training mode or in inference mode. This argument is passed to the cell when calling it. This is only relevant if or is used (optional). Defaults to .\n‚Ä¢ initial_state: List of initial state tensors to be passed to the first call of the cell (optional, causes creation of zero-filled initial state tensors). Defaults to ."
    },
    {
        "link": "https://tensorflow.org/api_docs/python/tf/keras/Model",
        "document": "Stay organized with collections Save and categorize content based on your preferences.\n\nA model grouping layers into an object with training/inference features.\n\nUsed in the notebooks\n\nThere are three ways to instantiate a :\n\nYou start from , you chain layer calls to specify the model's forward pass, and finally you create your model from inputs and outputs:\n\nA new Functional API model can also be created by using the intermediate tensors. This enables you to quickly extract sub-components of the model.\n\nNote that the and models are not created with objects, but with the tensors that originate from objects. Under the hood, the layers and weights will be shared across these models, so that user can train the , and use or to do feature extraction. The inputs and outputs of the model can be nested structures of tensors as well, and the created models are standard Functional API models that support all the existing APIs.\n\nIn that case, you should define your layers in and you should implement the model's forward pass in .\n\nIf you subclass , you can optionally have a argument (boolean) in , which you can use to specify a different behavior in training and inference:\n\nOnce the model is created, you can config the model with losses and metrics with , train the model with , or use the model to do prediction with .\n\nIn addition, is a special case of model where the model is purely a stack of single-input, single-output layers.\n\nCompiles the model with the information given in config.\n\nThis method uses the information in the config (optimizer, loss, metrics, etc.) to compile the model.\n\nCompute the total loss, validate it, and return it.\n\nSubclasses can optionally override this method to provide custom loss computation logic.\n\nUpdate metric states and collect all metrics to be returned.\n\nSubclasses can optionally override this method to provide custom metric updating and collection logic.\n\nReturns the loss value & metrics values for the model in test mode.\n\nComputation is done in batches (see the arg.)\n\nThis method lets you export a model to a lightweight SavedModel artifact that contains the model's forward pass only (its method) and can be served via e.g. TF-Serving. The forward pass is registered under the name (see example below).\n\nThe original code of the model (including any custom layers you may have used) is no longer necessary to reload the artifact -- it is entirely standalone.\n\nIf you would like to customize your serving endpoints, you can use the lower-level class. The method relies on internally.\n\nTrains the model for a fixed number of epochs (dataset iterations).\n\nUnpacking behavior for iterator-like inputs: A common pattern is to pass an iterator like object such as a or a to , which will in fact yield not only features ( ) but optionally targets ( ) and sample weights ( ). Keras requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for and respectively. Any other type provided will be wrapped in a length-one tuple, effectively treating everything as . When yielding dicts, they should still adhere to the top-level tuple structure, e.g. . Keras will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the . The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: where it is unclear if the tuple was intended to be unpacked into , , and or passed through as a single element to .\n\nThis method is the reverse of , capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by ).\n\nReturns a serialized config with information for compiling the model.\n\nThis method returns a config dictionary containing all the information (optimizer, loss, metrics, etc.) with which the model was compiled.\n\nRetrieves a layer based on either its name (unique) or index.\n\nIf and are both provided, will take precedence. Indices are based on order of horizontal graph traversal (bottom-up).\n\nIf any of the metric result is a dict (containing multiple metrics), each of them gets added to the top level returned dict of this method.\n\nWeights are loaded based on the network's topology. This means the architecture should be the same as when the weights were saved. Note that layers that don't have weights are not taken into account in the topological ordering, so adding or removing layers is fine as long as they don't have weights.\n\nIf you have modified your model, for instance by adding a new layer (with weights) or by changing the shape of the weights of a layer, you can choose to ignore errors and continue loading by setting . In this case any layer with mismatching weights will be skipped. A warning will be displayed for each skipped layer.\n\nComputation is done in batches. This method is designed for batch processing of large numbers of inputs. It is not intended for use inside of loops that iterate over your data and process small numbers of inputs at a time.\n\nFor small numbers of inputs that fit in one batch, directly use for faster execution, e.g., , or if you have layers such as that behave differently during inference.\n\nNote that is an alias for .\n‚Ä¢ The model's optimizer's state (if any)\n\nThus models can be reinstantiated in the exact same state.\n\nTest the model on a single batch of samples.\n\nTo load a network from a JSON save file, use ."
    },
    {
        "link": "https://dipanshu10.medium.com/implementing-lstm-networks-with-python-a-guide-using-tensorflow-and-keras-915b58f502ce",
        "document": "Master LSTM Networks With Python: A Guide Using TensorFlow and Keras\n\nHave you ever wondered how your smartphone predicts your next word when texting, or how voice assistants understand your commands so accurately?\n\nBehind these technological marvels lies a powerful neural network architecture called Long Short-Term Memory (LSTM).In the world of deep learning, traditional neural networks struggle with a critical limitation: they can‚Äôt remember past information when processing future inputs. This is where LSTM networks shine. Developed to solve the notorious ‚Äúvanishing gradient problem‚Äù that plagues conventional Recurrent Neural Networks (RNNs), LSTMs have revolutionized how we process sequential data.\n\nLong Short-Term Memory(LSTM) networks are a type or recurrent neural network (RNN) designed to address the vanishing gradient problem in traditional RNNs. they are particularly effective for preprocessing and predicting time series data, making them valubale in various applications such are natural language processing ,speech recoginition and financial forecasting."
    },
    {
        "link": "https://github.com/christianversloot/machine-learning-articles/blob/main/build-an-lstm-model-with-tensorflow-and-keras.md",
        "document": "Long Short-Term Memory (LSTM) based neural networks have played an important role in the field of Natural Language Processing. In addition, they have been used widely for sequence modeling. The reason why LSTMs have been used widely for this is because the model connects back to itself during a forward pass of your samples, and thus benefits from context generated by previous predictions when prediction for any new sample.\n\nIn this article, we're going to take a look at how we can build an LSTM model with TensorFlow and Keras. For doing so, we're first going to take a brief look at what LSTMs are and how they work. Don't worry, we won't cover this in much detail, because we already did so in another article. It is necessary though to understand what is happening before we actually get to work. That's how you build intuition for the models you'll use for Machine Learning tasks.\n\nOnce we know about LSTMs, we're going to take a look at how we can build one with TensorFlow. More specifically, we're going to use , or TensorFlow's tightly coupled (or frankly, embedded) version of Keras for the job. First of all, we're going to see how LSTMs are represented as . We'll then move on and actually build the model. With step-by-step explanations, you will understand what is going on at each line and build an understanding of LSTM models in code.\n\nThe code example below gives you a working LSTM based model with TensorFlow 2.x and Keras. If you want to understand it in more detail, make sure to read the rest of the article below.\n\nBefore we will actually write any code, it's important to understand what is happening inside an LSTM. First of all, we must say that an LSTM is an improvement upon what is known as a vanilla or traditional Recurrent Neural Network, or RNN. Such networks look as follows:\n\nA fully recurrent network. Created by fdeloche at Wikipedia, licensed as CC BY-SA 4.0. No changes were made.\n\nIn a vanilla RNN, an input value ( ) is passed through the model, which has a hidden or learned state at that point in time. The model produces the output which is in the target representation. Using this way of working, we can convert inputs in English into outputs in German, to give just an example. Vanilla RNNs are therefore widely used as sequence-to-sequence models.\n\nHowever, we can do the same with classic neural networks. Their benefit compared to classic MLPs is that they pass the output back to themselves, so that it can be used during the next pass. This provides the neural network with context with respect to previous inputs (which in semantically confusing tasks like translation can sometimes be really important). Classic RNNs are therefore nothing more than a fully-connected network that passes neural outputs back to the neurons.\n\nSo far, so good. RNNs really boosted the state-of-the-art back in the days. But well, there's a problem. It emerges when you want to train classic Recurrent Neural Networks. If you apply backpropagation to training a regular neural network, errors are computed backwards, so that the gradient update becomes known that can be applied by the optimizer. Recurrent backpropagation is something that is however not so easy or available, so another approach had to be taken. Effectively, this involved unfolding the network, effectively making copies of the network (with exactly the same initialization) and improving upon them. This way, we can compute gradients more easily, and chain them together. It allowed for the training of RNNs.\n\nBut chaining gradients together effectively means that you have to apply multiplications. And here's the catch: classic RNNs were combined with activation functions like Sigmoid and Tanh, but primarily Sigmoid. As the output of the derivative of these functions is almost always < 1.0, you get a severe case of vanishing gradients. Classic RNNs could therefore not be used when sequences got long; they simply got stuck or trained very slowly.\n\nEnter LSTMs. These Long Short-Term Memory networks effectively split up the output and memory. In so-called memory cells, they allow all functionality to happen, the prediction to be generated, and memory to be updated. Visually, this looks as follows:\n\nLet's take a brief look at all the components in a bit more detail:\n‚Ä¢ All functionality is embedded into a memory cell, visualized above with the rounded border.\n‚Ä¢ The and variables represent the outputs of the memory cell at respectively and . In plain English: the output of the previous cell into the current cell, and the output of the current cell to the next one.\n‚Ä¢ The and variables represent the memory itself, at the known time steps. As you can see, memory has been cut away from the output variable, being an entity on its own.\n‚Ä¢ We have three so-called gates, represented by the three blocks of elements within the cell:\n‚Ä¢ On the left, we see a forget gate. It takes the previous output and current input and by means of Sigmoid activation computes what can be forgotten and hence removed from memory related to current and previous input. By multiplying this with the memory, the removal is performed.\n‚Ä¢ In the middle, we see an input gate. It takes the previous output and current input and applies both a Sigmoid and Tanh activation. The Sigmoid activation effectively learns what must be kept from the inputs, whereas the Tanh normalizes the values into the range , stabilizing the training process. As you can see, the results are first multiplied (to ensure that normalization occurs) after which it is added into memory.\n‚Ä¢ On the right, we see an output gate. It takes a normalized value for memory through Tanh and a Sigmoid activated value for the previous output and current input, effectively learning what must be predicted for the current input value. This value is then output, and the memory and output values are also passed to the next cell.\n\nThe benefit of LSTMs with respect to simple RNNs lies in the fact that memory has been separated from the actual output mechanisms. As you can see, all vanishing gradient-causing mechanisms lie within the cell. In inter-cell communication, the only elements that are encountered during gradient computation are multiplication (x) and addition (+). These are linear operations, and by consequence the LSTM can ensure that gradients between cells are always 1.0. Hence, with LSTMs, the vanishing gradients problem is resolved.\n\nThis makes them a lot faster than vanilla RNNs.\n\nNow that we understand how LSTMs work in theory, let's take a look at constructing them in TensorFlow and Keras. Of course, we must take a look at how they are represented first. In TensorFlow and Keras, this happens through the class, and it is described as:\n\nIndeed, that's the LSTM we want, although it might not have all the gates yet - gates were changed in another paper that was a follow-up to the Hochreiter paper. Nevertheless, understanding the LSTM with all the gates is a good idea, because that's what most of them look like today.\n\nIn code, it looks as follows:\n\nThese are the attributes that can be configured:\n‚Ä¢ With units, we can define the dimensionality of the output space, as we are used to e.g. with Dense layers.\n‚Ä¢ The activation attribute defines the activation function that will be used. By default, it is the Tanh function.\n‚Ä¢ With recurrent_activation, you can define the activation function for the recurrent functionality.\n‚Ä¢ The use_bias attribute can be used to configure whether bias must be used to steer the model as well.\n‚Ä¢ The initializers can be used to initialize the weights of the kernels and recurrent segment, as well as the biases.\n‚Ä¢ The unit_forget_bias represents the bias value (+1) at the forget gate. This is recommended in a follow-up study to the original LSTM paper.\n‚Ä¢ The regularizers and constraints allow you to constrain the training process, possibly blocking vanishing and exploding gradients, and keeping the model at adequate complexity.\n‚Ä¢ Dropout can be added to avoid overfitting, to both the cell itself as well as the recurrent segment.\n‚Ä¢ With return_sequences, you can indicate whether you want only the prediction for the current input as the output, or that with all the previous predictions appended.\n‚Ä¢ With return_state, you can indicate whether you also want to have state returned besides the outputs.\n‚Ä¢ With go_backwards, you can indicate whether you want to have the sequence returned in reverse order.\n‚Ä¢ If you set stateful to True, the recurrent segment will work on a batch level rather than model level.\n‚Ä¢ Structure of your input (timesteps, batch, features or batch, timesteps, features) can be switched with time_major.\n‚Ä¢ With unroll, you can still unroll the network at training. If set to False, a symbolic loop will be used.\n‚Ä¢ Additional arguments can be passed with **kwargs.\n\nNow that we understand how LSTMs work and how they are represented within TensorFlow, it's time to actually build one with Python, TensorFlow and its Keras APIs. We'll walk you through the process with step-by-step examples. The process is composed of the following steps:\n‚Ä¢ Importing the Keras functionality that we need into the Python script.\n‚Ä¢ Listing the configuration for our LSTM model and preparing for training.\n‚Ä¢ Loading and preparing a dataset; we'll use the IMDB dataset today.\n\nOpen up a code editor and create a file, e.g. called , and let's go!\n\nLet's specify the model imports first:\n‚Ä¢ We'll need TensorFlow so we import it as .\n‚Ä¢ From the TensorFlow Keras Datasets, we import the one.\n‚Ä¢ We'll need word embeddings ( ), MLP layers ( ) and LSTM layers ( ), so we import them as well.\n‚Ä¢ Our loss function will be binary cross entropy.\n‚Ä¢ As we'll stack all layers on top of each other with , we need (the Keras Sequential API) for constructing our variable in the first place.\n‚Ä¢ For optimization we use an extension of classic gradient descent called Adam.\n‚Ä¢ Finally, we need to import . We're going to use the IMDB dataset which has sequences of reviews. While we'll specify a maximum length, this can mean that shorter sequences are present as well; these are not cutoff and therefore have different sizes than our desired one (i.e. the maximum length). We'll have to pad them with zeroes in order to make them of equal length.\n\nThe next step is specifying the model configuration. While strictly not necessary (we can also specify them hardcoded), I always think it's a good idea to group them together. This way, you can easily see how your model is configured, without having to take a look through all the aspects.\n\nBelow, we can see that our model will be trained with a batch size of 128, using binary crossentropy loss and Adam optimization, and only for five epochs (we only have to show you that it works). 20% of our training data will be used for validation purposes, and the output will be verbose, with verbosity mode set to 1 out of 0, 1 and 2. Our learned word embedding will have 15 hidden dimensions and each sequence passed through the model is 300 characters at max. Our vocabulary will contain 5000 words at max.\n\nYou might now also want to disable Eager Execution in TensorFlow. While it doesn't work for all, some people report that the training process speeds up after using it. However, it's not necessary to do so - simply test how it behaves on your machine:\n\nOnce this is complete, we can load and prepare the data. To make things easier, Keras comes with a standard set of datasets, of which the IMDB dataset can be used for sentiment analysis (essentially text classification with two classes). Using , we can load the data.\n\nOnce the data has been loaded, we apply . This ensures that sentences shorter than the maximum sentence length are brought to equal length by applying padding with, in this case, zeroes, because that often corresponds with the padding character.\n\nWe can then define the Keras model. As we are using the Sequential API, we can initialize the variable with . The first layer is an layer, which learns a word embedding that in our case has a dimensionality of 15. This is followed by an layer providing the recurrent segment (with default activation enabled), and a layer that has one output - through Sigmoid a number between 0 and 1, representing an orientation towards a class.\n\nThe model can then be compiled. This initializes the model that has so far been a skeleton, a foundation, but no actual model yet. We do so by specifying the optimizer, the loss function, and the additional metrics that we had specified before.\n\nThis is also a good place to generate a summary of what the model looks like.\n\nThen, we can instruct TensorFlow to start the training process.\n\nThe pairs passed to the model are the padded inputs and their corresponding class labels. Training happens with the batch size, number of epochs, verbosity mode and validation split that were also defined in the configuration section above.\n\nWe cannot evaluate the model on the same dataset that was used for training it. We fortunately have testing data available through the train/test split performed in the section, and can use built-in evaluation facilities to evaluate the model. We then print the test results on screen.\n\nIf you want to get the full model code just at once, e.g. for copy-and-run, here you go:\n\nTime to run the model! Open up a terminal where at least TensorFlow and Python have been installed, and run the model - .\n\nYou should see that the model starts training after e.g. a few seconds. If you have the IMDB dataset not downloaded to your machine, it will be downloaded first.\n\nEventually, you'll approximately see an 87.1% accuracy on the evaluation set:\n\nIf you face speed issues with training the TensorFlow LSTM on your GPU, you might decide to temporarily disable its access to your GPUs by adding the following before :\n\nLong Short-Term Memory Networks (LSTMs) are a type of recurrent neural network that can be used in Natural Language Processing, time series and other sequence modeling tasks. In this article, we covered their usage within TensorFlow and Keras in a step-by-step fashion.\n\nWe first briefly looked at LSTMs in general. What are they? What can they be used for? How do they improve compared to previous RNN based approaches? This analysis gives you the necessary context in order to understand what is going on within your code.\n\nWe then looked at how LSTMs are represented in TensorFlow and Keras. We saw that there is a separate layer that can be configured with a wide variety of attributes. In the article, we looked at the meaning for each attribute and saw how everything interrelates. Once understanding this, we moved on to actually implementing the model with TensorFlow. In a step-by-step phased approach, we explained in detail why we made certain choices, allowing you to see exactly how the model was constructed.\n\nAfter training on the IMDB dataset, we saw that the model achieves an accuracy of approximately 87.1% on the evaluation set.\n\nI hope that you have learned something from this article. If you did, please feel free to drop a message, as I'd love to hear from you üí¨ Please do the same if you have any questions, or click the Ask Questions button to the right. Thank you for reading MachineCurve today and happy engineering! üòé\n\nKeras Team. (n.d.). Keras documentation: The sequential class. Keras: the Python deep learning API. https://keras.io/api/models/sequential/"
    },
    {
        "link": "https://medium.com/@john.kosinski/preparing-and-shaping-timeseries-data-for-keras-lstm-input-part-one-5bb882bc2143",
        "document": "It‚Äôs often said that successful machine learning comes more from how the data is processed, than from the model architecture. Clearly both are important, but the importance of pre-processing the data might be sometimes underestimated.\n\nIn order to allow a model to learn as much as possible from a set of data, the important features of the data need to be extracted and arranged in such a way that the model can use them to generalize relationships. Often there is a long discovery process, in which the data is interrogated manually in order to determine or intuit what features to extract and how to present them. Training the model is usually one of the last steps in a lengthy process.\n\nIn this example I‚Äôm going to focus on price series data, for example a multi-year daily stock price time series, and I‚Äôm going to demonstrate an example of preprocessing the data to extract a few features, in order to prepare the data to be used to train a keras LSTM model.\n\nThis simplified example will consist of the following steps:\n\n1. Read the data\n\n2. Extract the daily range (high ‚Äî low)\n\n3. Remove the trend\n\n4. Handle outliers\n\n5. Scale the data\n\n7. Extract data about the trend, as a new column\n\n8. Shape the data into the correct shape to be used as input for a keras LSTM model\n\nThis article is available in jupyter notebook form, for both Part One and Part Two, here:\n\nAnd in complete form with minimal comments:\n\nKeras and tensorflow are not required for this example, as it‚Äôs only about preprocessing the data prior to training a model; there is\n\nno actual model involved in this example.\n\nThe data comes from Yahoo Finance historical data, and is available at https://github.com/jrkosinski/articles/blob/main/lstm-preprocessing/data/prices-d.csv\n\nThe data comes with the following columns:\n\n- Open\n\n- High\n\n- Low\n\n- Close\n\n- Adjusted Close\n\n- Volume\n\nAt the end of the example, the data will have been transformed, with 3 scaled and normalized columns:\n\n- Range\n\n- Change\n\n- Trend\n\n‚Ä¶ and will be in the 3-dimensional array shape that a keras LSTM model expects, split into training, validation, and testing sets.\n\nIn part one, this example will extract Range and Change from the timeseries data, remove the outliers, and scale the data between 0 and 1.\n\nIn part two, the example will take the result of that, retrend the data, and shape it appropriately for input into a keras LSTM model.\n\nThe data is read from a file downloaded from the free historical stock data at Yahoo Finance; the columns are OHLC, plus Adjusted Close and Volume. Feel free to use any Yahoo historical stock price data, but the exact data file that I used is here:\n\nhttps://github.com/jrkosinski/articles/blob/main/lstm-preprocessing/data/prices-d.csv\n\nThe only column that we won‚Äôt be touching at all is Volume, so I‚Äôll just remove that straightaway. Also we don‚Äôt need ‚ÄòClose‚Äô, as we‚Äôll use ‚ÄòAdj Close‚Äô instead, as the continuous series is better for most purposes. The other columns will be used to extract useful features, and then afterwards those source columns may be discarded from the DataFrame.\n\nThe absolute values of the Open, High, and Low won‚Äôt be useful to us. The daily range as a percentage of something (the Open, or previous day‚Äôs Close for example) could be useful though, so we should extract what‚Äôs useful, and get rid of what‚Äôs not.\n\n\n\nThe single line of code above that extracts the daily range is possible thanks to the non-native Python library pandas. In case you aren‚Äôt accustomed to using pandas (and just to demonstrate what that line actually does) it would be as if I had looped through the data and done this:\n\nJust to show what the Range column looks like, I will plot it.\n\nJust a few comments here; you can visually see a few things:\n\n- there is no discernable strong trend\n\n- the distribution is noticeably skewed right, with extreme positive outliers\n\n- the range of the data is not neatly between 0 and 1\n\nThe lack of trend is a good thing, we want that to feed into the model. The right-skew and the scale, we will fix in later steps. If you like to get empirical, we can show the low trendiness of the series using augmented Dickey-Fuller or some other metric.\n\n\n\nHere I‚Äôve plotted the distribution to better show the skewness of the data, though you can see it plainly enough in the previous plot. Removing outliers should make the data more balanced, so I will do that in a later\n\nstep.\n\n\n\nNow that we‚Äôve extracted the daily range from the Open, High, and Low columns, we don‚Äôt need those anymore, so I‚Äôll just remove them. The information that we needed is now in the Range column.\n\nQ: Isn‚Äôt information lost when the outliers are squashed?\n\nA: Yes, it is, that‚Äôs a good point; and it isn‚Äôt replaced with anything. So we‚Äôre gambling that the lost information isn‚Äôt worth as much as what we‚Äôre getting from removing outliers (ostensibly better results from the model). It is part of a discovery process wherein many things will be tried, and if the lost data is found to possibly be valuable, we can find other ways to extract the data and still feed in a normalized series.\n\nA data series may exhibit trend, and it may exhibit seasonality. Multi-year stock price data is less likely to show seasonality, but very likely to show a strong persistent trend. The problem with trended data (especially inflation-affected financial asset data, which tends to grow exponentially with inflation) is that it smashes early data into oblivion, making it nearly invisible to the model trying to generalize something from it. This is why absolute price is almost never fed into a model without first being heavily processed.\n\nNow for the closing price (‚ÄòAdj Close‚Äô column), again no one cares about the absolute price from day to day, what one cares about is the change in price from day to day. Pandas lets us achieve that with the following single line:\n\n\n\nBut the main problem is evident when we plot this over time. We can see that as the price rose over the years, the average daily change also increased, naturally. This is not ideal data to feed into a model, because (as you can see) earlier values will be de-emphasized to the point of nearly being ignored, whereas later data points will be relatively overemphasized. Since we would like the model to be able to glean meaningful data from the entire dataset, this is less than ideal. The plot below shows data that is likely to be difficult for a model to extract generalizations from.\n\nInstead, the percentage change is what we want. It‚Äôs less likely to be skewed or trended, and we can see this visually by plotting.\n\nNote that the very first value for the Pct Change column is a NaN. The reason is that to get this column, each value in the source column was compared to its previous timestep, and the first record has no previous record to which to compare.\n\nThe NaN can be removed reasonably by either basing the first change off of the Open (instead of the previous step‚Äôs Close), or just by simply removing the first row. I‚Äôll just remove the first row.\n\nThe model is best able to extract generalizations from the data if most of the data falls within a normal range. Outliers tend to skew the data such that some values (which may be important) will be almost ignored, whereas others will be overemphasized.\n\nJust to clarify I‚Äôm not talking here about outliers that may reflect mistakes in the data collection process necessarily (bad data samples can often just be discarded), but simply events of unusual magnitude. For example, price data that contains a swan event such as a flash crash that is many times larger than the normal daily move may de-emphasize the magnitude of all of the others days‚Äô data. This can cause a model trained on data that contains the swan event to come up with very different results compared to the same model trained on data that did not contain the event. That‚Äôs not ideal, because it indicates that something is interfering with the model‚Äôs ability to generalize from the available data.\n\nTake another peek at the Change column before handling outliers. Note that just by visual inspection, it‚Äôs clear that most of the data falls into a very small percentage of the full range of values. That should be corrected so that at least more than 50% of the data falls within more than 50% of the range (just ballpark figures here, there is no real rule of thumb. It would depend on the goals of the data, and your idea of what the ideal ‚Äòbalance‚Äô of data is for your situation; but we can see that the data here is clearly not well balanced).\n\n\n\nI don‚Äôt want to remove the rows that contain outliers, because that would be removing real data that we want to give to the model. I don‚Äôt want to zero the values either as that would skew the data in a different direction. Instead I would like to ‚Äòsquash‚Äô the values to within a reasonable range, where outliers >=n will be treated the same as values that are equal to n.\n\nNow we can visually see in the plot of Change that there‚Äôs a more balanced distribution of values, that will be more healthy for the model to digest. Again, there is no formula for achieving the perfect balance, and this is part of a discovery process in which we‚Äôd like to get enough information to intuit a good enough rule, or what is closer to an ideal.\n\nThe shape of the Range data is a bit different, so I‚Äôm going to do basically the same thing, but I‚Äôm going to pass different values for the upper and lower limits, so that the left of the distribution will be less affected by squashing, and the right of the distribution will be more affected (which is where it‚Äôs needed). I can do that by just passing lower values (lower than the default) for both _min_quantile_ and _max_quantile_. That will cause the function to squash more on the top and less (or not at all, in this case) on the bottom.\n\nAnd now, we likewise see a more even distribution of values.\n\nScaling input data between 0 and 1 is conventional when preparing inputs to LSTM or other types of models. While not strictly necessary, and there are cases in which it‚Äôs not advisable, it is generally considered good\n\npractice.\n\nThis function uses MinMaxScaler from scikit-learn package to fit the values of a given column between 0 and 1 (or any given values) and replaces the original column in the DataFrame with the new data.\n\nCheck the min and max of the Change column before scaling.\n\nCheck that it‚Äôs been changed by the scaling process.\n\nAnd very importantly, note that the shape of the data has not changed, it‚Äôs only been rescaled. The plot from before looks the same as after, except for the scale of the y axis.\n\nScaling of Range is exactly the same.\n\nAnd likewise, the scale has been the only thing changed.\n\nSo now the Range has been extracted and added to the dataset, the % daily Change has been extracted and added to the dataset, outliers in both columns have been handled, and the data has been scaled. All other columns have been removed, except for Adj Close; this will be used in\n\nPart Two to re-extract trend data.\n\nThis article is available in jupyter notebook form, for both Part One and Part Two, here: https://github.com/jrkosinski/articles/tree/main/lstm-preprocessing\n\nCheck out the article for Part Two as well."
    },
    {
        "link": "https://tensorflow.org/tutorials/structured_data/time_series",
        "document": "Stay organized with collections Save and categorize content based on your preferences.\n\nThis tutorial is an introduction to time series forecasting using TensorFlow. It builds a few different styles of models including Convolutional and Recurrent Neural Networks (CNNs and RNNs).\n\nThis is covered in two main parts, with subsections:\n‚Ä¢ Forecast multiple steps:\n‚Ä¢ Single-shot: Make the predictions all at once.\n‚Ä¢ Autoregressive: Make one prediction at a time and feed the output back to the model.\n\nThis tutorial uses a weather time series dataset recorded by the Max Planck Institute for Biogeochemistry.\n\nThis dataset contains 14 different features such as air temperature, atmospheric pressure, and humidity. These were collected every 10 minutes, beginning in 2003. For efficiency, you will use only the data collected between 2009 and 2016. This section of the dataset was prepared by Fran√ßois Chollet for his book Deep Learning with Python.\n\nThis tutorial will just deal with hourly predictions, so start by sub-sampling the data from 10-minute intervals to one-hour intervals:\n\nLet's take a glance at the data. Here are the first few rows:\n\nHere is the evolution of a few features over time:\n\nNext, look at the statistics of the dataset:\n\nOne thing that should stand out is the value of the wind velocity ( ) and the maximum value ( ) columns. This is likely erroneous.\n\nThere's a separate wind direction column, so the velocity should be greater than zero ( ). Replace it with zeros:\n\nBefore diving in to build a model, it's important to understand your data and be sure that you're passing the model appropriately formatted data.\n\nThe last column of the data, ‚Äîgives the wind direction in units of degrees. Angles do not make good model inputs: 360¬∞ and 0¬∞ should be close to each other and wrap around smoothly. Direction shouldn't matter if the wind is not blowing.\n\nRight now the distribution of wind data looks like this:\n\nBut this will be easier for the model to interpret if you convert the wind direction and velocity columns to a wind vector:\n\nThe distribution of wind vectors is much simpler for the model to correctly interpret:\n\nSimilarly, the column is very useful, but not in this string form. Start by converting it to seconds:\n\nSimilar to the wind direction, the time in seconds is not a useful model input. Being weather data, it has clear daily and yearly periodicity. There are many ways you could deal with periodicity.\n\nYou can get usable signals by using sine and cosine transforms to clear \"Time of day\" and \"Time of year\" signals:\n\nThis gives the model access to the most important frequency features. In this case you knew ahead of time which frequencies were important.\n\nIf you don't have that information, you can determine which frequencies are important by extracting features with Fast Fourier Transform. To check the assumptions, here is the of the temperature over time. Note the obvious peaks at frequencies near and :\n\nYou'll use a split for the training, validation, and test sets. Note the data is not being randomly shuffled before splitting. This is for two reasons:\n‚Ä¢ It ensures that chopping the data into windows of consecutive samples is still possible.\n‚Ä¢ It ensures that the validation/test results are more realistic, being evaluated on the data collected after the model was trained.\n\nIt is important to scale features before training a neural network. Normalization is a common way of doing this scaling: subtract the mean and divide by the standard deviation of each feature.\n\nThe mean and standard deviation should only be computed using the training data so that the models have no access to the values in the validation and test sets.\n\nIt's also arguable that the model shouldn't have access to future values in the training set when training, and that this normalization should be done using moving averages. That's not the focus of this tutorial, and the validation and test sets ensure that you get (somewhat) honest metrics. So, in the interest of simplicity this tutorial uses a simple average.\n\nNow, peek at the distribution of the features. Some features do have long tails, but there are no obvious errors like the wind velocity value.\n\nThe models in this tutorial will make a set of predictions based on a window of consecutive samples from the data.\n\nThe main features of the input windows are:\n‚Ä¢ The width (number of time steps) of the input and label windows.\n‚Ä¢ The time offset between them.\n‚Ä¢ Which features are used as inputs, labels, or both.\n\nThis tutorial builds a variety of models (including Linear, DNN, CNN and RNN models), and uses them for both:\n\nThis section focuses on implementing the data windowing so that it can be reused for all of those models.\n\nDepending on the task and type of model you may want to generate a variety of data windows. Here are some examples:\n‚Ä¢ None For example, to make a single prediction 24 hours into the future, given 24 hours of history, you might define a window like this:\n‚Ä¢ None A model that makes a prediction one hour into the future, given six hours of history, would need a window like this:\n\nThe rest of this section defines a class. This class can:\n‚Ä¢ Handle the indexes and offsets as shown in the diagrams above.\n‚Ä¢ Plot the content of the resulting windows.\n‚Ä¢ Efficiently generate batches of these windows from the training, evaluation, and test data, using s.\n\nStart by creating the class. The method includes all the necessary logic for the input and label indices.\n\nIt also takes the training, evaluation, and test DataFrames as input. These will be converted to s of windows later.\n\nHere is code to create the 2 windows shown in the diagrams at the start of this section:\n\nGiven a list of consecutive inputs, the method will convert them to a window of inputs and a window of labels.\n\nThe example you define earlier will be split like this:\n\nThis diagram doesn't show the axis of the data, but this function also handles the so it can be used for both the single output and multi-output examples.\n\nTry it out:\n\nTypically, data in TensorFlow is packed into arrays where the outermost index is across examples (the \"batch\" dimension). The middle indices are the \"time\" or \"space\" (width, height) dimension(s). The innermost indices are the features.\n\nThe code above took a batch of three 7-time step windows with 19 features at each time step. It splits them into a batch of 6-time step 19-feature inputs, and a 1-time step 1-feature label. The label only has one feature because the was initialized with . Initially, this tutorial will build models that predict single output labels.\n\nHere is a plot method that allows a simple visualization of the split window:\n\nThis plot aligns inputs, labels, and (later) predictions based on the time that the item refers to:\n\nYou can plot the other columns, but the example window configuration only has labels for the column.\n\nFinally, this method will take a time series DataFrame and convert it to a of pairs using the function:\n\nAdd properties for accessing them as s using the method you defined earlier. Also, add a standard example batch for easy access and plotting:\n\nNow, the object gives you access to the objects, so you can easily iterate over the data.\n\nThe property tells you the structure, data types, and shapes of the dataset elements.\n\nThe simplest model you can build on this sort of data is one that predicts a single feature's value‚Äî1 time step (one hour) into the future based only on the current conditions.\n\nSo, start by building models to predict the value one hour into the future.\n\nThe object creates s from the training, validation, and test sets, allowing you to easily iterate over batches of data.\n\nBefore building a trainable model it would be good to have a performance baseline as a point for comparison with the later more complicated models.\n\nThis first task is to predict temperature one hour into the future, given the current value of all features. The current values include the current temperature.\n\nSo, start with a model that just returns the current temperature as the prediction, predicting \"No change\". This is a reasonable baseline since temperature changes slowly. Of course, this baseline will work less well if you make a prediction further in the future.\n\nThat printed some performance metrics, but those don't give you a feeling for how well the model is doing.\n\nThe has a plot method, but the plots won't be very interesting with only a single sample.\n\nSo, create a wider that generates windows 24 hours of consecutive inputs and labels at a time. The new variable doesn't change the way the model operates. The model still makes predictions one hour into the future based on a single input time step. Here, the axis acts like the axis: each prediction is made independently with no interaction between time steps:\n\nThis expanded window can be passed directly to the same model without any code changes. This is possible because the inputs and labels have the same number of time steps, and the baseline just forwards the input to the output:\n\nBy plotting the baseline model's predictions, notice that it is simply the labels shifted right by one hour:\n\nIn the above plots of three examples the single step model is run over the course of 24 hours. This deserves some explanation:\n‚Ä¢ The blue line shows the input temperature at each time step. The model receives all features, this plot only shows the temperature.\n‚Ä¢ The green dots show the target prediction value. These dots are shown at the prediction time, not the input time. That is why the range of labels is shifted 1 step relative to the inputs.\n‚Ä¢ The orange crosses are the model's prediction's for each output time step. If the model were predicting perfectly the predictions would land directly on the .\n\nThe simplest trainable model you can apply to this task is to insert linear transformation between the input and output. In this case the output from a time step only depends on that step:\n\nA layer with no set is a linear model. The layer only transforms the last axis of the data from to ; it is applied independently to every item across the and axes.\n\nThis tutorial trains many models, so package the training procedure into a function:\n\nTrain the model and evaluate its performance:\n\nLike the model, the linear model can be called on batches of wide windows. Used this way the model makes a set of independent predictions on consecutive time steps. The axis acts like another axis. There are no interactions between the predictions at each time step.\n\nHere is the plot of its example predictions on the , note how in many cases the prediction is clearly better than just returning the input temperature, but in a few cases it's worse:\n\nOne advantage to linear models is that they're relatively simple to interpret. You can pull out the layer's weights and visualize the weight assigned to each input:\n\nSometimes the model doesn't even place the most weight on the input . This is one of the risks of random initialization.\n\nBefore applying models that actually operate on multiple time-steps, it's worth checking the performance of deeper, more powerful, single input step models.\n\nHere's a model similar to the model, except it stacks several a few layers between the input and the output:\n\nA single-time-step model has no context for the current values of its inputs. It can't see how the input features are changing over time. To address this issue the model needs access to multiple time steps when making predictions:\n\nThe , and models handled each time step independently. Here the model will take multiple time steps as input to produce a single output.\n\nCreate a that will produce batches of three-hour inputs and one-hour labels:\n\nNote that the 's parameter is relative to the end of the two windows.\n\nYou could train a model on a multiple-input-step window by adding a as the first layer of the model:\n\nThe main down-side of this approach is that the resulting model can only be executed on input windows of exactly this shape.\n\nThe convolutional models in the next section fix this problem.\n\nA convolution layer ( ) also takes multiple time steps as input to each prediction.\n\nBelow is the same model as , re-written with a convolution.\n‚Ä¢ The and the first are replaced by a .\n‚Ä¢ The is no longer necessary since the convolution keeps the time axis in its output.\n\nRun it on an example batch to check that the model produces outputs with the expected shape:\n\nTrain and evaluate it on the and it should give performance similar to the model.\n\nThe difference between this and the model is that the can be run on inputs of any length. The convolutional layer is applied to a sliding window of inputs:\n\nIf you run it on wider input, it produces wider output:\n\nNote that the output is shorter than the input. To make training or plotting work, you need the labels, and prediction to have the same length. So build a to produce wide windows with a few extra input time steps so the label and prediction lengths match:\n\nNow, you can plot the model's predictions on a wider window. Note the 3 input time steps before the first prediction. Every prediction here is based on the 3 preceding time steps:\n\nA Recurrent Neural Network (RNN) is a type of neural network well-suited to time series data. RNNs process a time series step-by-step, maintaining an internal state from time-step to time-step.\n\nYou can learn more in the Text generation with an RNN tutorial and the Recurrent Neural Networks (RNN) with Keras guide.\n\nIn this tutorial, you will use an RNN layer called Long Short-Term Memory ( ).\n\nAn important constructor argument for all Keras RNN layers, such as , is the argument. This setting can configure the layer in one of two ways:\n‚Ä¢ If , the default, the layer only returns the output of the final time step, giving the model time to warm up its internal state before making a single prediction:\n‚Ä¢ If , the layer returns an output for each input. This is useful for:\n\nWith , the model can be trained on 24 hours of data at a time.\n\nWith this dataset typically each of the models does slightly better than the one before it:\n\nThe models so far all predicted a single output feature, , for a single time step.\n\nAll of these models can be converted to predict multiple features just by changing the number of units in the output layer and adjusting the training windows to include all features in the ( ):\n\nNote above that the axis of the labels now has the same depth as the inputs, instead of .\n\nThe same baseline model ( ) can be used here, but this time repeating all features instead of selecting a specific :\n\nThe model from earlier took advantage of the fact that the sequence doesn't change drastically from time step to time step. Every model trained in this tutorial so far was randomly initialized, and then had to learn that the output is a a small change from the previous time step.\n\nWhile you can get around this issue with careful initialization, it's simpler to build this into the model structure.\n\nIt's common in time series analysis to build models that instead of predicting the next value, predict how the value will change in the next time step. Similarly, residual networks‚Äîor ResNets‚Äîin deep learning refer to architectures where each layer adds to the model's accumulating result.\n\nThat is how you take advantage of the knowledge that the change should be small.\n\nEssentially, this initializes the model to match the . For this task it helps models converge faster, with slightly better performance.\n\nThis approach can be used in conjunction with any model discussed in this tutorial.\n\nHere, it is being applied to the LSTM model, note the use of the to ensure that the initial predicted changes are small, and don't overpower the residual connection. There are no symmetry-breaking concerns for the gradients here, since the are only used on the last layer.\n\nHere is the overall performance for these multi-output models.\n\nThe above performances are averaged across all model outputs.\n\nBoth the single-output and multiple-output models in the previous sections made single time step predictions, one hour into the future.\n\nThis section looks at how to expand these models to make multiple time step predictions.\n\nIn a multi-step prediction, the model needs to learn to predict a range of future values. Thus, unlike a single step model, where only a single future point is predicted, a multi-step model predicts a sequence of the future values.\n\nThere are two rough approaches to this:\n‚Ä¢ Single shot predictions where the entire time series is predicted at once.\n‚Ä¢ Autoregressive predictions where the model only makes single step predictions and its output is fed back as its input.\n\nIn this section all the models will predict all the features across all output time steps.\n\nFor the multi-step model, the training data again consists of hourly samples. However, here, the models will learn to predict 24 hours into the future, given 24 hours of the past.\n\nHere is a object that generates these slices from the dataset:\n\nA simple baseline for this task is to repeat the last input time step for the required number of output time steps:\n\nSince this task is to predict 24 hours into the future, given 24 hours of the past, another simple approach is to repeat the previous day, assuming tomorrow will be similar:\n\nOne high-level approach to this problem is to use a \"single-shot\" model, where the model makes the entire sequence prediction in a single step.\n\nThis can be implemented efficiently as a with output units. The model just needs to reshape that output to the required .\n\nA simple linear model based on the last input time step does better than either baseline, but is underpowered. The model needs to predict time steps, from a single input time step with a linear projection. It can only capture a low-dimensional slice of the behavior, likely based mainly on the time of day and time of year.\n\nAdding a between the input and output gives the linear model more power, but is still only based on a single input time step.\n\nA convolutional model makes predictions based on a fixed-width history, which may lead to better performance than the dense model since it can see how things are changing over time:\n\nA recurrent model can learn to use a long history of inputs, if it's relevant to the predictions the model is making. Here the model will accumulate internal state for 24 hours, before making a single prediction for the next 24 hours.\n\nIn this single-shot format, the LSTM only needs to produce an output at the last time step, so set in .\n\nThe above models all predict the entire output sequence in a single step.\n\nIn some cases it may be helpful for the model to decompose this prediction into individual time steps. Then, each model's output can be fed back into itself at each step and predictions can be made conditioned on the previous one, like in the classic Generating Sequences With Recurrent Neural Networks.\n\nOne clear advantage to this style of model is that it can be set up to produce output with a varying length.\n\nYou could take any of the single-step multi-output models trained in the first half of this tutorial and run in an autoregressive feedback loop, but here you'll focus on building a model that's been explicitly trained to do that.\n\nThis tutorial only builds an autoregressive RNN model, but this pattern could be applied to any model that was designed to output a single time step.\n\nThe model will have the same basic form as the single-step LSTM models from earlier: a layer followed by a layer that converts the layer's outputs to model predictions.\n\nA is a wrapped in the higher level that manages the state and sequence results for you (Check out the Recurrent Neural Networks (RNN) with Keras guide for details).\n\nIn this case, the model has to manually manage the inputs for each step, so it uses directly for the lower level, single time step interface.\n\nThe first method this model needs is a method to initialize its internal state based on the inputs. Once trained, this state will capture the relevant parts of the input history. This is equivalent to the single-step model from earlier:\n\nThis method returns a single time-step prediction and the internal state of the :\n\nWith the 's state, and an initial prediction you can now continue iterating the model feeding the predictions at each step back as the input.\n\nThe simplest approach for collecting the output predictions is to use a Python list and a after the loop.\n\nTest run this model on the example inputs:\n\nThere are clearly diminishing returns as a function of model complexity on this problem:\n\nThe metrics for the multi-output models in the first half of this tutorial show the performance averaged across all output features. These performances are similar but also averaged across output time steps.\n\nThe gains achieved going from a dense model to convolutional and recurrent models are only a few percent (if any), and the autoregressive model performed clearly worse. So these more complex approaches may not be worth while on this problem, but there was no way to know without trying, and these models could be helpful for your problem.\n\nThis tutorial was a quick introduction to time series forecasting using TensorFlow.\n\nTo learn more, refer to:\n‚Ä¢ Chapter 15 of Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition.\n‚Ä¢ Lesson 8 of Udacity's intro to TensorFlow for deep learning, including the exercise notebooks.\n\nAlso, remember that you can implement any classical time series model in TensorFlow‚Äîthis tutorial just focuses on TensorFlow's built-in functionality."
    },
    {
        "link": "https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras",
        "document": "Unlike regression predictive modeling, time series also adds the complexity of a sequence dependence among the input variables.\n\nA powerful type of neural network designed to handle sequence dependence is called a recurrent neural network. The Long Short-Term Memory network or LSTM network is a type of recurrent neural network used in deep learning because very large architectures can be successfully trained.\n\nIn this post, you will discover how to develop LSTM networks in Python using the Keras deep learning library to address a demonstration time-series prediction problem.\n\nAfter completing this tutorial, you will know how to implement and develop LSTM networks for your own time series prediction problems and other more general sequence problems. You will know:\n‚Ä¢ How to develop LSTM networks for regression, window, and time-step-based framing of time series prediction problems\n‚Ä¢ How to develop and make predictions using LSTM networks that maintain state (memory) across very long sequences\n\nIn this tutorial, we will develop a number of LSTMs for a standard time series prediction problem. The problem and the chosen configuration for the LSTM networks are for demonstration purposes only; they are not optimized.\n\nThese examples will show exactly how you can develop your own differently structured LSTM networks for time series predictive modeling problems.\n\nKick-start your project with my new book Deep Learning for Time Series Forecasting, including step-by-step tutorials and the Python source code files for all examples.\n‚Ä¢ Update Oct/2016: There was an error in how RMSE was calculated in each example. Reported RMSEs were just plain wrong. Now, RMSE is calculated directly from predictions, and both RMSE and graphs of predictions are in the units of the original dataset. Models were evaluated using Keras 1.1.0, TensorFlow 0.10.0, and scikit-learn v0.18. Thanks to all those that pointed out the issue and to Philip O‚ÄôBrien for helping to point out the fix.\n‚Ä¢ Update Mar/2017: Updated example for Keras 2.0.2, TensorFlow 1.0.1 and Theano 0.9.0\n‚Ä¢ Update Apr/2017: For a more complete and better-explained tutorial of LSTMs for time series forecasting, see the post Time Series Forecasting with the Long Short-Term Memory Network in Python\n\nThe example in this post is quite dated. You can view some better examples using LSTMs on time series with:\n\nThe problem you will look at in this post is the International Airline Passengers prediction problem.\n\nThis is a problem where, given a year and a month, the task is to predict the number of international airline passengers in units of 1,000. The data ranges from January 1949 to December 1960, or 12 years, with 144 observations.\n\nBelow is a sample of the first few lines of the file.\n\nYou can load this dataset easily using the Pandas library. You are not interested in the date, given that each observation is separated by the same interval of one month. Therefore, when you load the dataset, you can exclude the first column.\n\nOnce loaded, you can easily plot the whole dataset. The code to load and plot the dataset is listed below.\n\nYou can see an upward trend in the dataset over time.\n\nYou can also see some periodicity in the dataset that probably corresponds to the Northern Hemisphere vacation period.\n\nLet‚Äôs keep things simple and work with the data as-is.\n\nNormally, it is a good idea to investigate various data preparation techniques to rescale the data and make it stationary.\n\nThe Long Short-Term Memory network, or LSTM network, is a recurrent neural network trained using Backpropagation Through Time that overcomes the vanishing gradient problem.\n\nAs such, it can be used to create large recurrent networks that, in turn, can be used to address difficult sequence problems in machine learning and achieve state-of-the-art results.\n\nInstead of neurons, LSTM networks have memory blocks connected through layers.\n\nA block has components that make it smarter than a classical neuron and a memory for recent sequences. A block contains gates that manage the block‚Äôs state and output. A block operates upon an input sequence, and each gate within a block uses the sigmoid activation units to control whether it is triggered or not, making the change of state and addition of information flowing through the block conditional.\n\nThere are three types of gates within a unit:\n‚Ä¢ Forget Gate: conditionally decides what information to throw away from the block\n‚Ä¢ Input Gate: conditionally decides which values from the input to update the memory state\n‚Ä¢ Output Gate: conditionally decides what to output based on input and the memory of the block\n\nEach unit is like a mini-state machine where the gates of the units have weights that are learned during the training procedure.\n\nYou can see how you may achieve sophisticated learning and memory from a layer of LSTMs, and it is not hard to imagine how higher-order abstractions may be layered with multiple such layers.\n\nYou can phrase the problem as a regression problem.\n\nThat is, given the number of passengers (in units of thousands) this month, what is the number of passengers next month?\n\nYou can write a simple function to convert the single column of data into a two-column dataset: the first column containing this month‚Äôs (t) passenger count and the second column containing next month‚Äôs (t+1) passenger count to be predicted.\n\nBefore you start, let‚Äôs first import all the functions and classes you will use. This assumes a working SciPy environment with the Keras deep learning library installed.\n\nBefore you do anything, it is a good idea to fix the random number seed to ensure your results are reproducible.\n\nYou can also use the code from the previous section to load the dataset as a Pandas dataframe. You can then extract the NumPy array from the dataframe and convert the integer values to floating point values, which are more suitable for modeling with a neural network.\n\nLSTMs are sensitive to the scale of the input data, specifically when the sigmoid (default) or tanh activation functions are used. It can be a good practice to rescale the data to the range of 0-to-1, also called normalizing. You can easily normalize the dataset using the MinMaxScaler preprocessing class from the scikit-learn library.\n\nAfter you model the data and estimate the skill of your model on the training dataset, you need to get an idea of the skill of the model on new unseen data. For a normal classification or regression problem, you would do this using cross validation.\n\nWith time series data, the sequence of values is important. A simple method that you can use is to split the ordered dataset into train and test datasets. The code below calculates the index of the split point and separates the data into the training datasets, with 67% of the observations used to train the model, leaving the remaining 33% for testing the model.\n\nNow, you can define a function to create a new dataset, as described above.\n\nThe function takes two arguments: the dataset, which is a NumPy array you want to convert into a dataset, and the look_back, which is the number of previous time steps to use as input variables to predict the next time period‚Äîin this case, defaulted to 1.\n\nThis default will create a dataset where X is the number of passengers at a given time (t), and Y is the number of passengers at the next time (t + 1).\n\nIt can be configured by constructing a differently shaped dataset in the next section.\n\nLet‚Äôs take a look at the effect of this function on the first rows of the dataset (shown in the unnormalized form for clarity).\n\nIf you compare these first five rows to the original dataset sample listed in the previous section, you can see the X=t and Y=t+1 pattern in the numbers.\n\nLet‚Äôs use this function to prepare the train and test datasets for modeling.\n\nThe LSTM network expects the input data (X) to be provided with a specific array structure in the form of [samples, time steps, features].\n\nCurrently, the data is in the form of [samples, features], and you are framing the problem as one time step for each sample. You can transform the prepared train and test input data into the expected structure using numpy.reshape() as follows:\n\nYou are now ready to design and fit your LSTM network for this problem.\n\nThe network has a visible layer with 1 input, a hidden layer with 4 LSTM blocks or neurons, and an output layer that makes a single value prediction. The default sigmoid activation function is used for the LSTM blocks. The network is trained for 100 epochs, and a batch size of 1 is used.\n\nOnce the model is fit, you can estimate the performance of the model on the train and test datasets. This will give you a point of comparison for new models.\n\nNote that you will invert the predictions before calculating error scores to ensure that performance is reported in the same units as the original data (thousands of passengers per month).\n\nFinally, you can generate predictions using the model for both the train and test dataset to get a visual indication of the skill of the model.\n\nBecause of how the dataset was prepared, you must shift the predictions so that they align on the x-axis with the original dataset. Once prepared, the data is plotted, showing the original dataset in blue, the predictions for the training dataset in green, and the predictions on the unseen test dataset in red.\n\nYou can see that the model did an excellent job of fitting both the training and the test datasets.\n\nFor completeness, below is the entire code example.\n\nNote: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n\nRunning the example produces the following output.\n\nYou can see that the model has an average error of about 23 passengers (in thousands) on the training dataset and about 49 passengers (in thousands) on the test dataset. Not that bad.\n\nLSTM for Regression Using the Window Method\n\nYou can also phrase the problem so that multiple, recent time steps can be used to make the prediction for the next time step.\n\nThis is called a window, and the size of the window is a parameter that can be tuned for each problem.\n\nFor example, given the current time (t) to predict the value at the next time in the sequence (t+1), you can use the current time (t), as well as the two prior times (t-1 and t-2) as input variables.\n\nWhen phrased as a regression problem, the input variables are t-2, t-1, and t, and the output variable is t+1.\n\nThe create_dataset() function created in the previous section allows you to create this formulation of the time series problem by increasing the look_back argument from 1 to 3.\n\nA sample of the dataset with this formulation is as follows:\n\nYou can re-run the example in the previous section with the larger window size. The whole code listing with just the window size change is listed below for completeness.\n\nNote: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n\nRunning the example provides the following output:\n\nYou can see that the error was increased slightly compared to that of the previous section. The window size and the network architecture were not tuned: This is just a demonstration of how to frame a prediction problem.\n\nYou may have noticed that the data preparation for the LSTM network includes time steps.\n\nSome sequence problems may have a varied number of time steps per sample. For example, you may have measurements of a physical machine leading up to the point of failure or a point of surge. Each incident would be a sample of observations that lead up to the event, which would be the time steps, and the variables observed would be the features.\n\nTime steps provide another way to phrase your time series problem. Like above in the window example, you can take prior time steps in your time series as inputs to predict the output at the next time step.\n\nInstead of phrasing the past observations as separate input features, you can use them as time steps of the one input feature, which is indeed a more accurate framing of the problem.\n\nYou can do this using the same data representation as in the previous window-based example, except when you reshape the data, you set the columns to be the time steps dimension and change the features dimension back to 1. For example:\n\nThe entire code listing is provided below for completeness.\n\nNote: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n\nRunning the example provides the following output:\n\nYou can see that the results are slightly better than the previous example, although the structure of the input data makes a lot more sense.\n\nThe LSTM network has memory capable of remembering across long sequences.\n\nNormally, the state within the network is reset after each training batch when fitting the model, as well as each call to model.predict() or model.evaluate().\n\nYou can gain finer control over when the internal state of the LSTM network is cleared in Keras by making the LSTM layer ‚Äústateful.‚Äù This means it can build a state over the entire training sequence and even maintain that state if needed to make predictions.\n\nIt requires that the training data not be shuffled when fitting the network. It also requires explicit resetting of the network state after each exposure to the training data (epoch) by calls to model.reset_states(). This means that you must create your own outer loop of epochs and within each epoch call model.fit() and model.reset_states(). For example:\n\nFinally, when the LSTM layer is constructed, the stateful parameter must be set to True. Instead of specifying the input dimensions, you must hard code the number of samples in a batch, the number of time steps in a sample, and the number of features in a time step by setting the batch_input_shape parameter. For example:\n\nThis same batch size must then be used later when evaluating the model and making predictions. For example:\n\nYou can adapt the previous time step example to use a stateful LSTM. The full code listing is provided below.\n\nNote: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n\nRunning the example provides the following output:\n\nYou do see that results are better than some, worse than others. The model may need more modules and may need to be trained for more epochs to internalize the structure of the problem.\n\nFinally, let‚Äôs take a look at one of the big benefits of LSTMs: the fact that they can be successfully trained when stacked into deep network architectures.\n\nLSTM networks can be stacked in Keras in the same way that other layer types can be stacked. One addition to the configuration that is required is that an LSTM layer prior to each subsequent LSTM layer must return the sequence. This can be done by setting the return_sequences parameter on the layer to True.\n\nYou can extend the stateful LSTM in the previous section to have two layers, as follows:\n\nThe entire code listing is provided below for completeness.\n\nNote: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n\nRunning the example produces the following output.\n\nThe predictions on the test dataset are again worse. This is more evidence to suggest the need for additional training epochs.\n\nIn this post, you discovered how to develop LSTM recurrent neural networks for time series prediction in Python with the Keras deep learning network.\n‚Ä¢ How to create an LSTM for a regression and a window formulation of the time series problem\n‚Ä¢ How to create an LSTM with a time step formulation of the time series problem\n‚Ä¢ How to create an LSTM with state and stacked LSTMs with state to learn long sequences\n\nDo you have any questions about LSTMs for time series prediction or about this post?\n\n Ask your questions in the comments below, and I will do my best to answer.\n\nThe example in this post is quite dated. See these better examples available for using LSTMs on time series:"
    },
    {
        "link": "https://kaggle.com/code/iamleonie/time-series-tips-tricks-for-training-lstms",
        "document": ""
    },
    {
        "link": "https://justintodata.com/forecast-time-series-lstm-with-tensorflow-keras",
        "document": "In this tutorial, we present a deep learning time series analysis example with Python. You‚Äôll see:\n‚Ä¢ How to preprocess/transform the dataset for time series forecasting.\n‚Ä¢ How to handle large time series datasets when we have limited computer memory.\n‚Ä¢ How to fit Long Short-Term Memory (LSTM) with TensorFlow Keras neural networks model.\n\nIf you want to analyze large time series dataset with machine learning techniques, you‚Äôll love this guide with practical tips.\n\nThe dataset we are using is the Household Electric Power Consumption from Kaggle. It provides measurements of electric power consumption in one household with a one-minute sampling rate.\n\nThere are 2,075,259 measurements gathered within 4 years. Different electrical quantities and some sub-metering values are available. But we‚Äôll only focus on three features:\n\nIn this project, we will predict the amount of Global_active_power 10 minutes ahead.\n\nTo begin, let‚Äôs process the dataset to get ready for time series analysis.\n\nWe transform the dataset df by:\n‚Ä¢ creating feature date_time in DateTime format by combining Date and Time.\n‚Ä¢ ordering the features by time in the new dataset.\n\nNow we have a dataset df as below.\n\nNext, we split the dataset into training, validation, and test datasets.\n\ndf_test holds the data within the last 7 days in the original dataset. df_val has data 14 days before the test dataset. df_train has the rest of the data.\n\nRelated article: Time Series Analysis, Visualization & Forecasting with LSTM\n\nThis article forecasted the Global_active_power only 1 minute ahead of historical data. \n\nBut practically, we want to forecast over a more extended period, which we‚Äôll do in this article.\n\nBefore we can fit the TensorFlow Keras LSTM, there are still other processes that need to be done.\n\nLet‚Äôs deal with them little by little!\n\nAs mentioned earlier, we want to forecast the Global_active_power that‚Äôs 10 minutes in the future.\n\nThe graph below visualizes the problem: using the lagged data (from t-n to t-1) to predict the target (t+10).\n\nIt is not efficient to loop through the dataset while training the model. So we want to transform the dataset with each row representing the historical data and the target.\n\nIn this way, we only need to train the model using each row of the above matrix.\n\nNow here comes the challenges:\n‚Ä¢ How do we convert the dataset to the new structure?\n‚Ä¢ How do we handle this larger new data structure when our computer memory is limited?\n\nAs a result, the function create_ts_files is defined:\n‚Ä¢ to convert the original dataset to the new dataset above.\n‚Ä¢ at the same time, to divide the new dataset into smaller files, which is easier to process.\n\nWithin this function, we define the following parameters:\n‚Ä¢ start_index: the earliest time to be included in all the historical data for forecasting. \n\nIn this practice, we want to include history from the very beginning, so we set the default of it to be 0.\n‚Ä¢ end_index: the latest time to be included in all the historical data for forecasting.\n\nIn this practice, we want to include all the history, so we set the default of it to be None.\n‚Ä¢ history_length: this is n mentioned earlier, which is the number of timesteps to look back for each forecasting.\n‚Ä¢ step_size: the stride of the history window. \n\nGlobal_active_power doesn‚Äôt change fast throughout time. So to be more efficient, we can let step_size = 10. In this way, we downsample to use every 10 minutes of data in the past to predict the future amount. We are only looking at t-1, t-11, t-21 until t-n to predict t+10.\n‚Ä¢ target_step: the number of periods in the future to predict.\n\nAs mentioned earlier, we are trying to predict the global_active_power 10 minutes ahead. So this feature = 10.\n‚Ä¢ num_rows_per_file: the number of records to put in each file.\n\nThis is necessary to divide the large new dataset into smaller files.\n‚Ä¢ data_folder: the one single folder that will contain all the files.\n\nIn the end, just know that this function creates a folder with files. \n\nAnd each file contains a pandas dataframe that looks like the new dataset in the chart above. \n\nEach of these dataframes has columns:\n‚Ä¢ y, which is the target to predict. This will be the value at t + target_step (t + 10).\n‚Ä¢ x_lag{i}, the value at time t + target_step ‚Äì i (t + 10 ‚Äì 11, t + 10 ‚Äì 21, and so on), i.e., the lagged value compared to y.\n\nAt the same time, the function also returns the number of lags (len(col_names)-1) in the dataframes. This number will be required when defining the shape for TensorFlow models later.\n\nBefore applying the function create_ts_files, we also need to:\n‚Ä¢ scale the global_active_power to work with Neural Networks.\n‚Ä¢ define step_size within historical data to be 10 minutes.\n‚Ä¢ set the target_step to be 10, so that we are forecasting the global_active_power 10 minutes after the historical data.\n\nAfter these, we apply the create_ts_files to:\n‚Ä¢ create 158 files (each including a pandas dataframe) within the folder ts_data.\n‚Ä¢ return num_timesteps as the number of lags.\n\nAs the function runs, it prints the name of every 10 files.\n\nThe folder ts_data is around 16 GB, and we were only using the past 7 days of data to predict. Now you can see why it‚Äôs necessary to divide the dataset into smaller dataframes!\n\nIn this procedure, we create a class TimeSeriesLoader to transform and feed the dataframes into the model.\n\nThere are built-in functions from Keras such as Keras Sequence, tf.data API. But they are not very efficient for this purpose.\n\nWithin this class, we define:\n‚Ä¢ __init__: the initial settings of the object, including: \n\n‚Äì ts_folder, which will be ts_data that we just created.\n\n‚Äì filename_format, which is the string format of the file names in the ts_folder. \n\nFor example, when the files are ts_file0.pkl, ts_file1.pkl, ‚Ä¶, ts_file100.pkl, the format would be ‚Äòts_file{}.pkl‚Äô.\n‚Ä¢ get_chunk: this method takes the dataframe from one of the files, processes it to be ready for training.\n‚Ä¢ shuffle_chunks: this method shuffles the order of the chunks that are returned in get_chunk. This is a good practice for modeling.\n\nThe definitions might seem a little confusing. But keep reading, you‚Äôll see this object in action within the next step.\n\nAfter defining, we apply this TimeSeriesLoader to the ts_data folder.\n\nNow with the object tss points to our dataset, we are finally ready for LSTM!\n\nAs mentioned before, we are going to build an LSTM model based on the TensorFlow Keras library.\n\nWe all know the importance of hyperparameter tuning based on our guide. But in this article, we are simply demonstrating the model fitting without tuning.\n\nThe procedures are below:\n‚Ä¢ define the shape of the input dataset:\n\n‚Äì num_timesteps, the number of lags in the dataframes we set in Step #2.\n\n‚Äì the number of time series as 1. Since we are only using one feature of global_active_power.\n‚Ä¢ define the number of units, 4*units*(units+2) is the number of parameters of the LSTM. \n\nThe higher the number, the more parameters in the model.\n‚Ä¢ define the dropout rate, which is used to prevent overfitting.\n‚Ä¢ specify the output layer to have a linear activation function.\n\nThen we also define the optimization function and the loss function. Again, tuning these hyperparameters to find the best option would be a better practice.\n\nTo take a look at the model we just defined before running, we can print out the summary.\n\nYou can see that the output shape looks good, which is n / step_size (7*24*60 / 10 = 1008). The number of parameters that need to be trained looks right as well (4*units*(units+2) = 480).\n\nWe train each chunk in batches, and only run for one epoch. Ideally, you would train for multiple epochs for neural networks.\n\nAfter fitting the model, we may also evaluate the model performance using the validation dataset.\n\nSame as the training dataset, we also create a folder of the validation data, which prepares the validation dataset for model fitting.\n\nBesides testing using the validation dataset, we also test against a baseline model using only the most recent history point (t + 10 ‚Äì 11).\n\nThe detailed Python code is below.\n\nThe validation dataset using LSTM gives Mean Squared Error (MSE) of 0.418. While the baseline model has MSE of 0.428. The LSTM does slightly better than the baseline.\n\nWe could do better with hyperparameter tuning and more epochs. Plus, some other essential time series analysis tips such as seasonality would help too.\n\nThank you for reading!\n\nHope you found something useful in this guide. Leave a comment if you have any questions.\n\nBefore you leave, don‚Äôt forget to sign up for the Just into Data newsletter! Or connect with us on Twitter, Facebook.\n\nSo you won‚Äôt miss any new data science articles from us!"
    },
    {
        "link": "https://geeksforgeeks.org/data-visualization-using-matplotlib",
        "document": "Matplotlib is a powerful and widely-used Python library for creating static, animated and interactive data visualizations. In this article, we will provide a guide on Matplotlib and how to use it for data visualization with practical implementation.\n\nMatplotlib offers a wide variety of plots such as line charts, bar charts, scatter plot and histograms making it versatile for different data analysis tasks. The library is built on top of NumPy making it efficient for handling large datasets. It provides a lot of flexibility in code.\n\nWe will use the pip command to install this module. If you do not have pip installed then refer to the article, Download and install pip Latest Version.\n\nTo install Matplotlib type the below command in the terminal.\n\nIf you are using Jupyter Notebook, you can install it within a notebook cell by using:\n\nMatplotlib provides a module called pyplot which offers a MATLAB-like interface for creating plots and charts. It simplifies the process of generating various types of visualizations by providing a collection of functions that handle common plotting tasks.\n\nMatplotlib supports a variety of plots including line charts, bar charts, histograms, scatter plots, etc. Let‚Äôs understand them with implementation using pyplot.\n\nLine chart is one of the basic plots and can be created using the plot() function. It is used to represent a relationship between two data X and Y on a different axis.\n\nA bar chart is a graph that represents the category of data with rectangular bars with lengths and heights that is proportional to the values which they represent. The bar plots can be plotted horizontally or vertically. A bar chart describes the comparisons between the different categories. It can be created using the bar() method.\n\nIn the below example we will use the tips dataset. Tips database is the record of the tip given by the customers in a restaurant for two and a half months in the early 1990s. It contains 6 columns as total_bill, tip, sex, smoker, day, time, size.\n\nA histogram is basically used to represent data provided in a form of some groups. It is a type of bar plot where the X-axis represents the bin ranges while the Y-axis gives information about frequency. The hist() function is used to compute and create histogram of x.\n\nScatter plots are used to observe relationships between variables. The scatter() method in the matplotlib library is used to draw a scatter plot.\n\nPie chart is a circular chart used to display only one series of data. The area of slices of the pie represents the percentage of the parts of the data. The slices of pie are called wedges. It can be created using the pie() method.\n\nA Box Plot is also known as a Whisker Plot and is a standardized way of displaying the distribution of data based on a five-number summary: minimum, first quartile (Q1), median (Q2), third quartile (Q3) and maximum. It can also show outliers.Let‚Äôs see an example of how to create a Box Plot using Matplotlib in Python:\n‚Ä¢ and : Customize the appearance of the boxes and median lines respectively.\n\nThe box shows the interquartile range (IQR) the line inside the box shows the median and the ‚Äúwhiskers‚Äù extend to the minimum and maximum values within 1.5 * IQR from the first and third quartiles. Any points outside this range are considered outliers and are plotted as individual points.\n\nA Heatmap is a data visualization technique that represents data in a matrix form where individual values are represented as colors. Heatmaps are particularly useful for visualizing the magnitude of multiple features in a two-dimensional surface and identifying patterns, correlations and concentrations. Let‚Äôs see an example of how to create a Heatmap using Matplotlib in Python:\n‚Ä¢ None : Displays the data as an image (heatmap). The argument specifies the color map used for the heatmap.\n‚Ä¢ None : Ensures that each data point is shown as a block of color without smoothing.\n\nThe color bar on the side provides a scale to interpret the colors with darker colors representing lower values and lighter colors representing higher values. This type of plot is often used in fields like data analysis, bioinformatics and finance to visualize data correlations and distributions across a matrix.\n\nMatplotlib allows extensive customization and styling of plots including changing colors, adding labels and modifying plot styles.\n\nLet‚Äôs apply the customization techniques we‚Äôve learned to the basic plots we created earlier. This will involve enhancing each plot with titles, axis labels, limits, tick labels, and legends to make them more informative and visually appealing.\n\nLet‚Äôs see how to customize the line chart. We will be using the following properties:\n‚Ä¢ color: Changing the color of the line\n‚Ä¢ linewidth: Customizing the width of the line\n‚Ä¢ marker: For changing the style of actual plotted point\n‚Ä¢ markersize: For changing the size of the markers\n‚Ä¢ linestyle: For defining the style of the plotted line\n\nTo make bar charts more informative and visually appealing various customization options are available. Customization that is available for the Bar Chart are:\n‚Ä¢ edgecolor: Color of edges of the bar\n\nThe lines in between the bars refer to the different values in the Y-axis of the particular value of the X-axis.\n\nTo make histogram plots more effective and tailored to your data, you can apply various customizations:\n‚Ä¢ alpha: blending value, between 0 (transparent) and 1 (opaque)\n\nScatter plots are versatile tools for visualizing relationships between two variables. Customizations that are available for the scatter plot are to enhance their clarity and effectiveness:\n‚Ä¢ s: marker size (can be scalar or array of size equal to size of x or y)\n‚Ä¢ c: color of sequence of colors for markers\n‚Ä¢ alpha: blending value, between 0 (transparent) and 1 (opaque)\n\nPie charts are a great way to visualize proportions and parts of a whole. To make your pie charts more effective and visually appealing, consider the following customization techniques:\n‚Ä¢ explode: Moving the wedges of the plot\n‚Ä¢ autopct: Label the wedge with their numerical value.\n‚Ä¢ color: Attribute is used to provide color to the wedges.\n‚Ä¢ shadow: Used to create shadow of wedge.\n\nBefore moving any further with Matplotlib let‚Äôs discuss some important classes that will be used further in the tutorial. These classes are:\n\nConsider the figure class as the overall window or page on which everything is drawn. It is a top-level container that contains one or more axes. A figure can be created using the figure() method.\n\nAxes class is the most basic and flexible unit for creating sub-plots. A given figure may contain many axes, but a given axes can only be present in one figure. The axes() function creates the axes object.\n\nJust like pyplot class, axes class also provides methods for adding titles, legends, limits, labels, etc. Let‚Äôs see a few of them ‚Äì\n‚Ä¢ None ax.set_title() is used to add title.\n‚Ä¢ None To set the limits we use ax.set_xlim(), ax.set_ylim()\n‚Ä¢ None ax.set_xticklabels(), ax.set_yticklabels() are used to tick labels.\n‚Ä¢ None To add legend we use ax.legend()\n\nWe have learned about the basic components of a graph that can be added so that it can convey more information. One method can be by calling the plot function again and again with a different set of values as shown in the above example. Now let‚Äôs see how to plot multiple graphs using some functions and also how to plot subplots.\n\nThe add_axes() method method allows you to manually add axes to a figure in Matplotlib. It takes a list of four values to specify the position and size of the axes.\n\nsubplot() method adds a plot to a specified grid position within the current figure. It takes three arguments: the number of rows, columns, and the plot index. Now Let‚Äôs understand it with the help of example:\n\nThe subplot2grid() creates axes object at a specified location inside a grid and also helps in spanning the axes object across multiple rows or columns. In simpler words, this function is used to create multiple charts within the same figure.\n\nFor saving a plot in a file on storage disk, savefig() method is used. A file can be saved in many formats like .png, .jpg, .pdf, etc.\n\nIn this guide, we have explored the fundamentals of Matplotlib, from installation to advanced plotting techniques. By mastering these concepts, Whether you are working with simple line charts or complex heatmaps you can create and customize a wide range of visualizations to effectively communicate data insights.\n\nWhat is the difference between and in Matplotlib?\n\nCan Matplotlib be used for interactive visualizations?\n\nWhat are some advanced customization options available in Matplotlib?\n\nHow do I handle large datasets with Matplotlib?"
    },
    {
        "link": "https://machinelearningplus.com/plots/top-50-matplotlib-visualizations-the-master-plots-python",
        "document": "A compilation of the Top 50 matplotlib plots most useful in data analysis and visualization. This list lets you choose what visualization to show for what situation using python‚Äôs matplotlib and seaborn library.\n\nThe charts are grouped based on the 7 different purposes of your visualization objective. For example, if you want to picturize the relationship between 2 variables, check out the plots under the ‚ÄòCorrelation‚Äô section. Or if you want to show how a value changed over time, look under the ‚ÄòChange‚Äô section and so on.\n\nAn effective chart is one which:\n‚Ä¢ Conveys the right and necessary information without distorting facts.\n‚Ä¢ Simple in design, you don‚Äôt have to strain in order to get it.\n‚Ä¢ Aesthetics support the information rather than overshadow it.\n\nMatplotlib is popularly used for visualizing plots. Check out these free video tutorials to learn how to get started with Matplotlib and create your your first plot.\n‚Ä¢ Scatter plot with line of best fit\n‚Ä¢ Plotting with different scales using secondary Y axis\n\nRun this once before the plot‚Äôs code. The individual charts, however, may redefine its own aesthetics.\n\nIf you want to have a video walkthrough of how to setup Matplotlib, check this free video lesson.\n\nThe plots under correlation is used to visualize the relationship between 2 or more variables. That is, how does one variable change with respect to another.\n\nScatteplot is a classic and fundamental plot used to study the relationship between two variables. If you have multiple groups in your data you may want to visualise each group in a different color. In , you can conveniently do this using .\n\nSometimes you want to show a group of points within a boundary to emphasize their importance. In this example, you get the records from the dataframe that should be encircled and pass it to the described in the code below.\n\n3. Scatter plot with linear regression line of best fit\n\nIf you want to understand how two variables change with respect to each other, the line of best fit is the way to go. The below plot shows how the line of best fit differs amongst various groups in the data. To disable the groupings and to just draw one line-of-best-fit for the entire dataset, remove the parameter from the call below.\n\nEach regression line in its own column\n\nAlternately, you can show the best fit line for each group in its own column. You cando this by setting the parameter inside the .\n\nOften multiple datapoints have exactly the same X and Y values. As a result, multiple points get plotted over each other and hide. To avoid this, jitter the points slightly so you can visually see them. This is convenient to do using seaborn‚Äôs .\n\nAnother option to avoid the problem of points overlap is the increase the size of the dot depending on how many points lie in that spot. So, larger the size of the point more is the concentration of points around that.\n\nMarginal histograms have a histogram along the X and Y axis variables. This is used to visualize the relationship between the X and Y along with the univariate distribution of the X and the Y individually. This plot if often used in exploratory data analysis (EDA).\n\nMarginal boxplot serves a similar purpose as marginal histogram. However, the boxplot helps to pinpoint the median, 25th and 75th percentiles of the X and the Y.\n\nCorrelogram is used to visually see the correlation metric between all possible pairs of numeric variables in a given dataframe (or 2D array).\n\nPairwise plot is a favorite in exploratory analysis to understand the relationship between all possible pairs of numeric variables. It is a must have tool for bivariate analysis.\n\nIf you want to see how the items are varying based on a single metric and visualize the order and amount of this variance, the diverging bars is a great tool. It helps to quickly differentiate the performance of groups in your data and is quite intuitive and instantly conveys the point.\n\nDiverging texts is similar to diverging bars and it preferred if you want to show the value of each items within the chart in a nice and presentable way.\n\nDivering dot plot is also similar to the diverging bars. However compared to diverging bars, the absence of bars reduces the amount of contrast and disparity between the groups.\n\nLollipop with markers provides a flexible way of visualizing the divergence by laying emphasis on any significant datapoints you want to bring attention to and give reasoning within the chart appropriately.\n\nBy coloring the area between the axis and the lines, the area chart throws more emphasis not just on the peaks and troughs but also the duration of the highs and lows. The longer the duration of the highs, the larger is the area under the line.\n\nOrdered bar chart conveys the rank order of the items effectively. But adding the value of the metric above the chart, the user gets the precise information from the chart itself. It is a classic way of visualizing items based on counts or any given metric. Check this free video tutorial on implementing and interpreting ordered bar charts.\n\nLollipop chart serves a similar purpose as a ordered bar chart in a visually pleasing way.\n\nThe dot plot conveys the rank order of the items. And since it is aligned along the horizontal axis, you can visualize how far the points are from each other more easily.\n\nSlope chart is most suitable for comparing the ‚ÄòBefore‚Äô and ‚ÄòAfter‚Äô positions of a given person/item.\n\nDumbbell plot conveys the ‚Äòbefore‚Äô and ‚Äòafter‚Äô positions of various items along with the rank ordering of the items. Its very useful if you want to visualize the effect of a particular project / initiative on different objects.\n\nHistogram shows the frequency distribution of a given variable. The below representation groups the frequency bars based on a categorical variable giving a greater insight about the continuous variable and the categorical variable in tandem. Create histogram and learn how to interpret them in this free video tutorial.\n\nThe histogram of a categorical variable shows the frequency distribution of a that variable. By coloring the bars, you can visualize the distribution in connection with another categorical variable representing the colors.\n\nDensity plots are a commonly used tool visualise the distribution of a continuous variable. By grouping them by the ‚Äòresponse‚Äô variable, you can inspect the relationship between the X and the Y. The below case if for representational purpose to describe how the distribution of city mileage varies with respect the number of cylinders.\n\nDensity curve with histogram brings together the collective information conveyed by the two plots so you can have them both in a single figure instead of two.\n\nJoy Plot allows the density curves of different groups to overlap, it is a great way to visualize the distribution of a larger number of groups in relation to each other. It looks pleasing to the eye and conveys just the right information clearly. It can be easily built using the package which is based on .\n\nDistributed dot plot shows the univariate distribution of points segmented by groups. The darker the points, more is the concentration of data points in that region. By coloring the median differently, the real positioning of the groups becomes apparent instantly.\n\nBox plots are a great way to visualize the distribution, keeping the median, 25th 75th quartiles and the outliers in mind. However, you need to be careful about interpreting the size the boxes which can potentially distort the number of points contained within that group. So, manually providing the number of observations in each box can help overcome this drawback. Check this free video lesson to visualize distribution of a numeric variable using box plot.\n\nFor example, the first two boxes on the left have boxes of the same size even though they have 5 and 47 obs respectively. So writing the number of observations in that group becomes necessary.\n\nDot + Box plot Conveys similar information as a boxplot split in groups. The dots, in addition, gives a sense of how many data points lie within each group.\n\nViolin plot is a visually pleasing alternative to box plots. The shape or area of the violin depends on the number of observations it holds. However, the violin plots can be harder to read and it not commonly used in professional settings. Thsi free video tutorial will train you how to implement violin plots.\n\nPopulation pyramid can be used to show either the distribution of the groups ordered by the volumne. Or it can also be used to show the stage-by-stage filtering of the population as it is used below to show how many people pass through each stage of a marketing funnel.\n\nCategorical plots provided by the library can be used to visualize the counts distribution of 2 ore more categorical variables in relation to each other.\n\nThe chart can be created using the package and is used to show the compositions of groups in a larger population.\n\nPie chart is a classic way to show the composition of groups. However, its not generally advisable to use nowadays because the area of the pie portions can sometimes become misleading. So, if you are to use pie chart, its highly recommended to explicitly write down the percentage or numbers for each portion of the pie.\n\nTree map is similar to a pie chart and it does a better work without misleading the contributions by each group.\n\nBar chart is a classic way of visualizing items based on counts or any given metric. In below chart, I have used a different color for each item, but you might typically want to pick one color for all items unless you to color them by groups. The color names get stored inside in the code below. You can change the color of the bars by setting the parameter in .\n\nTime series plot is used to visualise how a given metric changes over time. Here you can see how the Air Passenger traffic changed between 1949 and 1969. Check this free video tutorial on how to implement line plots for analyzing time series.\n\nThe below time series plots all the the peaks and troughs and annotates the occurence of selected special events.\n\nThe ACF plot shows the correlation of the time series with its own lags. Each vertical line (on the autocorrelation plot) represents the correlation between the series and its lag starting from lag 0. The blue shaded region in the plot is the significance level. Those lags that lie above the blue line are the significant lags.\n\nSo how to interpret this?\n\nFor AirPassengers, we see upto 14 lags have crossed the blue line and so are significant. This means, the Air Passengers traffic seen upto 14 years back has an influence on the traffic seen today.\n\nPACF on the other had shows the autocorrelation of any given lag (of time series) against the current series, but with the contributions of the lags-inbetween removed.\n\nNote: If you want to learn how to interpret and draw ACF and PACF plots, check this free video tutorial.\n\nCross correlation plot shows the lags of two time series with each other.\n\nTime series decomposition plot shows the break down of the time series into trend, seasonal and residual components.\n\nYou can plot multiple time series that measures the same value on the same chart as shown below.\n\n41. Plotting with different scales using secondary Y axis\n\nIf you want to show two time series that measures two different quantities at the same point in time, you can plot the second series againt the secondary Y axis on the right.\n\nLearn to draw a multiple axis time series using this free video tutorial.\n\nTime series with error bands can be constructed if you have a time series dataset with multiple observations for each time point (date / timestamp). Below you can see a couple of examples based on the orders coming in at various times of the day. And another example on the number of orders arriving over a duration of 45 days.\n\nIn this approach, the mean of the number of orders is denoted by the white line. And a 95% confidence bands are computed and drawn around the mean.\n\nStacked area chart gives an visual representation of the extent of contribution from multiple time series so that it is easy to compare against each other.\n\nAn unstacked area chart is used to visualize the progress (ups and downs) of two or more series with respect to each other. In the chart below, you can clearly see how the personal savings rate comes down as the median duration of unemployment increases. The unstacked area chart brings out this phenomenon nicely.\n\nCalendar map is an alternate and a less preferred option to visualise time based data compared to a time series. Though can be visually appealing, the numeric values are not quite evident. It is however effective in picturising the extreme values and holiday effects nicely.\n\nThe seasonal plot can be used to compare how the time series performed at same day in the previous season (year / month / week etc).\n\nA Dendrogram groups similar points together based on a given distance metric and organizes them in tree like links based on the point‚Äôs similarity.\n\nCluster Plot canbe used to demarcate points that belong to the same cluster. Below is a representational example to group the US states into 5 groups based on the USArrests dataset. This cluster plot uses the ‚Äòmurder‚Äô and ‚Äòassault‚Äô columns as X and Y axis. Alternately you can use the first to principal components as rthe X and Y axis.\n\nAndrews Curve helps visualize if there are inherent groupings of the numerical features based on a given grouping. If the features (columns in the dataset) doesn‚Äôt help discriminate the group ( , then the lines will not be well segregated as you see below.\n\nParallel coordinates helps to visualize if a feature helps to segregate the groups effectively. If a segregation is effected, that feature is likely going to be very useful in predicting that group.\n\nThat‚Äôs all for now! If you encounter some error or bug please notify here."
    },
    {
        "link": "https://stackoverflow.com/questions/58410187/how-to-plot-predicted-values-vs-the-true-value",
        "document": "I'm new to visualization using matplotlib. I will like to make a plot of my machine learning model's predicted value vs the actual value.\n\nI made a prediction using random forest algorithm and will like to visualize the plot of true values and predicted values.\n\nI used the below code, but the plot isn't showing clearly the relationship between the predicted and actual values.\n\nThis is what I get:\n\nThis is what I expect the plot to look like:\n\nBelow is the table of data i want to plot.(Note: This is just part of the result as it is impossible to include the full data (shape 8221, 1) here. i look forward to your help."
    },
    {
        "link": "https://oreilly.com/library/view/python-data-science/9781491912126/ch04.html",
        "document": "We‚Äôll now take an in-depth look at the Matplotlib tool for visualization in Python. Matplotlib is a multiplatform data visualization library built on NumPy arrays, and designed to work with the broader SciPy stack. It was conceived by John Hunter in 2002, originally as a patch to IPython for enabling interactive MATLAB-style plotting via gnuplot from the IPython command line. IPython‚Äôs creator, Fernando Perez, was at the time scrambling to finish his PhD, and let John know he wouldn‚Äôt have time to review the patch for several months. John took this as a cue to set out on his own, and the Matplotlib package was born, with version 0.1 released in 2003. It received an early boost when it was adopted as the plotting package of choice of the Space Telescope Science Institute (the folks behind the Hubble Telescope), which financially supported Matplotlib‚Äôs development and greatly expanded its capabilities.\n\nOne of Matplotlib‚Äôs most important features is its ability to play well with many operating systems and graphics backends. Matplotlib supports dozens of backends and output types, which means you can count on it to work regardless of which operating system you are using or which output format you wish. This cross-platform, everything-to-everyone approach has been one of the great strengths of Matplotlib. It has led to a large userbase, which in turn has led to an active developer base and Matplotlib‚Äôs powerful tools and ubiquity within the scientific Python world.\n\nIn recent years, however, the interface and style of Matplotlib have begun to show their age. Newer tools like ggplot and ggvis in the R language, along with web visualization toolkits based on D3js and HTML5 canvas, often make Matplotlib feel clunky and old-fashioned. Still, I‚Äôm of the opinion that we cannot ignore Matplotlib‚Äôs strength as a well-tested, cross-platform graphics engine. Recent Matplotlib versions make it relatively easy to set new global plotting styles (see ‚ÄúCustomizing Matplotlib: Configurations and Stylesheets‚Äù), and people have been developing new packages that build on its powerful internals to drive Matplotlib via cleaner, more modern APIs‚Äîfor example, Seaborn (discussed in ‚ÄúVisualization with Seaborn‚Äù), ggplot, HoloViews, Altair, and even Pandas itself can be used as wrappers around Matplotlib‚Äôs API. Even with wrappers like these, it is still often useful to dive into Matplotlib‚Äôs syntax to adjust the final plot output. For this reason, I believe that Matplotlib itself will remain a vital piece of the data visualization stack, even if new tools mean the community gradually moves away from using the Matplotlib API directly.\n\nPerhaps the simplest of all plots is the visualization of a single function . Here we will take a first look at creating a simple plot of this type. As with all the following sections, we‚Äôll start by setting up the notebook for plotting and importing the functions we will use: For all Matplotlib plots, we start by creating a figure and an axes. In their simplest form, a figure and axes can be created as follows (Figure 4-5): In Matplotlib, the figure (an instance of the class ) can be thought of as a single container that contains all the objects representing axes, graphics, text, and labels. The axes (an instance of the class ) is what we see above: a bounding box with ticks and labels, which will eventually contain the plot elements that make up our visualization. Throughout this book, we‚Äôll commonly use the variable name to refer to a figure instance, and to refer to an axes instance or group of axes instances. Once we have created an axes, we can use the function to plot some data. Let‚Äôs start with a simple sinusoid (Figure 4-6): Alternatively, we can use the pylab interface and let the figure and axes be created for us in the background (Figure 4-7; see ‚ÄúTwo Interfaces for the Price of One‚Äù for a discussion of these two interfaces): If we want to create a single figure with multiple lines, we can simply call the function multiple times (Figure 4-8): That‚Äôs all there is to plotting simple functions in Matplotlib! We‚Äôll now dive into some more details about how to control the appearance of the axes and lines. The first adjustment you might wish to make to a plot is to control the line colors and styles. The function takes additional arguments that can be used to specify these. To adjust the color, you can use the keyword, which accepts a string argument representing virtually any imaginable color. The color can be specified in a variety of ways (Figure 4-9): # specify color by name If no color is specified, Matplotlib will automatically cycle through a set of default colors for multiple lines. Similarly, you can adjust the line style using the keyword (Figure 4-10): # For short, you can use the following codes: Example of various line styles If you would like to be extremely terse, these and codes can be combined into a single nonkeyword argument to the function (Figure 4-11): Controlling colors and styles with the shorthand syntax These single-character color codes reflect the standard abbreviations in the RGB (Red/Green/Blue) and CMYK (Cyan/Magenta/Yellow/blacK) color systems, commonly used for digital color graphics. There are many other keyword arguments that can be used to fine-tune the appearance of the plot; for more details, I‚Äôd suggest viewing the docstring of the function using IPython‚Äôs help tools (see ‚ÄúHelp and Documentation in IPython‚Äù). Matplotlib does a decent job of choosing default axes limits for your plot, but sometimes it‚Äôs nice to have finer control. The most basic way to adjust axis limits is to use the and methods (Figure 4-12): If for some reason you‚Äôd like either axis to be displayed in reverse, you can simply reverse the order of the arguments (Figure 4-13): Example of reversing the y-axis A useful related method is (note here the potential confusion between axes with an e, and axis with an i). The method allows you to set the and limits with a single call, by passing a list that specifies (Figure 4-14): The method goes even beyond this, allowing you to do things like automatically tighten the bounds around the current plot (Figure 4-15): It allows even higher-level specifications, such as ensuring an equal aspect ratio so that on your screen, one unit in is equal to one unit in (Figure 4-16): Example of an ‚Äúequal‚Äù layout, with units matched to the output resolution For more information on axis limits and the other capabilities of the method, refer to the docstring. As the last piece of this section, we‚Äôll briefly look at the labeling of plots: titles, axis labels, and simple legends. Titles and axis labels are the simplest such labels‚Äîthere are methods that can be used to quickly set them (Figure 4-17): You can adjust the position, size, and style of these labels using optional arguments to the function. For more information, see the Matplotlib documentation and the docstrings of each of these functions. When multiple lines are being shown within a single axes, it can be useful to create a plot legend that labels each line type. Again, Matplotlib has a built-in way of quickly creating such a legend. It is done via the (you guessed it) method. Though there are several valid ways of using this, I find it easiest to specify the label of each line using the keyword of the plot function (Figure 4-18): As you can see, the function keeps track of the line style and color, and matches these with the correct label. More information on specifying and formatting plot legends can be found in the docstring; additionally, we will cover some more advanced legend options in ‚ÄúCustomizing Plot Legends‚Äù. While most functions translate directly to methods (such as ‚Üí , ‚Üí , etc.), this is not the case for all commands. In particular, functions to set limits, labels, and titles are slightly modified. For transitioning between MATLAB-style functions and object-oriented methods, make the following changes: In the object-oriented interface to plotting, rather than calling these functions individually, it is often more convenient to use the method to set all these properties at once (Figure 4-19): Example of using ax.set to set multiple properties at once\n\nMatplotlib‚Äôs default tick locators and formatters are designed to be generally sufficient in many common situations, but are in no way optimal for every plot. This section will give several examples of adjusting the tick locations and formatting for the particular plot type you‚Äôre interested in. Before we go into examples, it will be best for us to understand further the object hierarchy of Matplotlib plots. Matplotlib aims to have a Python object representing everything that appears on the plot: for example, recall that the is the bounding box within which plot elements appear. Each Matplotlib object can also act as a container of sub-objects; for example, each can contain one or more objects, each of which in turn contain other objects representing plot contents. The tick marks are no exception. Each has attributes and , which in turn have attributes that contain all the properties of the lines, ticks, and labels that make up the axes. Within each axis, there is the concept of a major tick mark and a minor tick mark. As the names would imply, major ticks are usually bigger or more pronounced, while minor ticks are usually smaller. By default, Matplotlib rarely makes use of minor ticks, but one place you can see them is within logarithmic plots (Figure 4-73): Example of logarithmic scales and labels We see here that each major tick shows a large tick mark and a label, while each minor tick shows a smaller tick mark with no label. We can customize these tick properties‚Äîthat is, locations and labels‚Äîby setting the and objects of each axis. Let‚Äôs examine these for the x axis of the plot just shown: We see that both major and minor tick labels have their locations specified by a (which makes sense for a logarithmic plot). Minor ticks, though, have their labels formatted by a ; this says that no labels will be shown. We‚Äôll now show a few examples of setting these locators and formatters for various plots. Perhaps the most common tick/label formatting operation is the act of hiding ticks or labels. We can do this using and , as shown here (Figure 4-74): Notice that we‚Äôve removed the labels (but kept the ticks/gridlines) from the x axis, and removed the ticks (and thus the labels as well) from the y axis. Having no ticks at all can be useful in many situations‚Äîfor example, when you want to show a grid of images. For instance, consider Figure 4-75, which includes images of different faces, an example often used in supervised machine learning problems (for more information, see ‚ÄúIn-Depth: Support Vector Machines‚Äù): # Get some face data from scikit-learn Notice that each image has its own axes, and we‚Äôve set the locators to null because the tick values (pixel number in this case) do not convey relevant information for this particular visualization. Reducing or Increasing the Number of Ticks One common problem with the default settings is that smaller subplots can end up with crowded labels. We can see this in the plot grid shown in Figure 4-76: Particularly for the x ticks, the numbers nearly overlap, making them quite difficult to decipher. We can fix this with the , which allows us to specify the maximum number of ticks that will be displayed. Given this maximum number, Matplotlib will use internal logic to choose the particular tick locations (Figure 4-77): # For every axis, set the x and y major locator This makes things much cleaner. If you want even more control over the locations of regularly spaced ticks, you might also use , which we‚Äôll discuss in the following section. Matplotlib‚Äôs default tick formatting can leave a lot to be desired; it works well as a broad default, but sometimes you‚Äôd like to do something more. Consider the plot shown in Figure 4-78, a sine and a cosine: There are a couple changes we might like to make. First, it‚Äôs more natural for this data to space the ticks and grid lines in multiples of . We can do this by setting a , which locates ticks at a multiple of the number you provide. For good measure, we‚Äôll add both major and minor ticks in multiples of (Figure 4-79): But now these tick labels look a little bit silly: we can see that they are multiples of , but the decimal representation does not immediately convey this. To fix this, we can change the tick formatter. There‚Äôs no built-in formatter for what we want to do, so we‚Äôll instead use , which accepts a user-defined function giving fine-grained control over the tick outputs (Figure 4-80): This is much better! Notice that we‚Äôve made use of Matplotlib‚Äôs LaTeX support, specified by enclosing the string within dollar signs. This is very convenient for display of mathematical symbols and formulae; in this case, is rendered as the Greek character . The offers extremely fine-grained control over the appearance of your plot ticks, and comes in very handy when you‚Äôre preparing plots for presentation or publication. We‚Äôve mentioned a couple of the available formatters and locators. We‚Äôll conclude this section by briefly listing all the built-in locator and formatter options. For more information on any of these, refer to the docstrings or to the Matplotlib online documentation. Each of the following is available in the namespace: Ticks and range are a multiple of base Finds up to a max number of ticks at nice locations Set the strings from a list of labels Set the strings manually for the labels Use a format string for each value We‚Äôll see additional examples of these throughout the remainder of the book.\n\nOne common type of visualization in data science is that of geographic data. Matplotlib‚Äôs main tool for this type of visualization is the Basemap toolkit, which is one of several Matplotlib toolkits that live under the namespace. Admittedly, Basemap feels a bit clunky to use, and often even simple visualizations take much longer to render than you might hope. More modern solutions, such as leaflet or the Google Maps API, may be a better choice for more intensive map visualizations. Still, Basemap is a useful tool for Python users to have in their virtual toolbelts. In this section, we‚Äôll show several examples of the type of map visualization that is possible with this toolkit. Installation of Basemap is straightforward; if you‚Äôre using conda you can type this and the package will be downloaded: We add just a single new import to our standard boilerplate: Once you have the Basemap toolkit installed and imported, geographic plots are just a few lines away (the graphics in Figure 4-102 also require the package in Python 2, or the package in Python 3): The meaning of the arguments to Basemap will be discussed momentarily. The useful thing is that the globe shown here is not a mere image; it is a fully functioning Matplotlib axes that understands spherical coordinates and allows us to easily over-plot data on the map! For example, we can use a different map projection, zoom in to North America, and plot the location of Seattle. We‚Äôll use an etopo image (which shows topographical features both on land and under the ocean) as the map background (Figure 4-103): Plotting data and labels on the map This gives you a brief glimpse into the sort of geographic visualizations that are possible with just a few lines of Python. We‚Äôll now discuss the features of Basemap in more depth, and provide several examples of visualizing map data. Using these brief examples as building blocks, you should be able to create nearly any map visualization that you desire. The first thing to decide when you are using maps is which projection to use. You‚Äôre probably familiar with the fact that it is impossible to project a spherical map, such as that of the Earth, onto a flat surface without somehow distorting it or breaking its continuity. These projections have been developed over the course of human history, and there are a lot of choices! Depending on the intended use of the map projection, there are certain map features (e.g., direction, area, distance, shape, or other considerations) that are useful to maintain. The Basemap package implements several dozen such projections, all referenced by a short format code. Here we‚Äôll briefly demonstrate some of the more common ones. We‚Äôll start by defining a convenience routine to draw our world map along with the longitude and latitude lines: # lats and longs are returned as a dictionary # cycle through these lines and set the desired style The simplest of map projections are cylindrical projections, in which lines of constant latitude and longitude are mapped to horizontal and vertical lines, respectively. This type of mapping represents equatorial regions quite well, but results in extreme distortions near the poles. The spacing of latitude lines varies between different cylindrical projections, leading to different conservation properties, and different distortion near the poles. In Figure 4-104, we show an example of the equidistant cylindrical projection, which chooses a latitude scaling that preserves distances along meridians. Other cylindrical projections are the Mercator ( ) and the cylindrical equal-area ( ) projections. The additional arguments to Basemap for this view specify the latitude ( ) and longitude ( ) of the lower-left corner ( ) and upper-right corner ( ) for the desired map, in units of degrees. Pseudo-cylindrical projections relax the requirement that meridians (lines of constant longitude) remain vertical; this can give better properties near the poles of the projection. The Mollweide projection ( ) is one common example of this, in which all meridians are elliptical arcs (Figure 4-105). It is constructed so as to preserve area across the map: though there are distortions near the poles, the area of small patches reflects the true area. Other pseudo-cylindrical projections are the sinusoidal ( ) and Robinson ( ) projections. The extra arguments to here refer to the central latitude ( ) and longitude ( ) for the desired map. Perspective projections are constructed using a particular choice of perspective point, similar to if you photographed the Earth from a particular point in space (a point which, for some projections, technically lies within the Earth!). One common example is the orthographic projection ( ), which shows one side of the globe as seen from a viewer at a very long distance. Thus, it can show only half the globe at a time. Other perspective-based projections include the gnomonic projection ( ) and stereographic projection ( ). These are often the most useful for showing small portions of the map. Here is an example of the orthographic projection (Figure 4-106): A conic projection projects the map onto a single cone, which is then unrolled. This can lead to very good local properties, but regions far from the focus point of the cone may become very distorted. One example of this is the Lambert conformal conic projection ( ), which we saw earlier in the map of North America. It projects the map onto a cone arranged in such a way that two standard parallels (specified in by and ) have well-represented distances, with scale decreasing between them and increasing outside of them. Other useful conic projections are the equidistant conic ( ) and the Albers equal-area ( ) projection (Figure 4-107). Conic projections, like perspective projections, tend to be good choices for representing small to medium patches of the globe. If you‚Äôre going to do much with map-based visualizations, I encourage you to read up on other available projections, along with their properties, advantages, and disadvantages. Most likely, they are available in the Basemap package. If you dig deep enough into this topic, you‚Äôll find an incredible subculture of geo-viz geeks who will be ready to argue fervently in support of their favorite projection for any given application! Earlier we saw the and methods for projecting global images on the map, as well as the and methods for drawing lines of constant latitude and longitude. The Basemap package contains a range of useful functions for drawing borders of physical features like continents, oceans, lakes, and rivers, as well as political boundaries such as countries and US states and counties. The following are some of the available drawing functions that you may wish to explore using IPython‚Äôs help features:\n‚Ä¢ Draw a mask between the land and sea, for use with projecting images on one or the other Draw the map boundary, including the fill color for oceans Fill the continents with a given color; optionally fill lakes with another color\n‚Ä¢ Draw an etopo relief image onto the map For the boundary-based features, you must set the desired resolution when creating a Basemap image. The argument of the class sets the level of detail in boundaries, either (crude), (low), (intermediate), (high), (full), or if no boundaries will be used. This choice is important: setting high-resolution boundaries on a global map, for example, can be very slow. Here‚Äôs an example of drawing land/sea boundaries, and the effect of the resolution parameter. We‚Äôll create both a low- and high-resolution map of Scotland‚Äôs beautiful Isle of Skye. It‚Äôs located at 57.3¬∞N, 6.2¬∞W, and a map of 90,000√ó120,000 kilometers shows it well (Figure 4-108): Notice that the low-resolution coastlines are not suitable for this level of zoom, while high-resolution works just fine. The low level would work just fine for a global view, however, and would be much faster than loading the high-resolution border data for the entire globe! It might require some experimentation to find the correct resolution parameter for a given view; the best route is to start with a fast, low-resolution plot and increase the resolution as needed. Perhaps the most useful piece of the Basemap toolkit is the ability to over-plot a variety of data onto a map background. For simple plotting and text, any function works on the map; you can use the instance to project latitude and longitude coordinates to coordinates for plotting with , as we saw earlier in the Seattle example. In addition to this, there are many map-specific functions available as methods of the instance. These work very similarly to their standard Matplotlib counterparts, but have an additional Boolean argument , which if set to allows you to pass raw latitudes and longitudes to the method, rather than projected coordinates. Some of these map-specific methods are: We‚Äôll see examples of a few of these as we continue. For more information on these functions, including several example plots, see the online Basemap documentation. Recall that in ‚ÄúCustomizing Plot Legends‚Äù, we demonstrated the use of size and color in a scatter plot to convey information about the location, size, and population of California cities. Here, we‚Äôll create this plot again, but using Basemap to put the data in context. We start with loading the data, as we did before: # Extract the data we're interested in Next, we set up the map projection, scatter the data, and then create a colorbar and legend (Figure 4-109): This shows us roughly where larger populations of people have settled in California: they are clustered near the coast in the Los Angeles and San Francisco areas, stretched along the highways in the flat central valley, and avoiding almost completely the mountainous regions along the borders of the state. As an example of visualizing some more continuous geographic data, let‚Äôs consider the ‚Äúpolar vortex‚Äù that hit the eastern half of the United States in January 2014. A great source for any sort of climatic data is NASA‚Äôs Goddard Institute for Space Studies. Here we‚Äôll use the GIS 250 temperature data, which we can download using shell commands (these commands may have to be modified on Windows machines). The data used here was downloaded on 6/12/2016, and the file size is approximately 9 MB: The data comes in NetCDF format, which can be read in Python by the library. You can install this library as shown here: We read the data as follows: The file contains many global temperature readings on a variety of dates; we need to select the index of the date we‚Äôre interested in‚Äîin this case, January 15, 2014: Now we can load the latitude and longitude data, as well as the temperature anomaly for this index: Finally, we‚Äôll use the method to draw a color mesh of the data. We‚Äôll look at North America, and use a shaded relief map in the background. Note that for this data we specifically chose a divergent colormap, which has a neutral color at zero and two contrasting colors at negative and positive values (Figure 4-110). We‚Äôll also lightly draw the coastlines over the colors for reference: The data paints a picture of the localized, extreme temperature anomalies that happened during that month. The eastern half of the United States was much colder than normal, while the western half and Alaska were much warmer. Regions with no recorded temperature show the map background.\n\nMatplotlib has proven to be an incredibly useful and popular visualization tool, but even avid users will admit it often leaves much to be desired. There are several valid complaints about Matplotlib that often come up:\n‚Ä¢ Prior to version 2.0, Matplotlib‚Äôs defaults are not exactly the best choices. It was based off of MATLAB circa 1999, and this often shows.\n‚Ä¢ Matplotlib‚Äôs API is relatively low level. Doing sophisticated statistical visualization is possible, but often requires a lot of boilerplate code.\n‚Ä¢ Matplotlib predated Pandas by more than a decade, and thus is not designed for use with Pandas s. In order to visualize data from a Pandas , you must extract each and often concatenate them together into the right format. It would be nicer to have a plotting library that can intelligently use the labels in a plot. An answer to these problems is Seaborn. Seaborn provides an API on top of Matplotlib that offers sane choices for plot style and color defaults, defines simple high-level functions for common statistical plot types, and integrates with the functionality provided by Pandas s. To be fair, the Matplotlib team is addressing this: it has recently added the tools (discussed in ‚ÄúCustomizing Matplotlib: Configurations and Stylesheets‚Äù), and is starting to handle Pandas data more seamlessly. The 2.0 release of the library will include a new default stylesheet that will improve on the current status quo. But for all the reasons just discussed, Seaborn remains an extremely useful add-on. Here is an example of a simple random-walk plot in Matplotlib, using its classic plot formatting and colors. We start with the typical imports: Now we create some random walk data: Although the result contains all the information we‚Äôd like it to convey, it does so in a way that is not all that aesthetically pleasing, and even looks a bit old-fashioned in the context of 21st-century data visualization. Now let‚Äôs take a look at how it works with Seaborn. As we will see, Seaborn has many of its own high-level plotting routines, but it can also overwrite Matplotlib‚Äôs default parameters and in turn get even simple Matplotlib scripts to produce vastly superior output. We can set the style by calling Seaborn‚Äôs method. By convention, Seaborn is imported as : Now let‚Äôs rerun the same two lines as before (Figure 4-112): # same plotting code as above! The main idea of Seaborn is that it provides high-level commands to create a variety of plot types useful for statistical data exploration, and even some statistical model fitting. Let‚Äôs take a look at a few of the datasets and plot types available in Seaborn. Note that all of the following could be done using raw Matplotlib commands (this is, in fact, what Seaborn does under the hood), but the Seaborn API is much more convenient. Often in statistical data visualization, all you want is to plot histograms and joint distributions of variables. We have seen that this is relatively straightforward in Matplotlib (Figure 4-113): Rather than a histogram, we can get a smooth estimate of the distribution using a kernel density estimation, which Seaborn does with (Figure 4-114): Histograms and KDE can be combined using (Figure 4-115): If we pass the full two-dimensional dataset to , we will get a two-dimensional visualization of the data (Figure 4-116): We can see the joint distribution and the marginal distributions together using . For this plot, we‚Äôll set the style to a white background (Figure 4-117): There are other parameters that can be passed to ‚Äîfor example, we can use a hexagonally based histogram instead (Figure 4-118): When you generalize joint plots to datasets of larger dimensions, you end up with pair plots. This is very useful for exploring correlations between multidimensional data, when you‚Äôd like to plot all pairs of values against each other. We‚Äôll demo this with the well-known Iris dataset, which lists measurements of petals and sepals of three iris species: Visualizing the multidimensional relationships among the samples is as easy as calling (Figure 4-119): A pair plot showing the relationships between four variables Sometimes the best way to view data is via histograms of subsets. Seaborn‚Äôs makes this extremely simple. We‚Äôll take a look at some data that shows the amount that restaurant staff receive in tips based on various indicator data (Figure 4-120): Out[14]: total_bill tip sex smoker day time size 0 16.99 1.01 Female No Sun Dinner 2 1 10.34 1.66 Male No Sun Dinner 3 2 21.01 3.50 Male No Sun Dinner 3 3 23.68 3.31 Male No Sun Dinner 2 4 24.59 3.61 Female No Sun Dinner 4 An example of a faceted histogram Factor plots can be useful for this kind of visualization as well. This allows you to view the distribution of a parameter within bins defined by any other parameter (Figure 4-121): An example of a factor plot, comparing distributions given various discrete factors Similar to the pair plot we saw earlier, we can use to show the joint distribution between different datasets, along with the associated marginal distributions (Figure 4-122): The joint plot can even do some automatic kernel density estimation and regression (Figure 4-123): Time series can be plotted with . In the following example (visualized in Figure 4-124), we‚Äôll use the Planets data that we first saw in ‚ÄúAggregation and Grouping‚Äù: We can learn more by looking at the method of discovery of each of these planets, as illustrated in Figure 4-125: Number of planets discovered by year and type (see the online appendix for a full-scale figure) For more information on plotting with Seaborn, see the Seaborn documentation, a tutorial, and the Seaborn gallery. Here we‚Äôll look at using Seaborn to help visualize and understand finishing results from a marathon. I‚Äôve scraped the data from sources on the Web, aggregated it and removed any identifying information, and put it on GitHub where it can be downloaded (if you are interested in using Python for web scraping, I would recommend Web Scraping with Python by Ryan Mitchell). We will start by downloading the data from the Web, and loading it into Pandas: By default, Pandas loaded the time columns as Python strings (type ); we can see this by looking at the attribute of the : Let‚Äôs fix this by providing a converter for the times: That looks much better. For the purpose of our Seaborn plotting utilities, let‚Äôs next add columns that give the times in seconds: To get an idea of what the data looks like, we can plot a over the data (Figure 4-126): The relationship between the split for the first half-marathon and the finishing time for the full marathon The dotted line shows where someone‚Äôs time would lie if they ran the marathon at a perfectly steady pace. The fact that the distribution lies above this indicates (as you might expect) that most people slow down over the course of the marathon. If you have run competitively, you‚Äôll know that those who do the opposite‚Äîrun faster during the second half of the race‚Äîare said to have ‚Äúnegative-split‚Äù the race. Let‚Äôs create another column in the data, the split fraction, which measures the degree to which each runner negative-splits or positive-splits the race: Where this split difference is less than zero, the person negative-split the race by that fraction. Let‚Äôs do a distribution plot of this split fraction (Figure 4-127): The distribution of split fractions; 0.0 indicates a runner who completed the first and second halves in identical times Out of nearly 40,000 participants, there were only 250 people who negative-split their marathon. Let‚Äôs see whether there is any correlation between this split fraction and other variables. We‚Äôll do this using a , which draws plots of all these correlations (Figure 4-128): The relationship between quantities within the marathon dataset It looks like the split fraction does not correlate particularly with age, but does correlate with the final time: faster runners tend to have closer to even splits on their marathon time. (We see here that Seaborn is no panacea for Matplotlib‚Äôs ills when it comes to plot styles: in particular, the x-axis labels overlap. Because the output is a simple Matplotlib plot, however, the methods in ‚ÄúCustomizing Ticks‚Äù can be used to adjust such things if desired.) The difference between men and women here is interesting. Let‚Äôs look at the histogram of split fractions for these two groups (Figure 4-129): The distribution of split fractions by gender The interesting thing here is that there are many more men than women who are running close to an even split! This almost looks like some kind of bimodal distribution among the men and women. Let‚Äôs see if we can suss out what‚Äôs going on by looking at the distributions as a function of age. A nice way to compare distributions is to use a violin plot (Figure 4-130): This is yet another way to compare the distributions between men and women. Let‚Äôs look a little deeper, and compare these violin plots as a function of age. We‚Äôll start by creating a new column in the array that specifies the decade of age that each person is in (Figure 4-131): A violin plot showing the split fraction by gender and age Looking at this, we can see where the distributions of men and women differ: the split distributions of men in their 20s to 50s show a pronounced over-density toward lower splits when compared to women of the same age (or of any age, for that matter). Also surprisingly, the 80-year-old women seem to outperform everyone in terms of their split time. This is probably due to the fact that we‚Äôre estimating the distribution from small numbers, as there are only a handful of runners in that range: Back to the men with negative splits: who are these runners? Does this split fraction correlate with finishing quickly? We can plot this very easily. We‚Äôll use , which will automatically fit a linear regression to the data (Figure 4-132): Apparently the people with fast splits are the elite runners who are finishing within ~15,000 seconds, or about 4 hours. People slower than that are much less likely to have a fast second split."
    },
    {
        "link": "https://stackoverflow.com/questions/65539013/how-to-plot-a-graph-of-actual-vs-predict-values-in",
        "document": "The problem you seem to have is that you mix and into one \"plot\" (meaning here the function)\n\nUsing or function (which you also mixed up), the first parameter are the coordinates on the x-axis and the second parameter are the coordinates on the y-axis.\n\nSo 1.) you need to one with only and then one with only . To do this you 2.) need either to have 2D data, or as it seems to be in your case, just use indexes for the x-axis by using the functionality.\n\nHere is some code with random data, that might get you started:\n\nThis will give you something like this:"
    }
]