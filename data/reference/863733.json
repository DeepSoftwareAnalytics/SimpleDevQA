[
    {
        "link": "https://community.openai.com/t/maximum-token-length-allowed/137151",
        "document": ""
    },
    {
        "link": "https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them",
        "document": ""
    },
    {
        "link": "https://platform.openai.com/docs/guides/rate-limits",
        "document": ""
    },
    {
        "link": "https://community.openai.com/t/what-is-the-maximum-response-length-output-tokens-for-each-gpt-model/524066",
        "document": ""
    },
    {
        "link": "https://tolulade-ademisoye.medium.com/understanding-openai-api-token-request-limits-0c6f39cc58f2",
        "document": "Are you embarking on your journey to create your first large language model-based application? Youâ€™re welcome :) ! This post will guide you through the basics and setup needed to tap into the OpenAI model server.\n\nOnce you create your Openai platform account to make use of their API navigate to the settings>>limits section â€” youâ€™ll likely encounter something like this.\n\nOpenai currently has 6 tiers of customer categories API access. New accounts default to the free tier. Keep a keen eye on the usage tier as it significantly impacts and limits your API consumption during development.\n\nEach tier has its designated rate limit, a crucial consideration in your product development journey.\n\nYour choice of model for the project plays a pivotal role in determining the rate limit and development timeline, especially if youâ€™re crafting a proof of concept.\n\nIn this medium post, I shared some strategies to help you navigate the model selection process.\n\nTokens in natural language processing represent the breakdown of text into words or letters. To illustrate, take a sentence like;\n\nYou can break this into tokens like; words or letters;\n\nWith this understanding, your application design should chunk documents into layers of words, all while keeping an eye on the token volume. Why? Depending on your user tier on OpenAI, there are token limits.\n\nImagine deploying your app and constantly hitting token limit errors â€” no one wants that!\n\nThe token limit refers to the amount or volume of words (broken down for analysis) your application intends to send to the OpenAI server.\n\nConversely, the request limit denotes the number of calls your application can make to the API server. Simple, right? However, these request limits could make or break your applicationâ€™s success during development or production. Request limit, in other words, is the maximum number of API calls per a given time your application can make.\n\nFirst things first, decide on the user tier for your project. Each tier has caps on token limits (tpm- token per minute) and request limits (rpm/rpd â€” request per day or minute)\n\nNext up, decide on the model type. Take a glance at the image above; notice how each model has different tpm and rpm/rpd?\n\nChoose one that aligns with your projectâ€™s token request needs.\n\nYes, you should design and develop your language model application to send several requests per minute (for example, 30 requests per minute), with built-in waits or retries when it hits the limit. Always experiment.\n\nAlternatively, you can design your application to gracefully handle set token limits â€” returning a statement or retrying when needed.\n\nFriendly Reminder: Always check your bill settings. Itâ€™s easy to run out of OpenAI credits due to requests and token limits, leaving you confused about failing API calls.\n\nThanks a lot for reading. If youâ€™re on the lookout for a Machine Learning Engineer consultant or AI Engineer consultant, donâ€™t hesitate to reach out. Book a session with me â€” â€” letâ€™s dive into the exciting world of AI together.\n\nYou may also buy me a coffee to support my work."
    },
    {
        "link": "https://bretcameron.com/blog/three-strategies-to-overcome-open-ai-token-limits",
        "document": "Right now, itâ€™s possible to harness the power of LLMs (large language models) through several public APIs â€” giving us access to the likes of ChatGPT, Claude and Gemini for use in our own applications.\n\nAt a small scale, using these APIs is straightforward. But if we want to support longer conversations or larger amounts of context, sooner or later we will bump into token limits and weâ€™ll need to implement our own workarounds.\n\nIn this article, Iâ€™ll walk you through three strategies for overcoming these token limits, so that you can support much longer conversations or much more context â€” or both!\n\nWeâ€™ll be using OpenAIâ€™s API in Node.js, but the strategies weâ€™ll learn can be adapted to pretty-much every mainstream LLM in any server-side programming language.\n\nWhether youâ€™re writing a chatbot or using AI tools for something else, under-the-hood most LLMs use a conversational format. Understanding how to make the most of this format is key to making more capable and efficient applications. So, how can we approach the problem of scaling our conversations?\n\nThis article is not focused on training or fine-tuning models. While fine-tuning can be a fantastic strategy for handling a very large volume of text, itâ€™s also slow: depending on the amount of training data, it can take several hours to fine-tune a model. In many contexts, this is simply too slow.\n\nInstead, weâ€™ll focus on techniques that will significantly increase the length of conversations, but your users will only be waiting for somewhere between a few minutes and a few seconds.\n\nThese approaches are sometimes referred to as a â€œmemory systemâ€ or â€œlong-term memoryâ€; typically, they involve altering the way a conversation is stored.\n\nIn this article, weâ€™ll focus on three approaches to dealing with the issue of scale:\nâ€¢ Strategy 1: Rolling Context. Send a rolling window of X many past messages, and discard earlier messages.\nâ€¢ Strategy 2: Summarised Message History. Use AI to summarise messages, then add the summaries to the message history rather than using full messages.\nâ€¢ Strategy 3: Categorised Excerpts. Divide messages into excerpts based on topic, then provide only the data of topics relevant to the question being asked.\n\nThereâ€™s an alternative approach to this problem offered by OpenAI: fine-tuning. This is an excellent solution for providing a large amount of context to an AI model. With this, you can provide a huge amount of conversation data in a JSONL file (where every line is a separate JSON object).\n\nBut thereâ€™s one main caveat. The fine-tuning process can take a long time â€” in my experience, at least a few minutes for small training samples and sometimes, for larger amounts of training data, several hours.\n\nDepending on your needs, this may be absolutely fine or it may be a significant problem. For example, if your users want to do something right now, asking them to come back in a few hours might mean a lot of people never return.\n\nThatâ€™s where the three strategies come in. They may not offer the same scale as fine-tuning, but they allow us to get more out of the model limits while adding little or no delay to the responses. Here is a summary of the different approaches:\n\nBefore we start coding, letâ€™s take a step back to understand what tokens are and why token limitations exist.\n\nFirst, we need to understand what tokens are. Most LLMs convert plaintext into a series of numerical tokens, each representing:\n\nWhen using natural language, you can assume that the token count will always be larger than the word count. In the example below, four words becomes six tokens:\n\nYou can see this process in-action by trying OpenAIâ€™s free tokenizer: https://platform.openai.com/tokenizer.\n\nThis encoding (or â€œtokenizationâ€) is done for a number of reasons. Primarily, neural networks (and computers, in general) can understand and interpret numbers much better than text; using numbers allows neural networks to perform operations like matrix multiplications more efficiently.\n\nDifferent LLMs use different tokenization strategies, so when weâ€™re calculating token counts, we need to make sure weâ€™re using the same encoding system as the model weâ€™re using. For OpenAI models in Node.js, weâ€™ll use the package https://www.npmjs.com/package/js-tiktoken.\n\nWeâ€™ll be using OpenAIâ€™s model, which has a token limit per-conversation of 8,192 tokens. This limit includes the entire message history, as well as the amount of tokens available for use in the AIâ€™s reply.\n\nWhy do conversations have token limits?\n\nHitting the token limit can feel frustrating, but there are good reasons for OpenAI and its competitors to enforce these limits.\n\nLarger conversations require more memory and computational power, as the model needs to maintain and refer back to the context throughout the interaction. Without a limit, OpenAI is susceptible to Denial-of-Service (DoS) attacks, which could make their API less reliable and cost them a lot of money.\n\nThis constraint is good for us API users too: since every request must send the whole conversation as context, if we let conversations become arbitrarily large, we could also end up paying a lot more than we anticipated. Working around this constraint forces us to make our applications more efficient and cost-effective â€” which is no bad thing!\n\nIf youâ€™d like to see the final version of all the examples in this article, check out my example repository.\n\nTo follow along, make sure you have Node.js installed on your machine. Then create a new directory, go into it and start a new Node.js project with an file:\n\nWeâ€™ll also need to install a few dependencies:\nâ€¢ â€” a library that makes it easier to interact with the OpenAI API.\nâ€¢ â€” a pure JavaScript port of tiktoken, the original BPE tokenizer used by OpenAIâ€™s models.\n\nTo keep this tutorial focused, we wonâ€™t worry about creating a frontend application, adding API routes, or storing our data in a database. For now, weâ€™ll store everything in application memory.\n\nI will be using TypeScript, as I believe this has become the standard for professional JavaScript applications. So letâ€™s add and (for executing TypeScript) as development dependencies.\n\nOnce those are installed, open up your new project in your favourite editor, and letâ€™s make a few edits to our file.\n\nFinally, inside , letâ€™s create a simple function that we can use as the entry-point to our app.\n\nIf we run , we should see our log. Weâ€™re almost ready to get coding, but first, letâ€™s create an API key inside OpenAI.\n\nHead over to OpenAIâ€™s API Dashboard and create an account. You will need to and add a payment method, but you can set a very low monthly usage cap in the Limits page. Following this tutorial should cost well under $1.\n\nYou could create a new project, but Iâ€™ll use the â€œDefault projectâ€. Click into your project and click â€œAPI keysâ€. Then click â€œCreate new secret keyâ€.\n\nThis will open a modal. Youâ€™ll want a key for a â€œservice accountâ€. Give it a name and assign it to your project.\n\nWith , we can create a new file in the root directory and store our environment variables there. Add the secret key you just created to your local environment variables as .\n\nYouâ€™ll also need to know your Organization ID, which can be found in the â€œGeneralâ€ section for the project. Save this to an environment variable called .\n\nYour file should end up looking like this.\n\nMake sure not to commit these to version control. Add a file, like this.\n\nThen, to ensure these variables are loaded into , import at the top of and run its function.\n\nTo get our first reply, known as a completion, we simply need to create a new client with our credentials and then send a message with the role .\n\nIf everythingâ€™s working, running this with should give you a reply. I got:\n\nThis isnâ€™t very easy to test, so letâ€™s add the ability to continue the conversation via the terminal using Nodeâ€™s built-in library. Weâ€™ll import the version, so we can use async/await.\n\nIf the user types a message, weâ€™ll send a reply from the AI. The conversation will continue indefinitely, unless the user types â€œexitâ€, â€œquitâ€ or â€œcloseâ€, in which case weâ€™ll break from the loop.\n\nThis is workable, but when the AI gives a longer response, weâ€™ll be waiting around with no sign that a message is loading. Instead, we can stream in the response using the option and write the response to the terminal as it comes in. To achieve that, we can change the loop to this:\n\nThis feels much more responsive and will make it easier to test our code.\n\nBecause weâ€™re adding both our messages and the AIâ€™s responses to our array, the AI can respond to questions about previous parts of the conversation. So we can have fulfilling conversations like this:\n\nNow that we have the ability to send and receive messages with ease, we can move onto using strategies that will allow us to get around the tokenization limits of our chosen model.\n\nThe simplest method to avoid hitting token limits is to only return the most recent messages each run.\n\nTo start with, we can choose a number of messages that we will allow in our â€œcontext windowâ€ and simply remove items from the start of the array whenever that limit is exceeded.\n\nAt the bottom of our loop, we can add another loop that checks if we go above a certain number of messages â€” letâ€™s say 10 â€” and the uses the method until weâ€™re within the limit.\n\nIf we have an initial system message with important context that we want to protect, we could instead use to ensure the message at index 0 is not removed.\n\nOr, what if we want to maximize the available tokens? To do that, weâ€™ll need to use to encode our messages. One approach to this would be to keep a second array of .\n\nWeâ€™ll also create a new encoding to allow us to calculate the number of tokens being used and weâ€™ll create a simple function to sum the count of the entire array.\n\nWe need to be careful to add and remove items to both and at the same time.\n\nThen, we can adapt our nested loop to remove items from both arrays if the token limit is exceeded. In practice, we could choose a number close to the actual token limit of the model (leaving some headroom for the reply).\n\nFor example, since weâ€™ve specified the reply should have a value of 300 and we know GPT-4 has a conversation token limit of 8,192 tokens, we could use tokens as a reasonable limit.\n\nBut to make it easier to test, for now Iâ€™ll use something much smaller:\n\nOverall, our full file might look something like this:\n\nThis system is relatively easy to implement and it would ensure that the conversation token limit is never exceeded. However, depending on the use-case, it may be problematic that weâ€™re losing earlier messages. What if, for example, we have a lot of context in the early messages that we want to keep?\n\nOne way to support more scale than our first approach is to use AI to succinctly summarise the messages and to store those summaries in the instead of the full messages, saving us precious tokens.\n\nCombined with the first strategy, we could go significantly further before needing to remove messages from the .\n\nThe tradeoff here is that it will take time for the AI to come up with the summaries, but we can minimize the impact of this by asking for summaries in the background, rather than waiting for them to be created.\n\nOur simple and arrays are no longer fit for purpose, because â€” if we want to summarise content in the background and we only know the index â€” we could get into trouble if items are added or removed before the summaries are finished. To account for this, we can delete and turn into something more versatile.\n\nFirst, weâ€™ll create a which includes the and also an , which will give us confidence that the summaries will not be added to the wrong index!\n\nLetâ€™s not mess around with our IDs. Iâ€™ll install a dedicated library to ensure theyâ€™re watertight:\n\nThen, in our code, we can add our new type:\n\nWhen we add items to we can include these new fields:\n\nWeâ€™ll also need to filter out our bespoke fields when sending the messages to OpenAI:\n\nNext, letâ€™s create a function that will summarise our message. As the is in global scope, our function can update it as a side-effect. Weâ€™ll also set to be the number of tokens of the original message or 100 â€” whichever is higher â€” since the AI will provide summaries of simple messages like â€œhiâ€. We need to receive these longer summaries so that we can discount them.\n\nLetâ€™s also add a log so we can see the summaries when they come in.\n\nWe can then call this function without awaiting it, so that it runs in the background and doesnâ€™t block other code from running.\n\nLetâ€™s see an example of how this code performs. When I asked the AI â€œhow are youâ€, the summary that I got had a higher token count than the original message, so we could safely ignore that summary.\n\nHowever, the AIâ€™s reply was longer and could be summarised.\n\nIn the background, I received the following summary.\n\nThis summary does a good job of retaining the key information and cuts the down from 28 in the original message to 19 in the summary â€” about 68% of the original size.\n\nWith an estimated saving of 68%, we could replace roughly 12,000 tokens of original text with summaries that fit within the 8,000 token limit of GPT-4. And this saving is likely to be much more significant for larger messages, so depending on how the AI is being used, the potential savings could be much more significant.\n\nIn addition, because the summaries happen in the background, the end user is completely unaware of the additional calculations that are taking place.\n\nHereâ€™s a complete file that uses this approach:\n\nOf course, there are risks with this strategy: mainly that important information could be lost. Depending on your particular application, it may be possible to adapt the prompt to help the AI distinguish between useful and useless information.\n\nItâ€™s also upping our usage of the AI when a message comes in, so it will only start saving us money if we do need to support longer conversations, while costing us more for shorter conversations.\n\nBut â€” if our goal is to maximize the useful context we can fit into a single conversation â€”we could go further still: these summaries likely still contain information that is not useful for the question being asked and therefore they are still taking up valuable tokens. In the next section, we will go a step further and reduce the number of tokens per conversation even more.\n\nIn our final approach to this problem, we will break down our message into excerpts categorised by topic. For certain use-cases, this could be an extremely powerful approach because, when a user asks a question, only excerpts relevant to the question will be inserted into the conversation history. Filling our message history with only relevant data could be a much more efficient way to get the most out of every token.\n\nTo do this, as in the strategy above, weâ€™ll need to get additional responses from the AI whenever a message is created. This will increase the costs in shorter conversations, so the value of this approach depends on the use-case: but for longer conversations where maximizing the amount of context is critical, this approach allows us to be much more efficient.\n\nBack to the code. Weâ€™ll store our topics inside a new object:\n\nTo help test this, Iâ€™ll take a paragraph from the Guardianâ€™s review of Dune 2. Weâ€™ll use the following prompt, which will return the topics in a JSON format.\n\nTo help the AI come up with consistent topic names, Iâ€™m passing in the keys of the object. Otherwise, we might get very similar but different topic categories, which will be harder to match-up.\n\nNext, letâ€™s write a new function called , which will split each message out into its topics.\n\nThereâ€™s quite a lot to unpack here. First, because the AI will be adding topic names and characters required for a JSON object, the response will often be larger than the original message.\n\nTo ensure we donâ€™t get any problems, I have set the of the response to be twice the token count of the original message â€” well over what it is likely to be in reality. This means that our original message plus the maximum possible reply must, when added together, be below the modelâ€™s overall conversation limit. In other words, the original message must be under a third of the overall token limit. If it isnâ€™t, weâ€™ll throw an error.\n\nThen, we need to parse the response string into JSON. In my tests, I had no problems with the AI returning JSON as expected, but because we cannot guarantee the response string can be parsed, so we need to handle that case with a try/catch. In my example, I chose to re-try the function if it errored up to three times before giving up, by adding arguments for the and .\n\nI mainly tested a single paragraph, but as an example, in one of my tests I got an JSON containing the following four topics:\n\nIâ€™ll share a few of the excerpts related to â€˜Other Charactersâ€™ and â€˜Performance Reviewâ€™.\n\nNow, if our user asked a question about supporting characters or the actorsâ€™ performances, we could supply the excerpts above as evidence. To do that, weâ€™ll need another prompt, supplying our categories and asking the AI which are appropriate. Weâ€™ll need to get this response when a user sends a message, so it will add a bit mean the response they get will be a little delayed; that is one of the trade-offs of this approach.\n\nHereâ€™s a prompt that will ask for the relevant topics, provided as an array of strings.\n\nNext, weâ€™ll create a function to send this prompt and return the parsed response an array of strings.\n\nIn my test, I asked this question and passed it as an argument to :\n\nThe AI correctly inferred that was the most appropriate of the given topics.\n\nNow we need to ask the question to the AI, providing the excerpts inside as context. We can get a list of like below.\n\nThis will work when we already have relevant data in our object, but it will not handle new conversations very well. For that, we can include the last 10 messages of the conversation unaltered, using .\n\nIncluding the recent messages in full will also allow us to run the anatomisation function in the background. So we can replace our uses of with , and that will gradually build up our object as the conversation continues.\n\nThis is far from perfect, and there are plenty of edge-cases we could account for and optimisations we could make. But here is a full file containing a working example of this strategy:\n\nOne of the weaknesses of this approach is that we could end up with a bunch of generic topics that arenâ€™t useful. But if our AI tool is focused on a particular niche or use-case, we may well be able to specify the topics of interest in advance, and force the AI to organise excerpts into only those specific topics.\n\nFor example, an AI focused on film reviews could be given a list of topics to look out for (set design, costumes, audio, music, and so on) which could mitigate the risk of categories being created that weâ€™re not interested in.\n\nOverall, the value of any individual strategy depends a lot on your particular use-case. Itâ€™s also possible to combine elements of multiple strategy, for example by using the first strategy to trim the message history whenever we exceed a certain token count.\n\nYou can find a repository containing all the examples in this article below:\n\nWhile researching this article, I found this article by LangChain gave me some useful ideas. So if you want to go deeper, be sure to check them out!\n\nWhether you are new to the world of programming with LLMs or youâ€™re a seasoned veteran, I hope this article has given you some inspiration for how to handle conversation memory in a way that goes beyond the token limits set for a particular model."
    },
    {
        "link": "https://help.openai.com/en/articles/6891753-what-are-the-best-practices-for-managing-my-rate-limits-in-the-api",
        "document": ""
    },
    {
        "link": "https://prompthub.us/blog/tokens-and-tokenization-understanding-cost-speed-and-limits-with-openais-apis",
        "document": "If youâ€™re building with AI, chances are youâ€™ve run into issues and questions around tokens. They are at the core of all Large Language Models (LLMs), and are the building blocks of all prompts and responses. \n\n\n\nUnderstanding tokenizationâ€”what it is, its significance, and its impact on API usageâ€”is crucial for anyone integrating AI into their product, or even using apps like ChatGPT. \n\n\n\nLetâ€™s define some terms, and then run through some real-world examples to better understand how to optimize prompts and interactions with OpenAIâ€™s APIs.\n\nA token is the basic unit of information that an LLM reads to understand and generate text. A token can be an individual character, a full word, or even part of a word, as you can see in the example below.\n\nWhy do models use tokens?\n\nWhen you write a prompt, it is broken down into tokens immediately. This is because LLMs donâ€™t interpret language in the same way humans do. They are trained on a large amount of text data, which they use to learn patterns. Tokenization turns English into a format the model can work with.\n\nHow tokens are split depends on the language and specific tokenization process used.\n\nWhat actually is tokenization?\n\nTokenization is the process of breaking down text into tokens. Like how \"Evergreen\" was broken down into two tokens. In general, there are three major tokenization processes.\n\n1. Word-based - each word is a token. Evergreen = 1 token.\n\n-Advantages: You can include every word in a language. \n\n-Downsides: The model's large vocabulary increases memory usage and time complexity.\n\n2. Character-based - Each character is a token. Evergreen = 9 tokens.\n\n-Advantages: Fewer total tokens needed, leading to simplified memory and time complexity\n\n-Disadvantages: Difficulty in learning meaningful context. â€œtâ€ gives less context than â€œtreeâ€.\n\n3. Subword-based - Common subwords are treated as separate tokens. Evergreen = 2 tokens\n\n-Advantages: Covers a large vocabulary with less tokens, by flexibly handling word forms \n\n-Disadvantages: Increased memory and time complexity compared to character-based\n\nOpenAI uses a type of subword-based tokenization called Byte Pair Enconding. This tokenization method is one of the main reasons that these models have gotten so good at understanding, and can generate nuanced responses.\n\nTokens are one of the main driving factors related to cost, response time, and performance when using OpenAIâ€™s APIs (and most other providers). Understanding how they relate can help you write better prompts, cut costs, and improve user experience.\n\nOpenAI charges different rates for different models, and updates these figures often. In general, longer prompts require more tokens and cost more.\n\nAlong with cost, each model has a certain token limit. This limit relates to the tokens used in the prompt AND the response. In practice, tokens from the system message tokens are included as well, though OpenAI hasn't explcitly documented this (yet).\n\nIf you are using a model that has a max tokens limit of 4,000 and your prompt has 3500 tokens, then you only have 500 tokens left for your response. If the model generates a completion of more then 500 tokens, then the request will fail and no response will be returned.\n\nAlthough instructing the model in your prompt not to return an output over 500 characters may get you a response, you'll reduce the quality of the response. It goes against one of the main principles of prompt engineering, give the model room to think. If youâ€™re interested in some of the othe prompt engineering best practices, you can check out our recent article here: 10 Tips for Writing Better Prompts with Any Model.\n\n\n\nThe more tokens used in a request will slow down the API response time. Keep in mind in the frame of your application. If having speedy responses is more important than detailed responses you might want to adjust the token limit. \n\n\n\nExperimenting with different models and token limits takes just a few clicks in PromptHub, allowing for side-by-side comparisons that highlight the tangible differences when running prompts on a model with a lower token limit.\n\nWhile the token limit for a model includes the system message, prompt and response, the Max Tokens parameter only relates to the tokens used in the response. \n\nSo if you need to constrain your response, you can test using this parameter.\n\n\n\nFor more info on how Max Tokens works, we did a deep dive on all things related to OpenAI parameters in our article Understanding OpenAI Parameters: Optimize your Prompts for Better Outputs.\n\n\n\nHow to get around token limits\n\nThis will be an article of itâ€™s on in the future. For now here's a high level overview of the options.\n\nChunking - Divide your text input into smaller pieces that fit within the token limit, and send them as separate requests. Have the model generate responses for each chunk and combine the results. You may lose some contextual coherence depending on your chunking boundries.\n\nSummarization - Condense the entire content before sending it to the model. Rather than sending each chapter of a book, you summarize each chapter into small peices. Unfortunately, this method may cause some loss in detail and nuance.\n\nPrioritization - Make sure the most important part of your prompt fits within the token limit. This takes time and requires you to make some tough decisions in regard to what to keep.\n\nHow to make sure you are optimizing usage of OpenAIâ€™s APIs\n\nTo optimize the use of OpenAIâ€™s APIs in regard to tokens, you need consider a few things.\n\nMonitor Token Usage: Keep a tab on the token count for your requests (prompts and responses). PromptHub allows you to view the token count for each request and even test it against different models and versions of your prompts.\n\nBe Mindful of Prompt Length: While longer prompts tend to outperform shorter prompts, it's still good to revisit your prompts to see if they can be condensed. Focused prompts can often produce similiar results as longer prompts, but at a fraction of the cost.\n\nBreak Text When Needed: Use chunking or any of the methods above when handling large text inputs that exceed the token limit threshold.\n\nExperiment time! Lets see how adjusting the token limit impacts the output of the same prompt-system message combination. \n\n\n\nVersion 1 will have a maximum token limit of 600, and Version 2 will have a limit of 1,500.\n\nâš™ï¸ System message: As a travel agent, your goal is to provide personalized recommendations for tropical destinations based on the customer's preferences. Feel free to ask clarifying questions and offer detailed suggestions. Remember to keep the conversation engaging and informative.\n\nðŸ’¬ User message (prompt): You are a travel agent assisting a customer with their vacation plans. The customer wants recommendations for a tropical destination. Write a conversation between the travel agent (you) and the customer discussing various options and providing suggestions.\n\nI ran this experiment in PromptHub, using our comparison tools to analyze the responses side-by-side. PromptHubâ€™s Github style versioning and difference-checking make A/B testing easy.\n\nVersion 1 (600 token limit) is on the left, and Version 2 (1,500 token limit) is on the right.\n\nVersion 1 is more concise, a product of the lower token limit. Version 2 goes into more detail, providing more information on destination specifics (outdoor activites, cultural experiences, budget etc).\n\nBoth versions have some trade-offs. Version 1 is succinct, uses tokens efficiently, and processes the request twice as fast. Version 2 gives way more details and provides a richer conversation.\n\nThis shows the importance of experimenting with token limits (along with other parameters and models) to figure out the optimal set up for your use case. Sometimes short and sweet is better, sometimes you need more detail.\nâ€¢ Optimizing token usage is crucial for cost-effective and efficient use of OpenAI's models. Tokens directly influence both the cost of API usage and response time.\nâ€¢ Managing token limits is key in order to avoid reaching the maximum token limit and hindering the model's ability to produce a response.\nâ€¢ Chunking requests, summarizing content, and writing effect prompts can help manage situations where token limits are exceeded.\nâ€¢ Experiment with different token limits to see the trade-offs between response length and depth"
    },
    {
        "link": "https://cookbook.openai.com/examples/how_to_handle_rate_limits",
        "document": ""
    },
    {
        "link": "https://tolulade-ademisoye.medium.com/understanding-openai-api-token-request-limits-0c6f39cc58f2",
        "document": "Are you embarking on your journey to create your first large language model-based application? Youâ€™re welcome :) ! This post will guide you through the basics and setup needed to tap into the OpenAI model server.\n\nOnce you create your Openai platform account to make use of their API navigate to the settings>>limits section â€” youâ€™ll likely encounter something like this.\n\nOpenai currently has 6 tiers of customer categories API access. New accounts default to the free tier. Keep a keen eye on the usage tier as it significantly impacts and limits your API consumption during development.\n\nEach tier has its designated rate limit, a crucial consideration in your product development journey.\n\nYour choice of model for the project plays a pivotal role in determining the rate limit and development timeline, especially if youâ€™re crafting a proof of concept.\n\nIn this medium post, I shared some strategies to help you navigate the model selection process.\n\nTokens in natural language processing represent the breakdown of text into words or letters. To illustrate, take a sentence like;\n\nYou can break this into tokens like; words or letters;\n\nWith this understanding, your application design should chunk documents into layers of words, all while keeping an eye on the token volume. Why? Depending on your user tier on OpenAI, there are token limits.\n\nImagine deploying your app and constantly hitting token limit errors â€” no one wants that!\n\nThe token limit refers to the amount or volume of words (broken down for analysis) your application intends to send to the OpenAI server.\n\nConversely, the request limit denotes the number of calls your application can make to the API server. Simple, right? However, these request limits could make or break your applicationâ€™s success during development or production. Request limit, in other words, is the maximum number of API calls per a given time your application can make.\n\nFirst things first, decide on the user tier for your project. Each tier has caps on token limits (tpm- token per minute) and request limits (rpm/rpd â€” request per day or minute)\n\nNext up, decide on the model type. Take a glance at the image above; notice how each model has different tpm and rpm/rpd?\n\nChoose one that aligns with your projectâ€™s token request needs.\n\nYes, you should design and develop your language model application to send several requests per minute (for example, 30 requests per minute), with built-in waits or retries when it hits the limit. Always experiment.\n\nAlternatively, you can design your application to gracefully handle set token limits â€” returning a statement or retrying when needed.\n\nFriendly Reminder: Always check your bill settings. Itâ€™s easy to run out of OpenAI credits due to requests and token limits, leaving you confused about failing API calls.\n\nThanks a lot for reading. If youâ€™re on the lookout for a Machine Learning Engineer consultant or AI Engineer consultant, donâ€™t hesitate to reach out. Book a session with me â€” â€” letâ€™s dive into the exciting world of AI together.\n\nYou may also buy me a coffee to support my work."
    }
]