[
    {
        "link": "https://medium.com/@aleksej.gudkov/implementing-a-hash-table-in-python-a-step-by-step-guide-a7ef0f231d3c",
        "document": "A hash table is a data structure that maps keys to values using a hash function for fast lookups, insertions, and deletions. While Python provides a built-in dictionary ( ) that functions as a hash table, understanding how to implement one from scratch is a valuable exercise in learning data structures and algorithms.\n\nThis guide will walk you through implementing a hash table in Python, covering the core concepts of hashing, collision resolution, and common operations."
    },
    {
        "link": "https://github.com/touqir14/Microdict",
        "document": "A high performance python hash table library that is generally faster and consumes significantly less memory than Python Dictionaries. It currently supports Python 3.5+.\n\nPython Dictionaries are fast but their memory consumption can also be high at the same time. This is partly due to the nature of Python keeping data, within RAM, in the form of PyObjects, which consume far more memory than native types such as Integers and Character Arrays. As a result, Python Dictionaries can be prohibitive in many cases while building memory intensive python applications. This motivated me to develop a typed python hash table library that consumes significantly (upto 7 times) less memory compared to python dictionaries. It is also faster than python dictionaries. Moreover, it's underlying C implementation can also outperform Google's highly optimized Swiss Table and Facebook's F14 hash tables. See the Performance Section.\n\nYou can install Microdict using pip : .\n\nMicrodict is absolutely built using C extensions and as such building it will require Python C api header files. Build and install the package using from the terminal after cloning the repository. Microdict is tested to work on Linux, Mac OSX, and Windows systems. You will need GCC 7+ on linux/mac osx systems and Visual C++ 14+ compiler on Windows systems to build the package. For the best performance use on a 64 bit system.\n\nOnce installed, type the following code snippet in your python interpreter to run the tests:\n\nThe following code snippet shows common uses of the library.\n\n. ( ) # Generates a dictionary with key and value type of signed 32 bit integer. [ ] # Just like python dictionaries, setting a key value pair. ( ) ( [ ]) : ( . ( )) : ( . ( )) # Removes [1,2] from the hashtable and prints 2. [ ] [ ] [ ] : ( ) . (): ( ) , . (): ( , ) # Will print all the items. . () . ([ , ]) . () . () # Returns a python dictionary containing all the items in d2 [ ] [ ] . ( ) # d2 now will additionally have the pairs [120, 5] and [42, 9] [ ] [ ] . ( ) # d2 now will additionally have the pairs [111, 89] and [123, 1]. ( ( . ())) Faster approach shown below. d2.get_items() creates and returns a list of all items. So if you don't need a list container, iterate using d2.items() for memory efficiency. Same applies for other d2.get_* methods shown below. ( . ()) ( ( . ())) ( . ()) ( ( )) ( . ())\nâ€¢ : Returns a Microdict hash table of any of the types given above.\nâ€¢ dtype: A python string type ( ) that sets the hash table type to be created. It can be any one of the above types.\nâ€¢ key_len: A python Integer type ( ). It sets the maximum number of bytes the characters of a key (UTF-8 string) requires. Passing a UTF-8 encoded string key which consumes more bytes than key_len will not be accepted. This argument is only applicable when . It only accepts a value of at most 65355 and a larger value will raise a .\nâ€¢ val_len: A python Integer type( ). It sets the maximum number of bytes the characters of a value (UTF-8 string) requires. Passing a UTF-8 encoded string value which consumes more bytes than val_len will not be accepted. This argument is only applicable when . It only accepts a value of at most 65355 and a larger value will raise a .\nâ€¢ : Prints a series of lines of the form : , where forms a type given above.\n\nThe following are the methods that are common to all hash table types returned by .\nâ€¢ : Returns None. If key_list is not provided, the clear method will delete all items from the hash table.\nâ€¢ key_list: It is an optional argument but not a keyword optional argument (keyword must not be provided). So, it suffices: . If provided, it must be of type . The entries within key_list are the keys that will be removed from the hash table. By default, any entry that is not present in the hash table will be skipped. For any of the integer hash table types, any non python entry will be skipped. It is upto the programmer to pass the proper sized integer to prevent overflows. For type, the entries must be python UTF-8 strings with UTF-8 character bytes upto key_len as set by and otherwise, that entry will be skipped.\nâ€¢ : Returns a deep copy of the Microdict Hash table of the same type as the caller Hash table object.\nâ€¢ : Creates and returns a python containing all the items (key, value) in the hash table.\nâ€¢ : Creates and returns a python containing all the keys in the hash table.\nâ€¢ : Creates and returns a python containing all the values in the hash table.\nâ€¢ : Used to iterate over items using a loop. Example :\nâ€¢ : Deletes a key from the hash table and returns its corresponding value. If the key is not present, then a is raised.\nâ€¢ key: For any of the integer hash table types, any non python entry will raise a . It is upto the programmer to pass the proper sized integer to prevent overflows. For type, the entries must be python UTF-8 strings with UTF-8 character bytes upto key_len as set by and otherwise, a will be raised.\nâ€¢ : Creates and returns a python dictionary containing all items present in the Microdict hash table.\nâ€¢ : Inserts all the items present in the dictionary into the Microdict hash table.\nâ€¢ dictionary: Must be either a python dictionary or a Microdict hash table. If it is a python dictionary, then all its items that are of the same type as the Microdict hash table will be inserted. The rest will be skipped by default. The conditions given in the method documentation of clear regarding type constraints apply here too.\nâ€¢ : Used to iterate over values using a loop. Example : .\n\nEach of the cells in the tables below are of the format (Speed Gain, Space Gain).\n\nExperiments were carried out for the types , , (key and value length kept to 8 characters). Speed Gain and Space Gain are computed by averaging over the results of 30 experiments carried out using (unique) randomly generated data. Space consumption was computed using the psutil library. Time consumption was computed using the method. All the experiments were conducted with Python 3.8.2 on a 64 bit Ubuntu 20.04 LTS system. The following table shows the benchmarks against Python Dictionary for retrieving all values using the full set of keys and the operator.\n\nMicrodict's underlying C implementation was benchmarked against Swiss Table abseil::flat_hash_map and F14 folly::F14FastMap to further test its capabilities. The data types were set as above using the same settings. The following tables show the results for retrieving all values using the keys."
    },
    {
        "link": "https://dev.to/theramoliya/python-use-hash-tables-dictionaries-for-fast-data-retrieval-1oom",
        "document": "Hash tables, often implemented as dictionaries in Python, are a fundamental data structure for efficiently storing and retrieving data. They provide constant-time average-case lookup, insertion, and deletion operations, making them valuable for various applications. Here's an example:\n\n\n\n# Initialize count if element is new\n\nIn this example, we use a dictionary ( ) to efficiently count the occurrences of elements in a list. We iterate through the list, checking whether each element exists in the dictionary. If it does, we increment its count; otherwise, we initialize it with a count of 1. This approach provides a straightforward and efficient way to compute element frequencies.\n\nHash tables are versatile and can be applied to various problems, such as caching, frequency counting, memoization, and more. Leveraging Python dictionaries effectively can lead to clean and performant solutions in many scenarios."
    },
    {
        "link": "https://realpython.com/python-hash-table",
        "document": "Invented over half a century ago, the hash table is a classic data structure that has been fundamental to programming. To this day, it helps solve many real-life problems, such as indexing database tables, caching computed values, or implementing sets. It often comes up in job interviews, and Python uses hash tables all over the place to make name lookups almost instantaneous.\n\nEven though Python comes with its own hash table called , it can be helpful to understand how hash tables work behind the curtain. A coding assessment may even task you with building one. This tutorial will walk you through the steps of implementing a hash table from scratch as if there were none in Python. Along the way, youâ€™ll face a few challenges thatâ€™ll introduce important concepts and give you an idea of why hash tables are so fast.\n\nIn addition to this, youâ€™ll get a hands-on crash course in test-driven development (TDD) and will actively practice it while building your hash table in a step-by-step fashion. Youâ€™re not required to have any prior experience with TDD, but at the same time, you wonâ€™t get bored even if you do!\nâ€¢ How you can implement a hash table from scratch in Python\nâ€¢ How you can deal with hash collisions and other challenges\nâ€¢ What the desired properties of a hash function are\nâ€¢ How Pythonâ€™s works behind the scenes\n\nItâ€™ll help if youâ€™re already familiar with Python dictionaries and have basic knowledge of object-oriented programming principles. To get the complete source code and the intermediate steps of the hash table implemented in this tutorial, follow the link below:\n\nGet to Know the Hash Table Data Structure Before diving deeper, you should familiarize yourself with the terminology because it can get slightly confusing. Colloquially, the term hash table or hash map is often used interchangeably with the word dictionary. However, thereâ€™s a subtle difference between the two concepts as the former is more specific than the latter. In computer science, a dictionary is an abstract data type made up of keys and values arranged in pairs. Moreover, it defines the following operations for those elements:\nâ€¢ Find a value associated with the given key In a sense, this abstract data type resembles a bilingual dictionary, where the keys are foreign words, and the values are their definitions or translations to other languages. But there doesnâ€™t always have to be a sense of equivalence between keys and values. A phone book is another example of a dictionary, which combines names with the corresponding phone numbers. Note: Anytime you map one thing to another or associate a value with a key, youâ€™re essentially using a kind of a dictionary. Thatâ€™s why dictionaries are also known as maps or associative arrays. Dictionaries have a few interesting properties. One of them is that you can think of a dictionary as a mathematical function that projects one or more arguments to exactly one value. The direct consequences of that fact are the following:\nâ€¢ Only Key-Value Pairs: You canâ€™t have a key without the value or the other way around in a dictionary. They always go together.\nâ€¢ Arbitrary Keys and Values: Keys and values can belong to two disjoint sets of the same or separate types. Both keys and values may be almost anything, such as numbers, words, or even pictures.\nâ€¢ Unordered Key-Value Pairs: Because of the last point, dictionaries donâ€™t generally define any order for their key-value pairs. However, that might be implementation-specific.\nâ€¢ Unique Keys: A dictionary canâ€™t contain duplicate keys, because that would violate the definition of a function.\nâ€¢ Non-Unique Values: The same value can be associated with many keys, but it doesnâ€™t have to. There are related concepts that extend the idea of a dictionary. For example, a multimap lets you have more than one value per key, while a bidirectional map not only maps keys to values but also provides mapping in the opposite direction. However, in this tutorial, youâ€™re only going to consider the regular dictionary, which maps exactly one value to each key. Hereâ€™s a graphical depiction of a hypothetical dictionary, which maps some abstract concepts to their corresponding English words: Itâ€™s a one-way map of keys to values, which are two completely different sets of elements. Right away, you can see fewer values than keys because the word bow happens to be a homonym with multiple meanings. Conceptually, this dictionary still contains four pairs, though. Depending on how you decided to implement it, you could reuse repeated values to conserve memory or duplicate them for simplicity. Now, how do you code such a dictionary in a programming language? The right answer is you donâ€™t, because most modern languages come with dictionaries as either primitive data types or classes in their standard libraries. Python ships with a built-in type, which already wraps a highly optimized data structure written in C so that you donâ€™t have to write a dictionary yourself. Pythonâ€™s lets you perform all the dictionary operations listed at the beginning of this section: With the square bracket syntax ( ), you can add a new key-value pair to a dictionary. You can also update the value of or delete an existing pair identified by a key. Finally, you can look up the value associated with the given key. That said, you may ask a different question. How does the built-in dictionary actually work? How does it map keys of arbitrary data types, and how does it do it so quickly? Finding an efficient implementation of this abstract data type is known as the dictionary problem. One of the most well-known solutions takes advantage of the hash table data structure that youâ€™re about to explore. However, note that it isnâ€™t the only way to implement a dictionary in general. Another popular implementation builds on top of a red-black tree. Have you ever wondered why accessing sequence elements in Python works so quickly, regardless of which index you request? Say you were working with a very long string of characters, like this one: There are 2.6 billion characters from repeating ASCII letters in the variable above, which you can count with Pythonâ€™s function. Yet, getting the first, middle, last, or any other character from this string is equally quick: # The last element, same as text[len(text) - 1] The same is true for all sequence types in Python, such as lists and tuples. How come? The secret to such a blazing speed is that sequences in Python are backed by an array, which is a random-access data structure. It follows two principles:\nâ€¢ Every element in the array has a fixed size known up front. When you know the memory address of an array, which is called the offset, then you can get to the desired element in the array instantly by calculating a fairly straightforward formula: Formula to Calculate the Memory Address of a Sequence Element You start at the arrayâ€™s offset, which is also the address of the arrayâ€™s first element, with the index zero. Next, you move forward by adding the required number of bytes, which you get by multiplying the element size by the target elementâ€™s index. It always takes the same amount of time to multiply and add a few numbers together. Note: Unlike arrays, Pythonâ€™s lists can contain heterogeneous elements of varying sizes, which would break the above formula. To mitigate this, Python adds another level of indirection by introducing an array of pointers to memory locations rather than storing values directly in the array: Pointers are merely integer numbers, which always take up the same amount of space. Itâ€™s customary to denote memory addresses using the hexadecimal notation. Python and some other languages prefix such numbers with . Okay, so you know that finding an element in an array is quick, no matter where that element is physically located. Can you take the same idea and reuse it in a dictionary? Yes! Hash tables get their name from a trick called hashing, which lets them translate an arbitrary key into an integer number that can work as an index in a regular array. So, instead of searching a value by a numeric index, youâ€™ll look it up by an arbitrary key without a noticeable performance loss. Thatâ€™s neat! In practice, hashing wonâ€™t work with every key, but most built-in types in Python can be hashed. If you follow a few rules, then youâ€™ll be able to create your own hashable types too. Youâ€™ll learn more about hashing in the next section.\n\nA hash function performs hashing by turning any data into a fixed-size sequence of bytes called the hash value or the hash code. Itâ€™s a number that can act as a digital fingerprint or a digest, usually much smaller than the original data, which lets you verify its integrity. If youâ€™ve ever fetched a large file from the Internet, such as a disk image of a Linux distribution, then you may have noticed an MD5 or SHA-2 checksum on the download page. Aside from verifying data integrity and solving the dictionary problem, hash functions help in other fields, including security and cryptography. For example, you typically store hashed passwords in databases to mitigate the risk of data leaks. Digital signatures involve hashing to create a message digest before encryption. Blockchain transactions are another prime example of using a hash function for cryptographic purposes. Note: A cryptographic hash function is a special type of hash function that must meet a few additional requirements. In this tutorial, youâ€™ll only encounter the most basic form of the hash function used in the hash table data structure, though. While there are many hashing algorithms, they all share a few common properties that youâ€™re about to discover in this section. Implementing a good hash function correctly is a difficult task that may require the understanding of advanced math involving prime numbers. Fortunately, you donâ€™t usually need to implement such an algorithm by hand. Python comes with a built-in hashlib module, which provides a variety of well-known cryptographic hash functions, as well as less secure checksum algorithms. The language also has a global function, used primarily for quick element lookup in dictionaries and sets. You can study how it works first to learn about the most important properties of hash functions. Before taking a stab at implementing a hash function from scratch, hold on for a moment and analyze Pythonâ€™s to distill its properties. This will help you understand what problems are involved when designing a hash function of your own. Note: The choice of a hash function can dramatically impact your hash tableâ€™s performance. Therefore, youâ€™ll rely on the built-in function when building a custom hash table later on in this tutorial. Implementing a hash function in this section only serves as an exercise. For starters, try your hand at calling on a few data type literals built into Python, such as numbers and strings, to see how the function behaves: There are already several observations that you can make by looking at the result. First, the built-in hash function may return different values on your end for some of the inputs shown above. While the numeric input always seems to produce an identical hash value, the string most likely doesnâ€™t. Why is that? It may seem like is a non-deterministic function, but that couldnâ€™t be further from the truth! When you call with the same argument within your existing interpreter session, then youâ€™ll keep getting the same result: Thatâ€™s because hash values are immutable and donâ€™t change throughout an objectâ€™s lifetime. However, as soon as you exit Python and start it again, then youâ€™ll almost certainly see different hash values across Python invocations. You can test this by trying the option to run a one-liner script in your terminal: python -c print(hash('Lorem')) python -c print(hash('Lorem')) python -c print(hash('Lorem')) python -c python -c python -c Thatâ€™s expected behavior, which was implemented in Python as a countermeasure against a Denial-of-Service (DoS) attack that exploited a known vulnerability of hash functions in web servers. Attackers could abuse a weak hash algorithm to deliberately create so-called hash collisions, overloading the server and making it inaccessible. Ransom was a typical motive for the attack as most victims made money through an uninterrupted online presence. Today, Python enables hash randomization by default for some inputs, such as strings, to make the hash values less predictable. This makes a bit more secure and the attack more difficult. You can disable randomization, though, by setting a fixed seed value through the environment variable, for example: 1 python -c print(hash('Lorem')) python -c print(hash('Lorem')) python -c print(hash('Lorem')) python -c python -c python -c Now, each Python invocation yields the same hash value for a known input. This can help in partitioning or sharing data across a cluster of distributed Python interpreters. Just be careful and understand the risks involved in disabling hash randomization. All in all, Pythonâ€™s is indeed a deterministic function, which is one of the most fundamental features of the hash function. Additionally, seems fairly universal as it takes arbitrary inputs. In other words, it takes values of various types and sizes. The function accepts strings and floating-point numbers regardless of their length or magnitude without complaining. In fact, you can calculate a hash value of more exotic types too: Here, you called the hash function on Pythonâ€™s object, the function itself, and even a custom class with a few of its instances. That said, not all objects have a corresponding hash value. The function will raise an exception if you try calling it against one of those few objects: The inputâ€™s underlying type will determine whether or not you can calculate a hash value. In Python, instances of built-in mutable typesâ€”like lists, sets, and dictsâ€”arenâ€™t hashable. Youâ€™ve gotten a hint of why that is, but youâ€™ll learn more in a later section. For now, you can assume that most data types should work with a hash function in general. Another interesting characteristic of is that it always produces a fixed-size output no matter how big the input was. In Python, the hash value is an integer with a moderate magnitude. Occasionally, it may come out as a negative number, so take that into account if you plan to rely on hash values in one way or another: The natural consequence of a fixed-size output is that most of the original information gets irreversibly lost during the process. Thatâ€™s fine since you want the resulting hash value to act as a unified digest of arbitrarily large data, after all. However, because the hash function projects a potentially infinite set of values onto a finite space, this can lead to a hash collision when two different inputs produce the same hash value. Note: If youâ€™re mathematically inclined, then you could use the pigeonhole principle to describe hash collisions more formally: Given m items and n containers, if m > n, then thereâ€™s at least one container with more than one item. In this context, items are a potentially infinite number of values that you feed into the hash function, while containers are their hash values assigned from a finite pool. Hash collisions are an essential concept in hash tables, which youâ€™ll revisit later in more depth when implementing your custom hash table. For now, you can think of them as highly undesirable. You should avoid hash collisions as much as possible because they can lead to very inefficient lookups and could be exploited by hackers. Therefore, a good hash function must minimize the likelihood of a hash collision for both security and efficiency. In practice, this often means that the hash function must assign uniformly distributed values over the available space. You can visualize the distribution of hash values produced by Pythonâ€™s by plotting a textual histogram in your terminal. Copy the following block of code and save it in a file named : It uses a instance to conveniently represent the histogram of hash values of the provided items. The hash values are spread over the specified number of containers by wrapping them with the modulo operator. Now, you can take one hundred printable ASCII characters, for example, then calculate their hash values and show their distribution: When there are only two containers, you should expect roughly a fifty-fifty distribution. Adding more containers should result in filling them more or less evenly. As you can see, the built-in function is pretty good but not perfect at distributing the hash values uniformly. Related to that, the uniform distribution of hash values is typically pseudo-random, which is especially important for cryptographic hash functions. This prevents potential attackers from using statistical analysis to try and predict the correlation between input and output of the hash function. Consider altering a single letter in a string, and check how that affects the resulting hash value in Python: Itâ€™s a completely different hash value now, despite only one letter being different. Hash values are often subject to an avalanche effect, as even the smallest input change gets amplified. Nevertheless, this feature of the hash function isnâ€™t essential for the sake of implementing a hash table data structure. In most cases, Pythonâ€™s exhibits yet another nonessential feature of cryptographic hash functions, which stems from the pigeonhole principle mentioned earlier. It behaves like a one-way function because finding its inverse is next to impossible in the majority of cases. However, there are notable exceptions: Hash values of small integers are equal to themselves, which is an implementation detail that CPython uses for simplicity and efficiency. Bear in mind that the actual hash values donâ€™t matter as long as you can calculate them in a deterministic way. Last but not least, calculating a hash value in Python is fast, even for very big inputs. On a modern computer, calling with a string of 100 million characters as the argument returns instantaneously. If it werenâ€™t so fast, then the additional overhead of the hash value computation would offset the benefits of hashing in the first place. Based on what youâ€™ve learned so far about Pythonâ€™s , you can now draw conclusions about the desired properties of a hash function in general. Hereâ€™s a summary of those features, comparing the regular hash function with its cryptographic flavor: The goals of both hash function types overlap, so they share a few common features. On the other hand, a cryptographic hash function provides additional guarantees around security. Before building your own hash function, youâ€™ll take a look at another function built into Python thatâ€™s seemingly its most straightforward substitute. Compare an Objectâ€™s Identity With Its Hash Probably one of the most straightforward hash function implementations imaginable in Python is the built-in , which tells you an objectâ€™s identity. In the standard Python interpreter, identity is the same as the objectâ€™s memory address expressed as an integer: The function has most of the desired hash function properties. After all, itâ€™s super fast and works with any input. It returns a fixed-size integer in a deterministic way. At the same time, you canâ€™t easily retrieve the original object based on its memory address. The memory addresses themselves are immutable during an objectâ€™s lifetime and somewhat randomized between interpreter runs. So, why does Python insist on using a different function for hashing then? First of all, the intent of is different from , so other Python distributions may implement identity in alternative ways. Second, memory addresses are predictable without having a uniform distribution, which is both insecure and severely inefficient for hashing. Finally, equal objects should generally produce the same hash code even if they have distinct identities. Note: Later, youâ€™ll learn more about the contract between the equality of values and the corresponding hash codes. With that out of the way, you can finally think of making your own hash function. Designing a hash function that meets all requirements from scratch is hard. As mentioned before, youâ€™ll be using the built-in function to create your hash table prototype in the next section. However, trying to build a hash function from the ground up is a great way of learning how it works. By the end of this section, youâ€™ll only have a rudimentary hash function thatâ€™s far from perfect, but youâ€™ll have gained valuable insights. In this exercise, you can limit yourself to only one data type at first and implement a crude hash function around it. For example, you could consider strings and sum up the ordinal values of the individual characters in them: You iterate over the text using a generator expression, then turn each individual character into the corresponding Unicode code point with the built-in function, and finally sum the ordinal values together. This will throw out a single number for any given text provided as an argument: Right away, youâ€™ll notice a few problems with this function. Not only is it string-specific, but it also suffers from poor distribution of hash codes, which tend to form clusters at similar input values. A slight change in the input has little effect on the observed output. Even worse, the function remains insensitive to character order in the text, which means anagrams of the same word, such as Loren and Loner, lead to a hash code collision. To fix the first problem, try converting the input to a string with a call to . Now, your function will be able to deal with any type of argument: You can call with an argument of any data type, including a string, a floating-point number, or a Boolean value. Note that this implementation will only be as good as the corresponding string representation. Some objects may not have a textual representation suitable for the code above. In particular, custom class instances without the special methods and properly implemented are a good example. Plus, you wonâ€™t be able to distinguish between different data types anymore: In reality, youâ€™d want to treat the string and the floating-point number as distinct objects with different hash codes. One way to mitigate this would be to trade for , which encloses the representation of strings with additional apostrophes ( ): Thatâ€™ll improve your hash function to some extent: Strings are now distinguishable from numbers. To tackle the issue with anagrams, like Loren and Loner, you might modify your hash function by taking into consideration the characterâ€™s value as well as its position within the text: Here, you take the sum of products derived from multiplying the ordinal values of characters and their corresponding indices. Notice you enumerate the indices from one rather than zero. Otherwise, the first character would always be discarded as its value would be multiplied by zero. Now your hash function is fairly universal and doesnâ€™t cause nearly as many collisions as before, but its output can grow arbitrarily large because the longer the string, the bigger the hash code. Also, itâ€™s quite slow for larger inputs: \"This has a somewhat medium length.\" \"This is very long and slow!\" You can always address unbounded growth by taking the modulo ( ) of your hash code against a known maximum size, such as one hundred: \"This has a somewhat medium length.\" \"This is very long and slow!\" Remember that choosing a smaller pool of hash codes increases the likelihood of hash code collisions. If you donâ€™t know the number of your input values up front, then itâ€™s best to leave that decision until later if you can. You may also impose a limit on your hash codes by assuming a reasonable maximum value, such as , which represents the highest value of integers supported natively in Python. Ignoring the functionâ€™s slow speed for a moment, youâ€™ll notice another peculiar issue with your hash function. It results in suboptimal distribution of hash codes through clustering and by not taking advantage of all the available slots: The distribution is uneven. Moreover, there are six containers available, but one is missing from the histogram. This problem stems from the fact that the two apostrophes added by cause virtually all keys in this example to result in an even hash number. You can avoid this by removing the left apostrophe if it exists: The call to will only affect a string if it starts with the specified prefix to strip. Naturally, you can continue improving your hash function further. If youâ€™re curious about the implementation of for strings and byte sequences in Python, then it currently uses the SipHash algorithm, which might fall back to a modified version of FNV if the former is unavailable. To find out which hash algorithm your Python interpreter is using, reach for the module: At this point, you have a pretty good grasp of the hash function, how itâ€™s supposed to work, and what challenges you might face in implementing it. In the upcoming sections, youâ€™ll use a hash function to build a hash table. The choice of a particular hash algorithm will influence the hash tableâ€™s performance. With that knowledge as your foundation, you can feel free to stick with the built-in from now on.\n\nIn this section, youâ€™re going to create a custom class representing the hash table data structure. It wonâ€™t be backed by a Python dictionary, so you can build a hash table from scratch and practice what youâ€™ve learned so far. At the same time, youâ€™ll model your implementation after the built-in dictionary by mimicking its most essential features. Note: This is just a quick reminder that implementing a hash table is only an exercise and an educational tool to teach you about the problems that this data structure solves. Just like coding a custom hash function before, a pure-Python hash table implementation has no practical use in real-life applications. Below is a list of the high-level requirements for your hash table, which youâ€™ll be implementing now. By the end of this section, your hash table will exhibit the following core features. Itâ€™ll let you:\nâ€¢ Find a value by key in the hash table\nâ€¢ Update the value associated with an existing key\nâ€¢ Check if the hash table has a given key In addition to these, youâ€™ll implement a few nonessential but still useful features. Specifically, you should be able to:\nâ€¢ Return a default value if the corresponding key is not found\nâ€¢ Report the number of key-value pairs stored in the hash table\nâ€¢ Make the hash table comparable by using the equality test operator While implementing these features, youâ€™ll actively exercise test-driven development by gradually adding more features to your hash table. Note that your prototype will only cover the basics. Youâ€™ll learn how to cope with some more advanced corner cases a bit later in this tutorial. In particular, this section wonâ€™t cover how to: Feel free to use the supplementary materials as control checkpoints if you get stuck or if youâ€™d like to skip some of the intermediate refactoring steps. Each subsection ends with a complete implementation stage and the corresponding tests that you can start from. Grab the following link and download the supporting materials with the complete source code and the intermediate steps used in this tutorial: Source Code: Click here to download the source code that youâ€™ll use to build a hash table in Python. Take a Crash Course in Test-Driven Development Now that you know the high-level properties of a hash function and its purpose, you can exercise a test-driven development approach to build a hash table. If youâ€™ve never tried this programming technique before, then it boils down to three steps that you tend to repeat in a cycle:\nâ€¢ ðŸŸ¥ Red: Think of a single test case and automate it using a unit testing framework of your choice. Your test will fail at this point, but thatâ€™s okay. Test runners typically indicate a failing test with the red color, hence the name of this cycle phase.\nâ€¢ ðŸŸ© Green: Implement the bare minimum to make your test pass, but no more! This will ensure higher code coverage and spare you from writing redundant code. The test report will light up to a satisfying green color afterward.\nâ€¢ â™»ï¸ Refactor: Optionally, modify your code without changing its behavior as long as all test cases still pass. You can use this as an opportunity to remove duplication and improve the readability of your code. Python comes with the unittest package out of the box, but the third-party pytest library has an arguably shallower learning curve, so youâ€™ll use that in this tutorial instead. Go ahead and install in your virtual environment now: Remember that you can verify each implementation stage against several control checkpoints. Next, create a file named and define a dummy test function in it to check if pytest will pick it up: \"This is just a dummy test\" The framework leverages the built-in statement to run your tests and report their results. That means you can just use regular Python syntax, without importing any specific API until absolutely necessary. It also detects test files and test functions as long as their names start with the prefix. The statement takes a Boolean expression as an argument, followed by an optional error message. When the condition evaluates to , then nothing happens, as if there were no assertion at all. Otherwise, Python will raise an and display the message on the standard error stream (stderr). Meanwhile, pytest intercepts assertion errors and builds a report around them. Now, open the terminal, change your working directory to wherever you saved that test file, and run the command without any arguments. Its output should look similar to this: python -m pytest assert 2 + 2 == 22, \"This is just a dummy test\" E AssertionError: This is just a dummy test pytest > assert 2 + 2 == 22, \"This is just a dummy test\" E AssertionError: This is just a dummy test Uh-oh. Thereâ€™s a failure in your test. To find the root cause, increase the verbosity of pytestâ€™s output by appending the flag to the command. Now you can pinpoint where the problem lies: def test_should_always_pass(): > assert 2 + 2 == 22, \"This is just a dummy test\" E AssertionError: This is just a dummy test E assert 4 == 22 E +4 E -22 The output shows what the actual and expected values were for the failed assertion. In this case, adding two plus two results in four rather than twenty-two. You can fix the code by correcting the expected value: \"This is just a dummy test\" When you rerun pytest, there should be no test failures anymore: Thatâ€™s it! Youâ€™ve just learned the essential steps in automating test cases for your hash table implementation. Naturally, you can use an IDE such as PyCharm or an editor like VS Code that integrates with testing frameworks if thatâ€™s more convenient for you. Next up, youâ€™re going to put this new knowledge into practice. Remember to follow the red-green-refactor cycle described earlier. Therefore, you must start by identifying your first test case. For example, you should be able to instantiate a new hash table by calling the hypothetical class imported from the module. This call should return a non-empty object: At this point, your test will refuse to run because of an unsatisfied import statement at the top of the file. Youâ€™re in the red phase, after all. The red phase is the only time when youâ€™re allowed to add new features, so go on and create another module named and put the class definition in it: Itâ€™s a bare-bones class placeholder, but it should be just enough to make your test pass. By the way, if youâ€™re using a code editor, then you can conveniently split the view into columns, displaying the code under test and the corresponding tests side by side: If youâ€™re curious about the color scheme depicted in the screenshot above, then itâ€™s the Dracula Theme. Itâ€™s available for many code editors and not just PyCharm. Once running pytest succeeds, then you can start thinking of another test case since thereâ€™s barely anything to refactor yet. For example, a basic hash table should contain a sequence of values. At this early stage, you can assume the sequence to have a fixed size established at the hash tableâ€™s creation time. Modify your existing test case accordingly: You specify the size using a keyword argument. However, before adding new code to the class, rerun your tests to confirm that youâ€™ve entered the red phase again. Witnessing a failing test can be invaluable. When you implement a piece of code later, youâ€™ll know whether it satisfies a particular group of tests or if they remain unaffected. Otherwise, your tests could lie to you by verifying something different than you thought! After confirming that youâ€™re in the red phase, declare the method in the class with the expected signature, but donâ€™t implement its body: Boom! Youâ€™re back in the green phase once more, so how about a bit of refactoring this time? For instance, you could rename the argument to if thatâ€™s more descriptive to you. Donâ€™t forget to change the test case first, then run pytest, and always update the code under test as the final step: As you can tell, the red-green-refactor cycle consists of brief stages, often lasting no more than a few seconds each. So, why donâ€™t you continue by adding more test cases? It would be nice if your data structure could report back the hash tableâ€™s capacity using Pythonâ€™s built-in function. Add another test case and observe how it fails miserably: To handle correctly, you must implement the special method in your class and remember the capacity supplied through the class initializer: You may think that TDD isnâ€™t moving you in the right direction. Thatâ€™s not how you might have envisioned the hash table implementation. Whereâ€™s the sequence of values that you started with in the beginning? Unfortunately, taking baby steps and making many course corrections along the way is something that test-driven development gets a lot of criticism for. Therefore, it may not be appropriate for projects involving lots of experimentation. On the other hand, implementing a well-known data structure such as a hash table is a perfect application of this software development methodology. You have clear expectations that are straightforward to codify as test cases. Soon, youâ€™ll see for yourself that taking the next step will lead to a slight change in the implementation. Donâ€™t worry about it, though, because perfecting the code itself is less important than making your test cases pass. As you keep adding more constraints through the test cases, you frequently have to rethink your implementation. For example, a new hash table should probably start with empty slots for the stored values: In other words, a new hash table should expose a attribute having the requested length and being filled with the elements. By the way, itâ€™s common to use such verbose function names because theyâ€™ll appear in your test report. The more readable and descriptive your tests, the more quickly youâ€™ll figure out what part needs fixing. Note: As a rule of thumb, your test cases should be as independent and atomic as possible, which usually means using only one assertion per function. Nevertheless, your test scenarios may sometimes need a bit of setup or teardown. They may also involve a few steps. In such cases, itâ€™s customary to structure your function around the so-called given-when-then convention: The given part describes the initial state and preconditions to your test case, while when represents the action of your code under test, and then is responsible for asserting the resulting outcome. This is one of many possible ways to satisfy your existing tests: You replace the attribute with a list of the requested length containing only elements. Multiplying a number by a list or the other way around is a quick way to populate that list with the given value or values. Other than that, you update the special method so that all tests will pass. Note: A Python dictionary has a length corresponding to the actual number of key-value pairs stored instead of its internal capacity. Youâ€™ll achieve a similar effect soon. Now that youâ€™re acquainted with TDD, itâ€™s time to dive deeper and add the remaining features to your hash table. Now that you can create a hash table, itâ€™s time to give it some storage capabilities. The traditional hash table is backed by an array capable of storing only one data type. Because of this, hash table implementations in many languages, such as Java, require you to declare the type for their keys and values up front: This particular hash table maps strings to integers, for example. However, because arrays arenâ€™t native to Python, youâ€™ll keep using a list instead. As a side effect, your hash table will be able to accept arbitrary data types for both the keys and values, just like Pythonâ€™s . Note: Python has an efficient collection, but itâ€™s intended for numeric values only. You may sometimes find it convenient for processing raw binary data. Now add another test case for inserting key-value pairs into your hash table using the familiar square bracket syntax: First, you create a hash table with one hundred empty slots and then populate it with three pairs of keys and values of various types, including strings, floating-point numbers, and Booleans. Finally, you assert that the hash table contains the expected values in whatever order. Note that your hash table only remembers the values but not the associated keys at the moment! The most straightforward and perhaps slightly naive implementation that would satisfy this test is as follows: It completely ignores the key and just appends the value to the right end of the list, increasing its length. You may very well immediately think of another test case. Inserting elements into the hash table shouldnâ€™t grow its size. Similarly, removing an element shouldnâ€™t shrink the hash table, but you only take a mental note of that, because thereâ€™s no ability to remove key-value pairs yet. Note: You could also write a placeholder test and tell pytest to skip it unconditionally until later: It leverages one of the decorators provided by pytest. In the real world, youâ€™d want to create separate test cases with descriptive names dedicated to testing these behaviors. However, because this is only a tutorial, youâ€™re going to add a new assertion to the existing function for brevity: Youâ€™re in the red phase now, so rewrite your special method to keep the capacity fixed at all times: You turn an arbitrary key into a numeric hash value and use the modulo operator to constrain the resulting index within the available address space. Great! Your test report lights up green again. Note: The code above relies on Pythonâ€™s built-in function, which has an element of randomization, as you already learned. Therefore, your test might fail in rare cases when two keys happen to produce an identical hash code by coincidence. Because youâ€™ll deal with hash code collisions later, you can disable hash randomization or use a predictable seed when running pytest in the meantime: Make sure to pick a hash seed that wonâ€™t cause any collisions in your sample data. Finding one can involve a bit of a trial and error. In my case, value seemed to work fine. But can you think of some edge cases, maybe? What about inserting into your hash table? It would create a conflict between a legitimate value and the designated sentinel value that you chose to indicate blanks in your hash table. Youâ€™ll want to avoid that. As always, start by writing a test case to express the desired behavior: One way to work around this would be to replace in your method with another unique value that users are unlikely to insert. For example, you could define a special constant by creating a brand-new object to represent blank spaces in your hash table: You only need a single blank instance to mark the slots as empty. Naturally, youâ€™ll need to update an older test case to get back to the green phase: Then, write a positive test case exercising the happy path for handling the insertion of a value: You create an empty hash table with one hundred slots and insert associated with some arbitrary key. It should work like a charm if youâ€™ve been closely following the steps so far. If not, then look at the error messages because they often contain clues as to what went wrong. Alternatively, check the sample code available for download at this link: Source Code: Click here to download the source code that youâ€™ll use to build a hash table in Python. In the next subsection, youâ€™ll add the ability to retrieve values by their associated keys. To retrieve a value from the hash table, youâ€™ll want to leverage the same square brackets syntax as before, only without using the assignment statement. Youâ€™ll also need a sample hash table. To avoid duplicating the same setup code across the individual test cases in your test suite, you can wrap it in a test fixture exposed by pytest: A test fixture is a function annotated with the decorator. It returns sample data for your test cases, such as a hash table populated with known keys and values. Your pytest will automatically call that function for you and inject its result into any test function that declares an argument with the same name as your fixture. In this case, the test function expects a argument, which corresponds to your fixture name. To be able to find values by key, you can implement the element lookup through another special method called in your class: You calculate the index of an element based on the hash code of the provided key and return whatever sits under that index. But what about missing keys? As of now, you might return a blank instance when a given key hasnâ€™t been used before, an outcome which isnâ€™t all that helpful. To replicate how a Python would work in such a case, you should raise a exception instead: You create an empty hash table and try accessing one of its values through a missing key. The pytest framework includes a special construct for testing exceptions. Up above, you use the context manager to expect a specific kind of exception within the following code block. Handling this case is a matter of adding a conditional statement to your accessor method: If the value at the given index is blank, then you raise the exception. By the way, you use the operator instead of the equality test operator ( ) to ensure that youâ€™re comparing the identities rather than values. Although the default implementation of the equality test in custom classes falls back to comparing the identities of their instances, most built-in data types distinguish between the two operators and implement them differently. Because you can now determine whether a given key has an associated value in your hash table, you might as well implement the operator to mimic a Python dictionary. Remember to write and cover these test cases individually to respect test-driven development principles: Both test cases take advantage of the test fixture that you prepared earlier and verify the special method, which you can implement in the following way: When accessing the given key raises a , you intercept that exception and return to indicate a missing key. Otherwise, you combine the â€¦ block with an clause and return . Exception handling is great but can sometimes be inconvenient, which is why lets you specify an optional default value. You can replicate the same behavior in your custom hash table: The code of looks similar to the special method youâ€™ve just implemented: You attempt to return the value corresponding to the provided key. In case of an exception, you return the default value, which is an optional argument. When the user leaves it unspecified, then it equals . For completeness, youâ€™ll add the capability to delete a key-value pair from your hash table in the upcoming subsection. Python dictionaries let you delete previously inserted key-value pairs using the built-in keyword, which removes information about both the key and the value. Hereâ€™s how this could work with your hash table: First, you verify if the sample hash table has the desired key and value. Then, you delete both by indicating only the key and repeat the assertions but with inverted expectations. You can make this test pass as follows: You calculate the index associated with a key and remove the corresponding value from the list unconditionally. However, you immediately remember your mental note from earlier about asserting that your hash table should not shrink when you delete elements from it. So, you add two more assertions: These will ensure that the size of your hash tableâ€™s underlying list remains unaffected. Now, you need to update your code so that it marks a slot as blank instead of throwing it away completely: Considering that youâ€™re in the green phase again, you can take this opportunity to spend some time refactoring. The same index formula appears three times in different places. You can extract it and simplify the code: Suddenly, after applying only this slight modification, a pattern emerges. Deleting an item is equivalent to inserting a blank object. So, you can rewrite your deletion routine to take advantage of the mutator method: Assigning a value through the square brackets syntax delegates to the method. All right, thatâ€™s enough refactoring for now. Itâ€™s much more important to think of other test cases at this point. For example, what happens when you request to delete a missing key? Pythonâ€™s raises a exception in such a case, so you can do the same: Covering this test case is relatively straightforward as you can rely on the code that you wrote when implementing the operator: If you find the key in your hash table, then you overwrite the associated value with the sentinel value to remove that pair. Otherwise, you raise an exception. All right, thereâ€™s one more basic hash table operation to cover, which youâ€™ll do next. The insertion method should already take care of updating a key-value pair, so youâ€™re only going to add a relevant test case and check if it works as expected: After modifying the value, hello, of an existing key and changing it to hallo, you also check if other key-value pairs, as well as the hash tableâ€™s length, remain untouched. Thatâ€™s it. You already have a basic hash table implementation, but a few extra features that are relatively cheap to implement are still missing. Itâ€™s time to address the elephant in the room. Python dictionaries let you iterate over their keys, values, or key-value pairs called items. However, your hash table is really only a list of values with fancy indexing on top of it. If you ever wanted to retrieve the original keys put into your hash table, then youâ€™d be out of luck because the current hash table implementation wonâ€™t ever remember them. In this subsection, youâ€™ll refactor your hash table heavily to add the ability to retain keys and values. Bear in mind that there will be several steps involved, and many tests will start failing as a result of that. If youâ€™d like to skip those intermediate steps and see the effect, then jump ahead to defensive copying. Wait a minute. You keep reading about key-value pairs in this tutorial, so why not replace values with tuples? After all, tuples are straightforward in Python. Even better, you could use named tuples to take advantage of their named element lookup. But first, you need to think of a test. Note: Remember to focus on the high-level user-facing functionality when figuring out a test case. Donâ€™t go about testing a piece of code that you may anticipate based on your programmerâ€™s experience or gut feeling. Itâ€™s the tests that should ultimately drive your implementation in TDD, not the other way around. First of all, youâ€™re going to need another attribute in your class to hold the key-value pairs: The order of key-value pairs is unimportant at this point, so you can assume that they might come out in any order each time you request them. However, instead of introducing another field to the class, you could reuse by renaming it to and making other necessary adjustments. There are a few. Just be aware that this will temporarily make some tests fail until you fix the implementation. Note: If youâ€™re using a code editor, then you can conveniently rename a variable or a class member with a single click of a button by leveraging the refactoring capabilities. In PyCharm, for example, you can right-click on a variable, choose Refactor from the context menu, and then Renameâ€¦. Or you can use the corresponding keyboard shortcut: Thatâ€™s the most straightforward and reliable way of changing the name of a variable in your project. The code editor will update all variable references across your project files. When youâ€™ve renamed to in and , then youâ€™ll also need to update the special method. In particular, it should store tuples of the key and the associated value now: Inserting an element in your hash table wraps the key and the value in a tuple and then puts that tuple at the desired index in your list of pairs. Note that your list will initially contain only blank objects instead of tuples, so youâ€™ll be using two different data types in your list of pairs. One is a tuple, while the other one could be anything but a tuple to denote a blank slot. Because of that, you donâ€™t need any special sentinel constant anymore to mark a slot as empty. You can safely remove your constant and replace it with the plain again where necessary, so go ahead and do that now. Note: Removing code may be hard to accept at first, but less is better! As you can see, test-driven development can sometimes make you run in circles. You can take another step back to regain control over deleting an item: Unfortunately, your method can no longer take advantage of the square brackets syntax because this would result in wrapping whatever sentinel value you chose in an unnecessary tuple. You must use an explicit assignment statement here to avoid needlessly complicated logic down the road. The last important piece of the puzzle is updating the method: You peek at an index, expecting to find a key-value pair. If you get nothing at all, then you raise an exception. On the other hand, if you see something interesting, then you grab the tupleâ€™s second element at index one, which corresponds to the mapped value. However, you can write this more elegantly using a named tuple, as suggested earlier: The class consists of the and attributes, which are free to take values of any data type. Simultaneously, your class inherits all the regular tupleâ€™s behaviors because it extends the parent. Note that you have to explicitly call on your key and value in the method to take advantage of the named attribute access in . Note: Despite using a custom data type to represent key-value pairs, you can write your tests to expect either instances or regular tuples, thanks to the compatibility of both types. Naturally, you have a few test cases to update at this point before the report can turn green again. Take your time and carefully review your test suite. Alternatively, look at the code from the supporting materials if you feel stuck, or take a peek here: There will be another test case that needs special care. Specifically, itâ€™s about verifying that an empty hash table has no values when created. Youâ€™ve just replaced a list of values with a list of pairs. To fish the values out again, you can use a list comprehension such as this: If youâ€™re concerned about stuffing too much logic into the test case, then youâ€™d be absolutely right. After all, you want to test the hash tableâ€™s behavior. But donâ€™t worry about that just yet. Youâ€™ll revisit this test case shortly. Once youâ€™re back in the green phase, try to figure out possible corner cases. For example, are exposed as a public attribute that anyone could intentionally or unintentionally tamper with. In practice, accessor methods should never leak your internal implementation but should make defensive copies to protect mutable attributes from external modifications: Whenever you request the key-value pairs from a hash table, you expect to get a brand-new object with a unique identity. You can hide a private field behind a Python property, so create one and replace every reference to with in your class only. The leading underscore is a standard naming convention in Python that indicates internal implementation: When you request a list of key-value pairs stored in your hash table, youâ€™ll get their shallow copy each time. Because you wonâ€™t have a reference to your hash tableâ€™s internal state, itâ€™ll remain unaffected by potential changes to its copy. Note: The order of class methods that you arrived at might slightly differ from that in the code block presented above. Thatâ€™s okay because method ordering doesnâ€™t matter from Pythonâ€™s standpoint. However, itâ€™s customary to start with static or class methods, followed by your classâ€™s public interface, which youâ€™re most likely to look at. The internal implementation should typically appear at the very end. To avoid having to jump around your code, itâ€™s a good idea to organize methods in a way that resembles a story. Specifically, a higher-level function should be listed before the low-level functions that are called. To further mimic in your property, the resulting list of pairs shouldnâ€™t include blank slots. In other words, there shouldnâ€™t be any values in that list: To satisfy this test, you can filter empty values out by adding a condition to the list comprehension in your property: You donâ€™t need an explicit call to because the list comprehension creates a new list. For every pair in the original list of the key-value pairs, you check if that particular pair is truthy and keep it in the resulting list. However, this will break two other tests that you need to update now: Thatâ€™s not ideal because one of your tests reaches out for the internal implementation instead of focusing on public interfaces. Nevertheless, such testing is known as white-box testing, which has its place. Get the Keys and Values Do you remember the test case that you modified by adding a list comprehension to retrieve the values from your key-value pairs? Well, here it is again if you need to refresh your memory: The highlighted line looks just like what youâ€™d need to implement the property, which you replaced with earlier. You may update the test function to take advantage of again: It might feel as though it was a wasted effort. But, these values are now evaluated dynamically through a getter property, whereas they were stored in a fixed-size list before. To satisfy this test, you can reuse part of its old implementation, which employs a list comprehension: Notice that you no longer have to specify the optional filtering condition here, because thereâ€™s already one lurking behind the property. Purists might think of using a set comprehension instead of a list comprehension to communicate the lack of guarantees for the order of values. However, that would result in losing information about duplicate values in the hash table. Protect yourself from such a possibility up front by writing another test case: If you have a hash table with names and ages, for example, and more than one person has the same age, then should keep all repeated age values. You can sort the ages to ensure repeatable test runs. While this test case doesnâ€™t require writing new code, itâ€™ll guard against regressions. Itâ€™s worthwhile to check the expected values, their types, and their number. However, you canâ€™t compare two lists directly because the actual values in a hash table might appear in an unpredictable order. To disregard the order in your test, you could convert both lists to sets or sort them like before. Unfortunately, sets remove potential duplicates, while sorting isnâ€™t possible when lists contain incompatible types. To reliably check if two lists have exactly the same elements of arbitrary types with potential duplicates while ignoring their order, you might use the following Python idiom: It leverages the built-in function, but itâ€™s quite verbose. Youâ€™re probably better off using the plugin. Donâ€™t forget to install it into your virtual environment first: Next, import the function into your test suite and use it to wrap the hash tableâ€™s values: Doing so converts the values to an unordered list, which redefines the equality test operator so that it doesnâ€™t take order into account when comparing list elements. Additionally, values of an empty hash table should be an empty list, while the property should always return a new list copy: On the other hand, the hash tableâ€™s keys must be unique, so it makes sense to emphasize that by returning a set rather than a list of keys. After all, sets are by definition unordered collections of items with no duplicates: Thereâ€™s no empty set literal in Python, so you have to call the built-in function directly in this case. The implementation of the corresponding getter function will look familiar: It resembles the property. The difference is that you use curly brackets instead of square brackets and refer to the attribute instead of in your named tuple. Alternatively, you could use if you wanted to, but it would seem less readable. This also reminds you about the need for a similar test case that you missed when covering the attribute. It makes sense to return a set of pairs for the sake of consistency: So, the property will also use a set comprehension from now on: You donâ€™t need to worry about losing any information, because each key-value pair is unique. At this point, you should be in the green phase again. Note that you can take advantage of the property to convert your hash table to a plain old dictionary and use and to test that: To disregard the order of elements, remember to wrap the dictionary keys and key-value pairs with sets before making the comparison. In contrast, your hash tableâ€™s values come out as a list, so be sure to use the function to compare lists while ignoring the element order. Okay, your hash table is really beginning to take shape now! Thereâ€™s one small detail that you intentionally left broken for simplicity until now. Itâ€™s the length of your hash table, which currently reports its maximum capacity even when there are only empty slots. Fortunately, it doesnâ€™t take much effort to fix this. Find your function named , rename as shown below, and check if an empty hash tableâ€™s length is equal to zero rather than one hundred: To make the capacity independent from the length, modify your special method so that it refers to the public property with filtered pairs instead of the private list of all slots: You just removed the leading underscore from the variable name. But that small change is now causing a whole bunch of tests to end abruptly with an error and a few to fail. Note: Failed tests are less severe because their assertions evaluate to , while errors indicate utterly unexpected behavior in your code. It seems that most test cases suffer from the same unhandled exception due to division by zero when mapping the key to an index. That makes sense because uses the hash tableâ€™s length to find the remainder from dividing the hashed key by the number of slots available. However, the hash tableâ€™s length has a different meaning now. You need to take the length of the internal list instead: Thatâ€™s much better now. The three test cases that still fail use wrong assumptions about the hash tableâ€™s length. Change those assumptions to make the tests pass: Youâ€™re back in the game, but the was a red flag that should immediately make you want to add additional test cases: Creating a with a non-positive capacity doesnâ€™t make much sense. How could you have a container with a negative length? So you should raise an exception if someone were to attempt that, either on purpose or by accident. The standard way to indicate such incorrect input arguments in Python is by raising a exception: If youâ€™re diligent, then you should probably also check for invalid argument types, but thatâ€™s beyond the scope of this tutorial. You can treat it as a voluntary exercise. Next, add another scenario for testing the length of a non-empty hash table provided as a fixture by pytest: There are three key-value pairs, so the length of the hash table should also be three. This test shouldnâ€™t require any additional code. Finally, because youâ€™re in the refactoring phase now, you can add a bit of syntactic sugar by introducing the property and using it where possible: The capacity is constant and determined at the hash tableâ€™s creation time. You can derive it from the length of the underlying list of pairs: As you introduce new vocabulary to your problem domain, it helps you discover new opportunities for more accurate, unambiguous naming. For example, youâ€™ve seen the word pairs used interchangeably to refer to both the actual key-value pairs stored in your hash table and an internal list of slots for pairs. It might be a good opportunity to refactor your code by changing to everywhere. Then, one of your earlier test cases will communicate its intent more clearly: Perhaps it would make even more sense to rename this test by replacing the word value with pair in it: You might think that such philosophical musings arenâ€™t necessary. However, the more descriptive your names, the more readable your code will beâ€”if not for others, then certainly for you in the future. There are even books and jokes about it. Your tests are a form of documentation, so it pays off to maintain the same level of attention to detail for them as for your code under test. Python lets you iterate over a dictionary through the keys, the values, or the key-value pairs known as items. You want the same behavior in your custom hash table, so you start by sketching out a few test cases: Iteration by keys, values, or key-value pairs works out of the box with the current implementation because the underlying sets and lists can already handle that. On the other hand, to make instances of your class iterable, you must define another special method, which will let you cooperate directly with loops: Unlike before, you hand over a reference to the instance here, but the behavior is the same as if you were iterating over the property. This behavior is compatible with the built-in in Python. The special method you need, , must return an iterator object, which the loop uses internally: This is an example of a generator iterator, which takes advantage of the yield keyword in Python. Note: The expression delegates the iteration to a sub-generator, which can be another iterable object, such as a list or a set. The keyword lets you define an in-place iterator using a functional style without creating another class. The special method named gets called by the loop when it starts. Okay, only a few nonessential features are missing from your hash table at this point. The next bit that youâ€™re going to implement in this section will make your hash table look pleasant when printed onto the standard output. The textual representation of your hash table will look similar to a Python literal: The literal uses curly braces, commas, and colons, while keys and values have their respective representations. For example, strings are enclosed in single apostrophes. As you donâ€™t know the exact order of the key-value pairs, you check if the string representation of your hash table conforms to one of the possible pair permutations. To make your class work with the built-in function, you must implement the corresponding special method in : You iterate over keys and values through the property and use f-strings to format the individual pairs. Notice the conversion flag in the template string, which enforces calling instead of the default on keys and values. This ensures more explicit representation, which varies between data types. For example, it wraps strings in single apostrophes. The difference between and is subtler. In general, both are meant for converting objects to strings. However, while you can expect to return human-friendly text, often returns a valid piece of Python code that you can evaluate to recreate the original object: In this example, the string representation of a Python fraction is , but the canonical string representation of the same object represents a call to the class. You can achieve a similar effect in your class. Unfortunately, your class initializer doesnâ€™t allow for creating new instances out of values at the moment. Youâ€™ll address this by introducing a class method thatâ€™ll let you create instances from Python dictionaries: This plays nicely with your earlier conversion, which was in the opposite direction. However, youâ€™ll need to assume a sufficiently large capacity for the hash table to hold all the key-value pairs from the original dictionary. A reasonable estimate seems to be ten times the number of pairs. You can hard-code it for now: You create a new hash table and set its capacity using an arbitrary factor. Then, you insert key-value pairs by copying them from the dictionary passed as an argument to the method. You can allow for overriding the default capacity if you want to, so add a similar test case: To make the capacity optional, you may take advantage of the short-circuit evaluation of Boolean expressions: If the isnâ€™t specified, then you fall back to the default behavior, which multiplies the dictionaryâ€™s length by ten. With this, youâ€™re finally able to provide a canonical string representation for your instances: As before, you check for all the possible attribute order permutations in the resulting representation. The canonical string representation of a hash table looks mostly the same as with , but it also wraps the dict literal in a call to your new class method. The corresponding implementation delegates to the special method by calling the built-in function: Note that you donâ€™t hard-code the class name, to avoid issues if you ever choose to rename your class at a later time. Your hash table prototype is almost ready, as youâ€™ve implemented nearly all core and nonessential features from the list mentioned in the introduction to this section. Youâ€™ve just added the ability to create a hash table from a Python and provided string representations for its instances. The last and final bit is ensuring that hash table instances can be compared by value. Hash tables are like sets in the sense that they donâ€™t impose any specific order for their contents. In fact, hash tables and sets have more in common than you might think, as theyâ€™re both backed by a hash function. Itâ€™s the hash function that makes the key-value pairs in a hash table unordered. However, remember that starting from Python 3.6, does indeed retain insertion order as an implementation detail. Two hash tables should compare equal if and only if they have the same set of key-value pairs. However, Python compares object identities by default because it doesnâ€™t know how to interpret values of custom data types. So, two instances of your hash table would always compare unequal even if they shared the same set of key-value pairs. To fix this, you can implement the equality test operator ( ) by providing the special method in your class. Additionally, Python will call this method for you to evaluate the not equal operator ( ) unless you also implement explicitly. You want the hash table to be equal to itself, its copy, or another instance with the same key-value pairs regardless of their order. Conversely, a hash table should not be equal to an instance with a different set of key-value pairs or a completely different data type: You use , introduced in the previous subsection, to quickly populate new hash tables with values. You can take advantage of the same class method to make a new copy of a hash table instance. Hereâ€™s the code that satisfies these test cases: The special method takes some object to compare as an argument. If that object happens to be the current instance of , then you return because the same identity implies value equality. Otherwise, you proceed by comparing the types and the sets of key-value pairs. The conversion to sets helps make the ordering irrelevant even if you decide to turn into another ordered list in the future. By the way, the resulting copy should have not only the same key-value pairs but also the same capacity as the source hash table. At the same time, two hash tables with different capacities should still compare equal: These two tests will work with your current implementation, so you donâ€™t need to code anything extra. Your custom hash table prototype is still missing a couple of nonessential features that the built-in dictionary provides. You may try adding them yourself as an exercise. For example, you could replicate other methods from Python dictionaries, such as or . Other than that, you might implement one of the bitwise operators supported by since Python 3.9, which allow for a union operation. Well done! This is the finished test suite for the tutorial, and your hash table has passed all unit tests. Give yourself a well-deserved pat on the back because that was a heck of a great job. Or was it? Suppose you reduce the capacity of your hash table to only account for the inserted pairs. The following hash table should accommodate all three key-value pairs stored in the source dictionary: However, when you reveal the keys and values of the resulting hash table, youâ€™ll sometimes find that there are fewer items. To make this code snippet repeatable, run it with hash randomization disabled by setting the environment variable to . More often than not, youâ€™ll end up losing information even though thereâ€™s enough space available in the hash table. Thatâ€™s because most hash functions arenâ€™t perfect, causing hash collisions, which youâ€™ll learn how to resolve next.\n\nIn this section, youâ€™ll explore the two most common strategies for dealing with hash code collisions, and youâ€™ll implement them in your class. If youâ€™d like to follow the examples below, then donâ€™t forget to set the variable to to disable Pythonâ€™s hash randomization. By now, you should already have a good idea of what a hash collision is. Itâ€™s when two keys produce identical hash code, leading distinct values to collide with each other at the same index in the hash table. For example, in a hash table with one hundred slots, the keys and happen to share the same index when hash randomization is disabled: As of now, your hash table canâ€™t detect attempts at storing different values in the same slot: Not only do you end up overwriting the pair identified by the key, but even worse, retrieving the value of this now-nonexistent key gives you an incorrect answer! There are three strategies available to you for addressing hash collisions: Choose a perfect hash function to avoid hash collisions in the first place. Spread the collided values in a predictable way that lets you retrieve them later. Keep the collided values in a separate data structure to search through. While perfect hashing is only possible when you know all the values up front, the other two hash collision resolution methods are more practical, so youâ€™ll take a closer look at them in this tutorial. Note that open addressing can be represented by several specific algorithms, including: In contrast, closed addressing is best known for separate chaining. Additionally, thereâ€™s also coalesced hashing, which combines the ideas behind both open and closed addressing into one algorithm. To follow test-driven development, youâ€™re going to need to design a test case first. But how do you test a hash collision? Pythonâ€™s built-in function uses hash randomization for some of its data types by default, which makes predicting its behavior extremely difficult. Choosing a hash seed manually with the environment variable would be impractical and make your test cases fragile. The best way to work around this is using a mocking library, such as Pythonâ€™s : Patching temporarily replaces one object with another. For example, you can substitute the built-in function with a fake one that always returns the same expected value, making your tests repeatable. This substitution only has an effect during the function call, after which the original is brought back again. You can apply the decorator against your whole test function or limit the mock objectâ€™s scope with a context manager: With a context manager, you can access the built-in function as well as its mocked version within the same test case. You could even have multiple flavors of the mocked function if you wanted to. The parameter lets you specify an exception to raise or a sequence of values that your mock object will return on consecutive invocations. In the remaining part of this tutorial, youâ€™ll continue adding more features to your class without strictly following test-driven development. New tests will be omitted for brevity, while modifying the class will cause some of the existing test to fail. However, youâ€™ll find a working test suite in the accompanying materials available for download. Stop for a moment to understand the theory behind hash code collisions. When it comes to handling them, linear probing is one of the oldest, most straightforward, and most effective techniques, all at the same time. It requires a few extra steps for the insert, lookup, delete, and update operations, which youâ€™re about to learn. Consider a sample hash table representing the Python glossary with common acronyms. It has a capacity of ten slots in total, but four of them are already taken by the following key-value pairs: Now you want to put another term into your glossary to define the MRO acronym, which stands for method resolution order. You calculate the hash code of the key and truncate it with the modulo operator to get an index between zero and nine: Instead of using the modulo operator, you could alternatively truncate the hash code with a proper bitmask, which is how Pythonâ€™s works internally. Note: To get consistent hash codes, set the environment variable to to disable hash randomization. Great! Thereâ€™s a free slot in your hash table at index five, where you can insert a new key-value pair: So far, so good. Your hash table is still 50 percent free space, so you keep adding more terms until you try inserting the EAFP acronym. It turns out the hash code of EAFP truncates to one, which is the index of a slot occupied by the BDFL term: The likelihood of hashing two different keys into colliding hash codes is relatively small. However, projecting those hash codes onto a small range of array indices is a different story. With linear probing, you can detect and mitigate such collisions by storing the collided key-value pairs next to each other: Easier To Ask For Forgiveness Than Permission Although BDFL and EAFP keys give the same index equal to one, only the first inserted key-value pair winds up taking it. Whichever pair comes second will be placed next to the occupied index. Therefore, linear probing makes the hash table sensitive to the insertion order. Note: When you use linear probing or other hash collision resolution methods, then you canâ€™t rely merely on the hash code to find the corresponding slot. You also need to compare the keys. Consider adding another acronym, ABC, for abstract base classes, whose hash code truncates to index eight. You canâ€™t insert it in the following position this time because itâ€™s already taken by WSGI. Under normal circumstances, youâ€™d continue looking for a free slot further down, but because you reached the last index, you must wrap around and insert the new acronym at index zero: Easier To Ask For Forgiveness Than Permission To search for key-value pairs stuffed into the hash table like this, follow a similar algorithm. Start by looking at the expected index first. For example, to find the value associated with the ABC key, calculate its hash code and map it to an index: Thereâ€™s a key-value pair stored at index eight, but it has a different key equal to PEP, so you skip it by increasing the index. Again, that slot is occupied by an unrelated term, WSGI, so you bounce off and wrap around to finally find your pair with a matching key at index zero. Thatâ€™s your answer. In general, there are three possible stopping conditions for the search operation:\nâ€¢ You exhausted all slots without finding the matching key.\nâ€¢ You found an empty slot, which also indicates a missing key. The last point makes deleting an existing key-value pair more tricky. If you just removed an entry from the hash table, then youâ€™d introduce a blank slot, which would stop the lookup there regardless of any previous collisions. To make the collided key-value pairs reachable again, youâ€™d have to either rehash them or use a lazy deletion strategy. The latter is less difficult to implement but has the additional cost of increasing the number of necessary lookup steps. Essentially, rather than deleting a key-value pair, you replace it with a sentinel value, depicted by a red cross (âŒ) below, which makes finding entries that had previously collided possible. Say you wanted to delete the BDFL and PEP terms: Easier To Ask For Forgiveness Than Permission Youâ€™ve replaced the corresponding key-value pairs with two instances of the sentinel value. Later, when you look for the ABC key, for example, you bounce off the sentinel at index eight, then continue to WSGI, and finally arrive at index zero with the matching key. Without one of the sentinels, youâ€™d stop the search much earlier, falsely concluding thereâ€™s no such key. Note: Your hash tableâ€™s capacity remains unaffected because youâ€™re free to overwrite sentinels when inserting new key-value pairs. On the other hand, if you were to fill up the hash table and delete most of its elements, then youâ€™d practically end up with the linear search algorithm. So far, youâ€™ve learned about insertion, deletion, and lookup. However, thereâ€™s one catch about updating the value of an existing entry in a hash table with linear probing. When searching for a pair to update, you should only skip the slot if itâ€™s occupied by another pair with a different key or if it contains a sentinel value. On the other hand, if the slot is empty or has a matching key, then you should set the new value. In the next subsection, youâ€™re going to modify your class to use linear probing for hash collision resolution. Use Linear Probing in the Class After a brief detour into linear probing theory, youâ€™re back to coding now. Because linear probing will be used in all four basic CRUD operations in the hash table, it helps to write a helper method in your class to encapsulate the logic of visiting the hash tableâ€™s slots: Given a key, you start by using the corresponding hash code to find its expected index in the hash table. Then, you loop through all the available slots in the hash table, starting from the calculated index. At each step, you return the current index and the associated pair, which might be empty or marked as deleted. Afterward, you increase the index, wrapping around the origin if necessary. Next, you can rewrite your method. Donâ€™t forget about the need for a new sentinel value. This value will let you differentiate between slots that have never been occupied and those that had collided before but are now deleted: If the slot is empty or contains a pair with the matching key, then you reassign a new key-value pair at the current index and stop the linear probing. Otherwise, if another pair occupies that slot and has a different key, or the slot is marked as deleted, then you move forward until you find a free slot or exhaust all the available slots. If you run out of available slots, you raise a exception to indicate the hash tableâ€™s insufficient capacity. Note: Coincidentally, the method also covers updating the value of an existing pair. Because pairs are represented by immutable tuples, you replace the entire pair with the matching key and not just its value component. Getting and deleting key-value pairs from a hash table using linear probing works almost the same way: The only difference is in the highlighted lines. To delete a pair, you must know where itâ€™s located in the hash table to replace it with the sentinel value. On the other hand, youâ€™re only interested in the corresponding value when searching by key. If this code duplication bothers you, then you can try refactoring it as an exercise. However, having it written explicitly helps make the point. Note: This is a textbook implementation of the hash table, which probes elements by comparing keys using the equality test operator ( ). However, itâ€™s a potentially costly operation, which real-life implementations avoid by storing the hash code, along with keys and values, in triplets rather than pairs. Hash codes are cheap to compare, on the other hand. You can leverage the hash-equal contract to speed things up. If two hash codes are different, then theyâ€™re guaranteed to stem from different keys, so thereâ€™s no need to perform a costly equality test in the first place. This trick dramatically reduces the number of key comparisons. Thereâ€™s yet another important detail to note. The hash tableâ€™s slots can no longer be in one of only two statesâ€”that is, empty or occupied. Inserting the sentinel value into the hash table to mark a slot as deleted messes up the hash tableâ€™s , , and properties and reports the length incorrectly. To fix this, you have to filter out both and values when returning key-value pairs: With that little update, your hash table should now be able to deal with hash collisions by spreading the collided pairs and looking for them in a linear fashion: Despite having the same hash code equal to twenty-four, both collided keys, and , appear next to each other on the list. Notice theyâ€™re listed in the same order in which you added them to the hash table. Try swapping the insertion order or changing the hash tableâ€™s capacity and observe how that affects the slots. As of now, the hash tableâ€™s capacity remains fixed. Before implementing linear probing, you remained oblivious and kept overwriting collided values. Now, you can detect when thereâ€™s no more space in your hash table and raise a corresponding exception. However, wouldnâ€™t it be better to let the hash table dynamically scale its capacity as necessary? There are two different strategies when it comes to resizing a hash table. You can wait until the very last moment and only resize the hash table when it becomes full, or you can do so eagerly after reaching a certain threshold. Both ways have their pros and cons. The lazy strategy is arguably more straightforward to implement, so youâ€™ll take a closer look at it first. However, it can lead to more collisions and worse performance. The only time you absolutely have to increase the number of slots in your hash table is when the insertion of a new pair fails, raising the exception. Go ahead and replace the statement with a call to another helper method that youâ€™ll create, followed by a recursive call to through the square brackets syntax: When you determine that all slots are occupied, by either a legitimate pair or the sentinel value, you must allocate more memory, copy the existing pairs, and try inserting that new key-value pair again. Note: Putting your old key-value pairs into a bigger hash table will make them hash to entirely different slots. Rehashing takes advantage of the extra slots that you just created, reducing the number of collisions in the new hash table. Moreover, it saves space by reclaiming slots that used to be marked as deleted with the sentinel value. You donâ€™t need to worry about past collisions, because the key-value pairs will find new slots anyway. Now, implement resizing and rehashing in the following way: Create a local copy of the hash table. Because itâ€™s difficult to predict how many more slots you may need, take a wild guess and double the capacity size. Then, iterate over your existing set of key-value pairs and insert them into the copy. Finally, reassign the attribute in your instance so that it points to the enlarged list of slots. Your hash table can now dynamically increase its size when needed, so give it a spin. Create an empty hash table with a capacity of one and try inserting some key-value pairs into it: In successfully inserting twenty key-value pairs, you never got any errors. With this rough depiction, you can clearly see the doubling of slots, which takes place when the hash table becomes full and needs more slots. Thanks to the automatic resizing that youâ€™ve just implemented, you can assume a default capacity for new hash tables. This way, creating an instance of the class will no longer require specifying the initial capacity, although doing so could improve performance. A common choice for the initial capacity is a small power of two, such as eight: That makes it possible to create hash tables with a call to the parameterless initializer . Note that you can also update your class method to use the dictionaryâ€™s length as the initial capacity. Previously, you multiplied the dictionaryâ€™s length by an arbitrary factor, which was necessary to make your tests pass, due to unhandled hash collisions. As stated before, thereâ€™s one problem with the lazy resizing strategy, and thatâ€™s the increased likelihood of collisions. Youâ€™re going to address that next. Waiting until your hash table becomes saturated isnâ€™t optimal. You can try an eager strategy to resize the hash table before reaching its total capacity, keeping the collisions from happening in the first place. How do you decide on the best moment to resize and rehash? Use the load factor! The load factor is the ratio of the number of currently occupied slots, including the deleted ones, to all slots in the hash table. The higher the load factor, the bigger the chance of a hash collision, which results in worse lookup performance. Therefore, you want the load factor to remain relatively small at all times. Bumping up the hash tableâ€™s size is due whenever the load factor reaches a certain threshold. The choice of a particular threshold is a classic example of the spaceâ€“time trade-off in computer science. More frequent hash table resizing is cheaper and leads to better performance at the cost of more memory consumption. Conversely, waiting longer can save you some memory, but the key lookups will be slower. The chart below depicts the relationship between the amount of allocated memory and the average number of collisions: The data behind this chart measures the average number of collisions caused by inserting one hundred elements into an empty hash table with an initial capacity of one. The measurement was repeated many times for various load factor thresholds, at which the hash table resized itself in discrete jumps by doubling its capacity. The intersection of both plots, which appears at around 0.75, indicates the thresholdâ€™s sweet spot, with the lowest amount of memory and number of collisions. Using a higher load factor threshold doesnâ€™t provide significant memory savings, but it increases the number of collisions exponentially. A smaller threshold improves the performance but for a high price of mostly wasted memory. Remember that all you really need is one hundred slots! You can experiment with different load factor thresholds, but resizing the hash table when 60 percent of its slots are taken might be a good starting point. Hereâ€™s how you can implement the load factor calculation in your class: You start by filtering slots that are truthy, which would be anything but , and then take the ratio according to the load factorâ€™s definition. Note that if you decide to use a comprehension expression, then it must be a list comprehension to count all sentinel value occurrences. In this case, using a set comprehension would filter out the repeated markers of deleted pairs, leaving only one instance and resulting in a wrong load factor. Next, modify your class to accept an optional load factor threshold and use it to eagerly resize and rehash the slots: \"Load factor must be a number between (0, 1]\" The load factor threshold defaults to 0.6, which means 60 percent of all slots are occupied. You use a weak inequality ( ) instead of a strict one ( ) to account for the load factor threshold at its maximum value, which can never be greater than one. If the load factor equals one, then you must also resize the hash table before inserting another key-value pair. Brilliant! Your hash table has just become a bit faster. That concludes the open addressing example in this tutorial. Next up, youâ€™re going to resolve hash collisions using one of the most popular closed addressing techniques. Separate chaining is another extremely popular hash collision resolution method, perhaps even more widespread than linear probing. The idea is to group similar items by a common feature into so-called buckets to narrow down the search space. For example, you could imagine harvesting fruits and collecting them into color-coded baskets: Fruits Grouped by Color in Each Basket Each basket contains fruits of roughly the same color. So, when youâ€™re craving an apple, for example, you only need to search through the basket labeled with red. In an ideal world, each basket should contain no more than one element, making the search instantaneous. You can think of the labels as hash codes and the fruits with the same color as the collided key-value pairs. A hash table based on separate chaining is a list of references to buckets, typically implemented as linked lists that form chains of elements: Each linked list contains key-value pairs whose keys share the same hash code due to a collision. When looking for a value by key, you need to locate the right bucket first, then traverse it using linear search to find the matching key, and finally return the corresponding value. Linear search just means going through each item in the bucket, one by one, until you find the right key. Note: Linked listsâ€™ elements have a small memory overhead because every node contains a reference to the next element. At the same time, such a memory layout makes appending and removing elements very quick compared to a regular array. To adapt your class to use separate chaining, start by removing the method and replacing slots with buckets. Now, instead of having a value or a pair at each index, youâ€™ll make each index hold a bucket that might be empty or not. Each bucket will be a linked list: \"Load factor must be a number between (0, 1]\" Instead of implementing a linked list from scratch, you may take advantage of Pythonâ€™s double-ended queue, or deque, available in the module, which uses a doubly-linked list under the hood. It lets you append and remove elements more efficiently than a plain list. Donâ€™t forget to update the , , and properties so that they refer to buckets instead of slots: You no longer need the sentinel value to mark elements as deleted, so go ahead and remove the constant. This makes the key-value pairs and the load factorâ€™s definitions more straightforward. The capacity is synonymous with the number of buckets because you want to keep at most one key-value pair in each bucket, minimizing the number of hash code collisions. Note: The load factor defined like this can become greater than one when the number of key-value pairs stored in the hash table exceeds the number of buckets. By the way, allowing too many collisions will effectively degenerate your hash table into a flat list with linear time complexity, significantly degrading its performance. Attackers might take advantage of this fact by artificially creating as many collisions as possible. With separate chaining, all basic hash table operations boil down to finding the right bucket and searching through it, which makes the corresponding methods look similar: Deque instances take care of updating their internal references when you delete an item by index. If you had used a custom linked list, youâ€™d have to rewire the collided key-value pairs manually after each modification. As before, updating an existing key-value pair requires replacing the old one with a brand-new one because key-value pairs are immutable. If youâ€™d like to avoid repeating yourself, then try refactoring the three methods above using structural pattern matching, introduced in Python 3.10. Youâ€™ll find one possible solution in the accompanying materials. Okay, you know how to cope with hash code collisions, and youâ€™re now ready to move on. Next up, youâ€™ll make your hash table return keys and values in their insertion order.\n\nYour hash table is complete and fully functional now. It can map arbitrary keys to values using the built-in function. It can detect and resolve hash code collisions and even retain the insertion order of the key-value pairs. You could theoretically use it over Python if you wanted to, without noticing much difference apart from the poor performance and occasionally more verbose syntax. Note: As mentioned before, you should rarely need to implement a data structure such as a hash table yourself. Python comes with many useful collections that have unparalleled performance and are tested in the field by countless developers. For specialized data structures, you should check PyPI for third-party libraries before attempting to make one of your own. Youâ€™ll save yourself a lot of time and significantly reduce the risk of bugs. Until now, youâ€™ve taken it for granted that most built-in types in Python can work as hash table keys. However, to use any hash table implementation in practice, youâ€™ll have to restrict keys to only hashable types and understand the implications of doing so. Thatâ€™ll be especially helpful when you decide to bring custom data types into the equation. You learned earlier that some data types, including most primitive data types in Python, are hashable, while others arenâ€™t. The primary trait of hashability is the ability to calculate the hash code of a given object: For example, instances of the data type in Python are hashable, while ordinary sets donâ€™t implement hashing at all. Hashability directly impacts whether objects of certain types can become dictionary keys or set members, as both of these data structures use the function internally: While an instance of is hashable, the corresponding set with precisely the same values is not. Be aware that you can still use an unhashable data type for the dictionary values. Itâ€™s the dictionary keys that must be able to calculate their corresponding hash codes. Note: When you insert an unhashable object into a hashable container, such as a , then that container also becomes unhashable. Hashability is closely related to mutability, or the ability to change the internal state of an object during its lifetime. The relationship between the two is a bit like changing an address. When you move to another place, youâ€™re still the same person, but your old friends might have a hard time trying to find you. Unhashable types in Python, such as lists, sets, or dictionaries, happen to be mutable containers, as you can modify their values by adding or removing elements. On the other hand, most built-in hashable types in Python are immutable. Does this mean mutable types canâ€™t be hashable? The correct answer is they can be both mutable and hashable, but they rarely should be! Mutating a key would result in changing its memory address within a hash table. Consider this custom class as an example: This class represents a person with a name. Python provides a default implementation for the special method in your classes, which merely uses the objectâ€™s identity to derive its hash code: Each individual instance has a unique hash code even when itâ€™s logically equal to other instances. To make the objectâ€™s value determine its hash code, you can override the default implementation of like so: You call on the attribute so that instances of the class with equal names always have the same hash code. This is convenient for looking them up in a dictionary, for example. Note: You can explicitly mark your class as unhashable by setting its attribute equal to : This will prevent from working on instances of your class. Another trait of hashable types is the ability to compare their instances by value. Recall that a hash table compares keys with the equality test operator ( ), so you must implement another special method, , in your class to allow for that: This code fragment should look familiar if you went through the equality test of hash tables before. In a nutshell, you check if the other object is the exact same instance, an instance of another type, or another instance of the same type and equal value to the attribute. Note: Coding special methods such as and can be repetitive, tedious, and error-prone. If youâ€™re on Python 3.7 or above, then you can achieve the same effect more compactly by using data classes: While a data class generates based on your class attributes, you must set the option to enable the correct method generation. Having implemented and , you can use the class instances as dictionary keys: Perfect! It doesnâ€™t matter if you find an employee by the reference that you created earlier or a brand-new instance. Unfortunately, things get complicated when Bob suddenly decides to change his name and go by Bobby instead: File , line , in : File , line , in : File , line , in : You no longer have a way of retrieving the corresponding value even though you still use the original key object that you inserted before. Whatâ€™s more surprising, though, is that you canâ€™t access the value through a new key object with the updated personâ€™s name or with the old one. Can you tell why? The hash code of the original key determined which bucket the associated value got stored in. Mutating the state of your key made its hash code indicate a completely different bucket or slot, which doesnâ€™t contain the expected value. But using a key with the old name doesnâ€™t help either. While it points to the right bucket, the stored key has mutated, making equality comparison between and evaluate to rather than matching. Therefore, hash codes must be immutable for the hashing to work as expected. Since hash codes are typically derived from an objectâ€™s attributes, their state should be fixed and never change over time. In practice, this also means that objects intended as hash table keys should be immutable themselves. To sum up, a hashable data type has the following traits:\nâ€¢ Has a method to calculate the instanceâ€™s hash code\nâ€¢ Has an method to compare instances by value\nâ€¢ Has immutable hash codes, which donâ€™t change during instancesâ€™ lifetimes The fourth and final trait of hashable types is that they must comply with the hash-equal contract, which youâ€™ll learn more about in the following subsection. In short, objects with equal values must have identical hash codes. To avoid problems when using custom classes as hash table keys, they should comply with the hash-equal contract. If thereâ€™s one thing to remember about that contract, itâ€™s that when you implement , you should always implement a corresponding . The only time you donâ€™t have to implement both methods is when you use a wrapper such as a data class or an immutable named tuple that already does this for you. Also, not implementing both methods can be okay as long as youâ€™re absolutely sure that you wonâ€™t ever use objects of your data type as dictionary keys or set members. But can you be so sure? Note: If you canâ€™t use a data class or a named tuple, and youâ€™d like to manually compare and hash more than one field in a class, then wrap them in a tuple: It makes sense to define a private property to return that tuple if there are relatively many fields in your class. While you can implement both methods however you like, they must satisfy the hash-equal contract, which states that two equal objects must hash to equal hash codes. This makes it possible to find the right bucket based on a provided key. However, the reverse isnâ€™t true, because collisions may occasionally cause the same hash code to be shared by unequal values. You can express this more formally by using these two implications: The hash-equal contract is a one-way contract. If two keys are logically equal, then their hash codes must also be equal. On the other hand, if two keys share the same hash code, itâ€™s likely theyâ€™re the same key, but itâ€™s also possible that theyâ€™re different keys. You need to compare them to be sure if they really match. By the law of contraposition, you can derive another implication from the first one: If you know that two keys have different hash codes, then thereâ€™s no point in comparing them. That can help improve performance, as the equality test tends to be costly. Some IDEs offer to automatically generate the and methods for you, but you need to remember to regenerate them every time you modify your class attributes. Therefore, stick to Pythonâ€™s data classes or named tuples whenever you can to guarantee the proper implementation of hashable types."
    },
    {
        "link": "https://github.com/Devinterview-io/hash-table-data-structure-interview-questions/blob/main/README.md",
        "document": "A Hash Table, also known as a Hash Map, is a data structure that provides a mechanism to store and retrieve data based on key-value pairs.\n\nIt is an associative array abstract data type where the key is hashed, and the resulting hash value is used as an index to locate the corresponding value in a bucket or slot.\nâ€¢ Unique Keys: Each key in the hash table maps to a single value.\nâ€¢ Dynamic Sizing: Can adjust its size based on the number of elements.\nâ€¢ Fast Operations: Average time complexity for most operations is .\n\nThe hash function converts each key to a numerical hash value, which is then used to determine the storage location, or \"bucket.\"\n\nBuckets are containers within the hash table that hold the key-value pairs. The hash function aims to distribute keys uniformly across buckets to minimize collisions.\nâ€¢ Open-Addressing: Finds the next available bucket if a collision occurs.\nâ€¢ Chaining: Stores colliding keys in a linked list within the same bucket.\n\nHere is the Python code:\n\nHashing is a method that maps data to fixed-size values, known as hash values, for efficient storage and retrieval. A hash function generates these values, serving as the data's unique identifier or key.\nâ€¢ Data Integrity: A good hash function ensures even minor data changes will yield different hash values.\nâ€¢ Collision Management: While hash collisions can occur, they are manageable and typically rare.\n\nA hash function generates a fixed-size hash value, serving as the data's unique identifier or key for various applications like data indexing and integrity checks. Hash values are typically represented as hexadecimal numbers.\nâ€¢ Deterministic: The same input will always produce the same hash value.\nâ€¢ Fixed-Length Output: Outputs have a consistent length, making them suitable for data structures like hash tables.\nâ€¢ One-Way Functionality: Reconstructing the original input from the hash value should be computationally infeasible.\nâ€¢ Avalanche Effect: Small input changes should result in significantly different hash values.\nâ€¢ Collision Resistance: While it's difficult to completely avoid duplicate hash values for unique inputs, well-designed hash functions aim to minimize this risk.\n\nHere is the Python code:\n\nWhen discussing Hashing and Hash Tables, it's crucial to recognize that one is a technique while the other is a data structure. Here's a side-by-side comparison:\nâ€¢ Hashing: A computational technique that converts input (often a string) into a fixed-size value, referred to as a hash value.\nâ€¢ Hash Tables: A data structure that utilizes hash values as keys to efficiently store and retrieve data in memory.\nâ€¢ Hashing: To generate a unique identifier (hash value) for a given input, ensuring consistency and rapid computation.\nâ€¢ Hash Tables: To facilitate quick access to data by mapping it to a hash value, which determines its position in the table.\nâ€¢ Hashing: Common in cryptography for secure data transmission, data integrity checks, and digital signatures.\nâ€¢ Hash Tables: Used in programming and database systems to optimize data retrieval operations.\nâ€¢ Hashing: Focuses solely on deriving a hash value from input data.\nâ€¢ Hash Tables: Incorporates mechanisms to manage issues like hash collisions (when two distinct inputs produce the same hash value) and may also involve dynamic resizing to accommodate growing datasets.\nâ€¢ Hashing: Producing a hash value is typically an operation, given a consistent input size.\nâ€¢ Hash Tables: On average, operations such as data insertion, retrieval, and deletion have time complexity, although worst-case scenarios can degrade performance.\nâ€¢ Hashing: Transient in nature; once the hash value is generated, the original input data isn't inherently stored or preserved.\nâ€¢ Hash Tables: Persistent storage structure; retains both the input data and its corresponding hash value in the table.\n\nThe parity function can serve as a rudimentary example of a hash function. It takes a single input, usually an integer, and returns either or based on whether the input number is even or odd.\nâ€¢ Deterministic: Given the same number, the function will always return the same output.\nâ€¢ Efficient: Computing the parity of a number can be done in constant time.\nâ€¢ Small Output Space: The output is limited to two possible values, or .\nâ€¢ One-Way Function: Given one of the outputs ( or ), it is impossible to determine the original input.\n\nHere is the Python code:\n\nWhile the parity function serves as a simple example, real-world hash functions are much more complex. They often use cryptographic algorithms to provide a higher level of security and other features like the ones mentioned below:\nâ€¢ Cryptographic Hash Functions: Designed to be secure against various attacks like pre-image, second-pre-image, and collision.\nâ€¢ Non-Cryptographic Hash Functions: Used for general-purpose applications, data storage, and retrieval.\nâ€¢ Perfect Hash Functions: Provide a unique hash value for each distinct input.\n\nHash tables are at the core of many languages, such as Java, Python, and C++:\nâ€¢ Key Insights: Hash tables allow for fast data lookups based on key values. They use a technique called hashing to map keys to specific memory locations, enabling time complexity for key-based operations like insertion, deletion, and search.\nâ€¢ Key Mapping: This is often implemented through a hash function, a mathematical algorithm that converts keys into unique integers (hash values or hash codes) within a fixed range. The hash value then serves as an index to directly access the memory location where the key's associated value is stored.\nâ€¢ \nâ€¢ Challenges: Two different keys can potentially hash to the same index, causing what's known as a collision.\nâ€¢ Solutions:\nâ€¢ Separate Chaining: Affected keys and their values are stored in separate data structures, like linked lists or trees, associated with the index. The hash table then maps distinct hash values to each separate structure.\nâ€¢ Open Addressing: When a collision occurs, the table is probed to find an alternative location (or address) for the key using specific methods.\nâ€¢ \nâ€¢ Data Structure: Python uses an array of pointers to linked lists. Each linked list is called a \"bucket\" and contains keys that hash to the same index.\nâ€¢ Size Dynamics: Python uses dynamic resizing, and its load factor dictates when resizing occurs.\nâ€¢ \nâ€¢ Data Structure: The class uses an array of TreeNodes and LinkedLists, converting to trees after a certain threshold.\nâ€¢ Size Dynamics: Java also resizes dynamically, triggered when the number of elements exceeds a load factor.\nâ€¢ \nâ€¢ Data Structure: Starting from C++11, the standard library typically implements hash tables as resizable arrays of linked lists.\nâ€¢ Size Dynamics: It also resizes dynamically when the number of elements surpasses a certain load factor.\n\nDo note that C++ has several that may get used in hash tables, and it also uses separate chaining as a collision avoidance technique.\n\nWhile hash tables provide constant-time lookups in the average case, several factors can influence their performance:\nâ€¢ Hash Function Quality: A good hash function distributes keys more evenly, promoting better performance. Collisions, especially frequent or hard-to-resolve ones, can lead to performance drops.\nâ€¢ Load Factor: This is the ratio of elements to buckets. When it gets too high, the structure becomes less efficient, and resizing can be costly. Java and Python automatically manage load factors during resizing.\nâ€¢ Resizing Overhead: Periodic resizing (to manage the load factor) can pause lookups, leading to a one-time performance hit.\n\nIn hash functions and tables, a hash collision occurs when two distinct keys generate the same hash value or index. Efficiently addressing these collisions is crucial for maintaining the hash table's performance.\nâ€¢ Hash Function Limitations: No hash function is perfect; certain datasets may result in more collisions.\nâ€¢ Limited Hash Space: Due to the Pigeonhole Principle, if there are more unique keys than slots, collisions are inevitable.\nâ€¢ Direct: When two keys naturally map to the same index.\nâ€¢ Secondary: Arising during the resolution of a direct collision, often due to strategies like chaining or open addressing.\n\nThe likelihood of a collision is influenced by the hash function's design, the number of available buckets, and the table's load factor.\n\nWorst-case scenarios where all keys collide to the same index, can degrade a hash table's performance from to .\nâ€¢ Chaining: Each slot in the hash table contains a secondary data structure, like a linked list, to hold colliding keys.\nâ€¢ Open Addressing: The hash table looks for the next available slot to accommodate the colliding key.\nâ€¢ Cryptographic Hash Functions: These are primarily used to ensure data integrity and security due to their reduced collision probabilities. However, they're not commonly used for general hash tables because of their slower performance.\n\nConsider a hash table that employs chaining to resolve collisions:\n\nInserting a new key that hashes to index 3 causes a collision. With chaining, the new state becomes:\nâ€¢ Hash Collisions are Inevitable: Due to inherent mathematical and practical constraints.\nâ€¢ Strategies Matter: The efficiency of a hash table can be significantly influenced by the chosen collision resolution strategy.\nâ€¢ Probability Awareness: Being aware of collision probabilities is vital, especially in applications demanding high performance or security.\n\nIn hash tables, collisions occur when different keys yield the same index after being processed by the hash function. Let's look at common collision-handling techniques:\n\nHow it Works: Each slot in the table becomes the head of a linked list. When a collision occurs, the new key-value pair is appended to the list of that slot.\nâ€¢ Cache performance might not be optimal due to linked list traversal.\n\nHow it Works: If a collision occurs, the table is probed linearly (i.e., one slot at a time) until an empty slot is found.\nâ€¢ Clustering can occur, slowing down operations as the table fills up.\n\nHow it Works: Uses two hash functions. If the first one results in a collision, the second hash function determines the step size for probing.\n\nHere is the Python code:\n\nHere is the Python code:\n\nHere is the Python code:\n\nA good hash function is fundamental for efficient data management in hash tables or when employing techniques such as hash-based encryption. Here, let's go through what it means for a hash function to be high-quality.\nâ€¢ Deterministic: The function should consistently map the same input to the same hash value.\nâ€¢ Fast Performance: Ideally, the function should execute in time for typical use.\n\nCollision is the term when two different keys have the same hash value.\nâ€¢ Few collisions: A good hash function minimizes the number of collisions when it is possible.\nâ€¢ Uniformly Distributed Hash Values: Each output hash value or, in the context of a hash table, each storage location should be equally likely for the positive performance of the hash table.\nâ€¢ Security Considerations: In the context of cryptographic hash functions, the one-way nature is essential, meaning it is computationally infeasible to invert the hash value.\nâ€¢ Value Independence: Small changes in the input should result in substantially different hash values, also known as the avalanche effect.\n\nHere is the Python code:\n\nThis hash function sums up the ASCII values of the characters in to get its hash value. While this function is easy to implement and understand, it is not a good choice in practice as it may not meet the characteristics mentioned above.\n\nHere is the Python code:\n\nIn this example, the library allows us to implement a cryptographic hash function using the SHA-256 algorithm. This algorithm is characterized by high security and the one-way nature essential for securing sensitive data. \n\n\n\nSeparate Chaining is a collision resolution technique employed in Hash Tables. This method entails maintaining a list of entries, often implemented as a linked list, for each bucket.\nâ€¢ Effective Collision Handling: It provides a consistent utility irrespective of the number of keys hashed.\nâ€¢ Easy to Implement and Understand: This technique is relatively simple and intuitive to implement.\nâ€¢ Bucket Assignment: Each key hashes to a specific \"bucket,\" which could be a position in an array or a dedicated location in memory.\nâ€¢ Internal List Management: Keys within the same bucket are stored sequentially. Upon a collision, keys are appended to the appropriate bucket.\nâ€¢ Best-Case : The target key is the only entry in its bucket.\nâ€¢ Worst-Case : All keys in the table hash to the same bucket, and a linear search through the list is necessary.\n\nHere is the Python code:\n\nOpen Addressing is a collision resolution technique where a hash table dynamically looks for alternative slots to place a colliding element.\nâ€¢ Linear Probing: When a collision occurs, investigate cells in a consistent sequence. The operation is mathematically represented as:\n\nHere, is the key's hash value, is the table size, and iterates within the modulo operation.\nâ€¢ Quadratic Probing: The cells to search are determined by a quadratic function:\n\nPositive constants and are used as increment factors. If the table size is a prime number, these constants can equal 1 and 1, respectively. This scheme can still result in clustering.\nâ€¢ Double Hashing: Unlike with linear or quadratic probing, the second hash function computes the stride of the probe. The operation is:\n\nNote: It's crucial for the new probe sequence to cover all positions in the table, thereby ensuring that every slot has the same probability of being the data's final location. \n\n\n\nHash tables offer impressive time and space performance under most conditions. Here's a detailed breakdown:\nâ€¢ Best Case : With uniform distribution and no collisions, fundamental operations like lookup, insertion, and deletion are constant-time.\nâ€¢ Average and Amortized Case : Even with occasional collisions and rehashing, most operations typically remain constant-time. While rehashing can sometimes take , its infrequency across many operations ensures an overall constant-time complexity.\nâ€¢ Worst Case : Arises when all keys map to one bucket, forming a linked list. Such cases are rare and typically result from suboptimal hash functions or unique data distributions.\n\nThe primary storage revolves around the elements in the table. Additional overhead from the structure itself is minimal, ensuring an upper bound of . The \"load factor,\" indicating the ratio of stored elements to the table's capacity, can impact memory use. \n\n\n\nThe load factor is a key performance metric for hash tables, serving as a threshold for resizing the table. It balances time and space complexity by determining when the table is full enough to warrant expansion.\n\nThe load factor is calculated using the following formula:\n\nIt represents the ratio of stored elements to available buckets.\nâ€¢ Space Efficiency: It helps to minimize the table's size, reducing memory usage.\nâ€¢ Collision Management: A high load factor can result in more collisions, affecting performance.\nâ€¢ Resizing: The load factor is a trigger for resizing the table, which helps in redistributing elements.\nâ€¢ Standard Defaults: Libraries like Java's or Python's usually have well-chosen default load factors.\nâ€¢ Balanced Values: For most use-cases, a load factor between 0.5 and 0.75 strikes a good balance between time and space.\nâ€¢ Dynamic Adjustments: Some advanced hash tables, such as Cuckoo Hashing, may adapt the load factor for performance tuning.\n\nThe initial capacity is the starting number of buckets, usually a power of 2, while the load factor is a relative measure that triggers resizing. They serve different purposes in the operation and efficiency of hash tables. \n\n\n\nThe load factor is a key parameter that influences the performance and memory efficiency of a hashtable. It's a measure of how full the hashtable is and is calculated as:\n\nWhen the load factor exceeds a certain predetermined threshold, known as the rehashing or resizing threshold, the table undergoes a resizing operation. This triggers the following:\nâ€¢ Rehashing: Every key-value pair is reassigned to a new bucket, often using a new hash function.\nâ€¢ Reallocating memory: The internal data structure grows or shrinks to ensure a better balance between load factor and performance.\nâ€¢ Insertions: As the load factor increases, the number of insertions before rehashing decreases. Consequently, insertions can be faster with a smaller load factor, albeit with a compromise on memory efficiency.\nâ€¢ Retrievals: In a well-maintained hash table, retrievals are expected to be faster with a smaller load factor.\n\nHowever, these ideal situations might not hold in practice due to these factors:\nâ€¢ Cache Efficiency: A smaller load factor might result in better cache performance, ultimately leading to improved lookup speeds.\nâ€¢ Access Patterns: If insertions and deletions are frequent, a higher load factor might be preferable to avoid frequent rehashing, which can lead to performance overhead.\n\nHere is the Python code:\n\nResizing a hash table is essential to maintain efficiency as the number of elements in the table grows. Let's look at different strategies for resizing and understand their time complexities.\n\nResizing a hash table with separate chaining and open addressing entails different methods and time complexities:\nâ€¢ Separate Chaining: Direct and has a time complexity of for individual insertions.\nâ€¢ Open Addressing: Requires searching for a new insertion position, which can make the insertion in the worst-case scenario. For handling existing elements during resizing, the complexity is . Common heuristics and strategies, such as Lazy Deletion or Double Hashing, can help mitigate this complexity.\n\nArray resizing is the most common method due to its simplicity and efficiency. It achieves dynamic sizing by doubling the array size when it becomes too crowded and halving when occupancy falls below a certain threshold.\nâ€¢ Doubling Size: This action, also known as rehashing, takes time, where is the current number of elements. For each item currently in the table, the hash and placement in the new, larger table require time.\nâ€¢ Halving Size: The table needs rescaling when occupancy falls below a set threshold. The current table is traversed, and all elements are hashed for placement in a newly allocated, smaller table, taking time.\n\nHere is the Python code:\n\nThe efficiency of a hash table relies significantly on the hash function used. An inadequate hash function can lead to clustering, where multiple keys map to the same bucket, degrading performance to O(n), essentially turning the hash table into an unordered list.\n\nOn the other hand, a good hash function achieves a uniform distribution of keys, maximizing the O(1) operations. Achieving balance requires careful selection of the hash function and understanding its impact on performance.\nâ€¢ Uniformity: Allowing for an even distribution of keys across buckets.\nâ€¢ Consistency: Ensuring repeated calls with the same key return the same hash.\nâ€¢ Minimizing Collisions: A good function minimizes the chances of two keys producing the same hash.\nâ€¢ Identity Function: While simple, it doesn't support uniform distribution. It is most useful for testing or as a placeholder while the system is in development.\nâ€¢ Modulous Function: Useful for small tables and keys that already have a uniform distribution. Be cautious with such an implementation, especially with table resizing.\nâ€¢ Division Method: This hash function employs division to spread keys across defined buckets. The efficiency of this method can depend on the prime number that is used as the divisor.\nâ€¢ MurmurHash: A non-cryptographic immersion algorithm, known for its speed and high level of randomness, making it suitable for general-purpose hashing.\nâ€¢ MD5 and SHA Algorithms: While designed for cryptographic use, they still can be used in hashing where security is not a primary concern. However, they are slower than non-cryptographic functions for hashing purposes.\nâ€¢ Keep it Simple: Hash functions don't have to be overly complex. Sometimes, a straightforward approach suffices.\nâ€¢ Understand the Data: The nature of the data being hashed can often point to the most appropriate type of function.\nâ€¢ Protect Against Malicious Data: If your data source isn't entirely trustworthy, consider a more resilient hash function."
    },
    {
        "link": "https://geeksforgeeks.org/introduction-to-red-black-tree",
        "document": "Binary search trees are a fundamental data structure, but their performance can suffer if the tree becomes unbalanced. Red Black Trees are a type of balanced binary search tree that use a set of rules to maintain balance, ensuring logarithmic time complexity for operations like insertion, deletion, and searching, regardless of the initial shape of the tree. Red Black Trees are self-balancing, using a simple color-coding scheme to adjust the tree after each modification.\n\nA Red-Black Tree is a self-balancing binary search tree where each node has an additional attribute: a color, which can be either red or black. The primary objective of these trees is to maintain balance during insertions and deletions, ensuring efficient data retrieval and manipulation.\n\nA Red-Black Tree have the following properties:\nâ€¢ Node Color : Each node is either red or black\nâ€¢ Root Property : The root of the tree is always black\nâ€¢ Red Property : Red nodes cannot have red children (no two consecutive red nodes on any path).\nâ€¢ Black Property : Every path from a node to its descendant null nodes (leaves) has the same number of black\n\nThese properties ensure that the longest path from the root to any leaf is no more than twice as long as the shortest path, maintaining the treeâ€™s balance and efficient performance.\n\nThe Correct Red-Black Tree in above image ensures that every path from the root to a leaf node has the same number of black nodes. In this case,â€‹ there is one (excluding the root node).\n\nThe Incorrect Red Black Tree does not follow the red-black properties as two red nodes are adjacent to each other. Another problem is that one of the paths to a leaf node has zero black nodes, whereas the other two contain a black node.\n\nMost of the BST operations (e.g., search, max, min, insert, delete.. etc) take O(h) time where h is the height of the BST. The cost of these operations may become O(n) for a skewed Binary tree. If we make sure that the height of the tree remains O(log n) after every insertion and deletion, then we can guarantee an upper bound of O(log n) for all these operations. The height of a Red-Black tree is always O(log n) where n is the number of nodes in the tree.\n\nThe AVL trees are more balanced compared to Red-Black Trees, but they may cause more rotations during insertion and deletion. So if your application involves frequent insertions and deletions, then Red-Black trees should be preferred. And if the insertions and deletions are less frequent and search is a more frequent operation, then AVL tree should be preferred over the Red-Black Tree.\n\nA simple example to understand balancing is, that a chain of 3 nodes is not possible in the Red-Black tree. We can try any combination of colors and see if all of them violate the Red-Black tree property.\nâ€¢ black height of the red-black tree is the number of black nodes on a path from the root node to a leaf node. Leaf nodes are also counted as black h black height >= h/2\nâ€¢ black depth of a node is defined as the number of black nodes from the root to that node i.e the number of black ancestors.\n\nInserting a new node in a Red-Black Tree involves a two-step process: performing a standard binary search tree (BST) insertion, followed by fixing any violations of Red-Black properties.\nâ€¢ BST Insert : Insert the new node like in a standard BST.\nâ€¢ Fix Violations\nâ€¢ None If the parent of the new node is black\nâ€¢ None If the parent is red , the tree might violate the Red Property, requiring fixes.\n\nAfter inserting the new node as a red node, we might encounter several cases depending on the colors of the nodeâ€™s parent and uncle (the sibling of the parent):\nâ€¢ Case 1: Uncle is Red : Recolor the parent and uncle to black , and the grandparent to red . Then move up the tree to check for further violations.\nâ€¢ Case 2: Uncle is Black\nâ€¢ Sub-case 2.2: Node is a left child : Perform a right rotation on the grandparent and recolor appropriately.\n\nSearching for a node in a Red-Black Tree is similar to searching in a standard Binary Search Tree (BST). The search operation follows a straightforward path from the root to a leaf, comparing the target value with the current nodeâ€™s value and moving left or right accordingly.\nâ€¢ Start at the Root : Begin the search at the root node.\nâ€¢ Traverse the Tree\nâ€¢ None If the target value is equal to the current nodeâ€™s value, the node is found.\nâ€¢ None If the target value is less than the current nodeâ€™s value, move to the left child.\nâ€¢ None If the target value is greater than the current nodeâ€™s value, move to the right child.\nâ€¢ Repeat : Continue this process until the target value is found or a NIL node is reached (indicating the value is not present in the tree).\n\nDeleting a node from a Red-Black Tree also involves a two-step process: performing the BST deletion, followed by fixing any violations that arise.\nâ€¢ Fix Double Black\nâ€¢ None If a black node is deleted, a â€œdouble blackâ€ condition might arise, which requires specific fixes.\n\nWhen a black node is deleted, we handle the double black issue based on the siblingâ€™s color and the colors of its children:\nâ€¢ Case 1: Sibling is Red : Rotate the parent and recolor the sibling and parent.\nâ€¢ Case 2: Sibling is Black\nâ€¢ Sub-case 2.1: Siblingâ€™s children are black : Recolor the sibling and propagate the double black upwards.\nâ€¢ Sub-case 2.2: At least one of the siblingâ€™s children is red\nâ€¢ If the siblingâ€™s far child is red : Perform a rotation on the parent and sibling, and recolor appropriately.\nâ€¢ If the siblingâ€™s near child is red : Rotate the sibling and its child, then handle as above.\n\nRotations are fundamental operations in maintaining the balanced structure of a Red-Black Tree (RBT). They help to preserve the properties of the tree, ensuring that the longest path from the root to any leaf is no more than twice the length of the shortest path. Rotations come in two types: left rotations and right rotations.\n\nA left rotation at node ð‘¥x moves ð‘¥x down to the left and its right child ð‘¦y up to take ð‘¥xâ€™s place.\nâ€¢ y to be the right child of x\n\nA right rotation at node ð‘¥x moves ð‘¥x down to the right and its left child ð‘¦y up to take ð‘¥xâ€™s place.\nâ€¢ y to be the left child of x\n\nRotations in Red-Black Trees are typically performed during insertions and deletions to maintain the properties of the tree. Below are the scenarios for rotations:\n\nWhen a new node is inserted, it is always colored red. This can create violations of Red-Black Tree properties, specifically:\nâ€¢ None The root must be black\nâ€¢ Case 1: Recoloring and Propagating Upwards\nâ€¢ None If the parent and uncle of the new node are both red , recolor the parent and uncle to black , and the grandparent to red . Then, recursively apply the fix-up to the grandparent.\nâ€¢ Case 2: Rotation and Recoloring\nâ€¢ None If the new nodeâ€™s uncle is black and the new node is the right child of a left child (or vice versa), perform a rotation to move the new node up and align it.\nâ€¢ None If the new nodeâ€™s uncle is black and the new node is the left child of a left child (or right of a right), perform a rotation and recolor the parent and grandparent to fix the violation.\n\nAfter deletion, the tree might need fixing to restore properties:\nâ€¢ None When a black node is removed, or a red node is replaced by a black node, a double-black situation can arise.\nâ€¢ Case 1: Sibling is Red\nâ€¢ None Recolor the sibling and the parent, and perform a rotation.\nâ€¢ Case 2: Sibling is Black with Black Children\nâ€¢ None Recolor the sibling to red and move the problem up to the parent.\nâ€¢ Case 3: Sibling is Black with at least one Red Child\nâ€¢ None Rotate and recolor to fix the double-black issue.\nâ€¢ Balanced: Red-Black Trees are self-balancing, meaning they automatically maintain a balance between the heights of the left and right subtrees. This ensures that search, insertion, and deletion operations take O(log n) time in the worst case.\nâ€¢ Efficient search, insertion, and deletion: Due to their balanced structure, Red-Black Trees offer efficient operations. Search, insertion, and deletion all take O(log n) time in the worst case.\nâ€¢ Simple to implement: The rules for maintaining the Red-Black Tree properties are relatively simple and straightforward to implement.\nâ€¢ Widely used: Red-Black Trees are a popular choice for implementing various data structures, such as maps, sets, and priority queues.\nâ€¢ More complex than other balanced trees: Compared to simpler balanced trees like AVL trees, Red-Black Trees have more complex insertion and deletion rules.\nâ€¢ Constant overhead: Maintaining the Red-Black Tree properties adds a small overhead to every insertion and deletion operation.\nâ€¢ Not optimal for all use cases: While efficient for most operations, Red-Black Trees might not be the best choice for applications where frequent insertions and deletions are required, as the constant overhead can become significant.\nâ€¢ Implementing maps and sets: Red-Black Trees are often used to implement maps and sets, where efficient search, insertion, and deletion are crucial.\nâ€¢ Priority queues: Red-Black Trees can be used to implement priority queues, where elements are ordered based on their priority.\nâ€¢ File systems: Red-Black Trees are used in some file systems to manage file and directory structures.\nâ€¢ In-memory databases: Red-Black Trees are sometimes used in in-memory databases to store and retrieve data efficiently.\nâ€¢ Graphics and game development: Red-Black Trees can be used in graphics and game for tasks like collision detection and pathfinding.\n\n2. How does a Red-Black Tree maintain its balance?\n\n3. What are the advantages of using a Red-Black Tree?\n\n4. What are the disadvantages of using a Red-Black Tree?\n\n5. What are some common applications of Red-Black Trees?\nâ€¢ None What is the difference between Heap and Red-Black Tree?"
    },
    {
        "link": "https://quora.com/In-what-situations-are-red-black-trees-a-good-data-structure-to-use",
        "document": "Something went wrong. Wait a moment and try again."
    },
    {
        "link": "https://geeksforgeeks.org/how-to-optimize-memory-usage-and-performance-when-dealing-with-large-datasets-using-treemap-in-java",
        "document": "How to Optimize Memory Usage and Performance when Dealing with Large Datasets Using TreeMap in Java?\n\nJava programs' memory utilization and performance depend on their ability to handle huge datasets effectively. TreeMap is a Red-Black tree-based Map interface implementation that can be efficiently tuned to handle big datasets.\n\nThis post examines techniques for maximizing speed and memory use using TreeMap to handle big datasets.\n\nA Red-Black tree is used in Java's TreeMap to offer a sorted map implementation. It makes key-value pair retrieval and iteration efficient. The optimization of memory and speed necessitates taking use patterns, data distribution, and tree balance into account.\nâ€¢ Keeping the Tree in Balance: For best results, the Red-Black tree should be balanced regularly.\nâ€¢ Batch Processing: By processing data in groups as opposed to one at a time, memory overhead may be minimized.\nâ€¢ Selecting Efficient Data Structures: You may reduce memory utilization by using custom objects or primitives, examples of efficient data structures you can use inside TreeMap.\n\nLet's look at an example where we use TreeMap to maximize efficiency and memory use while working with a big dataset."
    },
    {
        "link": "https://stackoverflow.com/questions/20734/red-black-trees",
        "document": "Lots and lots of heat here, but not much light, so lets see if we can provide some.\n\nFirst, a RB tree is an associative data structure, unlike, say an array, which cannot take a key and return an associated value, well, unless that's an integer \"key\" in a 0% sparse index of contiguous integers. An array cannot grow in size either (yes, I know about realloc() too, but under the covers that requires a new array and then a memcpy()), so if you have either of these requirements, an array won't do. An array's memory efficiency is perfect. Zero waste, but not very smart, or flexible - realloc() not withstanding.\n\nSecond, in contrast to a bsearch() on an array of elements, which IS an associative data structure, a RB tree can grow (AND shrink) itself in size dynamically. The bsearch() works fine for indexing a data structure of a known size, which will remain that size. So if you don't know the size of your data in advance, or new elements need to be added, or deleted, a bsearch() is out. Bsearch() and qsort() are both well supported in classic C, and have good memory efficiency, but are not dynamic enough for many applications. They are my personal favorite though because they're quick, easy, and if you're not dealing with real-time apps, quite often are flexible enough. In addition, in C/C++ you can sort an array of pointers to data records, pointing to the struc{} member, for example, you wish to compare, and then rearranging the pointer in the pointer array such that reading the pointers in order at the end of the pointer sort yields your data in sorted order. Using this with memory-mapped data files is extremely memory efficient, fast, and fairly easy. All you need to do is add a few \"*\"s to your compare function/s.\n\nThird, in contrast to a hashtable, which also must be a fixed size, and cannot be grown once filled, a RB tree will automagically grow itself and balance itself to maintain its O(log(n)) performance guarantee. Especially if the RB tree's key is an int, it can be faster than a hash, because even though a hashtable's complexity is O(1), that 1 can be a very expensive hash calculation. A tree's multiple 1-clock integer compares often outperform 100-clock+ hash calculations, to say nothing of rehashing, and malloc()ing space for hash collisions and rehashes. Finally, if you want ISAM access, as well as key access to your data, a hash is ruled out, as there is no ordering of the data inherent in the hashtable, in contrast to the natural ordering of data in any tree implementation. The classic use for a hash table is to provide keyed access to a table of reserved words for a compiler. It's memory efficiency is excellent.\n\nFourth, and very low on any list, is the linked, or doubly-linked list, which, in contrast to an array, naturally supports element insertions and deletions, and as that implies, resizing. It's the slowest of all the data structures, as each element only knows how to get to the next element, so you have to search, on average, (element_knt/2) links to find your datum. It is mostly used where insertions and deletions somewhere in the middle of the list are common, and especially, where the list is circular and feeds an expensive process which makes the time to read the links relatively small. My general RX is to use an arbitrarily large array instead of a linked list if your only requirement is that it be able to increase in size. If you run out of size with an array, you can realloc() a larger array. The STL does this for you \"under the covers\" when you use a vector. Crude, but potentially 1,000s of times faster if you don't need insertions, deletions or keyed lookups. It's memory efficiency is poor, especially for doubly-linked lists. In fact, a doubly-linked list, requiring two pointers, is exactly as memory inefficient as a red-black tree while having NONE of its appealing fast, ordered retrieval characteristics.\n\nFifth, trees support many additional operations on their sorted data than any other data structure. For example, many database queries make use of the fact that a range of leaf values can be easily specified by specifying their common parent, and then focusing subsequent processing on the part of the tree that parent \"owns\". The potential for multi-threading offered by this approach should be obvious, as only a small region of the tree needs to be locked - namely, only the nodes the parent owns, and the parent itself.\n\nIn short, trees are the Cadillac of data structures. You pay a high price in terms of memory used, but you get a completely self-maintaining data structure. This is why, as pointed out in other replies here, transaction databases use trees almost exclusively."
    },
    {
        "link": "https://happycoders.eu/algorithms/red-black-tree-java",
        "document": "The red-black tree is a widely used concrete implementation of a self-balancing binary search tree. In the JDK, it is used in TreeMap, and since Java 8, it is also used for bucket collisions in HashMap. How does it work?\n\nIn this article, you will learn:\nâ€¢ How do you insert elements into a red-black tree? How do you remove them?\nâ€¢ What are the rules for balancing a red-black tree?\nâ€¢ How to implement a red-black tree in Java?\nâ€¢ How to determine its time complexity?\nâ€¢ What distinguishes a red-black tree from other data structures?\n\nYou can find the source code for the article in this GitHub repository.\n\nA red-black tree is a self-balancing binary search tree, that is, a binary search tree that automatically maintains some balance.\n\nEach node is assigned a color (red or black). A set of rules specifies how these colors must be arranged (e.g., a red node may not have red children). This arrangement ensures that the tree maintains a certain balance.\n\nAfter inserting and deleting nodes, quite complex algorithms are applied to check compliance with the rules â€“ and, in case of deviations, to restore the prescribed properties by recoloring nodes and rotations.\n\nIn the literature, red-black trees are depicted with and without so-called NIL nodes. A NIL node is a leaf that does not contain a value. NIL nodes become relevant for the algorithms later on, e.g., to determine colors of uncle or sibling nodes.\n\nIn Java, NIL nodes can be represented simply by references; more on this later.\n\nThe following example shows two possible representations of a red-black tree. The first image shows the tree without (i.e., with implicit) NIL leaves; the second image shows the tree with explicit NIL leaves.\n\nIn the course of this tutorial, I will generally refrain from showing the NIL leaves. When explaining the insert and delete operations, I will show them sporadically if it facilitates understanding the respective algorithm.\n\nThe following rules enforce the red-black tree balance:\nâ€¢ Each node is either red or black.\nâ€¢ A red node must not have red children.\nâ€¢ All paths from a node to the leaves below contain the same number of black nodes.\n\nRule 2 is in parentheses because it does not affect the tree's balance. If a child of a red root is also red, the root must be colored black according to rule 4. However, if a red root has only black children, there is no advantage in coloring the root black.\n\nTherefore, rule 2 is often omitted in the literature.\n\nWhen explaining the insert and delete operations and in the Java code, I will point out where there would be differences if we would also implement rule 2. So much in advance: The difference is only one line of code per operation :)\n\nBy the way, from rules 4 and 5 follows that a red node always has either two NIL leaves or two black child nodes with values. If it had one NIL leaf and one black child with value, then the paths through this child would have at least one more black node than the path to the NIL leaf, which would violate rule 5.\n\nWe refer to the height of the red-black tree as the maximum number of nodes from the root to a NIL leaf, not including the root. The height of the red-black tree in the example above is 4:\n\nFrom rules 3 and 4 follows:\n\nThe path from the root to a leaf (not counting the root) is at most twice as long as the path from the root to a leaf.\n\nLet's assume that the shortest path has (in addition to the root) black nodes and no red nodes. Then we could add another red nodes before each black node without breaking rule 3 (which we could reword to: no two red nodes may follow each other).\n\nThe following example shows the shortest possible path through a red-black tree of height four on the left and the longest possible path on the right:\n\nThe paths to the NIL leaves on the left have a length (excluding the root) of 2. The paths to the NIL leaves on the bottom right have a length of 4.\n\nBlack height is the number of black nodes from a given node to its leaves. The black NIL leaves are counted, the start node is not.\n\nThe black height of the entire tree is the number of black nodes from the root (this is not counted) to the NIL leaves.\n\nThe black height of all red-black trees shown so far is 2.\n\nAs a starting point for implementing the red-black tree in Java, I use the Java source code for the binary search tree from the second part of the binary tree series.\n\nNodes are represented by the class. For simplicity, we use primitives as the node value.\n\nTo implement the red-black tree, besides the child nodes and , we need a reference to the parent node and the node's color. We store the color in a , defining red as and black as .\n\nWe implement the red-black tree in the class. This class extends the class presented in the second part of the series (which essentially provides a function).\n\nWe will add the operations (insert, search, delete) in the following sections, step by step.\n\nBut first, we have to define some helper functions.\n\nInsertion and deletion work basically as described in the article about binary search trees.\n\nAfter insertion and deletion, the red-black rules (see above) are reviewed. If they have been violated, they must be restored. That happens by recoloring nodes and by rotations.\n\nThe rotation works precisely like with AVL trees, which I described in the previous tutorial. I'll show you the corresponding diagrams again here. You can find detailed explanations in the section \"AVL tree rotation\" of the article just mentioned.\n\nThe following graphic shows a right rotation. The colors have no relation to those of the red-black tree. They are only used to track the node movements better.\n\nThe left node becomes the new root; the root becomes its right child. The right child of the pre-rotation left node becomes the left child of the post-rotation right node . The two white nodes and do not change their relative position.\n\nThe Java code is slightly longer than in the AVL tree â€“ for the following two reasons:\nâ€¢ We also need to update the references of the nodes (in the AVL tree, we worked without references).\nâ€¢ None We need to update the references to and from the pre-rotation top node's parent ( in the graphic). For the AVL tree, we did that indirectly by returning the new root of the rotated subtree and \"hooking\" the rotation into the recursive call of the insert and delete operations.\n\nYou can find the implementation of the right rotation in the source code starting at line 358:\n\nThe method called at the end sets the parent-child relationship between the parent node of the former root node of the rotated subtree and its new root node . You can find it in the code starting at line 388:\n\nLeft rotation works analogously: The right node moves up to the top. The root becomes the left child of . The left child of the formerly right node becomes the right child of the post-rotation left node . and do not change their relative position.\n\nHere is the Java code for the left rotation (source code, starting at line 373):\n\nLike any binary tree, the red-black tree provides operations to find, insert, and delete nodes. We will go through these operations step by step in the following sections.\n\nAt this point, I would like to recommend the red-black tree simulator by David Galles. It allows you to animate any insert, delete and search operations graphically.\n\nThe search works like in any binary search tree: We first compare the search key with the root. If the search key is smaller, we continue the search in the left subtree; if the search key is larger, we continue the search in the right subtree.\n\nWe repeat this until we either find the node we are looking for â€“ or until we reach a NIL leaf (in Java code: a reference). Reaching a NIL leaf would mean that the key we are looking for does not exist in the tree.\n\nFor a graphical representation of the search, see the example in the article about binary search trees.\n\nFor the red-black tree, we implement an iterative variant of the search. You can find it in the source code starting at line 14:\n\nThis code should be self-explanatory.\n\nIn the \"Searching\" section of the article mentioned above, you can also find a recursive version of the search.\n\nTo insert a new node, we first proceed as described in the \"binary search tree insertion\" section of the corresponding article. I.e., we search for the insertion position from the root downwards and attach the new node to a leaf or half-leaf.\n\nYou can find the code in the class, starting at line 29:\n\nWe initially color the new node red so that rule 5 is satisfied, i.e., all paths have the same number of black nodes after insertion.\n\nHowever, if the parent node of the inserted node is also red, we have violated rule 4. We then have to repair the tree by recoloring and/or rotating it so that all rules are satisfied again. That is done in the method, which is called in the last line of the method.\n\nDuring the repair, we have to deal with five different cases:\nâ€¢ Case 1: New node is the root\nâ€¢ None Case 2: Parent node is red and the root\nâ€¢ Case 4: Parent node is red, uncle node is black, inserted node is \"inner grandchild\"\nâ€¢ Case 5: Parent node is red, uncle node is black, inserted node is \"outer grandchild\"\n\nThe five cases are described below.\n\nCase 1: New Node Is the Root\n\nIf the new node is the root, we don't have to do anything else. Unless we work with rule 2 (\"the root is always black\"). In that case, we would have to color the root black.\n\nCase 2: Parent Node Is Red and the Root\n\nIn this case, rule 4 (\"no red-red!\") is violated. All we have to do now is to color the root black. That leads to rule 4 being complied with again.\n\nAnd rule 5? Since the root is not counted in this rule, all paths still have one black node (the NIL leaves not displayed in the graphic). And if we would count the root, then all paths would now have two black nodes instead of one â€“ that would also be allowed.\n\nIf we work with rule 2 (\"the root is always black\"), we have already colored the root black in case 1, and case 2 can no longer occur.\n\nWe use the term \"uncle node\" to refer to the sibling of the parent node; that is, the second child of the grandparent node next to the parent node. The following graphic should make this understandable: Inserted was the 81; its parent is the 75, the grandparent is the 19, and the uncle is the 18.\n\nBoth the parent and the uncle are red. In this case, we do the following:\n\nWe recolor parent and uncle nodes (18 and 75 in the example) black and the grandparent (19) red. Thus rule 4 (\"no red-red!\") is satisfied again at the inserted node. The number of black nodes per path does not change (in the example, it remains at 2).\n\nHowever, there could now be two red nodes in a row at the grandparent node â€“ namely, if the great-grandparent node (17 in the example) were also red. In this case, we would have to make further repairs. We would do this by calling the repair function recursively on the grandparent node.\n\nCase 4: Parent Node Is Red, Uncle Node Is Black, Inserted Node Is \"Inner Grandchild\"\n\nI must first explain this case: \"inner grandchild\" means that the path from the grandparent node to the inserted node forms a triangle, as shown in the following graphic using 19, 75, and 24. In this example, you can see that a NIL leaf is also considered a black uncle (according to rule 3).\n\nIn this case, we first rotate at the parent node in the .\n\nWhat does that mean?\n\nIf the inserted node is the child of its parent node, we rotate to the at the parent node. If the inserted node is the child, we rotate to the .\n\nIn the example, the inserted node (the 24) is a left child, so we rotate to the right at the parent node (75 in the example):\n\nSecond, we rotate at the grandparent node in the to the previous rotation. In the example, we rotate left around the 19:\n\nFinally, we color the node we just inserted (the 24 in the example) black and the original grandparent (the 19 in the example) red:\n\nSince there is now a black node at the top of the last rotated subtree, there cannot be a violation of rule 4 (\"no red-red!\") at that position.\n\nAlso, recoloring the original grandparent (19) red cannot violate rule 4. Its left child is the uncle, which is black by definition of this case. And the right child, as a result of the second rotation, is the left child of the inserted node, thus a black NIL leaf.\n\nThe inserted red 75 has two NIL leaves as children, so there is no violation of rule 4 here either.\n\nThe repair is now complete; a recursive call of the repair function is not necessary.\n\nCase 5: Parent Node Is Red, Uncle Node Is Black, Inserted Node Is \"Outer Grandchild\"\n\n\"Outer grandchild\" means that the path from grandparent to inserted node forms a line, such as the 19, 75, and 81 in the following example:\n\nIn this case, we rotate at the grandparent (19 in the example) in the opposite direction of the parent and inserted node (after all, both go in the same direction in this case). In the example, the parent and inserted nodes are both children, so we rotate at the grandparent:\n\nThen we recolor the former parent (75 in the example) black and the former grandparent (19) red:\n\nAs at the end of case 4, we have a black node at the top of the rotation, so there can be no violation of rule 4 (\"no red-red!\") there.\n\nThe left child of the 19 is the original uncle after rotation, so it is black by case definition. The right child of the 19 is the original left child of the parent node (75), which must also be a black NIL leaf; otherwise, the right place where we inserted the 81 would not have been free (because a red node always has either two black children with value or two black NIL children).\n\nThe red 81 is the inserted node and, therefore, also has two black NIL leaves.\n\nAt this point, we've completed the repair of the red-black tree.\n\nIf you have paid close attention, you will notice that case 5 corresponds precisely to the second rotation from case 4. In the code, this will be shown by the fact that only the first rotation is implemented for case 4, and then the program jumps to the code for case 5.\n\nYou can find the complete repair function in starting at line 64. I have marked cases 1 to 5 by comments. Cases 4 and 5 are split into 4a/4b and 5a/5b depending on whether the parent node is left (4a/5a) or right child (4b/5b) of the grandparent node.\n\nYou will find the helper function starting at line 152:\n\nUnlike the AVL tree, we cannot easily hook the repair function of the red-black tree into the existing recursion from . That is because we need to rotate not only at the node under which we inserted the new node but also at the grandparent if necessary (cases 3 and 4).\n\nYou will find numerous alternative implementations in the literature. These are sometimes minimally more performant than the way presented here since they combine multiple steps. That doesn't change the order of magnitude of the performance, but it can gain a few percent. It was important for me to implement the algorithm in a comprehensible way. The more performant algorithms are always more complex, too.\n\nI implemented the iterative insertion in two steps â€“ search first, then insertion â€“ unlike , where I combined the two. That makes reading the code a bit easier but requires an additional \" \" check to determine whether the new node needs to be inserted as a left or right child under its parent.\n\nIf you have just finished reading the chapter on inserting, you might want to take a short break. After all, deleting is even more complex.\n\nFirst, we proceed as described in the \"Binary Search Tree Deletion\" section of the article on binary search trees in general.\nâ€¢ If the node to be deleted has children, we simply remove it.\nâ€¢ If the node to be deleted has child, we remove the node and let its single child move up to its position.\nâ€¢ If the node to be deleted has children, we copy the content (not the color!) of the in-order successor of the right child into the node to be deleted and then delete the in-order successor according to rule 1 or 2 (the in-order successor has at most one child by definition).\n\nAfter that, we need to check the rules of the tree and repair it if necessary. To do this, we need to remember the deleted node's color and which node we have moved up.\nâ€¢ If the deleted node is red, we cannot have violated any rule: Neither can it result in two consecutive red nodes (rule 4), nor does it change the number of black nodes on any path (rule 5).\nâ€¢ However, if the deleted node is black, we are guaranteed to have violated rule 5 (unless the tree contained nothing but a black root), and rule 4 may also have been violated â€“ namely if both parent nodes and the moved-up child of the deleted node were red.\n\nFirst, here is the code for the actual deletion of a node (class RedBlackTree, line 163). Underneath the code, I will explain its parts:\n\nThe first lines of code search for the node to be deleted; the method terminates if that node can't be found.\n\nHow to proceed depends on the number of children nodes to be deleted.\n\nDeleting a Node With Zero or One Child\n\nIf the deleted node has at most one child, we call the method . You can find it in the source code starting at line 221:\n\nI have already introduced you to the method (which is called several times here) in the rotation.\n\nThe case where the deleted node is black and has no children is a special case. That is dealt with in the last block:\n\nWe have seen above that deleting a black node results in the number of black nodes no longer being the same on all paths. That is, we will have to repair the tree. The tree repair always starts (as you will see shortly) at the moved-up node.\n\nIf the deleted node has no children, one of its NIL leaves virtually moves up to its position. To be able to navigate from this NIL leaf to its parent node later, we need a special placeholder. I've implemented one in the class , which you can find in the source code starting at line 349:\n\nFinally, the method returns the moved-up node that the calling method stores in the variable.\n\nIf the node to be deleted has two children, we first use the method (line 244) to find the in-order successor of the subtree that starts at the right child:\n\nWe then copy the data of the in-order successor into the node to be deleted and call the method introduced above to remove the in-order successor from the tree. Again, we remember the moved-up node in .\n\nHere is once more the last -block of the method:\n\nAs stated above, deleting a node does not violate any rules. If, however, the deleted node is , we call the repair method .\n\nIf any, we've needed the temporary placeholder created in only for calling the repair function. We can therefore remove it afterward.\n\nWhen deleting, we have to consider one more case than when inserting. In contrast to the insertion, the color of the uncle is not relevant here but that of the deleted node's sibling.\nâ€¢ Case 3: Sibling is black and has two black children, parent is red\nâ€¢ Case 4: Sibling is black and has two black children, parent is black\nâ€¢ Case 5: Sibling is black and has at least one red child, \"outer nephew\" is black\nâ€¢ Case 6: Sibling is black and has at least one red child, \"outer nephew\" is red\n\nThe following sections describe the six cases in detail:\n\nIf we removed the root, another node moved up to its position. That could only happen if the root had zero or only one child. If the root had had two children, it would have been the in-order successor that would have been removed in the end and not the root node.\n\nIf the root had child, the new root is a black NIL node. Thus the tree is empty and valid:\n\nIf the root had child, then this had to be red and have no other children.\n\nExplanation: If the red child had another red child, rule 4 (\"no red-red!\") would have been violated. If the red child had a black child, then the paths through the red node would have at least one more black node than the NIL subtree of the root, and thus rule 5 would have been violated.\n\nThus, the tree consists of only one red root and is therefore also valid.\n\nShould we work with rule 2 (\"the root is always black\"), we would now recolor the root.\n\nFor all other cases, we first check the color of the sibling. That is the second child of the parent of the deleted node. In the following example, we delete the 9; its sibling is the red 19:\n\nIn this case, we first color the sibling black and the parent red:\n\nThat obviously violated rule 5: The paths in the right subtree of the parent each have two more black nodes than those in the left subtree. We fix this by rotating around the parent in the direction of the deleted node.\n\nIn the example, we have deleted the node of the parent node â€“ we, therefore, perform a rotation:\n\nNow we have two black nodes on the right path and two on the path to the 18. However, we have black node on the path to the left NIL leaf of 17 (remember: the root does not count, the NIL nodes do â€“ even the ones not drawn in the graphic).\n\nWe look at the new sibling of the deleted node (18 in the example). That new sibling is now definitely black because it is an original child of the red sibling from the beginning of the case.\n\nAlso, the new sibling has black children. Therefore, we color the sibling (the 18) red and the parent (the 17) black:\n\nNow all paths have two black nodes; we have a valid red-black tree again.\n\nIn fact, I have anticipated something in this last step. Namely, we have executed the rules of case 3 (that's why the image subtitle is in parentheses).\n\nIn this last step of case 2, we always have a black sibling. The fact that the black sibling had two black children, as required for case 3, was a coincidence. In fact, at the end of case 2, any of the cases 3 to 6 can occur and must be treated according to the following sections.\n\nCase 3: Sibling Is Black and Has Two Black Children, Parent Is Red\n\nIn the following example, we delete the 75 and let one of its black NIL leaves move up.\n\nThe deletion violates rule 5: In the rightmost path, we now have one black node less than in all others.\n\nThe sibling (the 18 in the example) is black and has two black children (the NIL leaves not shown). The parent (the 19) is red. In this case, we repair the tree as follows:\n\nWe recolor the sibling (the 18) red and the parent (the 19) black:\n\nThus we have a valid red-black tree again. The number of black nodes is the same on all paths (as required by rule 5). And since the sibling has only black children, coloring it red cannot violate rule 4 (\"no red-red!\").\n\nCase 4: Sibling Is Black and Has Two Black Children, Parent Is Black\n\nIn the following example, we delete the 18:\n\nThis leads (just like in case 3) to a violation of rule 5: On the path to the deleted node, we now have one black node less than on all other paths.\n\nIn contrast to case 3, in this case, the parent node of the deleted node is black. We first color the sibling red:\n\nThat means that the black height in the subtree that starts at the parent node is again uniform (2). In the left subtree, however, it is one higher (3). Rule 5 is therefore still violated.\n\nWe solve this problem by pretending that we deleted a black node between nodes 17 and 19 (which would have had the same effect). Accordingly, we call the repair function recursively on the parent node, i.e., the 19 (which would have been the moved-up node in this case).\n\nThe 19 has a black sibling (the 9) with two black children (3 and 12) and a red parent (17). Accordingly, we are now back to case 3.\n\nWe solve case 3 by coloring the parent black and the sibling red:\n\nThe black height is now two on all paths, so our red-black tree is valid again.\n\nCase 5: Sibling is black and has at least one red child, \"outer nephew\" is black\n\nIn this example, we delete the 18:\n\nAs a result, we again violated rule 5 since the subtree starting at the sibling now has a black height greater by one.\n\nWe examine the \"outer nephew\" of the deleted node. \"Outer nephew\" means the child of the sibling that is opposite the deleted node. In the example, this is the right (and by definition black) NIL leaf under the 75.\n\nIn the following graphic, you can see that parent, sibling and nephew together form a line (in the example: 19, 75, and its right NIL child).\n\nWe start the repair by coloring the inner nephew (the 24 in the example) black and the sibling (the 75) red:\n\nThen we perform a rotation at the sibling node in the opposite direction of the deleted node . In the example, we've deleted the parent's child, so we perform a rotation at the sibling (the 75):\n\nWe are doing some recoloring again:\nâ€¢ We recolor the sibling in the color of its parent (in the example, the 24 red).\nâ€¢ Then we recolor the parent (the 19) and the outer nephew of the deleted node, i.e., the right child of the new sibling (the 75 in the example) black:\n\nFinally, we perform a rotation on the parent node in the direction of the deleted node . In the example, the deleted node was a child, so we perform a rotation accordingly (at 19 in the example):\n\nThis last step restores compliance with all red-black rules. There are no two consecutive red nodes, and the number of black nodes is uniformly two on all paths. We've thus completed the repair of the tree.\n\nCase 6: Sibling is black and has at least one red child, \"outer nephew\" is red\n\nIn the last example, which is very similar to case 5, we also delete the 18:\n\nAs a result, as in case 5, we violated rule 5 because the path to the deleted node now contains one less black node.\n\nIn case 6, unlike case 5, the outer nephew (81 in the example) is red and not black.\n\nWe first recolor the sibling in the parent's color (in the example, the 75 red). Then we recolor the parent (the 19 in the example) and the outer nephew (the 81) black:\n\nSecond, we perform a rotation at the parent node in the direction of the deleted node . In the example, we've deleted a child; accordingly, we perform a rotation around the 19:\n\nThis rotation restores the red-black rules. No two red nodes follow each other, and the number of black nodes is the same on all paths (namely 2).\n\nThe rules in this last case are similar to the final two steps of case 5. In the source code, you will see that for case 5, only its first two steps are implemented, and the program then goes to case 6 to execute the last two steps.\n\nWith this, we have studied all six cases. Let's move on to the implementation of the repair function in Java.\n\nYou can find the method in the source code starting at line 252. I have marked cases 1 to 6 with comments.\n\nYou will find the helper methods and starting at line 334:\n\nYou can find the implementation for a black sibling knot with at least one red child (cases 5 and 6) starting at line 302:\n\nJust as for inserting, you will find numerous alternative approaches for deleting in the literature. I have tried to structure the code so that you can follow the code flow as well as possible.\n\nLike any binary tree, we can traverse the red-black tree in pre-order, post-order, in-order, reverse-in-order, and level-order. In the \"Binary Tree Traversal\" section of the introductory article on binary trees, I have described traversal in detail.\n\nIn that section, you will also find the corresponding Java source code, implemented in the classes , , and .\n\nThe traversal methods work on the interface. Since also implements this interface, we can easily apply the traversal methods to it as well.\n\nFor an introduction to the topic of time complexity and O-notation, see this article.\n\nWe can determine the cost of searching, inserting, and deleting a node in the binary tree as follows:\n\nWe follow a path from the root to the searched node (or to a NIL leaf). At each level, we perform a comparison. The effort for the comparison is constant.\n\nThe search cost is thus proportional to the tree height.\n\nWe denote by the number of tree nodes. In the \"Height of a Red-Black Tree\" section, we have recognized that the longest path is at most twice as long as the shortest path. It follows that the height of the tree is bounded by .\n\nA formal proof is beyond the scope of this article. You can read the proof on Wikipedia.\n\nThus, the time complexity for finding a node in a red-black tree is:\n\nWhen inserting, we first perform a search. We have just determined the search cost as .\n\nNext, we insert a node. The cost of this is constant regardless of the tree size, so .\n\nThen we check the red-black rules and restore them if necessary. We do this starting at the inserted node and ascending to the root. At each level, we perform one or more of the following operations:\nâ€¢ Checking the color of the parent node\nâ€¢ Determination of the uncle node and checking its color\nâ€¢ Recoloring one up to three nodes\nâ€¢ Performing one or two rotations\n\nEach of these operations has constant time, , in itself. The total time for checking and repairing the tree is therefore also proportional to its height.\n\nSo the time complexity for inserting into a red-black tree is also:\n\nJust as with insertion, we first search for the node to be deleted in time .\n\nAlso, the deletion cost is independent of the tree size, so it is constant .\n\nFor checking the rules and repairing the tree, one or more of the following operations occur â€“ at most once per level:\nâ€¢ Checking the color of the deleted node\nâ€¢ Determining the sibling and examining its color\nâ€¢ Checking the colors of the sibling's children\nâ€¢ Recoloring the sibling node and one of its children\nâ€¢ Performing one or two rotations\n\nThese operations also all have a constant complexity in themselves. Thus, the total effort for checking and restoring the rules after deleting a node is also proportional to the tree height.\n\nSo the time complexity for deleting from a red-black tree is also:\n\nThe following sections describe the differences and the advantages and disadvantages of the red-black tree compared to alternative data structures.\n\nThe red-black tree, as well as the AVL tree, are self-balancing binary search trees.\n\nIn the red-black tree, the longest path to the root is at most twice as long as the shortest path to the root. On the other hand, in the AVL tree, the depth of no two subtrees differs by more than 1.\n\nIn the red-black tree, balance is maintained by the node colors, a set of rules, and by rotating and recoloring nodes. In the AVL tree, the heights of the subtrees are compared, and rotations are performed when necessary.\n\nThese differences in the characteristics of the two types of trees lead to the following differences in performance and memory requirements:\nâ€¢ Due to the more even balancing of the AVL tree, search in an AVL tree is usually faster. In terms of magnitude, however, both are in the range .\nâ€¢ For insertion and deletion, the time complexity in both trees is . In a direct comparison, however, the red-black tree is faster because it rebalances less frequently.\nâ€¢ Both trees require additional memory: the AVL tree one byte per node for the height of the subtree starting at a node; the red-black tree one bit per node for the color information. This rarely makes a difference in practice since a single bit usually occupies at least one byte.\n\nIf you expect many insert/delete operations, then you should use a red-black tree. If, on the other hand, you expect more search operations, then you should choose the AVL tree.\n\nThe red-black tree is a concrete implementation of a self-balancing binary search tree. So every red-black tree is also a binary search tree.\n\nThere are also other types of binary search trees, such as the AVL tree mentioned above â€“ or trivial non-balanced implementations. Thus, not every binary search tree is also a red-black tree.\n\nThis tutorial taught you what a red-black tree is, which rules govern it and how these rules are evaluated and restored if necessary after inserting and deleting nodes. I also introduced you to a Java implementation that is as easy to understand as possible.\n\nThe JDK uses red-black trees in TreeMap (here is the source code on GitHub) and in bucket collisions in HashMap (here is the source code).\n\nWith this, I conclude the tutorial series on binary trees.\n\nIf I could help you better understand binary trees in general, binary search trees, AVL trees, and â€“ in this article â€“ red-black trees, I'm happy about a comment. Also, feel free to share the article using one of the share buttons at the end.\n\nDo you want to be informed when the next article is published on HappyCoders.eu? Then click here to sign up for the HappyCoders newsletter."
    }
]