[
    {
        "link": "https://discuss.pytorch.org/t/using-transformer-on-timeseries/104759",
        "document": "A set of examples around pytorch in Vision, Text, Reinforcement Learning, etc. - pytorch/examples"
    },
    {
        "link": "https://medium.com/@mkaanaslan99/time-series-forecasting-with-a-basic-transformer-model-in-pytorch-650f116a1018",
        "document": "Time series forecasting is an essential topic that‚Äôs both challenging and rewarding, with a wide variety of techniques available to practitioners. After experiencing in traditional machine learning methods for forecasting, I decided to venture into the realm of deep learning! My latest project involved participating in the ‚ÄòStore Sales ‚Äî Time Series Forecasting‚Äô¬π competition on Kaggle, which provided an excellent opportunity for practical application. Here is the link for the competition and its datasets.\n\nI constructed a simple yet effective transformer¬≤ model, inspired by its successes in the area of natural language processing, and applied it to predicting store sales. The results were quite encouraging! In this article, I aim to share insights into the data, the pre-processing steps taken, outline the architecture of the transformer model, and describe my overall approach to the forecasting challenge. Through this, I hope to offer a resource that can serve as a starting point for others interested in applying transformer models to their own time series prediction tasks.\n\nIn this competition, the challenge was to predict 16 days of future sales for various product families across 54 stores, creating a grand total of 1,782 unique time series. Data spanned from January 1, 2013, to August 15, 2017, and the goal was to forecast the following 16 days of sales. While I have streamlined the pre-processing for brevity, I‚Äôll delve into the structure of the final dataset: it contains 3,029,400 entries across 20 columns, serving as inputs to my model. Each row captures a snapshot of sales data for a particular product family at a specific store on a given date, with ‚Äòstore_nbr,‚Äô ‚Äòfamily,‚Äô and ‚Äòdate‚Äô as key columns. The data break down into three types of variables:\n\nPast covariates are those time-related variables known up to the last date of training data, which is August 15, 2017. These include numeric variables like ‚Äòsales,‚Äô indicative of a product family‚Äôs sales at a particular store; ‚Äòtransactions,‚Äô the total number of transactions at a store; ‚Äòstore_sales,‚Äô the aggregate sales at that store; and ‚Äòfamily_sales,‚Äô representing the total sales for that product family.\n\nFuture covariates, which are also time-dependent but known past the training cut-off and until the last forecast date of August 31, 2017, encompass variables like ‚Äòonpromotion‚Äô ‚Äî the count of promoted items in a product family ‚Äî and ‚Äòdcoilwtico,‚Äô the daily oil price. These numeric columns are complemented by the ‚Äòholiday‚Äô column, which denotes the presence of holidays or events and is categorically encoded to integers. Additionally, ‚Äòtime_idx,‚Äô ‚Äòweek_day,‚Äô ‚Äòmonth_day,‚Äô ‚Äòmonth,‚Äô and ‚Äòyear‚Äô columns provide temporal context, also encoded as integers. Though my model is encoder-only, I‚Äôve added the 16 days shifted values of ‚Äòonpromotion‚Äô and ‚Äòdcoilwtico‚Äô to incorporate future information without a decoder.\n\nStatic covariates remain unchanged over time and include identifiers such as ‚Äòstore_nbr,‚Äô ‚Äòfamily,‚Äô as well as the categorical variables of ‚Äòcity,‚Äô ‚Äòstate,‚Äô ‚Äòtype,‚Äô and ‚Äòcluster‚Äô ‚Äî detailing store characteristics ‚Äî all of which are integer-encoded. Importantly, I‚Äôve noted the distinct categories for embedding purposes and distinguished static from time-dependent categorical variables to account for the additional temporal dimension post-embedding. The resulting dataframe, named ‚Äòdata_all,‚Äô is structured as follows:\n\nBefore transforming the data into tensors suitable for my PyTorch model, I divided it into training and validation sets. The window size, a vital hyperparameter, denotes the sequence length per training sample, shaping inputs into the form (batch size, window size, number of features). Additionally, ‚Äònum_val‚Äô indicates the validation folds used, which is set to 2 in this context. Observations from January 1, 2013, up to June 28, 2017, are designated as the training dataset, with the periods from June 29, 2017, to July 14, 2017, and July 15, 2017, to July 30, 2017, serving as validation intervals.\n\nNumeric and categorical variables are then cast into tensor format, with a logarithmic transformation applied to numeric variables for scaling purposes. This approach adheres to convention while aiming for more normalized input distributions which can be beneficial during model training.\n\nCrafting an unbiased training strategy is essential for a fair and effective model. With 1,782 distinct time series in our dataset, a judicious approach to batching is required to maintain impartiality during training. Batch processing, if mishandled, can introduce several biases. For instance, a model might favor time series included in later batches due to more recent gradient updates. Moreover, a static time block indexing could prevent the model from experiencing the full spectrum of temporal sequences. To illustrate, training on series consistently segmented from index 0 to 15 would mean the model never learns patterns between indices 10 and 25.\n\nTo mitigate such biases, I implemented a dynamic slicing mechanism: each time series is divided into time blocks starting from a random index in the window range, ensuring the model gets exposure to diverse series segments. Yet, I consistently kept the final block fixed to encompass the freshest data points. I paired series indexes with these random time blocks, shuffled them to smooth out batch-order influence, and then batched them for training. In this setting, every batch will have random time series and their random time blocks. However in the end, the model will see every time series and their all observations in one epoch in the training phase.\n\nThe model will be trained unbiased with the current setting, however there is a type of bias that could be helpful. If the last batches of the data includes more latest observations, model could perform better in test set with recent dates. I introduced an additional hyper-parameter setting that instead of random shuffling, orders the dataset based on the starting time of the blocks. The data is then divided into five parts ‚Äî reflecting our five-year dataset ‚Äî each of which is internally shuffled which means the last batches will include observations from last year, but again randomly. As a result, the model‚Äôs final gradient updates are influenced by the latest year, theoretically improving forecasts for more recent periods.\n\nThe implemented data loaders will yield batches comprising ‚Äòx_numeric‚Äô, ‚Äòx_category‚Äô, ‚Äòx_static‚Äô, and ‚Äòy‚Äô tensors. Their respective shapes will align with the following: (batch_size, window_size, num_numeric_features), (batch_size, window_size, num_category_features), (batch_size, num_static_features), and (batch_size, forecast_length). Here‚Äôs how the functions are established:\n\nIn time series forecasting, capturing the sequence information is key, and for this purpose, I utilized the Transformer architecture as described in the seminal paper Attention is All You Need (2017)¬≤. My model generates block forecasts and, hence, doesn‚Äôt require causality in its attention mechanism ‚Äî no masking was applied to the attention blocks.\n\nLet‚Äôs start with the input: categorical features are passed through embedding layers to represent them in a dense form then fed to the transformer block. Then, a Multi-Layer Perceptron (MLP) takes the final encoded input to produce the forecast. The embedding dimension, the number of attention heads in each Transformer block, and the dropout probability are the model‚Äôs primary hyperparameters. Furthermore, stacking multiple Transformer blocks is controlled by the ‚Äònum_blocks‚Äô hyperparameter.\n\nBelow is the implementation of a single Transformer block and the overall forecasting model:\n\nAs illustrated in the diagram, the model accepts three separate input tensors: numeric features, categorical features, and static features. Categorical and static feature embeddings are averaged and combined with numeric features to form a tensor with shape (batch_size, window_size, embedding_size), ready for the Transformer block. This composite tensor also carries embedded time variables, providing essential positional information.\n\nThe Transformer block extracts sequential information, and the resulting tensor is then aggregated along the time dimension before being passed into an MLP to produce the final forecast of shape (batch_size, forecast_length). The competition uses the Root Mean Squared Logarithmic Error (RMSLE) as the evaluation metric formulated as:\n\nGiven that predictions undergo logarithmic transformation, prospects of predicting negative sales values lower than -1 ‚Äî which result in undefined errors ‚Äî needed to be curtailed. To avoid negative sales forecasts and the resulting NaN loss values, a final ReLU activation function is applied to the MLP layer to ensure non-negative predictions.\n\nAs I embarked on training the model, I set several hyperparameters: window size, time shuffle, embedding size, number of heads, number of transformer blocks, dropout probability, batch size, and learning rate. Limited GPU resources constrained my ability to extensively tune these hyperparameters, but iterative experimentation led me to settle on the following configuration as the most effective:\n\nI opted for the Adam optimizer and implemented a learning rate scheduler to gradually adjust the learning rate during training. Here‚Äôs how the training process was carried out:\n\nAfter training, the model that demonstrates the lowest validation loss is retained. Despite being limited to 200 epochs due to GPU resource limitations, selected hyperparameters are worked out: the best-performing model achieved a training loss of 0.387 and a validation loss of 0.457. When applied to the test set, this model achieved a RMSLE of 0.416, landing me in the 89th position ‚Äî placing me within the top 10% of the competition, all with a basic model and relatively modest dataset.\n\nLarger embedding sizes paired with a higher number of attention heads seemed to enhance performance, yet the best outcomes were realized with a solitary transformer block, suggesting that simplicity is virtue with limited data. I also observed an improvement when forgoing full shuffling in favor of sectional shuffling; introducing a slight time bias meaningfully amplified predictive accuracy.\n\nThere is certainly room to systematize hyperparameter tuning for further gains. Additionally, incorporating future covariates information may not be fully realized by the current model architecture, suggesting an encoder-decoder configuration, akin to the Temporal Fusion Transformer¬≥, could yield more significant benefits.\n\nI hope this article has imparted valuable insights into time series forecasting. As someone continuing to learn and grow in this field, I welcome and truly appreciate your feedback and discourse on this and related topics. Thank you for joining me on this exploration!"
    },
    {
        "link": "https://pytorch-forecasting.readthedocs.io/en/stable/tutorials/stallion.html",
        "document": "In this tutorial, we will train the on a very small dataset to demonstrate that it even does a good job on only 20k samples. Generally speaking, it is a large model and will therefore perform much better with more data.\n\nOur example is a demand forecast from the Stallion kaggle competition.\n\nFirst, we need to transform our time series into a pandas dataframe where each row can be identified with a time step and a time series. Fortunately, most datasets are already in this format. For this tutorial, we will use the Stallion dataset from Kaggle describing sales of various beverages. Our task is to make a six-month forecast of the sold volume by stock keeping units (SKU), that is products, sold by an agency, that is a store. There are about 21 000 monthly historic sales records. In addition to historic sales we have information about the sales price, the location of the agency, special days such as holidays, and volume sold in the entire industry. The dataset is already in the correct format but misses some important features. Most importantly, we need to add a time index that is incremented by one for each time step. Further, it is beneficial to add date features, which in this case means extracting the month from the date record. from pytorch_forecasting.data.examples import get_stallion_data data = get_stallion_data() # add time index data[\"time_idx\"] = data[\"date\"].dt.year * 12 + data[\"date\"].dt.month data[\"time_idx\"] -= data[\"time_idx\"].min() # add additional features data[\"month\"] = data.date.dt.month.astype(str).astype( \"category\" ) # categories have be strings data[\"log_volume\"] = np.log(data.volume + 1e-8) data[\"avg_volume_by_sku\"] = data.groupby( [\"time_idx\", \"sku\"], observed=True ).volume.transform(\"mean\") data[\"avg_volume_by_agency\"] = data.groupby( [\"time_idx\", \"agency\"], observed=True ).volume.transform(\"mean\") # we want to encode special days as one variable and thus need to first reverse one-hot encoding special_days = [ \"easter_day\", \"good_friday\", \"new_year\", \"christmas\", \"labor_day\", \"independence_day\", \"revolution_day_memorial\", \"regional_games\", \"fifa_u_17_world_cup\", \"football_gold_cup\", \"beer_capital\", \"music_fest\", ] data[special_days] = ( data[special_days].apply(lambda x: x.map({0: \"-\", 1: x.name})).astype(\"category\") ) data.sample(10, random_state=521) The next step is to convert the dataframe into a PyTorch Forecasting . Apart from telling the dataset which features are categorical vs continuous and which are static vs varying in time, we also have to decide how we normalise the data. Here, we standard scale each time series separately and indicate that values are always positive. Generally, the , that scales dynamically on each encoder sequence as you train, is preferred to avoid look-ahead bias induced by normalisation. However, you might accept look-ahead bias if you are having troubles to find a reasonably stable normalisation, for example, because there are a lot of zeros in your data. Or you expect a more stable normalization in inference. In the later case, you ensure that you do not learn ‚Äúweird‚Äù jumps that will not be present when running inference, thus training on a more realistic data set. We also choose to use the last six months as a validation set. max_prediction_length = 6 max_encoder_length = 24 training_cutoff = data[\"time_idx\"].max() - max_prediction_length training = TimeSeriesDataSet( data[lambda x: x.time_idx <= training_cutoff], time_idx=\"time_idx\", target=\"volume\", group_ids=[\"agency\", \"sku\"], min_encoder_length=max_encoder_length // 2, # keep encoder length long (as it is in the validation set) max_encoder_length=max_encoder_length, min_prediction_length=1, max_prediction_length=max_prediction_length, static_categoricals=[\"agency\", \"sku\"], static_reals=[\"avg_population_2017\", \"avg_yearly_household_income_2017\"], time_varying_known_categoricals=[\"special_days\", \"month\"], variable_groups={ \"special_days\": special_days }, # group of categorical variables can be treated as one variable time_varying_known_reals=[\"time_idx\", \"price_regular\", \"discount_in_percent\"], time_varying_unknown_categoricals=[], time_varying_unknown_reals=[ \"volume\", \"log_volume\", \"industry_volume\", \"soda_volume\", \"avg_max_temp\", \"avg_volume_by_agency\", \"avg_volume_by_sku\", ], target_normalizer=GroupNormalizer( groups=[\"agency\", \"sku\"], transformation=\"softplus\" ), # use softplus and normalize by group add_relative_time_idx=True, add_target_scales=True, add_encoder_length=True, ) # create validation set (predict=True) which means to predict the last max_prediction_length points in time # for each series validation = TimeSeriesDataSet.from_dataset( training, data, predict=True, stop_randomization=True ) # create dataloaders for model batch_size = 128 # set this between 32 to 128 train_dataloader = training.to_dataloader( train=True, batch_size=batch_size, num_workers=0 ) val_dataloader = validation.to_dataloader( train=False, batch_size=batch_size * 10, num_workers=0 ) To learn more about the , visit its documentation or the tutorial explaining how to pass datasets to models.\n\nIt is now time to create our model. We train the model with PyTorch Lightning. Prior to training, you can identify the optimal learning rate with the PyTorch Lightning learning rate finder. # configure network and trainer pl.seed_everything(42) trainer = pl.Trainer( accelerator=\"cpu\", # clipping gradients is a hyperparameter and important to prevent divergance # of the gradient for recurrent neural networks gradient_clip_val=0.1, ) tft = TemporalFusionTransformer.from_dataset( training, # not meaningful for finding the learning rate but otherwise very important learning_rate=0.03, hidden_size=8, # most important hyperparameter apart from learning rate # number of attention heads. Set to up to 4 for large datasets attention_head_size=1, dropout=0.1, # between 0.1 and 0.3 are good values hidden_continuous_size=8, # set to <= hidden_size loss=QuantileLoss(), optimizer=\"ranger\", # reduce learning rate if no improvement in validation loss after x epochs # reduce_on_plateau_patience=1000, ) print(f\"Number of parameters in network: {tft.size() / 1e3:.1f}k\") Seed set to 42 GPU available: True (mps), used: False TPU available: False, using: 0 TPU cores HPU available: False, using: 0 HPUs `Trainer.fit` stopped: `max_steps=100` reached. Learning rate set to 0.0097723722095581 Restoring states from the checkpoint path at /Users/hirwa/Desktop/open/for/pytorch-forecasting/docs/source/tutorials/.lr_find_9962a241-9a34-45b4-b4b4-5fae485145c7.ckpt Restored all states from the checkpoint at /Users/hirwa/Desktop/open/for/pytorch-forecasting/docs/source/tutorials/.lr_find_9962a241-9a34-45b4-b4b4-5fae485145c7.ckpt For the , the optimal learning rate seems to be slightly lower than the suggested one. Further, we do not directly want to use the suggested learning rate because PyTorch Lightning sometimes can get confused by the noise at lower learning rates and suggests rates far too low. Manual control is essential. We decide to pick 0.03 as learning rate. If you have troubles training the model and get an error , consider either uninstalling tensorflow or first execute # configure network and trainer early_stop_callback = EarlyStopping( monitor=\"val_loss\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\" ) lr_logger = LearningRateMonitor() # log the learning rate logger = TensorBoardLogger(\"lightning_logs\") # logging results to a tensorboard trainer = pl.Trainer( max_epochs=50, accelerator=\"cpu\", enable_model_summary=True, gradient_clip_val=0.1, limit_train_batches=50, # coment in for training, running valiation every 30 batches # fast_dev_run=True, # comment in to check that networkor dataset has no serious bugs callbacks=[lr_logger, early_stop_callback], logger=logger, ) tft = TemporalFusionTransformer.from_dataset( training, learning_rate=0.03, hidden_size=16, attention_head_size=2, dropout=0.1, hidden_continuous_size=8, loss=QuantileLoss(), log_interval=10, # uncomment for learning rate finder and otherwise, e.g. to 10 for logging every 10 batches optimizer=\"ranger\", reduce_on_plateau_patience=4, ) print(f\"Number of parameters in network: {tft.size() / 1e3:.1f}k\") GPU available: True (mps), used: False TPU available: False, using: 0 TPU cores HPU available: False, using: 0 HPUs Training takes a couple of minutes on my Macbook but for larger networks and datasets, it can take hours. The training speed is here mostly determined by overhead and choosing a larger or (i.e. network size) does not slow does training linearly making training on large datasets feasible. During training, we can monitor the tensorboard which can be spun up with . For example, we can monitor examples predictions on the training and validation set. | Name | Type | Params | Mode ------------------------------------------------------------------------------------------------ 0 | loss | QuantileLoss | 0 | train 1 | logging_metrics | ModuleList | 0 | train 2 | input_embeddings | MultiEmbedding | 1.3 K | train 3 | prescalers | ModuleDict | 256 | train 4 | static_variable_selection | VariableSelectionNetwork | 3.4 K | train 5 | encoder_variable_selection | VariableSelectionNetwork | 8.0 K | train 6 | decoder_variable_selection | VariableSelectionNetwork | 2.7 K | train 7 | static_context_variable_selection | GatedResidualNetwork | 1.1 K | train 8 | static_context_initial_hidden_lstm | GatedResidualNetwork | 1.1 K | train 9 | static_context_initial_cell_lstm | GatedResidualNetwork | 1.1 K | train 10 | static_context_enrichment | GatedResidualNetwork | 1.1 K | train 11 | lstm_encoder | LSTM | 2.2 K | train 12 | lstm_decoder | LSTM | 2.2 K | train 13 | post_lstm_gate_encoder | GatedLinearUnit | 544 | train 14 | post_lstm_add_norm_encoder | AddNorm | 32 | train 15 | static_enrichment | GatedResidualNetwork | 1.4 K | train 16 | multihead_attn | InterpretableMultiHeadAttention | 808 | train 17 | post_attn_gate_norm | GateAddNorm | 576 | train 18 | pos_wise_ff | GatedResidualNetwork | 1.1 K | train 19 | pre_output_gate_norm | GateAddNorm | 576 | train 20 | output_layer | Linear | 119 | train ------------------------------------------------------------------------------------------------ 29.4 K Trainable params 0 Non-trainable params 29.4 K Total params 0.118 Total estimated model params size (MB) 480 Modules in train mode 0 Modules in eval mode Hyperparamter tuning with [optuna](https://optuna.org/) is directly build into pytorch-forecasting. For example, we can use the function to optimize the TFT‚Äôs hyperparameters. # use Optuna to find ideal learning rate or use in-built learning rate finder # save study results - also we can resume tuning at a later point in time\n\nPyTorch Lightning automatically checkpoints training and thus, we can easily retrieve the best model and load it. # load the best model according to the validation loss # (given that we use early stopping, this is not necessarily the last epoch) best_model_path = trainer.checkpoint_callback.best_model_path best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path) After training, we can make predictions with . The method allows very fine-grained control over what it returns so that, for example, you can easily match predictions to your pandas dataframe. See its documentation for details. We evaluate the metrics on the validation dataset and a couple of examples to see how well the model is doing. Given that we work with only 21 000 samples the results are very reassuring and can compete with results by a gradient booster. We also perform better than the baseline model. Given the noisy data, this is not trivial. GPU available: True (mps), used: False TPU available: False, using: 0 TPU cores HPU available: False, using: 0 HPUs We can now also look at sample predictions directly which we plot with . As you can see from the figures below, forecasts look rather accurate. If you wonder, the grey lines denote the amount of attention the model pays to different points in time when making the prediction. This is a special feature of the Temporal Fusion Transformer. # raw predictions are a dictionary from which all kind of information including quantiles can be extracted raw_predictions = best_tft.predict( val_dataloader, mode=\"raw\", return_x=True, trainer_kwargs=dict(accelerator=\"cpu\") ) GPU available: True (mps), used: False TPU available: False, using: 0 TPU cores HPU available: False, using: 0 HPUs Looking at the worst performers, for example in terms of , gives us an idea where the model has issues with forecasting reliably. These examples can provide important pointers about how to improve the model. This kind of actuals vs predictions plots are available to all models. Of course, it is also sensible to employ additional metrics, such as MASE, defined in the module. However, for the sake of demonstration, we only use here. # calcualte metric by which to display predictions = best_tft.predict( val_dataloader, return_y=True, trainer_kwargs=dict(accelerator=\"cpu\") ) mean_losses = SMAPE(reduction=\"none\").loss(predictions.output, predictions.y[0]).mean(1) indices = mean_losses.argsort(descending=True) # sort losses for idx in range(10): # plot 10 examples best_tft.plot_prediction( raw_predictions.x, raw_predictions.output, idx=indices[idx], add_loss_to_title=SMAPE(quantiles=best_tft.loss.quantiles), ) GPU available: True (mps), used: False TPU available: False, using: 0 TPU cores HPU available: False, using: 0 HPUs Checking how the model performs across different slices of the data allows us to detect weaknesses. Plotted below are the means of predictions vs actuals across each variable divided into 100 bins using the Now, we can directly predict on the generated data using the and methods. The gray bars denote the frequency of the variable by bin, i.e. are a histogram. GPU available: True (mps), used: False TPU available: False, using: 0 TPU cores HPU available: False, using: 0 HPUs {'avg_population_2017': <Figure size 1000x500 with 2 Axes>, 'avg_yearly_household_income_2017': <Figure size 1000x500 with 2 Axes>, 'encoder_length': <Figure size 1000x500 with 2 Axes>, 'volume_center': <Figure size 1000x500 with 2 Axes>, 'volume_scale': <Figure size 1000x500 with 2 Axes>, 'time_idx': <Figure size 1000x500 with 2 Axes>, 'price_regular': <Figure size 1000x500 with 2 Axes>, 'discount_in_percent': <Figure size 1000x500 with 2 Axes>, 'relative_time_idx': <Figure size 1000x500 with 2 Axes>, 'volume': <Figure size 1000x500 with 2 Axes>, 'log_volume': <Figure size 1000x500 with 2 Axes>, 'industry_volume': <Figure size 1000x500 with 2 Axes>, 'soda_volume': <Figure size 1000x500 with 2 Axes>, 'avg_max_temp': <Figure size 1000x500 with 2 Axes>, 'avg_volume_by_agency': <Figure size 1000x500 with 2 Axes>, 'avg_volume_by_sku': <Figure size 1000x500 with 2 Axes>, 'agency': <Figure size 1000x500 with 2 Axes>, 'sku': <Figure size 1000x500 with 2 Axes>, 'special_days': <Figure size 1000x500 with 2 Axes>, 'month': <Figure size 1000x500 with 2 Axes>}\n\nTo predict on a subset of data we can filter the subsequences in a dataset using the method. Here we predict for the subsequence in the dataset that maps to the group ids ‚ÄúAgency_01‚Äù and ‚ÄúSKU_01‚Äù and whose first predicted value corresponds to the time index ‚Äú15‚Äù. We output all seven quantiles. This means we expect a tensor of shape as we predict for a single subsequence six time steps ahead and 7 quantiles for each time step. GPU available: True (mps), used: False TPU available: False, using: 0 TPU cores HPU available: False, using: 0 HPUs Of course, we can also plot this prediction readily: GPU available: True (mps), used: False TPU available: False, using: 0 TPU cores HPU available: False, using: 0 HPUs\n\nThe model has inbuilt interpretation capabilities due to how its architecture is build. Let‚Äôs see how that looks. We first calculate interpretations with and plot them subsequently with . {'attention': <Figure size 640x480 with 1 Axes>, 'static_variables': <Figure size 700x375 with 1 Axes>, 'encoder_variables': <Figure size 700x525 with 1 Axes>, 'decoder_variables': <Figure size 700x350 with 1 Axes>} Unsurprisingly, the past observed volume features as the top variable in the encoder and price related variables are among the top predictors in the decoder. The general attention patterns seems to be that more recent observations are more important and older ones. This confirms intuition. The average attention is often not very useful - looking at the attention by example is more insightful because patterns are not averaged out. Partial dependency plots are often used to interpret the model better (assuming independence of features). They can be also useful to understand what to expect in case of simulations and are created with . GPU available: True (mps), used: False TPU available: False, using: 0 TPU cores HPU available: False, using: 0 HPUs GPU available: True (mps), used: False TPU available: False, using: 0 TPU cores HPU available: False, using: 0 HPUs GPU available: True (mps), used: False TPU available: False, using: 0 TPU cores HPU available: False, using: 0 HPUs GPU available: True (mps), used: False TPU available: False, using: 0 TPU cores HPU available: False, using: 0 HPUs GPU available: True (mps), used: False TPU available: False, using: 0 TPU cores HPU available: False, using: 0 HPUs GPU available: True (mps), used: False TPU available: False, using: 0 TPU cores HPU available: False, using: 0 HPUs GPU available: True (mps), used: False TPU available: False, using: 0 TPU cores HPU available: False, using: 0 HPUs GPU available: True (mps), used: False TPU available: False, using: 0 TPU cores HPU available: False, using: 0 HPUs GPU available: True (mps), used: False TPU available: False, using: 0 TPU cores HPU available: False, using: 0 HPUs GPU available: True (mps), used: False TPU available: False, using: 0 TPU cores HPU available: False, using: 0 HPUs GPU available: True (mps), used: False TPU available: False, using: 0 TPU cores HPU available: False, using: 0 HPUs GPU available: True (mps), used: False TPU available: False, using: 0 TPU cores HPU available: False, using: 0 HPUs GPU available: True (mps), used: False TPU available: False, using: 0 TPU cores HPU available: False, using: 0 HPUs GPU available: True (mps), used: False TPU available: False, using: 0 TPU cores HPU available: False, using: 0 HPUs GPU available: True (mps), used: False TPU available: False, using: 0 TPU cores HPU available: False, using: 0 HPUs GPU available: True (mps), used: False TPU available: False, using: 0 TPU cores HPU available: False, using: 0 HPUs GPU available: True (mps), used: False TPU available: False, using: 0 TPU cores HPU available: False, using: 0 HPUs GPU available: True (mps), used: False TPU available: False, using: 0 TPU cores HPU available: False, using: 0 HPUs GPU available: True (mps), used: False TPU available: False, using: 0 TPU cores HPU available: False, using: 0 HPUs GPU available: True (mps), used: False TPU available: False, using: 0 TPU cores HPU available: False, using: 0 HPUs GPU available: True (mps), used: False TPU available: False, using: 0 TPU cores HPU available: False, using: 0 HPUs GPU available: True (mps), used: False TPU available: False, using: 0 TPU cores HPU available: False, using: 0 HPUs GPU available: True (mps), used: False TPU available: False, using: 0 TPU cores HPU available: False, using: 0 HPUs GPU available: True (mps), used: False TPU available: False, using: 0 TPU cores HPU available: False, using: 0 HPUs GPU available: True (mps), used: False TPU available: False, using: 0 TPU cores HPU available: False, using: 0 HPUs GPU available: True (mps), used: False TPU available: False, using: 0 TPU cores HPU available: False, using: 0 HPUs GPU available: True (mps), used: False TPU available: False, using: 0 TPU cores HPU available: False, using: 0 HPUs GPU available: True (mps), used: False TPU available: False, using: 0 TPU cores HPU available: False, using: 0 HPUs GPU available: True (mps), used: False TPU available: False, using: 0 TPU cores HPU available: False, using: 0 HPUs GPU available: True (mps), used: False TPU available: False, using: 0 TPU cores HPU available: False, using: 0 HPUs"
    },
    {
        "link": "https://github.com/sktime/pytorch-forecasting",
        "document": "PyTorch Forecasting is a PyTorch-based package for forecasting with state-of-the-art deep learning architectures. It provides a high-level API and uses PyTorch Lightning to scale training on GPU or CPU, with automatic logging.\n\nOur article on Towards Data Science introduces the package and provides background information.\n\nPyTorch Forecasting aims to ease state-of-the-art timeseries forecasting with neural networks for real-world cases and research alike. The goal is to provide a high-level API with maximum flexibility for professionals and reasonable defaults for beginners. Specifically, the package provides\n‚Ä¢ A base model class which provides basic training of timeseries models along with logging in tensorboard and generic visualizations such actual vs predictions and dependency plots\n‚Ä¢ Multiple neural network architectures for timeseries forecasting that have been enhanced for real-world deployment and come with in-built interpretation capabilities\n\nThe package is built on pytorch-lightning to allow training on CPUs, single and multiple GPUs out-of-the-box.\n\nIf you are working on windows, you need to first install PyTorch with\n\nOtherwise, you can proceed with\n\nAlternatively, you can install the package via conda\n\nPyTorch Forecasting is now installed from the conda-forge channel while PyTorch is install from the pytorch channel.\n\nTo use the MQF2 loss (multivariate quantile loss), also install\n\nVisit https://pytorch-forecasting.readthedocs.io to read the documentation with detailed tutorials.\n\nThe documentation provides a comparison of available models.\n‚Ä¢ Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting which outperforms DeepAR by Amazon by 36-69% in benchmarks\n‚Ä¢ N-BEATS: Neural basis expansion analysis for interpretable time series forecasting which has (if used as ensemble) outperformed all other methods including ensembles of traditional statical methods in the M4 competition. The M4 competition is arguably the most important benchmark for univariate time series forecasting.\n‚Ä¢ N-HiTS: Neural Hierarchical Interpolation for Time Series Forecasting which supports covariates and has consistently beaten N-BEATS. It is also particularly well-suited for long-horizon forecasting.\n‚Ä¢ DeepAR: Probabilistic forecasting with autoregressive recurrent networks which is the one of the most popular forecasting algorithms and is often used as a baseline\n‚Ä¢ Simple standard networks for baselining: LSTM and GRU networks as well as a MLP on the decoder\n‚Ä¢ A baseline model that always predicts the latest known value\n\nTo implement new models or other custom components, see the How to implement new models tutorial. It covers basic as well as advanced architectures.\n\nNetworks can be trained with the PyTorch Lighning Trainer on pandas Dataframes which are first converted to a TimeSeriesDataSet.\n\n. . . . . , # import dataset, network to train and metric to optimize , , . . # load data: this is pandas dataframe with at least a column for # * the target (what you want to predict) # * the timeseries ID (which should be a unique string to identify each timeseries) # * the time of the observation (which should be a monotonically increasing integer) ... # define the dataset, i.e. add metadata to pandas dataframe for the model to understand it ( [ : . ], ..., # column name of time of observation ..., # column name of target to predict [ ... ], , # how much history to use , # how far to predict into future [ ... ], [ ... ], # covariates known and unknown in the future to inform prediction [ ... ], [ ... ], [ ... ], [ ... ], ) # create validation dataset using the same normalization techniques as for the training dataset . ( , , . . . () , ) . ( , , ) . ( , , ) ( , , , , ) () . ( , , # run on CPU, if on multiple GPUs, use strategy=\"ddp\" , , [ , ], ( ) ) # define network to train - the architecture is mostly inferred from the dataset, so that only a few hyperparameters have to be set by the user . ( , , , , , (), , , ) ( ) ( ). ( , , , , , ) # and plot the result - always visually confirm that the suggested learning rate makes sense ( ) . ( , ) . () # fit the model on the data - redefine the model with the correct learning rate if necessary . ( , , , )"
    },
    {
        "link": "https://medium.com/@melissamatiasf/building-my-own-time-series-transformer-model-using-pytorch-bc915fc17c29",
        "document": "Over the past few months, as part of a Applied-AI bootcamp, I‚Äôve been working on an exciting capstone project that combines data science, machine learning, and business insights. The project is designed as a real-world use case for S.Pellegrino¬Æ Essenza, a well-known beverage brand, to help optimize their sales forecasting and customer relationship management. This project simulates the challenges companies face in the highly competitive U.S. sparkling water market, offering insights and strategies that could enhance customer retention and boost sales efficiency. In this article, I‚Äôll walk you through how I built a predictive sales model using AWS, the challenges I faced, the techniques I applied, and how this model is set to transform the way S.Pellegrino approaches sales and customer management. In 2019, S.Pellegrino¬Æ Essenza launched its line of flavored sparkling water in the U.S., offering Mediterranean-inspired flavors like Lemon, Peach, and Cherry. This launch tapped into a dynamic market for flavored sparkling water, which, according to Statista data, has been growing by nearly 6% each year.\n\nHowever, as with any rapidly growing market, S.Pellegrino began facing significant challenges:\n‚Ä¢ Lack of an Accurate Sales Forecast: With varied product lines, regional variables, and fluctuating buying patterns, accurate sales forecasting became difficult.\n‚Ä¢ Difficulty in effectively managing customer relationships: Sales teams struggled to prioritize customers and missed out on potential upselling opportunities or failed to identify at-risk customers. Here‚Äôs what we heard from a sales team leader:\n\n‚ÄúAs a leader, I‚Äôm struggling to prioritize relationships with customers who have upselling potential and those at risk. I need data on time to optimize my team‚Äôs efforts.‚Äù This feedback led to a clear goal for our project: to predict future sales performance across customer segments ‚Äî High, Mid, and Low ‚Äî over a 6-month period. We believed that by improving forecasting and customer segmentation, S.Pellegrino could enhance customer retention, reduce churn, and optimize team efficiency. To solve this problem, we needed to build a predictive model that could handle the complexities of sales data across various customer segments. But first, we had to ensure we had the right data. To make this model effective, we focused on three critical data dimensions:\n\n# Create total_sales (order quantity * unit price)\n\ndf['total_sales'] = df['order_quantity'] * df['unit_price']\n\n\n\n# Create discounted_sales (total_sales after applying discount)\n\ndf['discounted_sales'] = df['total_sales'] * (1 - df['discount_applied'])\n\n\n\n# Create time-based features (year, month from order_date)\n\ndf['order_date'] = pd.to_datetime(df['order_date'])\n\ndf['year'] = df['order_date'].dt.year\n\ndf['month'] = df['order_date'].dt.month\n\n\n\n# Calculate customer lifetime value (Total sales per customer)\n\ncustomer_lifetime_value = df.groupby('customer_id')['discounted_sales'].sum().reset_index()\n\ncustomer_lifetime_value.columns = ['customer_id', 'lifetime_value']\n\n\n\n# Calculate team and customer performance (Aggregating total sales per sales team and customer)\n\nteam_performance = df.groupby('sales_team_id')['total_sales'].sum().reset_index()\n\nteam_performance.columns = ['sales_team_id', 'team_sales']\n\n\n\ncustomer_performance = df.groupby('customer_id')['total_sales'].sum().reset_index()\n\ncustomer_performance.columns = ['customer_id', 'customer_sales']\n\n\n\n# Merge back the customer lifetime value into the main dataframe\n\ndf = df.merge(customer_lifetime_value, on='customer_id', how='left')\n\n\n\n# Merge team and customer performance back into the main dataframe\n\ndf = df.merge(team_performance, on='sales_team_id', how='left')\n\ndf = df.merge(customer_performance, on='customer_id', how='left')\n\n\n\n# Calculate time since last order (Recency)\n\ndf['time_since_last_order'] = df.groupby('customer_id')['order_date'].transform('max')\n\ndf['time_since_last_order'] = (df['order_date'].max() - df['time_since_last_order']).dt.days\n\n\n\n# Calculate average sales per order by team\n\ndf['avg_sales_by_team'] = df.groupby('sales_team_id')['order_quantity'].transform('mean')\n\n\n\n# Calculate average discount per team\n\ndf['avg_discount_per_team'] = df.groupby('sales_team_id')['discount_applied'].transform('mean')\n\n\n\n# Display the first few rows to check the new features\n\ndf[['order_number', 'total_sales', 'discounted_sales', 'lifetime_value', 'team_sales', 'customer_sales', 'avg_sales_by_team', 'avg_discount_per_team', 'time_since_last_order']].head() 2. Customer Behavior & Segmentation: Clustering customers based on behavior and purchasing patterns to better target each group. This included calculating time since last purchase, frequency, and lifetime value.\n\nfrom sklearn.cluster import KMeans\n\nfrom sklearn.preprocessing import StandardScaler\n\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\n\n\n\n# Calculate 'frequency' as the number of orders per customer\n\ndf['frequency'] = df.groupby('customer_id')['order_number'].transform('count')\n\n\n\n# Define the features you want to use for clustering\n\nfeatures_for_clustering = ['lifetime_value', 'time_since_last_order', 'frequency']\n\n\n\n# Scale the features\n\nscaler = StandardScaler()\n\ndf_scaled = df.copy() # Create a copy of the original DataFrame\n\ndf_scaled[features_for_clustering] = scaler.fit_transform(df_scaled[features_for_clustering])\n\n\n\n# Apply K-Means clustering\n\nkmeans = KMeans(n_clusters=3, random_state=42)\n\ndf_scaled['customer_cluster'] = kmeans.fit_predict(df_scaled[features_for_clustering])\n\n\n\n# Add customer cluster labels to the original (non-scaled) data\n\ndf['customer_cluster'] = df_scaled['customer_cluster']\n\n\n\n# Display first few rows to check cluster assignment\n\nprint(df[['customer_id', 'lifetime_value', 'time_since_last_order', 'frequency', 'customer_cluster']].head())\n\n\n\n# Visualize customer clusters based on lifetime value and recency\n\nplt.figure(figsize=(10,6))\n\nsns.scatterplot(x='lifetime_value', y='time_since_last_order', hue='customer_cluster', data=df, palette='Set1')\n\nplt.title('Customer Clusters based on Lifetime Value and Recency')\n\nplt.xlabel('Lifetime Value')\n\nplt.ylabel('Time since last purchase')\n\nplt.show() 3. CRM Integration & Automation: Integrating this data into CRM tools like Monday, Salesforce, or Slack to alert sales teams about potential upselling or churn-risk customers.\n\nBuilding the Model: Choosing the Right Approach With our sample data in place, I turned to machine learning to help me achieve my goal. After evaluating several options, I decided to implement a Time-Series Transformer model for forecasting. This model was the perfect fit because it allowed us to handle:\n‚Ä¢ The need for speed and scalability in a dynamic market. # Let's start by importing the necessary libraries and modules like PyTorch\n\n\n\nimport torch\n\nimport torch.nn as nn\n\nimport torch.optim as optim\n\nfrom torch.utils.data import DataLoader, TensorDataset The Transformer model has the advantage of capturing both short-term trends and long-term patterns in the data. It‚Äôs particularly suited for time-series forecasting because of its ability to focus on different parts of the input sequence (i.e., historical sales data) to make accurate future predictions.\n‚Ä¢ 7 inputs such as sales data, customer segments, and seasonal trends.\n‚Ä¢ 6 encoder and 6 decoder layers, which help the model learn from past data and predict future outcomes.\n‚Ä¢ Attention Mechanism to identify which trends and patterns in the data are most important for predictions. This approach gave us a high degree of accuracy in predicting future sales performance, allowing S.Pellegrino to better prioritize their customer relationships and team efforts. In this example, I use the sample dataset for demonstration purposes. In practice, you would use a larger dataset, preprocess the text, and create vocabulary mappings for source and target languages. src_vocab_size = 5000\n\ntgt_vocab_size = 5000\n\nd_model = 512\n\nnum_heads = 8\n\nnum_encoder_layers = 6\n\nnum_decoder_layers = 6 \n\nd_ff = 2048\n\nmax_seq_length = 100\n\ndropout = 0.1\n\ninput_dim = 7 \n\n\n\ntransformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)\n\n\n\n# Initialize the Transformer model with the parameters\n\nmodel = TimeSeriesTransformer(d_model, n_heads, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout, input_dim) To train the model, we split our dataset into 80% for training and 20% for testing. This ensured that the model learned effectively from historical data while being tested on unseen data to prevent overfitting. We applied several techniques to improve accuracy, including:\n‚Ä¢ Mean Squared Error (MSE): Measuring how close the predictions were to actual sales.\n‚Ä¢ Cross-Validation: Splitting the dataset into smaller subsets to validate the model‚Äôs performance. After several epochs of training, we fine-tuned the model, reducing the error rate and improving the accuracy of our predictions. Now we‚Äôll train the model using the sample data. In practice, you would use a larger dataset and split it into training and validation sets. learning_rate = 1e-4\n\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\nloss_fn = nn.MSELoss()\n\n\n\ndef train_model(model, optimizer, loss_fn, train_loader, test_loader, epochs=10):\n\n model.train()\n\n for epoch in range(10):\n\n running_loss = 0.0\n\n for i, (inputs, targets) in enumerate(train_loader):\n\n optimizer.zero_grad()\n\n outputs = model(inputs)\n\n loss = loss_fn(outputs, targets)\n\n loss.backward()\n\n optimizer.step()\n\n running_loss += loss.item()\n\n print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader)}\")\n\n return model\n\n\n\n# Train the model\n\nmodel = train_model(model, optimizer, loss_fn, train_loader, test_loader, epochs=10) Once the model was built and trained, we needed to make it user-friendly and accessible to the sales team. For this, we chose to prototype using Gradio, a powerful tool that allows users to interact with machine learning models through an intuitive interface.\n‚Ä¢ Get immediate predictions about customer risk or upselling potential.\n‚Ä¢ Visualize the sales forecast over the next 6 months, allowing for more strategic decision-making. import gradio as gr\n\nmodel = TimeSeriesTransformer(d_model=256, n_heads=4, num_encoder_layers=6, num_decoder_layers=6, dim_feedforward=1024, dropout=0.1, input_dim=7)\n\nmodel.load_state_dict(torch.load('load file'), strict=False)\n\nmodel.eval()\n\n\n\n# Mock to define a simple function to be used in the interface\n\ndef classify_and_predict(name, months):\n\n # Map customer name to a specific segment and mock sales and lifetime value\n\n if name.lower() == \"Nipro Corp\":\n\n segment = \"High value customer\"\n\n base_sales = 6000 # Mock base sales for Nipro \n\n elif name.lower() == \"Elorac Corp\":\n\n segment = \"Low value customer\"\n\n base_sales = 1000 # Mock base sales for Linda\n\n sales_team_id = 10 # Example Sales Team ID for Linda\n\n return segment, f\"${predicted_team_sales:.2f} (Team ID: {sales_team_id})\", f\"${predicted_lifetime_value:.2f}\" The Vision is to Integrate with Tools like Slack to have Real-Time Alerts and Actions Our long-term vision for this project is to fully integrate the model into CRM tools like Salesforce, Slack or Monday. By doing this, we can automate real-time notifications for the sales team, alerting them when a customer is at risk of churn or presents an upselling opportunity.\n\nüî• Building my own predictive sales model for S.Pellegrino¬Æ Essenza has been a rewarding challenge, both as a technical exercise and as a transformative learning experience. As part of a bootcamp where I collaborated with talented women in tech, I had the opportunity to dive deep into the full AI lifecycle ‚Äî from data collection and model building to deployment and integration with business tools. One key takeaway from this experience is how valuable it is for designers to understand the complete AI cycle. As designers, having knowledge of machine learning and AI not only enhances our ability to create more intuitive and impactful products, but also helps us communicate better with data scientists and engineers. This cross-functional knowledge enables us to design more holistic solutions that drive business outcomes, like optimizing sales forecasting, customer relationship management and business process."
    },
    {
        "link": "https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1",
        "document": "In languages, the order of the words and their position in a sentence really matters. The meaning of the entire sentence can change if the words are re-ordered. When implementing NLP solutions, recurrent neural networks have an inbuilt mechanism that deals with the order of sequences. The transformer model, however, does not use recurrence or convolution and treats each data point as independent of the other. Hence, positional information is added to the model explicitly to retain the information regarding the order of words in a sentence. Positional encoding is the scheme through which the knowledge of the order of objects in a sequence is maintained.\n\nFor this tutorial, we‚Äôll simplify the notations used in this remarkable paper, Attention Is All You Need by Vaswani et al. After completing this tutorial, you will know:\n‚Ä¢ What is positional encoding, and why it‚Äôs important\n‚Ä¢ Code and visualize a positional encoding matrix in Python using NumPy\n\nKick-start your project with my book Building Transformer Models with Attention. It provides self-study tutorials with working code to guide you into building a fully-working transformer model that can\n\ntranslate sentences from one language to another...\n\nThis tutorial is divided into four parts; they are:\n\nPositional encoding describes the location or position of an entity in a sequence so that each position is assigned a unique representation. There are many reasons why a single number, such as the index value, is not used to represent an item‚Äôs position in transformer models. For long sequences, the indices can grow large in magnitude. If you normalize the index value to lie between 0 and 1, it can create problems for variable length sequences as they would be normalized differently.\n\nTransformers use a smart positional encoding scheme, where each position/index is mapped to a vector. Hence, the output of the positional encoding layer is a matrix, where each row of the matrix represents an encoded object of the sequence summed with its positional information. An example of the matrix that encodes only the positional information is shown in the figure below.\n\nThis is a quick recap of sine functions; you can work equivalently with cosine functions. The function‚Äôs range is [-1,+1]. The frequency of this waveform is the number of cycles completed in one second. The wavelength is the distance over which the waveform repeats itself. The wavelength and frequency for different waveforms are shown below:\n\nLet‚Äôs dive straight into this. Suppose you have an input sequence of length $L$ and require the position of the $k^{th}$ object within this sequence. The positional encoding is given by sine and cosine functions of varying frequencies:\n\n$k$: Position of an object in the input sequence, $0 \\leq k < L/2$\n\n$P(k, j)$: Position function for mapping a position $k$ in the input sequence to index $(k,j)$ of the positional matrix\n\n$n$: User-defined scalar, set to 10,000 by the authors of Attention Is All You Need.\n\n$i$: Used for mapping to column indices $0 \\leq i < d/2$, with a single value of $i$ maps to both sine and cosine functions\n\nIn the above expression, you can see that even positions correspond to a sine function and odd positions correspond to cosine functions.\n\nTo understand the above expression, let‚Äôs take an example of the phrase ‚ÄúI am a robot,‚Äù with n=100 and d=4. The following table shows the positional encoding matrix for this phrase. In fact, the positional encoding matrix would be the same for any four-letter phrase with n=100 and d=4.\n\nHere is a short Python code to implement positional encoding using NumPy. The code is simplified to make the understanding of positional encoding easier.\n\nTo understand the positional encoding, let‚Äôs start by looking at the sine wave for different positions with n=10,000 and d=512. \n\n The following figure is the output of the above code: The following figure is the output of the above code: You can see that each position $k$ corresponds to a different sinusoid, which encodes a single position into a vector. If you look closely at the positional encoding function, you can see that the wavelength for a fixed $i$ is given by: Hence, the wavelengths of the sinusoids form a geometric progression and vary from $2\\pi$ to $2\\pi n$. The scheme for positional encoding has a number of advantages.\n‚Ä¢ The sine and cosine functions have values in [-1, 1], which keeps the values of the positional encoding matrix in a normalized range.\n‚Ä¢ As the sinusoid for each position is different, you have a unique way of encoding each position.\n‚Ä¢ You have a way of measuring or quantifying the similarity between different positions, hence enabling you to encode the relative positions of words. Let‚Äôs visualize the positional matrix on bigger values. Use Python‚Äôs method from the library. Setting n=10,000 as done in the original paper, you get the following: What Is the Final Output of the Positional Encoding Layer? The positional encoding layer sums the positional vector with the word encoding and outputs this matrix for the subsequent layers. The entire process is shown below. This section provides more resources on the topic if you are looking to go deeper.\n‚Ä¢ Attention Is All You Need, 2017. In this tutorial, you discovered positional encoding in transformers.\n‚Ä¢ What is positional encoding, and why it is needed.\n‚Ä¢ How to implement positional encoding in Python using NumPy\n‚Ä¢ How to visualize the positional encoding matrix Do you have any questions about positional encoding discussed in this post? Ask your questions in the comments below, and I will do my best to answer."
    },
    {
        "link": "https://medium.com/we-talk-data/in-depth-guide-on-pytorchs-nn-transformer-901ad061a195",
        "document": "Picture this: you‚Äôre working on a challenging NLP project, aiming to build something like a translation or text summarization system. You know that traditional RNN-based models often struggle with long-term dependencies and require a lot of parameter tuning to get the right sequence-level attention. This is where PyTorch‚Äôs steps in. With its core design inspired by the transformer architecture (originally by Vaswani et al., 2017), it enables you to build powerful sequence-to-sequence models that handle long dependencies with ease. Here‚Äôs the deal: the module isn‚Äôt just a high-level wrapper. It‚Äôs built to optimize the entire attention mechanism, allowing your model to focus dynamically on different parts of the input sequence without sequential processing‚Äîmeaning faster and more efficient training. You might be wondering: ‚ÄúWhy not just build a transformer from scratch?‚Äù Well, the comes with some handy built-in features that eliminate the boilerplate. Here‚Äôs a quick rundown of its top benefits:\n‚Ä¢ Native Multi-Head Self-Attention: Implementing multi-head attention from scratch is time-consuming. Here, does it for you, letting you configure the number of heads while handling the computations.\n‚Ä¢ Built-in Positional Encoding: Sequence data lacks inherent order, but transformers need that. This module provides a robust positional encoding mechanism to add order information, saving you from implementing it separately.\n‚Ä¢ Flexible Encoder-Decoder Setup: With options to customize the encoder and decoder stack, you can adapt this module to tasks beyond the standard language model setup ‚Äî think multimodal models or image transformers. truly shines in applications requiring high-fidelity attention mechanisms across sequences, such as:\n‚Ä¢ Language Translation: Transformers are the backbone of machine translation because of their capability to manage long and complex sentence structures.\n‚Ä¢ Text Summarization: For abstractive summarization, transformers capture both the context and essence of text sequences, enabling more fluent summaries.\n‚Ä¢ Speech Recognition and Generation: Recent advancements have shown transformers handling not just text but audio sequences, making them viable for tasks beyond NLP.\n\nNow, let‚Äôs break down the anatomy of so you can see where each component fits and customize it to your needs.\n‚Ä¢ Multi-Head Attention Multi-head attention allows the model to attend to different parts of a sequence in parallel. With , you can specify the number of attention heads ( ) to control how many \"perspectives\" the model has on the data. For example, in language translation, this feature enables the model to attend to different word relationships in a sentence‚Äîcontextually powerful.\n‚Ä¢ Positional Encoding Since transformers operate without recurrence, they need positional encodings to understand the order of elements in a sequence. This encoding is added to the word embeddings, giving the model information about the position of each word in the sequence. import torch\n\nimport math\n\n\n\ndef positional_encoding(seq_len, embed_dim):\n\n pe = torch.zeros(seq_len, embed_dim)\n\n for pos in range(seq_len):\n\n for i in range(0, embed_dim, 2):\n\n pe[pos, i] = math.sin(pos / (10000 ** (2 * i / embed_dim)))\n\n pe[pos, i + 1] = math.cos(pos / (10000 ** (2 * i / embed_dim)))\n\n return pe\n\n\n\npe = positional_encoding(seq_len=10, embed_dim=512)\n\nprint(pe.shape) # Output: torch.Size([10, 512]) This code helps you visualize how positional encoding brings order to the transformer‚Äôs input. 3. Feed-Forward Network Each transformer layer includes a feed-forward network, which processes each position independently. Configuring the hidden dimensions of this network allows you to control the depth and richness of the model‚Äôs understanding. Use higher hidden dimensions for complex tasks that require nuanced attention. Now, let‚Äôs see how these components come together in a simple configuration: This example initializes a transformer model with an encoder-decoder structure. You can start customizing from here based on your data and task requirements.\n\nIn this section, you‚Äôll create a baseline transformer model. Let‚Äôs get into some of the critical setup details: The input dimensions in are crucial. Set these dimensions based on your task‚Äîtext, images, or even multimodal data. Here‚Äôs a quick rundown:\n‚Ä¢ : This defines the embedding dimension for both source and target sequences. For text, this might be between 128 and 1024, depending on your computational budget and model needs.\n‚Ä¢ Sequence Length: Always pad your sequences to ensure uniform input dimensions, especially when dealing with variable-length data. This might surprise you, but setting the right parameters in can drastically influence your model‚Äôs effectiveness:\n‚Ä¢ : More heads mean more perspectives, but also more computational load. For NLP tasks, 8 to 12 heads is standard.\n‚Ä¢ and : Increasing layers deepens the model‚Äôs understanding but requires careful tuning. Start with 6 layers for each and scale up based on validation performance.\n‚Ä¢ : Controls the size of the hidden layers in each transformer block. Increasing this value lets your model capture complex patterns in the data. Here‚Äôs a basic encoder-decoder setup using to get you started: This setup provides a full encoder-decoder structure, allowing you to process sequences with attention, positional encoding, and embedded token representations. You can now adjust hyperparameters and extend this base model to fit your specific use case.\n\nWhen working with transformers, data preparation is often the silent backbone that can make or break your model‚Äôs performance. Here‚Äôs how to tackle it like a pro: Tokenization isn‚Äôt just about breaking text into words or subwords ‚Äî it‚Äôs about finding that sweet spot between vocabulary size and model efficiency. Techniques like Byte Pair Encoding (BPE) and SentencePiece allow you to create a compact, subword vocabulary that balances expressiveness with computational load. Let‚Äôs see how BPE tokenization can be implemented: from transformers import GPT2Tokenizer\n\n\n\n# Initialize a BPE tokenizer (from Hugging Face Transformers library)\n\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n\ntokenizer.add_special_tokens({'pad_token': '[PAD]'})\n\n\n\n# Sample text\n\ntext = \"Transformers handle sequences efficiently!\"\n\ntokens = tokenizer.encode_plus(text, padding='max_length', max_length=12, return_tensors='pt')\n\nprint(tokens['input_ids']) # Encoded token IDs with padding This code initializes a BPE tokenizer, which efficiently handles out-of-vocabulary words by breaking them into subwords. Notice the padding setup; it‚Äôs crucial to keep sequence lengths consistent for batch processing. Transformers don‚Äôt naturally recognize sequence order, so positional encodings are added to give tokens context. Here‚Äôs a way to generate and visualize positional encodings to get a feel for how they represent sequence order: import matplotlib.pyplot as plt\n\n\n\ndef visualize_positional_encoding(seq_len=50, embed_dim=128):\n\n pe = torch.zeros(seq_len, embed_dim)\n\n for pos in range(seq_len):\n\n for i in range(0, embed_dim, 2):\n\n pe[pos, i] = math.sin(pos / (10000 ** (2 * i / embed_dim)))\n\n pe[pos, i + 1] = math.cos(pos / (10000 ** (2 * i / embed_dim)))\n\n plt.figure(figsize=(10, 6))\n\n plt.imshow(pe.numpy())\n\n plt.colorbar()\n\n plt.title(\"Positional Encoding Matrix\")\n\n plt.xlabel(\"Embedding Dimension\")\n\n plt.ylabel(\"Position\")\n\n plt.show()\n\n\n\nvisualize_positional_encoding() This visualization helps you understand how the positional encoding matrix assigns unique patterns to each position in a sequence. This might surprise you, but how you batch data impacts transformer performance significantly. To efficiently handle variable-length sequences, use padding and masking, and sort your sequences by length before batching. Here‚Äôs a quick example: This ensures that your sequences are padded to the maximum length in the batch, keeping computational requirements balanced.\n\nNow, let‚Äôs look at taking to the next level by customizing it for advanced tasks, integrating custom layers, and building a robust training pipeline. Here‚Äôs the deal: for specific tasks like machine translation or text generation, the right set of hyperparameters can make all the difference. Below is a custom configuration for a translation model: This transformer setup has been fine-tuned with parameters optimal for translation tasks, allowing you to quickly configure the model for end-to-end applications. Sometimes, the standard transformer setup might not be enough. Adding custom layers, like a projection layer at the end or a custom embedding layer, can enhance your model‚Äôs flexibility. Here‚Äôs how to integrate a simple projection layer for your output: Adding custom layers like this lets you adapt to more specialized tasks, from image processing to multimodal models. Finally, let‚Äôs talk about setting up a complete training loop. Here, you‚Äôll see how to integrate data loading, masking, gradient accumulation, and evaluation steps into one cohesive pipeline: import torch.optim as optim\n\n\n\ndef train_transformer(model, data_loader, criterion, optimizer, num_epochs=10):\n\n model.train()\n\n for epoch in range(num_epochs):\n\n total_loss = 0\n\n for batch in data_loader:\n\n src, tgt = batch[\"src\"], batch[\"tgt\"]\n\n tgt_input = tgt[:, :-1]\n\n tgt_output = tgt[:, 1:]\n\n\n\n # Generate masks\n\n src_mask = generate_padding_mask(src)\n\n tgt_mask = generate_look_ahead_mask(tgt_input.size(1))\n\n\n\n # Forward pass\n\n optimizer.zero_grad()\n\n predictions = model(src, tgt_input, src_mask=src_mask, tgt_mask=tgt_mask)\n\n loss = criterion(predictions.transpose(1, 2), tgt_output)\n\n \n\n # Backward pass and optimization\n\n loss.backward()\n\n optimizer.step()\n\n total_loss += loss.item()\n\n\n\n print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(data_loader)}\")\n\n\n\n# Sample usage\n\nmodel = ExtendedTransformerModel(src_vocab_size=10000, tgt_vocab_size=10000)\n\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\ncriterion = nn.CrossEntropyLoss(ignore_index=0) This training loop shows how to bring all components together, from loading data and generating masks to computing gradients and updating weights. Fine-tune parameters like the learning rate and batch size based on your task and hardware constraints.\n\nAs you know, transformers are powerful but resource-intensive. Here‚Äôs how to make the most of your GPU memory, reduce computation time, and avoid common pitfalls. You might be wondering, ‚ÄúCan I make my model faster without compromising performance?‚Äù The answer is yes, with a few smart strategies:\n‚Ä¢ Mixed Precision Training: Using 16-bit floating-point precision instead of the default 32-bit can drastically cut down memory usage without sacrificing much accuracy. PyTorch makes it easy with : from torch.cuda.amp import autocast, GradScaler\n\n\n\nscaler = GradScaler() # Initialize gradient scaler for mixed precision\n\n\n\nfor inputs, targets in data_loader:\n\n optimizer.zero_grad()\n\n with autocast(): # Enables mixed precision\n\n outputs = model(inputs)\n\n loss = criterion(outputs, targets)\n\n scaler.scale(loss).backward() # Scale loss for stability\n\n scaler.step(optimizer)\n\n scaler.update() Mixed precision training is especially useful when training on large datasets or with limited GPU memory.\n‚Ä¢ Gradient Checkpointing: This technique saves memory by recomputing certain layers during the backward pass rather than storing all intermediate activations. You can wrap specific layers with : Transformers are notorious for subtle bugs, especially when dealing with sequence lengths and masking. Here‚Äôs what to watch for:\n‚Ä¢ Mismatched Input Dimensions: Always double-check that the dimensions of your inputs align with the expected and values in .\n‚Ä¢ Incorrect Masking: Forgetting or incorrectly setting masks can cause your model to attend to irrelevant tokens. Verify masks by visualizing them during debugging. If something seems off, inspect the output shape of each layer, especially after adding custom layers, to catch dimension mismatches early. One often overlooked factor is learning rate scheduling. Transformers, especially large ones, can benefit from warm-up and decay schedules, which help stabilize training. Here‚Äôs a custom learning rate scheduler with warm-up, inspired by the schedule in the original transformer paper: import math\n\n\n\ndef get_lr(step, d_model=512, warmup_steps=4000):\n\n return d_model ** -0.5 * min(step ** -0.5, step * warmup_steps ** -1.5)\n\n\n\n# Example usage within a training loop\n\nfor step in range(num_training_steps):\n\n lr = get_lr(step)\n\n optimizer.param_groups[0]['lr'] = lr This approach gives you a gradual learning rate increase during the initial steps, which prevents instability early on, followed by a smooth decay."
    },
    {
        "link": "https://datacamp.com/tutorial/building-a-transformer-with-py-torch",
        "document": "First introduced in the paper Attention is All You Need by Vaswani et al., Transformers have since become a cornerstone of many NLP tasks due to their unique design and effectiveness.\n\nAt the heart of Transformers is the attention mechanism, specifically the concept of 'self-attention,' which allows the model to weigh and prioritize different parts of the input data. This mechanism is what enables Transformers to manage long-range dependencies in data. It is fundamentally a weighting scheme that allows a model to focus on different parts of the input when producing an output.\n\nThis mechanism allows the model to consider different words or features in the input sequence, assigning each one a 'weight' that signifies its importance for producing a given output.\n\nFor instance, in a sentence translation task, while translating a particular word, the model might assign higher attention weights to words that are grammatically or semantically related to the target word. This process allows the Transformer to capture dependencies between words or features, regardless of their distance from each other in the sequence.\n\nTransformers' impact in the field of NLP cannot be overstated. They have outperformed traditional models in many tasks, demonstrating superior capacity to comprehend and generate human language in a more nuanced way.\n\nFor a deeper understanding of NLP, DataCamp's Introduction to Natural Language Processing in Python course is a recommended resource.\n\nBefore diving into building a Transformer, it is essential to set up the working environment correctly. First and foremost, PyTorch needs to be installed. PyTorch (current stable version - 2.0.1) can be easily installed through pip or conda package managers.\n\nFor pip, use the command:\n\nFor conda, use the command:\n\nFor using pytorch with a cpu kindly visit the pytorch documentation.\n\nAdditionally, it is beneficial to have a basic understanding of deep learning concepts, as these will be fundamental to understanding the operation of Transformers. For those who need a refresher, the DataCamp course Deep Learning in Python is a valuable resource that covers key concepts in deep learning.\n\nTo build the Transformer model the following steps are necessary:\n‚Ä¢ Combining the Encoder and Decoder layers to create the complete Transformer network\n\n1. Importing the necessary libraries and modules\n\nWe‚Äôll start with importing the PyTorch library for core functionality, the neural network module for creating neural networks, the optimization module for training networks, and the data utility functions for handling data. Additionally, we‚Äôll import the standard Python math module for mathematical operations and the copy module for creating copies of complex objects.\n\nThese tools set the foundation for defining the model's architecture, managing data, and establishing the training process.\n\nThe Multi-Head Attention mechanism computes the attention between each pair of positions in a sequence. It consists of multiple ‚Äúattention heads‚Äù that capture different aspects of the input sequence.\n\nTo know more about Multi-Head Attention, check out this Attention mechanisms section of the Large Language Models (LLMs) Concepts course.\n\nThe class is defined as a subclass of PyTorch's nn.Module.\n‚Ä¢ num_heads: The number of attention heads to split the input into.\n\nThe initialization checks if d_model is divisible by num_heads, and then defines the transformation weights for query, key, value, and output.\n‚Ä¢ Calculating Attention Scores: attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k). Here, the attention scores are calculated by taking the dot product of queries (Q) and keys (K), and then scaling by the square root of the key dimension (d_k).\n‚Ä¢ Applying Mask: If a mask is provided, it is applied to the attention scores to mask out specific values.\n‚Ä¢ Calculating Attention Weights: The attention scores are passed through a softmax function to convert them into probabilities that sum to 1.\n‚Ä¢ Calculating Output: The final output of the attention is calculated by multiplying the attention weights by the values (V).\n\nThis method reshapes the input x into the shape (batch_size, num_heads, seq_length, d_k). It enables the model to process multiple attention heads concurrently, allowing for parallel computation.\n\nAfter applying attention to each head separately, this method combines the results back into a single tensor of shape (batch_size, seq_length, d_model). This prepares the result for further processing.\n\nThe forward method is where the actual computation happens:\n‚Ä¢ Apply Linear Transformations: The queries (Q), keys (K), and values (V) are first passed through linear transformations using the weights defined in the initialization.\n‚Ä¢ Split Heads: The transformed Q, K, V are split into multiple heads using the split_heads method.\n‚Ä¢ Apply Scaled Dot-Product Attention: The scaled_dot_product_attention method is called on the split heads.\n‚Ä¢ Combine Heads: The results from each head are combined back into a single tensor using the combine_heads method.\n‚Ä¢ Apply Output Transformation: Finally, the combined tensor is passed through an output linear transformation.\n\nIn summary, the MultiHeadAttention class encapsulates the multi-head attention mechanism commonly used in transformer models. It takes care of splitting the input into multiple attention heads, applying attention to each head, and then combining the results. By doing so, the model can capture various relationships in the input data at different scales, improving the expressive ability of the model.\n\nThe class is a subclass of PyTorch's nn.Module, which means it will inherit all functionalities required to work with neural network layers.\n‚Ä¢ d_model: Dimensionality of the model's input and output.\n‚Ä¢ d_ff: Dimensionality of the inner layer in the feed-forward network.\n‚Ä¢ self.fc1 and self.fc2: Two fully connected (linear) layers with input and output dimensions as defined by d_model and d_ff.\n‚Ä¢ self.relu: ReLU (Rectified Linear Unit) activation function, which introduces non-linearity between the two linear layers.\n‚Ä¢ x: The input to the feed-forward network.\n‚Ä¢ self.fc1(x): The input is first passed through the first linear layer (fc1).\n‚Ä¢ self.relu(...): The output of fc1 is then passed through a ReLU activation function. ReLU replaces all negative values with zeros, introducing non-linearity into the model.\n‚Ä¢ self.fc2(...): The activated output is then passed through the second linear layer (fc2), producing the final output.\n\nIn summary, the PositionWiseFeedForward class defines a position-wise feed-forward neural network that consists of two linear layers with a ReLU activation function in between. In the context of transformer models, this feed-forward network is applied to each position separately and identically. It helps in transforming the features learned by the attention mechanisms within the transformer, acting as an additional processing step for the attention outputs.\n\nPositional Encoding is used to inject the position information of each token in the input sequence. It uses sine and cosine functions of different frequencies to generate the positional encoding.\n\nThe class is defined as a subclass of PyTorch's nn.Module, allowing it to be used as a standard PyTorch layer.\n‚Ä¢ d_model: The dimension of the model's input.\n‚Ä¢ max_seq_length: The maximum length of the sequence for which positional encodings are pre-computed.\n‚Ä¢ pe: A tensor filled with zeros, which will be populated with positional encodings.\n‚Ä¢ position: A tensor containing the position indices for each position in the sequence.\n‚Ä¢ div_term: A term used to scale the position indices in a specific way.\n‚Ä¢ The sine function is applied to the even indices and the cosine function to the odd indices of pe.\n‚Ä¢ Finally, pe is registered as a buffer, which means it will be part of the module's state but will not be considered a trainable parameter.\n\nThe forward method simply adds the positional encodings to the input x.\n\nIt uses the first x.size(1) elements of pe to ensure that the positional encodings match the actual sequence length of x.\n\nThe PositionalEncoding class adds information about the position of tokens within the sequence. Since the transformer model lacks inherent knowledge of the order of tokens (due to its self-attention mechanism), this class helps the model to consider the position of tokens in the sequence. The sinusoidal functions used are chosen to allow the model to easily learn to attend to relative positions, as they produce a unique and smooth encoding for each position in the sequence.\n\nFigure 2. The Encoder part of the transformer network (Source: image from the original paper)\n\nThe class is defined as a subclass of PyTorch's nn.Module, which means it can be used as a building block for neural networks in PyTorch.\n‚Ä¢ d_model: The dimensionality of the input.\n‚Ä¢ num_heads: The number of attention heads in the multi-head attention.\n‚Ä¢ d_ff: The dimensionality of the inner layer in the position-wise feed-forward network.\n‚Ä¢ dropout: The dropout rate used for regularization.\n‚Ä¢ self.norm1 and self.norm2: Layer normalization, applied to smooth the layer's input.\n‚Ä¢ self.dropout: Dropout layer, used to prevent overfitting by randomly setting some activations to zero during training.\n‚Ä¢ x: The input to the encoder layer.\n‚Ä¢ mask: Optional mask to ignore certain parts of the input.\n‚Ä¢ Self-Attention: The input x is passed through the multi-head self-attention mechanism.\n‚Ä¢ Add & Normalize (after Attention): The attention output is added to the original input (residual connection), followed by dropout and normalization using norm1.\n‚Ä¢ Feed-Forward Network: The output from the previous step is passed through the position-wise feed-forward network.\n‚Ä¢ Add & Normalize (after Feed-Forward): Similar to step 2, the feed-forward output is added to the input of this stage (residual connection), followed by dropout and normalization using norm2.\n‚Ä¢ Output: The processed tensor is returned as the output of the encoder layer.\n\nThe EncoderLayer class defines a single layer of the transformer's encoder. It encapsulates a multi-head self-attention mechanism followed by position-wise feed-forward neural network, with residual connections, layer normalization, and dropout applied as appropriate. These components together allow the encoder to capture complex relationships in the input data and transform them into a useful representation for downstream tasks. Typically, multiple such encoder layers are stacked to form the complete encoder part of a transformer model.\n‚Ä¢ d_model: The dimensionality of the input.\n‚Ä¢ num_heads: The number of attention heads in the multi-head attention.\n‚Ä¢ d_ff: The dimensionality of the inner layer in the feed-forward network.\n‚Ä¢ self.cross_attn: Multi-head attention mechanism that attends to the encoder's output.\n‚Ä¢ x: The input to the decoder layer.\n‚Ä¢ enc_output: The output from the corresponding encoder (used in the cross-attention step).\n‚Ä¢ src_mask: Source mask to ignore certain parts of the encoder's output.\n‚Ä¢ tgt_mask: Target mask to ignore certain parts of the decoder's input.\n‚Ä¢ Self-Attention on Target Sequence: The input x is processed through a self-attention mechanism.\n‚Ä¢ Add & Normalize (after Self-Attention): The output from self-attention is added to the original x, followed by dropout and normalization using norm1.\n‚Ä¢ Cross-Attention with Encoder Output: The normalized output from the previous step is processed through a cross-attention mechanism that attends to the encoder's output enc_output.\n‚Ä¢ Add & Normalize (after Cross-Attention): The output from cross-attention is added to the input of this stage, followed by dropout and normalization using norm2.\n‚Ä¢ Feed-Forward Network: The output from the previous step is passed through the feed-forward network.\n‚Ä¢ Add & Normalize (after Feed-Forward): The feed-forward output is added to the input of this stage, followed by dropout and normalization using norm3.\n‚Ä¢ Output: The processed tensor is returned as the output of the decoder layer.\n\nThe DecoderLayer class defines a single layer of the transformer's decoder. It consists of a multi-head self-attention mechanism, a multi-head cross-attention mechanism (that attends to the encoder's output), a position-wise feed-forward neural network, and the corresponding residual connections, layer normalization, and dropout layers. This combination enables the decoder to generate meaningful outputs based on the encoder's representations, taking into account both the target sequence and the source sequence. As with the encoder, multiple decoder layers are typically stacked to form the complete decoder part of a transformer model.\n\nNext, the Encoder and Decoder blocks are brought together to construct the comprehensive Transformer model.\n\n5. Combining the Encoder and Decoder layers to create the complete Transformer network\n\nFigure 4. The Transformer Network (Source: Image from the original paper)\n\nThe constructor takes the following parameters:\n‚Ä¢ d_model: The dimensionality of the model's embeddings.\n‚Ä¢ num_heads: Number of attention heads in the multi-head attention mechanism.\n‚Ä¢ num_layers: Number of layers for both the encoder and the decoder.\n‚Ä¢ d_ff: Dimensionality of the inner layer in the feed-forward network.\n\nAnd it defines the following components:\n\nThis method is used to create masks for the source and target sequences, ensuring that padding tokens are ignored and that future tokens are not visible during training for the target sequence.\n\nThis method defines the forward pass for the Transformer, taking source and target sequences and producing the output predictions.\n‚Ä¢ Input Embedding and Positional Encoding: The source and target sequences are first embedded using their respective embedding layers and then added to their positional encodings.\n‚Ä¢ Encoder Layers: The source sequence is passed through the encoder layers, with the final encoder output representing the processed source sequence.\n‚Ä¢ Decoder Layers: The target sequence and the encoder's output are passed through the decoder layers, resulting in the decoder's output.\n‚Ä¢ Final Linear Layer: The decoder's output is mapped to the target vocabulary size using a fully connected (linear) layer.\n\nThe final output is a tensor representing the model's predictions for the target sequence.\n\nThe Transformer class brings together the various components of a Transformer model, including the embeddings, positional encoding, encoder layers, and decoder layers. It provides a convenient interface for training and inference, encapsulating the complexities of multi-head attention, feed-forward networks, and layer normalization.\n\nThis implementation follows the standard Transformer architecture, making it suitable for sequence-to-sequence tasks like machine translation, text summarization, etc. The inclusion of masking ensures that the model adheres to the causal dependencies within sequences, ignoring padding tokens and preventing information leakage from future tokens.\n\nThese sequential steps empower the Transformer model to efficiently process input sequences and produce corresponding output sequences.\n\nFor illustrative purposes, a dummy dataset will be crafted in this example. However, in a practical scenario, a more substantial dataset would be employed, and the process would involve text preprocessing along with the creation of vocabulary mappings for both the source and target languages.\n\nThese values define the architecture and behavior of the transformer model:\n‚Ä¢ src_vocab_size, tgt_vocab_size: Vocabulary sizes for source and target sequences, both set to 5000.\n‚Ä¢ d_model: Dimensionality of the model's embeddings, set to 512.\n‚Ä¢ num_heads: Number of attention heads in the multi-head attention mechanism, set to 8.\n‚Ä¢ num_layers: Number of layers for both the encoder and the decoder, set to 6.\n‚Ä¢ d_ff: Dimensionality of the inner layer in the feed-forward network, set to 2048.\n\nThis line creates an instance of the Transformer class, initializing it with the given hyperparameters. The instance will have the architecture and behavior defined by these hyperparameters.\n\nThe following lines generate random source and target sequences:\n‚Ä¢ src_data: Random integers between 1 and src_vocab_size, representing a batch of source sequences with shape (64, max_seq_length).\n‚Ä¢ tgt_data: Random integers between 1 and tgt_vocab_size, representing a batch of target sequences with shape (64, max_seq_length).\n‚Ä¢ These random sequences can be used as inputs to the transformer model, simulating a batch of data with 64 examples and sequences of length 100.\n\nThe code snippet demonstrates how to initialize a transformer model and generate random source and target sequences that can be fed into the model. The chosen hyperparameters determine the specific structure and properties of the transformer. This setup could be part of a larger script where the model is trained and evaluated on actual sequence-to-sequence tasks, such as machine translation or text summarization.\n\nNext, the model will be trained utilizing the aforementioned sample data. However, in a real-world scenario, a significantly larger dataset would be employed, which would typically be partitioned into distinct sets for training and validation purposes.\n‚Ä¢ criterion = nn.CrossEntropyLoss(ignore_index=0): Defines the loss function as cross-entropy loss. The ignore_index argument is set to 0, meaning the loss will not consider targets with an index of 0 (typically reserved for padding tokens).\n‚Ä¢ optimizer = optim.Adam(...): Defines the optimizer as Adam with a learning rate of 0.0001 and specific beta values.\n‚Ä¢ transformer.train(): Sets the transformer model to training mode, enabling behaviors like dropout that only apply during training.\n\nThe code snippet trains the model for 100 epochs using a typical training loop:\n‚Ä¢ for epoch in range(100): Iterates over 100 training epochs.\n‚Ä¢ optimizer.zero_grad(): Clears the gradients from the previous iteration.\n‚Ä¢ output = transformer(src_data, tgt_data[:, :-1]): Passes the source data and the target data (excluding the last token in each sequence) through the transformer. This is common in sequence-to-sequence tasks where the target is shifted by one token.\n‚Ä¢ loss = criterion(...): Computes the loss between the model's predictions and the target data (excluding the first token in each sequence). The loss is calculated by reshaping the data into one-dimensional tensors and using the cross-entropy loss function.\n‚Ä¢ loss.backward(): Computes the gradients of the loss with respect to the model's parameters.\n‚Ä¢ optimizer.step(): Updates the model's parameters using the computed gradients.\n‚Ä¢ print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\"): Prints the current epoch number and the loss value for that epoch.\n\nThis code snippet trains the transformer model on randomly generated source and target sequences for 100 epochs. It uses the Adam optimizer and the cross-entropy loss function. The loss is printed for each epoch, allowing you to monitor the training progress. In a real-world scenario, you would replace the random source and target sequences with actual data from your task, such as machine translation.\n\nAfter training the model, its performance can be evaluated on a validation dataset or test dataset. The following is an example of how this could be done:\n‚Ä¢ transformer.eval(): Puts the transformer model in evaluation mode. This is important because it turns off certain behaviors like dropout that are only used during training.\n‚Ä¢ val_src_data: Random integers between 1 and src_vocab_size, representing a batch of validation source sequences with shape (64, max_seq_length).\n‚Ä¢ val_tgt_data: Random integers between 1 and tgt_vocab_size, representing a batch of validation target sequences with shape (64, max_seq_length).\n‚Ä¢ with torch.no_grad(): Disables gradient computation, as we don't need to compute gradients during validation. This can reduce memory consumption and speed up computations.\n‚Ä¢ val_output = transformer(val_src_data, val_tgt_data[:, :-1]): Passes the validation source data and the validation target data (excluding the last token in each sequence) through the transformer.\n‚Ä¢ val_loss = criterion(...): Computes the loss between the model's predictions and the validation target data (excluding the first token in each sequence). The loss is calculated by reshaping the data into one-dimensional tensors and using the previously defined cross-entropy loss function.\n\nThis code snippet evaluates the transformer model on a randomly generated validation dataset, computes the validation loss, and prints it. In a real-world scenario, the random validation data should be replaced with actual validation data from the task you are working on. The validation loss can give you an indication of how well your model is performing on unseen data, which is a critical measure of the model's generalization ability.\n\nFor further details about Transformers and Hugging Face, our tutorial, An Introduction to Using Transformers and Hugging Face, is useful.\n\nIn conclusion, this tutorial demonstrated how to construct a Transformer model using PyTorch, one of the most versatile tools for deep learning. With their capacity for parallelization and the ability to capture long-term dependencies in data, Transformers have immense potential in various fields, especially NLP tasks like translation, summarization, and sentiment analysis.\n\nFor those eager to deepen their understanding of advanced deep learning concepts and techniques, consider exploring the course Advanced Deep Learning with Keras on DataCamp. You can also read about building a simple neural network with PyTorch in a separate tutorial."
    },
    {
        "link": "https://towardsdatascience.com/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1",
        "document": "I have recently been getting more involved in the world of machine learning. When I‚Äôve had a problem understanding a complex issue or coding a neural network, the internet has seemed to have all the answers: from a simple linear regression to complex convolutional networks. At least that is what I thought‚Ä¶\n\nOnce I began getting better at this Deep Learning thing, I stumbled upon the all-glorious transformer. The original paper: \"Attention is all you need\", proposed an innovative way to construct neural networks. No more convolutions! The paper proposes an encoder-decoder neural network made up of repeated encoder and decoder blocks. The structure is the following:\n\nThe left block is the encoder, and the right block is the decoder. If you don‚Äôt understand the parts of this model yet, I highly recommend going over Harvard‚Äôs \"The Annotated Transformer\" guide where they code the transformer model in PyTorch from scratch. I will not be covering important concepts like \"multi-head attention\" or \"feed-forward layers\" in this tutorial, so you should know them before you continue reading. If you have already taken a look at the code from scratch, you are probably wondering if you are going to have to copy-paste that code all over the place for every project you make. Thankfully, no. Modern python libraries like PyTorch and Tensorflow already include easily accessible transformer models through an import. However, there is more to it than just importing the model and plugging it in. Today I will explain how to use and tune PyTorch nn.Transformer() module. I personally struggled trying to find information about how to implement, train, and infer from it, so I decided to create my own guide for all of you.\n\nTo start, we need to import PyTorch and some other libraries we are going to be using:\n\nNow, let‚Äôs take a closer look at the transformer module. I recommend starting by reading over PyTorch‚Äôs documentation about it. As they explain, there are no mandatory parameters. The module comes with the \"Attention is all you need\" model hyperparameters. To use it, let‚Äôs begin by creating a simple PyTorch model. I will only change some of the default parameters so our model doesn‚Äôt take unnecessarily long to train. I made those parameters part of our class:\n\nThe transformer blocks don‚Äôt care about the order of the input sequence. This, of course, is a problem. Saying \"I ate a pizza with pineapple\" is not the same as saying \"a pineapple ate I with pizza\". Thankfully, we have a solution: positional encoding. This is a way to \"give importance\" to elements depending on their position. A detailed explanation of how it works can be found here, but a quick explanation is that we create a vector for each element representing its position with regard to every other element in the sequence. Positional encoding follows this very complicated-looking formula which, in practice, we won‚Äôt really need to understand:\n\nFor the sake of organization and reusability, let‚Äôs create a separate class for the positional encoding layer (it looks hard but it is really just the formula, dropout, and a residual connection):\n\nNow that we have the only layer not included in PyTorch, we are ready to finish our model. Before adding the positional encoding, we need an embedding layer so that each element in our sequences is converted into a vector we can manipulate (instead of a fixed integer). We will also need a final linear layer so that we can convert the model‚Äôs output into the dimensions of our desired output. The final model should look something like this:\n\nI know‚Ä¶ It looks very intimidating, but if you understand what each part does, it is actually a pretty simple model to implement.\n\nYou may recall there was a special block in the model structure called \"masked multi-head attention\":\n\nSo‚Ä¶ what is masking? Before I can explain it to you, let‚Äôs quickly recapitulate what is going on with our tensors when we feed them into our model. First, we embed and encode (positional encoding) our source tensor. Then, our source tensor is encoded into an unintelligible encoded tensor that we feed into our decoder with our embedded and encoded (positionally) target vector. For our model to learn, we can‚Äôt just show it the whole target tensor! This would just give him the answer straight up.\n\nThe solution to this is a masking tensor. This tensor is made up of size (sequence length x sequence length) since for every element in the sequence, we show the model one more element. This matrix will be added to our target vector, so the matrix will be made up of zeros in the positions where the transformer can have access to the elements, and minus infinity where it can‚Äôt. An illustrated explanation might help you a bit more:\n\nIn case you didn‚Äôt know, tensors are matrices that can be stored in a GPU, and since they are matrices, all dimensions must have elements of the same size. Of course, this won‚Äôt happen when treating with tasks like NLP or different-sized images. Therefore, we use the so-called \"special tokens\". These tokens allow our model to know where the start of the sentence is (), where the end of the sentence is () and what elements are just there to fill up the remaining space so that our matrices have the sam sequence size (). These tokens must also be converted into their corresponding integer id (In our example they will be 2, 3, and 4 respectively). Padding a sequence looks something like this:\n\nTo tell our model that these tokens should be irrelevant, we use a binary matrix where there is a True value on the positions where the padding token is and False where it isn‚Äôt:\n\nTo create the two masking matrices we talked about, we need to extend our transformer model. If you know a bit of NumPy, you will have no problem understanding what these methods do. If you can‚Äôt understand it, I recommend opening a Jupyter notebook and going step by step to understand what they do.\n\nThe full extended model looks like this (note the change in the forward method as well):\n\nFor the sake of this project, I am going to create a set of fake data we can use to train our model. This data will be made up of sequences like:\n\nFeel free to skip to the next section if you aren‚Äôt interested in the data creation part.\n\nI won‚Äôt bother explaining what these functions do since they are pretty easy to understand with basic NumPy knowledge. I‚Äôll create all the sentences of size 8 so I don‚Äôt need padding, and I‚Äôll organize them randomly into batches of size 16:\n\nNow that we have data to work with, we can get to training our model. Let‚Äôs begin by creating an instance of our model, loss function, and optimizer. We will use the Stochastic Gradient Descent optimizer, the Cross-Entropy Loss function, and a learning rate of 0.01. I will also use my graphics card for this training since it will take less time, but it is not necessary.\n\nAn important concept we need to understand before continuing is that the target tensor we give as an input to the transformer must be shifted by one to the right (compared to the target output tensor). In other words, the tensor we want to give the model for training must have one extra element at the beginning and one less element at the end, and the tensor we compute the loss function with must be shifted in the other direction. This is so that if we give the model an element during inference, it gives us the next one.\n\nNow that we have grasped this concept, let‚Äôs get to coding! The training loop is a standard training loop except:\n‚Ä¢ The target tensor is passed to the model during the prediction\n‚Ä¢ A target mask is generated to hide the next words\n‚Ä¢ A padding mask might be generated and passed to the model as well\n\nThe validation loop is exactly the same as our training loop except we don‚Äôt read or update gradients:\n\nIn this example, I am training the model for 10 epochs. To simplify the training I created a fit function that calls the train and validation loop every epoch and prints the loss:\n\nThis produces the following out\n\nAfter training we obtain the following losses per epoch:\n\nAs we can see, our model seems to have learned something. It is time to check if it has, but‚Ä¶ how do we check it? We don‚Äôt have target tensors for data we have never seen. Here is where shifting our input target and output target tensor has an effect. As we saw before, our model learned to predict the next token when given an element. Therefore, we should be able to give our model the input tensor and the start token, and it should give us back the next element. If when the model predicts a token, we concatenate it with our previous input, we should slowly be able to add words to our output until our model predicts the token.\n\nHere is the code for that process:\n\nThe output of running this code is:\n\nSo the model has indeed gotten the gist of our sequences, but it still makes some mistakes when trying to predict the continuation. For example, in \"Example 4\", the model should predict a 1 as the first token, since the ending of the input is a 0. We can also see how during inference our sentences don‚Äôt need to have the same length, and the outputs will also not have the same length (see \"Example 5\").\n\nI believe this article can help a lot of beginner/intermediate Machine Learning developers learn how to work with transformer models in PyTorch, and, since the structure is the same in other languages, this tutorial is probably also useful for other frameworks like Tensorflow (hopefully).\n\nIf you have any suggestions or find any bugs feel free to leave a comment and I will fix it ASAP."
    },
    {
        "link": "https://stackoverflow.com/questions/61440281/is-positional-encoding-necessary-for-transformer-in-language-modeling",
        "document": "It is not clear for me - whether positional encoding is neccessary here ? As far as I understand - it is necessary for language translation task because the decoder should be able to position the word from the previous output within the sequence from encoder. But is it necessary in language modeling without the decoder ?\n\nIs it possible that the words in the encoder output are shuffled ?\n\nthere are no explanations in the original paper. And I didn't find explanations in tutorials (like here https://kazemnejad.com/blog/transformer_architecture_positional_encoding/).\n\n\"As each word in a sentence simultaneously flows through the Transformer‚Äôs encoder/decoder stack, The model itself doesn‚Äôt have any sense of position/order for each word.\"\n\nFrom my point of view - transformer encoder has info about the order because its input is an ordered sequence (similar to RNN).\n\nI tried to remove positional encoding from the model. It works, but with a worse performance.\n\nIs it useful to add such positional encoding to RNN ? Could it improve its performance ?"
    }
]