[
    {
        "link": "https://pypi.org/project/simpledbf",
        "document": "A required part of this site couldn’t load. This may be due to a browser extension, network issues, or browser settings. Please check your connection, disable any ad blockers, or try using a different browser."
    },
    {
        "link": "https://github.com/rnelsonchem/simpledbf",
        "document": "simpledbf is a Python library for converting basic DBF files (see Limitations) to CSV files, Pandas DataFrames, SQL tables, or HDF5 tables. This package is fully compatible with Python >=3.4, with almost complete Python 2.7 support as well. The conversion to CSV and SQL (see below) is entirely written in Python, so no additional dependencies are necessary. For other export formats, see Optional Requirements. This code was designed to be very simple, fast and memory efficient for convenient interactive or batch file processing; therefore, it lacks many features, such as the ability to write DBF files, that other packages might provide.\n\nBug fixes, questions, and update requests are encouraged and can be filed at the GitHub repo.\n\nThis code is derived from an ActiveState DBF example that works with Python2 and is distributed under a PSF license.\n\nThe most recent release of simpledbf can be installed using or , if you happen to be using the Anaconda Python distribution.\n\nThe development version can be installed from GitHub:\n\nAs an alternative, this package only contains a single file, so in principle, you could download the file from Github and put it in any folder of your choosing.\n\nThis package currently supports a subset of dBase III through 5 DBF files. In particular, support is missing for linked memo (i.e. DBT) files. This is mostly due to limitations in the types of files available to the author. Feel free to request an update if you can supply a DBF file with an associated memo file. DBF version 7, the most recent DBF file spec, is not currently supported by this package.\n\nExcept for HDF file export, this code should work fine with Python >=2.7. However, HDF files created in Python3 are compatible with all Python2 HDF packages, so in principle, you could make any HDF files in a temporary Python3 environment. If you are using the Anaconda Python distribution (recommended), then you can make a small Python3 working environment as follows:\n\nHDF file export is currently broken in Python2 due to a limitation in Pandas HDF export with unicode. This issue may be fixed future versions of Pandas/PyTables.\n\nThis module currently only defines a single class, , which is instantiated with a DBF file name, which can contain path info as well. An optional 'codec' keyword argument that controls the codec used for reading/writing files. The default is 'utf-8'. See the documentation for Python's codec standard library module for more codec options.\n\nThe object initially only reads the header information from the file, so you can inspect some of the properties. For example, is the number of records in the DBF file, and is a list of tuples with information about the data columns. See the DBF file spec for info on the column type characters. The \"DeletionFlag\" column is always present as a check for deleted records; however, it is never exported during conversion.\n\nThe docstring for this object contains a complete listing of attributes and their descriptions.\n\nThe method gives an approximate memory requirement for processing this DBF file. (~2x the total file size, which could be wildly inaccurate.) In addition, all of the output methods in this object take a keyword argument, which lets you split up the processing of large files into smaller chunks to limit the total memory usage of the conversion process. When this keyword argument is passed into , the approximate memory footprint of the chunk will also be given, which can be useful when trying to determine the maximum chunksize your memory will allow.\n\nThe object behaves like Python's file object in that it will be \"exhausted\" after export. To re-export the DBF data to a different format, first create a new instance using the same file name. This procedure is followed in the documentation below.\n\nThis package attempts to convert most blank strings and poorly formatted values to an empty value of your choosing. This is controlled by the keyword argument to all export functions. The default for CSV is an empty string (''), and for all other exports, it is 'nan' which converts empty/bad values to . NOTE The exception here is that float/int columns always use for all missing values for DBF->SQL->DataFrame conversion purposes. Pandas has very powerful functions for working with missing data, including converting NaN to other values (e.g. empty strings).\n\nUse the method to export the data to a CSV file. This method requires the name of a CSV file as an input. The default behavior is to append new data to an existing file, so be careful if the file already exists. The keyword argument controls the frequency that the file buffer will be flushed, which may not be necessary. The keyword changes the value used for missing/bad entries (default is ''). The keyword is a boolean that controls writing of the column names as the first row of the CSV file. The encoding of the resulting CSV file is determined by the codec that is set when opening the DBF file, see Loading.\n\nIf you are unhappy with the default CSV output of this module, Pandas also has very powerful CSV export capabilities for DataFrames.\n\nMost SQL databases can create tables directly from local CSV files. The pure-Python method creates two files: 1) a header-less CSV file containing the DBF contents, and 2) a SQL file containing the appropriate table creation and CSV import code. It is up to you to run the SQL file as a separate step. This function takes two mandatory arguments, which are simply the names of the SQL and CSV files, respectively. In addition, there are a number of optional keyword arguments as well. controls the output dialect. The default is 'sqlite', but 'postgres' is also accepted. sets the name of the SQL table that will be created. By default, this will be the name of the DBF file without the file extension. You should escape quote characters (\") in the CSV file. This is controlled with the keyword, which defaults to . (This changes '\"' in text strings to '\"\"', which the SQL server should ignore.) The , , and keywords are used to control the CSV file. See above.\n\nHere's an example for SQLite:\n\nHere's an example for Postgresql:\n\nThe method returns the DBF records as a Pandas DataFrame. If the size of the DBF file exceeds available memory, then passing the keyword argument will return a generator function. This generator yields DataFrames of len(<=chunksize) until all of the records have been processed. The keyword changes the value used for missing/bad entries (default is 'nan' which inserts ).\n\nWhen a DataFrame is constructed, it attempts to determine the dtype of each column. If you chunk the DataFrame output, it turns out that the dtype for a column can change. For example, if one chunk has a column with all strings, the dtype will be ; however, if in the next chunk that same column is full of , the resulting dtype will be set as . This has some consequences for writing to SQL and HDF tables as well. In principle, this behavior could be changed, but it is currently non-trivial to set the dtypes for DataFrame columns on construction. Please file a PR through GitHub if this is a big problem.\n\nThe method will transfer the DBF entries to an SQL database table of your choice using a combination of Pandas DataFrames and SQLalchemy. A valid SQLalchemy engine string argument is required to connect with the database. Database support will be limited to those supported by SQLalchemy. (This has been tested with SQLite and Postgresql.) Note, if you are transferring a large amount of data, this method will be very slow. If you have direct access to the SQL server, you might want to use the text-based SQL export instead.\n\nThis method accepts three optional arguments. is the name of the table you'd like to use. If this is not passed, your new table will have the same name as the DBF file without file extension. Again, the default here is to append to an existing table. If you want to start fresh, delete the existing table before using this function. The keyword processes the DBF file in chunks of records no larger than this size. The keyword changes the value used for missing/bad entries (default is 'nan' which inserts ).\n\nThe method transfers the DBF entries to an HDF5 table of your choice. This method uses a combination of Pandas DataFrames and PyTables, so both of these packages must be installed. This method requires a file name string for the HDF file, which will be created if it does not exist. Again, the default behavior is to append to an existing file of that name, so be careful here. The HDF file will be created using the highest level of compression (9) with the 'blosc' compression lib. This saves an enormous amount of disk space, with little degradation of performance; however, this compression library is non-standard, which can cause problems with other HDF libraries. Compression options are controlled use the and keyword arguments, which are identical to the ones described in the Pandas HDF compression docs.\n\nThis method uses the same optional arguments, and corresponding defaults, as (see above). A example with is shown below. In addition, a keyword argument is also available, which sets the columns that will be used as data columns in the HDF table. Data columns can be used for advanced searching and selection; however, there is some degredation of preformance for large numbers of data columns. See the Pandas data columns docs for a more detailed explanation.\n\nSee the chunksize issue for DataFrame export for information on a potential problem you may encounter with chunksize.\n\nBatch file export is trivial using simpledbf. For example, the following code processes all DBF files in the current directory into separate tables in a single HDF file."
    },
    {
        "link": "https://pypi.org/project/simpledbf/0.2.1",
        "document": "A required part of this site couldn’t load. This may be due to a browser extension, network issues, or browser settings. Please check your connection, disable any ad blockers, or try using a different browser."
    },
    {
        "link": "https://github.com/rnelsonchem/simpledbf/blob/master/simpledbf/simpledbf.py",
        "document": ""
    },
    {
        "link": "https://docs.python.org/3/library/filesys.html",
        "document": "The modules described in this chapter deal with disk files and directories. For example, there are modules for reading the properties of files, manipulating paths in a portable way, and creating temporary files. The full list of modules in this chapter is:\n\nOperating system interfaces, including functions to work with files at a lower level than Python file objects. Python’s built-in I/O library, including both abstract classes and some concrete classes such as file I/O. The standard way to open files for reading and writing with Python."
    },
    {
        "link": "https://github.com/rnelsonchem/simpledbf",
        "document": "simpledbf is a Python library for converting basic DBF files (see Limitations) to CSV files, Pandas DataFrames, SQL tables, or HDF5 tables. This package is fully compatible with Python >=3.4, with almost complete Python 2.7 support as well. The conversion to CSV and SQL (see below) is entirely written in Python, so no additional dependencies are necessary. For other export formats, see Optional Requirements. This code was designed to be very simple, fast and memory efficient for convenient interactive or batch file processing; therefore, it lacks many features, such as the ability to write DBF files, that other packages might provide.\n\nBug fixes, questions, and update requests are encouraged and can be filed at the GitHub repo.\n\nThis code is derived from an ActiveState DBF example that works with Python2 and is distributed under a PSF license.\n\nThe most recent release of simpledbf can be installed using or , if you happen to be using the Anaconda Python distribution.\n\nThe development version can be installed from GitHub:\n\nAs an alternative, this package only contains a single file, so in principle, you could download the file from Github and put it in any folder of your choosing.\n\nThis package currently supports a subset of dBase III through 5 DBF files. In particular, support is missing for linked memo (i.e. DBT) files. This is mostly due to limitations in the types of files available to the author. Feel free to request an update if you can supply a DBF file with an associated memo file. DBF version 7, the most recent DBF file spec, is not currently supported by this package.\n\nExcept for HDF file export, this code should work fine with Python >=2.7. However, HDF files created in Python3 are compatible with all Python2 HDF packages, so in principle, you could make any HDF files in a temporary Python3 environment. If you are using the Anaconda Python distribution (recommended), then you can make a small Python3 working environment as follows:\n\nHDF file export is currently broken in Python2 due to a limitation in Pandas HDF export with unicode. This issue may be fixed future versions of Pandas/PyTables.\n\nThis module currently only defines a single class, , which is instantiated with a DBF file name, which can contain path info as well. An optional 'codec' keyword argument that controls the codec used for reading/writing files. The default is 'utf-8'. See the documentation for Python's codec standard library module for more codec options.\n\nThe object initially only reads the header information from the file, so you can inspect some of the properties. For example, is the number of records in the DBF file, and is a list of tuples with information about the data columns. See the DBF file spec for info on the column type characters. The \"DeletionFlag\" column is always present as a check for deleted records; however, it is never exported during conversion.\n\nThe docstring for this object contains a complete listing of attributes and their descriptions.\n\nThe method gives an approximate memory requirement for processing this DBF file. (~2x the total file size, which could be wildly inaccurate.) In addition, all of the output methods in this object take a keyword argument, which lets you split up the processing of large files into smaller chunks to limit the total memory usage of the conversion process. When this keyword argument is passed into , the approximate memory footprint of the chunk will also be given, which can be useful when trying to determine the maximum chunksize your memory will allow.\n\nThe object behaves like Python's file object in that it will be \"exhausted\" after export. To re-export the DBF data to a different format, first create a new instance using the same file name. This procedure is followed in the documentation below.\n\nThis package attempts to convert most blank strings and poorly formatted values to an empty value of your choosing. This is controlled by the keyword argument to all export functions. The default for CSV is an empty string (''), and for all other exports, it is 'nan' which converts empty/bad values to . NOTE The exception here is that float/int columns always use for all missing values for DBF->SQL->DataFrame conversion purposes. Pandas has very powerful functions for working with missing data, including converting NaN to other values (e.g. empty strings).\n\nUse the method to export the data to a CSV file. This method requires the name of a CSV file as an input. The default behavior is to append new data to an existing file, so be careful if the file already exists. The keyword argument controls the frequency that the file buffer will be flushed, which may not be necessary. The keyword changes the value used for missing/bad entries (default is ''). The keyword is a boolean that controls writing of the column names as the first row of the CSV file. The encoding of the resulting CSV file is determined by the codec that is set when opening the DBF file, see Loading.\n\nIf you are unhappy with the default CSV output of this module, Pandas also has very powerful CSV export capabilities for DataFrames.\n\nMost SQL databases can create tables directly from local CSV files. The pure-Python method creates two files: 1) a header-less CSV file containing the DBF contents, and 2) a SQL file containing the appropriate table creation and CSV import code. It is up to you to run the SQL file as a separate step. This function takes two mandatory arguments, which are simply the names of the SQL and CSV files, respectively. In addition, there are a number of optional keyword arguments as well. controls the output dialect. The default is 'sqlite', but 'postgres' is also accepted. sets the name of the SQL table that will be created. By default, this will be the name of the DBF file without the file extension. You should escape quote characters (\") in the CSV file. This is controlled with the keyword, which defaults to . (This changes '\"' in text strings to '\"\"', which the SQL server should ignore.) The , , and keywords are used to control the CSV file. See above.\n\nHere's an example for SQLite:\n\nHere's an example for Postgresql:\n\nThe method returns the DBF records as a Pandas DataFrame. If the size of the DBF file exceeds available memory, then passing the keyword argument will return a generator function. This generator yields DataFrames of len(<=chunksize) until all of the records have been processed. The keyword changes the value used for missing/bad entries (default is 'nan' which inserts ).\n\nWhen a DataFrame is constructed, it attempts to determine the dtype of each column. If you chunk the DataFrame output, it turns out that the dtype for a column can change. For example, if one chunk has a column with all strings, the dtype will be ; however, if in the next chunk that same column is full of , the resulting dtype will be set as . This has some consequences for writing to SQL and HDF tables as well. In principle, this behavior could be changed, but it is currently non-trivial to set the dtypes for DataFrame columns on construction. Please file a PR through GitHub if this is a big problem.\n\nThe method will transfer the DBF entries to an SQL database table of your choice using a combination of Pandas DataFrames and SQLalchemy. A valid SQLalchemy engine string argument is required to connect with the database. Database support will be limited to those supported by SQLalchemy. (This has been tested with SQLite and Postgresql.) Note, if you are transferring a large amount of data, this method will be very slow. If you have direct access to the SQL server, you might want to use the text-based SQL export instead.\n\nThis method accepts three optional arguments. is the name of the table you'd like to use. If this is not passed, your new table will have the same name as the DBF file without file extension. Again, the default here is to append to an existing table. If you want to start fresh, delete the existing table before using this function. The keyword processes the DBF file in chunks of records no larger than this size. The keyword changes the value used for missing/bad entries (default is 'nan' which inserts ).\n\nThe method transfers the DBF entries to an HDF5 table of your choice. This method uses a combination of Pandas DataFrames and PyTables, so both of these packages must be installed. This method requires a file name string for the HDF file, which will be created if it does not exist. Again, the default behavior is to append to an existing file of that name, so be careful here. The HDF file will be created using the highest level of compression (9) with the 'blosc' compression lib. This saves an enormous amount of disk space, with little degradation of performance; however, this compression library is non-standard, which can cause problems with other HDF libraries. Compression options are controlled use the and keyword arguments, which are identical to the ones described in the Pandas HDF compression docs.\n\nThis method uses the same optional arguments, and corresponding defaults, as (see above). A example with is shown below. In addition, a keyword argument is also available, which sets the columns that will be used as data columns in the HDF table. Data columns can be used for advanced searching and selection; however, there is some degredation of preformance for large numbers of data columns. See the Pandas data columns docs for a more detailed explanation.\n\nSee the chunksize issue for DataFrame export for information on a potential problem you may encounter with chunksize.\n\nBatch file export is trivial using simpledbf. For example, the following code processes all DBF files in the current directory into separate tables in a single HDF file."
    },
    {
        "link": "https://pypi.org/project/simpledbf",
        "document": "A required part of this site couldn’t load. This may be due to a browser extension, network issues, or browser settings. Please check your connection, disable any ad blockers, or try using a different browser."
    },
    {
        "link": "https://stackoverflow.com/questions/57215656/problems-opening-dbf-files-in-python",
        "document": "I am trying to open en transform several DBF files to a dataframe. Most of them worked fine, but for one of the files I receive the error: \"UnicodeDecodeError: 'utf-8' codec can't decode byte 0xf6 in position 15: invalid start byte\"\n\nI have read this error on some other topics such as opening csv and xlsx and other files. The proposed solution was to include in the reading the file part. I haven't found a solution for DBF files unfortunately and I have very limited knowledge on DBF files.\n\nWhat I have tried so far:\n\nAnd last, if I try to install the module, I receive: SyntaxError: invalid syntax\n\nAny suggestions on how to solve this?"
    },
    {
        "link": "https://stackoverflow.com/questions/41898561/pandas-transform-a-dbf-table-into-a-dataframe",
        "document": "I want to read a file of an ArcGIS shapefile and dump it into a dataframe. I am currently using the dbf package.\n\nI have apparently been able to load the file as a Table, but have not been able to figure out how to parse it and turn it into a pandas dataframe. What is the way to do it?\n\nThis is where I am stuck at:\n\nPython returns this statement as output, which I frankly don't know what to make of:"
    },
    {
        "link": "https://gis.stackexchange.com/questions/288422/saving-dataframe-as-dbf-with-simpledbf-in-python",
        "document": "Stack Exchange network consists of 183 Q&A communities including Stack Overflow, the largest, most trusted online community for developers to learn, share their knowledge, and build their careers."
    }
]