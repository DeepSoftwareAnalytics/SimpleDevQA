[
    {
        "link": "https://learn.microsoft.com/en-us/dotnet/api/system.diagnostics.process?view=net-9.0",
        "document": "The following example uses an instance of the Process class to start a process.\n\nThe following example uses the Process class itself and a static Start method to start a process.\n\nThe following F# example defines a function that starts a process, captures all output and error information, and records the number of milliseconds that the process has run. The function has three parameters: the name of application to launch, the arguments to supply to the application, and the starting directory.\n\nThe code for the function was written by ImaginaryDevelopment and is available under the Microsoft Public License.\n\nA Process component provides access to a process that is running on a computer. A process, in the simplest terms, is a running app. A thread is the basic unit to which the operating system allocates processor time. A thread can execute any part of the code of the process, including parts currently being executed by another thread.\n\nThe Process component is a useful tool for starting, stopping, controlling, and monitoring apps. You can use the Process component, to obtain a list of the processes that are running, or you can start a new process. A Process component is used to access system processes. After a Process component has been initialized, it can be used to obtain information about the running process. Such information includes the set of threads, the loaded modules (.dll and .exe files), and performance information such as the amount of memory the process is using.\n\nThis type implements the IDisposable interface. When you have finished using the type, you should dispose of it either directly or indirectly. To dispose of the type directly, call its Dispose method in a / block. To dispose of it indirectly, use a language construct such as (in C#) or (in Visual Basic). For more information, see the \"Using an Object that Implements IDisposable\" section in the IDisposable interface documentation.\n\nThe process component obtains information about a group of properties all at once. After the Process component has obtained information about one member of any group, it will cache the values for the other properties in that group and not obtain new information about the other members of the group until you call the Refresh method. Therefore, a property value is not guaranteed to be any newer than the last call to the Refresh method. The group breakdowns are operating-system dependent.\n\nIf you have a path variable declared in your system using quotes, you must fully qualify that path when starting any process found in that location. Otherwise, the system will not find the path. For example, if is not in your path, and you add it using quotation marks: , you must fully qualify any process in when starting it.\n\nA system process is uniquely identified on the system by its process identifier. Like many Windows resources, a process is also identified by its handle, which might not be unique on the computer. A handle is the generic term for an identifier of a resource. The operating system persists the process handle, which is accessed through the Handle property of the Process component, even when the process has exited. Thus, you can get the process's administrative information, such as the ExitCode (usually either zero for success or a nonzero error code) and the ExitTime. Handles are an extremely valuable resource, so leaking handles is more virulent than leaking memory.\n\nIn .NET Framework, the Process class by default uses Console encodings, which are typically code page encodings, for the input, output, and error streams. For example code, on systems whose culture is English (United States), code page 437 is the default encoding for the Console class. However, .NET Core may make only a limited subset of these encodings available. If this is the case, it uses Encoding.UTF8 as the default encoding.\n\nIf a Process object depends on specific code page encodings, you can still make them available by doing the following before you call any Process methods:\n• None Retrieve the EncodingProvider object from the CodePagesEncodingProvider.Instance property.\n• None Pass the EncodingProvider object to the Encoding.RegisterProvider method to make the additional encodings supported by the encoding provider available.\n\nThe Process class will then automatically use the default system encoding rather than UTF8, provided that you have registered the encoding provider before calling any Process methods."
    },
    {
        "link": "https://stackoverflow.com/questions/7459397/how-to-easily-run-shell-commands-using-c",
        "document": "how do i use c# to run command prompt commands? Lets say i want to run these commands in a sequence:\n\nor something like that...Can someone make this method:\n\nsuch that your input is a series of string commands (i.e [\"ipconfig\",\"ping 192.168.192.168\",\"ping google.com\",\"nslookup facebook.com\") that should be executed on a single command prompt in the specific sequence in which they are put in the array. Thanks."
    },
    {
        "link": "https://stackoverflow.com/questions/1469764/run-command-prompt-commands",
        "document": "This may be a bit of a read so im sorry in advance. And this is my tried and tested way of doing this, there may be a simpler way but this is from me throwing code at a wall and seeing what stuck\n\nIf it can be done with a batch file then the maybe over complicated work around is have c# write a .bat file and run it. If you want user input you could place the input into a variable and have c# write it into the file. it will take trial and error with this way because its like controlling a puppet with another puppet.\n\nhere is an example, In this case the function is for a push button in windows forum app that clears the print queue.\n\nIF you want user input then you could try something like this.\n\nThis is for setting the computer IP as static but asking the user what the IP, gateway, and dns server is.\n\nyou will need this for it to work\n\nLike I said. It may be a little overcomplicated but it never fails unless I write the batch commands wrong."
    },
    {
        "link": "https://learn.microsoft.com/en-us/archive/msdn-magazine/2016/january/essential-net-csharp-scripting",
        "document": "With the arrival of Visual Studio 2015 Update 1, henceforth Update 1, comes a new C# read-evaluate-print-loop (REPL), available as a new interactive window within Visual Studio 2015 or as a new command-line interface (CLI) called CSI. In addition to bringing the C# language to the command line, Update 1 also introduces a new C# scripting language, traditionally saved into a CSX file.\n\nBefore delving into the details of the new C# scripting, it’s important to understand the target scenarios. C# scripting is a tool for testing out your C# and .NET snippets without the effort of creating multiple unit testing or console projects. It provides a lightweight option for quickly coding up a LINQ aggregate method call on the command line, checking the .NET API for unzipping files, or invoking a REST API to figure out what it returns or how it works. It provides an easy means to explore and understand an API without the overhead of a yet another CSPROJ file in your %TEMP% directory.\n\nAs with learning C# itself, the best way to get started with learning the C# REPL interface is to run it and begin executing commands. To launch it, run the command csi.exe from the Visual Studio 2015 developer command prompt or use the full path, C:\\Program Files (x86)\\MSBuild\\14.0\\bin\\csi.exe. From there, begin executing C# statements such as those shown in Figure 1.\n\nThe first thing to note is the obvious—it’s like C#—albeit a new dialect of C# (but without the ceremony that’s appreciated in a full production program and unnecessary in a quick-and-dirty prototype). Therefore, as you’d expect, if you want to call a static method you can write out the fully qualified method name and pass arguments within parentheses. As in C#, you declare a variable by prefixing it with the type and, optionally, assigning it a new value at declaration time. Again, as you’d expect, any valid method body syntax—try/catch/finally blocks, variable declaration, lambda expressions and LINQ—works seamlessly.\n\nAnd even on the command line, other C# features, such as string constructs (case sensitivity, string literals and string interpolation), are maintained. Therefore, when you’re using or outputting paths, backslashes need to be escaped using a C# escape character (“\\”) or a string literal, as do double backslashes in the output of a path like that of csi.exe. String interpolation works, too, as the “current directory” sample line in Figure 1 demonstrates.\n\nC# scripting allows far more than statements and expressions, though. You can declare custom types, embed type metadata via attributes, and even simplify verbosity using C# script-specific declaratives. Consider the spell-checking sample in Figure 2.\n\nFor the most part, this is just a standard C# class declaration. However, there are several specific C# scripting features. First, the #r directive serves to reference an external assembly. In this case, the reference is to Newtonsoft.Json.dll, which helps parse the JSON data. Note, however, this is a directive designed for referencing files in the file system. As such, it doesn’t require the unnecessary ceremony of a backslash escape sequence.\n\nSecond, you can take the entire listing and save it as a CSX file, then “import” or “inline” the file into the C# REPL window using #load Spell.csx. The #load directive allows you to include additional script files as if all the #load files were included in the same “project” or “compilation.” Placing code into a separate C# script file enables a type of file refactoring and, more important, the ability to persist the C# script over time.\n\nUsing declarations are another C# language feature allowed in C# scripting, one that Figure 2 leverages several times. Note that just as in C#, a using declaration is scoped to the file. Therefore, if you called #load Spell.csx from the REPL window, it wouldn’t persist the using Newtonsoft.Json declarative outside the Spell.csx. In other words, using Newtonsoft.Json from within Spell.csx wouldn’t persist into the REPL window without being declared again explicitly in the REPL window (and vice versa). Note that the C# 6.0 using static declarative is also supported. Therefore, a “using static System.Console” declarative eliminates the need to prefix any of the System.Console members with the type, allowing for REPL commands such as “WriteLine(\"Hello! My name is Inigo Montoya\").”\n\nOther constructs of note in C# scripting include the use of attributes, using statements, property and function declaration, and support for async/await. Given the latter support, it’s even possible to leverage await in the REPL window:\n\nHere are some further notes about the C# REPL interface:\n• You can’t run csi.exe from within Windows PowerShell Integrated Scripting Environment (ISE) as it requires direct console input, which isn’t supported from the “simulated” console windows of Windows PowerShell ISE. (For this reason, consider adding to the unsupported list of console applications—$psUnsupportedConsoleApplications.)\n• There is no “exit” or “quit” command to leave the CSI program. Instead, you use Ctrl+C to end the program.\n• Command history is persisted between csi.exe sessions launched from the same cmd.exe or PowerShell.exe session. For example, if you start csi.exe, invoke Console.WriteLine(\"HelloWorld\"), use Ctrl+C to exit, and then relaunch csi.exe, the up arrow will display the previous Console.WriteLine(\"HelloWorld\") command. Exiting the cmd.exe window and relaunching it will clear out the history.\n• Csi.exe supports the #help REPL command, which displays the output shown in Figure 3.\n• Csi.exe supports a number of command-line options, as shown in Figure 4.\n\nAs noted, csi.exe allows you to specify a default “profile” file that customizes your command window:\n• To clear the CSI console, invoke Console.Clear. (Consider a using static System.Console declarative to add support for simply invoking Clear.)\n• If you’re entering a multi-line command and make an error on an earlier line, you can use Ctrl+Z followed by Enter to cancel and return to an empty command prompt without executing (note that the ^Z will appear in the console).\n\nAs I mentioned, there’s also a new Visual Studio C# Interactive window in Update 1, as shown in Figure 5. The C# Interactive window is launched from the View | Other Windows | C# Interactive menu, which opens up an additional docked window. Like the csi.exe window, it’s a C# REPL window but with a few added features. First, it includes syntax color coding and IntelliSense. Similarly, compilation occurs in real time, as you edit, so syntax errors and the like will automatically be red-squiggle-underlined.\n\n\n\n Figure 5 Declaring a C# Script Function Outside of a Class Using the Visual Studio C# Interactive Window\n\nA common association with the C# Interactive Window is, of course, the Visual Studio Immediate and Command windows. While there are overlaps—after all, they’re both REPL windows against which you can execute .NET statements—they have significantly different purposes. The C# Immediate Window is bound directly to the debug context of your application, thus allowing you to inject additional statements into the context, examine data within the debug session, and even manipulate and update the data and debug context. Similarly, the Command Window provides a CLI for manipulating Visual Studio, including executing the various menus, but from the Command Window rather than from the menus themselves. (Executing the command View.C#Interactive, for example, opens the C# Interactive Window.) In contrast, the C# Interactive Window allows you to execute C#, including all the features relating to the C# REPL interface discussed in the previous section. However, the C# Interactive Window doesn’t have access to the debug context. It’s an entirely independent C# session without handles to either the debug context or even to Visual Studio. Like csi.exe, it’s an environment that lets you experiment with quick C# and .NET snippets to verify your understanding without having to start yet another Visual Studio console or unit testing project. Instead of having to launch a separate program, however, the C# Interactive Window is hosted within Visual Studio, where the developer is presumably already residing.\n\nHere are some notes about the C# Interactive Window:\n• The C# Interactive Window supports a number of additional REPL commands not found in csi.exe, including:\n• #cls/#clear to clear the contents of the editor window\n• #reset to restore the execution environment to its initial state while maintaining the command history\n• The keyboard shortcuts are a little unexpected, as the #help output in Figure 6 shows.\n\nFigure 6 Keyboard Shortcuts for the C# Interactive Window\n\nIt is important to note that Alt+UpArrow/DownArrow are the keyboard shortcuts for recalling the command history. Microsoft selected these over the simpler UpArrow/DownArrow because it wanted the Interactive window experience to match that of a standard Visual Studio code window.\n• Because the C# Interactive Window is hosted within Visual Studio, there isn’t the same opportunity to pass references, using declaratives or imports via the command line, as there is with csi.exe. Instead, the C# Interactive Window loads its default execution context from C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\IDE\\PrivateAssemblies\\CSharpInteractive.rsp, which identifies the assemblies to reference by default:\n\nThe combination of these two files is why you can use Console.WriteLine and Environment.CurrentDirectory rather than the fully qualified System.Console.WriteLine and System.Environ-ment.Current­Directory, respectively. In addition, referencing assemblies such as Microsoft.CSharp enables the use of language features like dynamic without anything further. (Modifying these files is how you’d change your “profile” or “preferences” so that the changes persist between sessions.)\n\nMore on the C# Script Syntax\n\nOne thing to keep in mind about the C# script syntax is that much of the ceremony important to a standard C# becomes appropriately optional in a C# script. For example, things like method bodies don’t need to appear within a function and C# script functions can be declared outside of the confines of a class. You could, for example, define a NuGet Install function that appeared directly in the REPL window, as shown in Figure 5. Also, perhaps somewhat surprisingly, C# scripting doesn’t support declaring namespaces. For example, you can’t wrap the Spell class in a Grammar namespace: namespace Grammar { class Spell {} }.\n\nIt’s important to note that you can declare the same construct (variable, class, function and so forth) over and over again. The last declaration shadows any earlier declaration.\n\nAnother important item to be conscious of is the behavior of the command-ending semicolon. Statements (variable assignments, for example) require a semicolon. Without the semicolon the REPL window will continue to prompt (via a period) for more input until the semicolon is entered. Expressions, on the other hand, will execute without the semicolon. Hence, System.Diagnostics.Pro­cess.Start(\"notepad\") will launch Notepad even without the ending semicolon. Furthermore, because the Start method call returns a process, string output of the expression will appear on the command line: [System.Diagnostics.Process (Notepad)]. Closing an expression with a semicolon, however, hides the output. Invoking Start with an ending semicolon, therefore, won’t produce any output, even though Notepad will still launch. Of course, Console.WriteLine(\"It would take a miracle.\"); will still output the text, even with the semicolon, because the method itself is displaying the output (not the return from the method).\n\nThe distinction between expressions and statements can result in subtle differences at times. For example, the statement string text = \"There’s a shortage of perfect b….\"; will result in no output, but text=\"Stop that rhyming and I mean it\" will return the assigned string (because the assignment returns the value assigned and there’s no semicolon to suppress the output).\n\nThe C# script directives for referencing additional assemblies (#r) and importing existing C# scripts (#load) are wonderful additions. (One could imagine complex solutions like project.json files to achieve the same thing that would not be as elegant.) Unfortunately, at the time of this writing, NuGet packages aren’t supported. To reference a file from NuGet requires installing the package to a directory and then referencing the specific DLL via the #r directive. (Microsoft assures me this is coming.)\n\nNote that at this time the directives refer to specific files. You can’t, for example, specify a variable in the directive. While you’d expect this with a directive, it prevents the possibility of dynamically loading an assembly. For example, you could dynamically invoke “nuget.exe install” to extract an assembly (again see Figure 5). Doing so, however, doesn’t allow your CSX file to dynamically bind to the extracted NuGet package because there’s no way to dynamically pass the assembly path to the #r directive.\n\nI confess I have a love-hate relationship with Windows PowerShell. I love the convenience of having the Microsoft .NET Framework on the command line and the possibility of passing .NET objects across the pipe, rather than the traditional text of so many of the CLIs that came before. That said, when it comes to the C# language, I am partisan—I love its elegance and power. (To this day, I’m still impressed with the language extensions that made LINQ possible.) Therefore, the idea that I could have the breadth of Windows PowerShell .NET combined with the elegance of the C# language meant I approached the C# REPL as a replacement for Windows PowerShell. After launching csi.exe, I immediately tried commands like cd, dir, ls, pwd, cls, alias and the like. Suffice it to say, I was disappointed because none of them worked. After pondering the experience and discussing it with the C# team, I realized that replacing Windows PowerShell was not what the team was focused on in version 1. Furthermore, it is the .NET Framework and, therefore, it supports extensibility both by adding your own functions for the preceding commands and even by updating the C# script implementation on Roslyn. I immediately set about defining functions for such commands. The start of such a library is available for download on GitHub at github.com/CSScriptEx.\n\nFor those of you looking for a more functional C# CLI that supports the previous command list out of the box now, consider ScriptCS at scriptcs.net (also on GitHub at github.com/scriptcs). It, too, leverages Roslyn, and includes alias, cd, clear, cwd, exit, help, install, references, reset, scriptpacks, usings and vars. Note that, with ScriptCS, the command prefix today is a colon (as in :reset) rather than a number sign (as in #reset). As an added bonus, ScriptCS also adds support for CSX files, in the form of colorization and IntelliSense, to Visual Studio Code.\n\nAt least for now, the purpose of the C# REPL interface is not to replace Windows PowerShell or even cmd.exe. To approach it as such at the start will result in disappointment. Rather, I suggest you approach C# scripting and the REPL CLIs as lightweight replacements for Visual Studio | New Project: UnitTestProject105 or the similarly purposed dotnetfiddle.net. These are C# and .NET targeted ways to increase your understanding of the language and .NET APIs. The C# REPL provides a means of coding up short snippets or program units you can noodle on until they’re ready to be cut and pasted into larger programs. It allows you to write more extensive scripts whose syntax is validated (even for little things like casing mismatches) as you write the code, rather than forcing you to execute the script only to discover you mistyped something. Once you understand its place, C# scripting and its interactive windows become a pleasure, the tool you’ve been looking for since version 1.0.\n\nAs interesting as C# REPL and C# scripting are on their own, consider that they also provide a stepping stone to being an extension framework for your own application—à la Visual Basic for Applications (VBA). With an interactive window and C# scripting support, you can imagine a world—not too far off—in which you can add .NET “macros” into your own applications again—without inventing a custom language, parser and editor. Now that would be a legacy COM feature worth bringing to the modern world none too soon.\n\nMark Michaelis is founder of IntelliTect, where he serves as its chief technical architect and trainer. For nearly two decades he has been a Microsoft MVP, and a Microsoft Regional Director since 2007. Michaelis serves on several Microsoft software design review teams, including C#, Microsoft Azure, SharePoint and Visual Studio ALM. He speaks at developer conferences and has written numerous books including his most recent, “Essential C# 6.0 (5th Edition)” (itl.tc/EssentialCSharp). Contact him on Facebook at facebook.com/Mark.Michaelis, on his blog at IntelliTect.com/Mark, on Twitter: @markmichaelis or via e-mail at mark@IntelliTect.com.\n\nThanks to the following Microsoft technical expert for reviewing this article: Kevin Bost and Kasey Uhlenhuth"
    },
    {
        "link": "https://bacancytechnology.com/qanda/dot-net/execute-shell-command-from-net-application",
        "document": "To execute a shell command from a .NET application, you can use the class, which is available in the namespace. Here is an example code snippet that demonstrates how to execute a shell command:\n\nIn this example, the command is executed with the argument and the URL of the Git repository to clone. The rest of the code is similar to the previous example, with the standard output of the process being redirected and printed to the console. You can replace the command with any other command that takes arguments."
    },
    {
        "link": "https://ffmpeg.org/ffmpeg-filters.html",
        "document": "This document describes filters, sources, and sinks provided by the libavfilter library.\n\nFiltering in FFmpeg is enabled through the libavfilter library.\n\nIn libavfilter, a filter can have multiple inputs and multiple outputs. To illustrate the sorts of things that are possible, we consider the following filtergraph.\n\nThis filtergraph splits the input stream in two streams, then sends one stream through the crop filter and the vflip filter, before merging it back with the other stream by overlaying it on top. You can use the following command to achieve this:\n\nThe result will be that the top half of the video is mirrored onto the bottom half of the output video.\n\nFilters in the same linear chain are separated by commas, and distinct linear chains of filters are separated by semicolons. In our example, are in one linear chain, and are separately in another. The points where the linear chains join are labelled by names enclosed in square brackets. In the example, the split filter generates two outputs that are associated to the labels and .\n\nThe stream sent to the second output of , labelled as , is processed through the filter, which crops away the lower half part of the video, and then vertically flipped. The filter takes in input the first unchanged output of the split filter (which was labelled as ), and overlay on its lower half the output generated by the filterchain.\n\nSome filters take in input a list of parameters: they are specified after the filter name and an equal sign, and are separated from each other by a colon.\n\nThere exist so-called that do not have an audio/video input, and that will not have audio/video output.\n\nThe program included in the FFmpeg directory can be used to parse a filtergraph description and issue a corresponding textual representation in the dot language.\n\nto see how to use .\n\nYou can then pass the dot description to the program (from the graphviz suite of programs) and obtain a graphical representation of the filtergraph.\n\nFor example the sequence of commands:\n\ncan be used to create and display an image representing the graph described by the string. Note that this string must be a complete self-contained graph, with its inputs and outputs explicitly defined. For example if your command line is of the form:\n\nyour string will need to be of the form:\n\nyou may also need to set the parameters and add a filter in order to simulate a specific input file.\n\nA filtergraph is a directed graph of connected filters. It can contain cycles, and there can be multiple links between a pair of filters. Each link has one input pad on one side connecting it to one filter from which it takes its input, and one output pad on the other side connecting it to one filter accepting its output.\n\nEach filter in a filtergraph is an instance of a filter class registered in the application, which defines the features and the number of input and output pads of the filter.\n\nA filter with no input pads is called a \"source\", and a filter with no output pads is called a \"sink\".\n\nA filtergraph has a textual representation, which is recognized by the / / and options in and / in , and by the function defined in .\n\nA filterchain consists of a sequence of connected filters, each one connected to the previous one in the sequence. A filterchain is represented by a list of \",\"-separated filter descriptions.\n\nA filtergraph consists of a sequence of filterchains. A sequence of filterchains is represented by a list of \";\"-separated filterchain descriptions.\n\nA filter is represented by a string of the form: [ ]...[ ] @ = [ ]...[ ]\n\nis the name of the filter class of which the described filter is an instance of, and has to be the name of one of the filter classes registered in the program optionally followed by \"@ \". The name of the filter class is optionally followed by a string \"= \".\n\nis a string which contains the parameters used to initialize the filter instance. It may have one of two forms:\n• A ’:’-separated list of . In this case, the keys are assumed to be the option names in the order they are declared. E.g. the filter declares three options in this order – , and . Then the parameter list means that the value is assigned to the option , to and to .\n• A ’:’-separated list of mixed direct and long pairs. The direct must precede the pairs, and follow the same constraints order of the previous point. The following pairs can be set in any preferred order.\n\nIf the option value itself is a list of items (e.g. the filter takes a list of pixel formats), the items in the list are usually separated by ‘ ’.\n\nThe list of arguments can be quoted using the character ‘ ’ as initial and ending mark, and the character ‘ ’ for escaping the characters within the quoted text; otherwise the argument string is considered terminated when the next special character (belonging to the set ‘ ’) is encountered.\n\nA special syntax implemented in the CLI tool allows loading option values from files. This is done be prepending a slash ’/’ to the option name, then the supplied value is interpreted as a path from which the actual value is loaded. E.g.\n\nwill load the text to be drawn from . API users wishing to implement a similar feature should use the functions together with custom IO code.\n\nThe name and arguments of the filter are optionally preceded and followed by a list of link labels. A link label allows one to name a link and associate it to a filter output or input pad. The preceding labels ... , are associated to the filter input pads, the following labels ... , are associated to the output pads.\n\nWhen two link labels with the same name are found in the filtergraph, a link between the corresponding input and output pad is created.\n\nIf an output pad is not labelled, it is linked by default to the first unlabelled input pad of the next filter in the filterchain. For example in the filterchain\n\nthe split filter instance has two output pads, and the overlay filter instance two input pads. The first output pad of split is labelled \"L1\", the first input pad of overlay is labelled \"L2\", and the second output pad of split is linked to the second input pad of overlay, which are both unlabelled.\n\nIn a filter description, if the input label of the first filter is not specified, \"in\" is assumed; if the output label of the last filter is not specified, \"out\" is assumed.\n\nIn a complete filterchain all the unlabelled filter input and output pads must be connected. A filtergraph is considered valid if all the filter input and output pads of all the filterchains are connected.\n\nLeading and trailing whitespaces (space, tabs, or line feeds) separating tokens in the filtergraph specification are ignored. This means that the filtergraph can be expressed using empty lines and spaces to improve redability.\n\nFor example, the filtergraph:\n\ncan be represented as:\n\nLibavfilter will automatically insert scale filters where format conversion is required. It is possible to specify swscale flags for those automatically inserted scalers by prepending to the filtergraph description.\n\nHere is a BNF description of the filtergraph syntax:\n\nFiltergraph description composition entails several levels of escaping. See (ffmpeg-utils)the \"Quoting and escaping\" section in the ffmpeg-utils(1) manual for more information about the employed escaping procedure.\n\nA first level escaping affects the content of each filter option value, which may contain the special character used to separate values, or one of the escaping characters .\n\nA second level escaping affects the whole filter description, which may contain the escaping characters or the special characters used by the filtergraph description.\n\nFinally, when you specify a filtergraph on a shell commandline, you need to perform a third level escaping for the shell special characters contained within it.\n\nFor example, consider the following string to be embedded in the drawtext filter description value:\n\nThis string contains the special escaping character, and the special character, so it needs to be escaped in this way:\n\nA second level of escaping is required when embedding the filter description in a filtergraph description, in order to escape all the filtergraph special characters. Thus the example above becomes:\n\n(note that in addition to the escaping special characters, also needs to be escaped).\n\nFinally an additional level of escaping is needed when writing the filtergraph description in a shell command, which depends on the escaping rules of the adopted shell. For example, assuming that is special and needs to be escaped with another , the previous string will finally result in:\n\nIn order to avoid cumbersome escaping when using a commandline tool accepting a filter specification as input, it is advisable to avoid direct inclusion of the filter or options specification in the shell.\n\nFor example, in case of the drawtext filter, you might prefer to use the option in place of to specify the text to render.\n\nSome filters support a generic option. For the filters supporting timeline editing, this option can be set to an expression which is evaluated before sending a frame to the filter. If the evaluation is non-zero, the filter will be enabled, otherwise the frame will be sent unchanged to the next filter in the filtergraph.\n\nThe expression accepts the following values:\n\nAdditionally, these filters support an command that can be used to re-define the expression.\n\nLike any other filtering option, the option follows the same rules.\n\nFor example, to enable a blur filter (smartblur) from 10 seconds to 3 minutes, and a curves filter starting at 3 seconds:\n\nSee to view which filters have timeline support.\n\nSome options can be changed during the operation of the filter using a command. These options are marked ’T’ on the output of . The name of the command is the name of the option and the argument is the new value.\n\n7 Options for filters with several inputs (framesync)\n\nSome filters with several inputs support a common set of options. These options can only be set by name, not with the short notation.\n\nWhen you configure your FFmpeg build, you can disable any of the existing filters using . The configure output will show the audio filters included in your build.\n\nBelow is a description of the currently available audio filters.\n\nApply Affine Projection algorithm to the first audio stream using the second audio stream.\n\nThis adaptive filter is used to estimate unknown audio based on multiple input audio samples. Affine projection algorithm can make trade-offs between computation complexity with convergence speed.\n\nA description of the accepted options follows.\n\nA compressor is mainly used to reduce the dynamic range of a signal. Especially modern music is mostly compressed at a high ratio to improve the overall loudness. It’s done to get the highest attention of a listener, \"fatten\" the sound and bring more \"power\" to the track. If a signal is compressed too much it may sound dull or \"dead\" afterwards or it may start to \"pump\" (which could be a powerful effect but can also destroy a track completely). The right compression is the key to reach a professional sound and is the high art of mixing and mastering. Because of its complex settings it may take a long time to get the right feeling for this kind of effect.\n\nCompression is done by detecting the volume above a chosen level and dividing it by the factor set with . So if you set the threshold to -12dB and your signal reaches -6dB a ratio of 2:1 will result in a signal at -9dB. Because an exact manipulation of the signal would cause distortion of the waveform the reduction can be levelled over the time. This is done by setting \"Attack\" and \"Release\". determines how long the signal has to rise above the threshold before any reduction will occur and sets the time the signal has to fall below the threshold to reduce the reduction again. Shorter signals than the chosen attack time will be left untouched. The overall reduction of the signal can be made up afterwards with the setting. So compressing the peaks of a signal about 6dB and raising the makeup to this level results in a signal twice as loud than the source. To gain a softer entry in the compression the flattens the hard edge at the threshold in the range of the chosen decibels.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThe filter accepts the following options:\n\nCopy the input audio source unchanged to the output. This is mainly useful for testing purposes.\n\nApply cross fade from one input audio stream to another input audio stream. The cross fade is applied for specified duration near the end of first stream.\n\nThe filter accepts the following options:\n• Cross fade from one input to another:\n• Cross fade from one input to another but without overlapping:\n\nThis filter splits audio stream into two or more frequency ranges. Summing all streams back will give flat output.\n\nThe filter accepts the following options:\n• Split input audio stream into two bands (low and high) with split frequency of 1500 Hz, each band will be in separate stream:\n• Same as above, but with higher filter order:\n• Same as above, but also with additional middle band (frequencies between 1500 and 8000):\n\nThis filter is bit crusher with enhanced functionality. A bit crusher is used to audibly reduce number of bits an audio signal is sampled with. This doesn’t change the bit depth at all, it just produces the effect. Material reduced in bit depth sounds more harsh and \"digital\". This filter is able to even round to continuous values instead of discrete bit depths. Additionally it has a D/C offset which results in different crushing of the lower and the upper half of the signal. An Anti-Aliasing setting is able to produce \"softer\" crushing sounds.\n\nAnother feature of this filter is the logarithmic mode. This setting switches from linear distances between bits to logarithmic ones. The result is a much more \"natural\" sounding crusher which doesn’t gate low signals for example. The human ear has a logarithmic perception, so this kind of crushing is much more pleasant. Logarithmic crushing is also able to get anti-aliased.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nDelay audio filtering until a given wallclock timestamp. See the cue filter.\n\nSamples detected as impulsive noise are replaced by interpolated samples using autoregressive modelling.\n\nSamples detected as clipped are replaced by interpolated samples using autoregressive modelling.\n\nThe filter accepts the following options:\n\nDelay one or more audio channels.\n\nSamples in delayed channel are filled with silence.\n\nThe filter accepts the following option:\n• Delay first channel by 1.5 seconds, the third channel by 0.5 seconds and leave the second channel (and any other channels that may be present) unchanged.\n• Delay second channel by 500 samples, the third channel by 700 samples and leave the first channel (and any other channels that may be present) unchanged.\n• Delay all channels by same number of samples:\n\nThis filter shall be placed before any filter that can produce denormals.\n\nA description of the accepted parameters follows.\n\nThis filter supports the all above options as commands.\n\nApplying both filters one after another produces original audio.\n\nA description of the accepted options follows.\n\nThis filter supports the all above options as commands.\n• Apply spectral compression to all frequencies with threshold of -50 dB and 1:6 ratio:\n• Similar to above but with 1:2 ratio and filtering only front center channel:\n• Apply spectral noise gate to all frequencies with threshold of -85 dB and with short attack time and short release time:\n• Apply spectral expansion to all frequencies with threshold of -10 dB and 1:2 ratio:\n• Apply limiter to max -60 dB to all frequencies, with attack of 2 ms and release of 10 ms:\n\nA description of the accepted options follows.\n\nThis filter supports the all above options as commands.\n\nA description of the accepted options follows.\n\nThis filter supports the all above options as commands.\n\nEchoes are reflected sound and can occur naturally amongst mountains (and sometimes large buildings) when talking or shouting; digital echo effects emulate this behaviour and are often used to help fill out the sound of a single instrument or vocal. The time difference between the original signal and the reflection is the , and the loudness of the reflected signal is the . Multiple echoes can have different delays and decays.\n\nA description of the accepted parameters follows.\n• Make it sound as if there are twice as many instruments as are actually playing:\n• If delay is very short, then it sounds like a (metallic) robot playing music:\n• A longer delay will sound like an open air concert in the mountains:\n• Same as above but with one more mountain:\n\nAudio emphasis filter creates or restores material directly taken from LPs or emphased CDs with different filter curves. E.g. to store music on vinyl the signal has to be altered by a filter first to even out the disadvantages of this recording medium. Once the material is played back the inverse filter has to be applied to restore the distortion of the frequency response.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nModify an audio signal according to the specified expressions.\n\nThis filter accepts one or more expressions (one for each channel), which are evaluated and used to modify a corresponding audio signal.\n\nIt accepts the following parameters:\n\nEach expression in can contain the following constants and functions:\n\nNote: this filter is slow. For faster processing you should use a dedicated filter.\n• Invert phase of the second channel:\n\nAn exciter is used to produce high sound that is not present in the original signal. This is done by creating harmonic distortions of the signal which are restricted in range and added to the original signal. An Exciter raises the upper end of an audio signal without simply raising the higher frequencies like an equalizer would do to create a more \"crisp\" or \"brilliant\" sound.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nA description of the accepted parameters follows.\n\nThis filter supports the all above options as commands.\n• Fade in first 15 seconds of audio:\n• Fade out last 25 seconds of a 900 seconds audio:\n\nA description of the accepted parameters follows.\n\nThis filter supports the some above mentioned options as commands.\n• Reduce white noise by 10dB, and use previously measured noise floor of -40dB:\n• Reduce white noise by 10dB, also set initial noise floor to -80dB and enable automatic tracking of noise floor so noise floor will gradually change during processing:\n• Reduce noise by 20dB, using noise floor of -40dB and using commands to take noise profile of first 0.4 seconds of input audio:\n• Leave almost only low frequencies in audio:\n\nThis filter is designed for applying long FIR filters, up to 60 seconds long.\n\nIt can be used as component for digital crossover filters, room equalization, cross talk cancellation, wavefield synthesis, auralization, ambiophonics, ambisonics and spatialization.\n\nThis filter uses the streams higher than first one as FIR coefficients. If the non-first stream holds a single channel, it will be used for all input channels in the first stream, otherwise the number of channels in the non-first stream must be same as the number of channels in the first stream.\n\nIt accepts the following parameters:\n• Apply reverb to stream using mono IR file as second input, complete command using ffmpeg:\n• Apply true stereo processing given input stereo stream, and two stereo impulse responses for left and right channel, the impulse response files are files with names l_ir.wav and r_ir.wav, and setting irnorm option value:\n• Similar to above example, but with explicitly set to estimated value and with disabled:\n\nSet output format constraints for the input audio. The framework will negotiate the most appropriate format to minimize conversions.\n\nIt accepts the following parameters:\n\nIf a parameter is omitted, all values are allowed.\n\nForce the output to either unsigned 8-bit or signed 16-bit stereo\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nA description of the accepted options follows.\n\nThis filter supports the all above options as commands.\n\nA gate is mainly used to reduce lower parts of a signal. This kind of signal processing reduces disturbing noise between useful signals.\n\nGating is done by detecting the volume below a chosen level and dividing it by the factor set with . The bottom of the noise floor is set via . Because an exact manipulation of the signal would cause distortion of the waveform the reduction can be levelled over time. This is done by setting and .\n\ndetermines how long the signal has to fall below the threshold before any reduction will occur and sets the time the signal has to rise above the threshold to reduce the reduction again. Shorter signals than the chosen attack time will be left untouched.\n\nThis filter supports the all above options as commands.\n\nIt accepts the following parameters:\n\nCoefficients in and format are separated by spaces and are in ascending order.\n\nCoefficients in format are separated by spaces and order of coefficients doesn’t matter. Coefficients in format are complex numbers with imaginary unit.\n\nDifferent coefficients and gains can be provided for every channel, in such case use ’|’ to separate coefficients or gains. Last provided coefficients will be used for all remaining channels.\n• Apply 2 pole elliptic notch at around 5000Hz for 48000 Hz sample rate:\n• Same as above but in format:\n\nThe limiter prevents an input signal from rising over a desired threshold. This limiter uses lookahead technology to prevent your signal from distorting. It means that there is a small delay after the signal is processed. Keep in mind that the delay it produces is the attack time you set.\n\nThe filter accepts the following options:\n\nDepending on picked setting it is recommended to upsample input 2x or 4x times with aresample before applying this filter.\n\nApply a two-pole all-pass filter with central frequency (in Hz) , and filter-width . An all-pass filter changes the audio’s frequency to phase relationship without changing its frequency to amplitude relationship.\n\nThe filter accepts the following options:\n\nThis filter supports the following commands:\n\nThe filter accepts the following options:\n\nMerge two or more audio streams into a single multi-channel stream.\n\nThe filter accepts the following options:\n\nIf the channel layouts of the inputs are disjoint, and therefore compatible, the channel layout of the output will be set accordingly and the channels will be reordered as necessary. If the channel layouts of the inputs are not disjoint, the output will have all the channels of the first input then all the channels of the second input, in that order, and the channel layout of the output will be the default value corresponding to the total number of channels.\n\nFor example, if the first input is in 2.1 (FL+FR+LF) and the second input is FC+BL+BR, then the output will be in 5.1, with the channels in the following order: a1, a2, b1, a3, b2, b3 (a1 is the first channel of the first input, b1 is the first channel of the second input).\n\nOn the other hand, if both input are in stereo, the output channels will be in the default order: a1, a2, b1, b2, and the channel layout will be arbitrarily set to 4.0, which may or may not be the expected value.\n\nAll inputs must have the same sample rate, and format.\n\nIf inputs do not have the same duration, the output will stop with the shortest.\n\nNote that this filter only supports float samples (the and audio filters support many formats). If the input has integer samples then aresample will be automatically inserted to perform the conversion to float samples.\n\nIt accepts the following parameters:\n• This will mix 3 input audio streams to a single output with the same duration as the first input and a dropout transition time of 3 seconds:\n• This will mix one vocal and one music input audio stream to a single output with the same duration as the longest input. The music will have quarter the weight as the vocals, and the inputs are not normalized:\n\nThis filter supports the following commands:\n\nMultiply first audio stream with second audio stream and store result in output audio stream. Multiplication is done by multiplying each sample from first stream with sample at same position from second stream.\n\nWith this element-wise multiplication one can create amplitude fades and amplitude modulations.\n\nIt accepts the following parameters:\n• Lower gain by 10 of central frequency 200Hz and width 100 Hz for first 2 channels using Chebyshev type 1 filter:\n\nThis filter supports the following commands:\n\nEach sample is adjusted by looking for other samples with similar contexts. This context similarity is defined by comparing their surrounding patches of size . Patches are searched in an area of around the sample.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nApply Normalized Least-Mean-(Squares|Fourth) algorithm to the first audio stream using the second audio stream.\n\nThis adaptive filter is used to mimic a desired filter by finding the filter coefficients that relate to producing the least mean square of the error signal (difference between the desired, 2nd input audio stream and the actual signal, the 1st input audio stream).\n\nA description of the accepted options follows.\n• One of many usages of this filter is noise reduction, input audio is filtered with same samples that are delayed by fixed amount, one such example for stereo audio is:\n\nThis filter supports the same commands as options, excluding option .\n\nPass the audio source unchanged to the output.\n\nPad the end of an audio stream with silence.\n\nThis can be used together with to extend audio streams to the same length as the video stream.\n\nA description of the accepted options follows.\n\nIf neither the nor the nor nor option is set, the filter will add silence to the end of the input stream indefinitely.\n\nNote that for ffmpeg 4.4 and earlier a zero or also caused the filter to add silence indefinitely.\n• Add 1024 samples of silence to the end of the input:\n• Make sure the audio output will contain at least 10000 samples, pad the input with silence if required:\n• Use to pad the audio input with silence, so that the video stream will always result the shortest and will be converted until the end in the output file when using the option:\n\nA phaser filter creates series of peaks and troughs in the frequency spectrum. The position of the peaks and troughs are modulated so that they vary over time, creating a sweeping effect.\n\nA description of the accepted parameters follows.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThis filter takes two audio streams for input, and outputs first audio stream. Results are in dB per channel at end of either input.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nAudio pulsator is something between an autopanner and a tremolo. But it can produce funny stereo effects as well. Pulsator changes the volume of the left and right channel based on a LFO (low frequency oscillator) with different waveforms and shifted phases. This filter have the ability to define an offset between left and right channel. An offset of 0 means that both LFO shapes match each other. The left and right channel are altered equally - a conventional tremolo. An offset of 50% means that the shape of the right channel is exactly shifted in phase (or moved backwards about half of the frequency) - pulsator acts as an autopanner. At 1 both curves match again. Every setting in between moves the phase shift gapless between all stages and produces some \"bypassing\" sounds with sine and triangle waveforms. The more you set the offset near 1 (starting from the 0.5) the faster the signal passes from the left to the right speaker.\n\nThe filter accepts the following options:\n\nResample the input audio to the specified parameters, using the libswresample library. If none are specified then the filter will automatically convert between its input and output.\n\nThis filter is also able to stretch/squeeze the audio data to make it match the timestamps or to inject silence / cut out audio to make it match the timestamps, do a combination of both or do neither.\n\nThe filter accepts the syntax [ :] , where expresses a sample rate and is a list of = pairs, separated by \":\". See the (ffmpeg-resampler)\"Resampler Options\" section in the ffmpeg-resampler(1) manual for the complete list of supported options.\n• Stretch/squeeze samples to the given timestamps, with a maximum of 1000 samples per second compensation:\n\nWarning: This filter requires memory to buffer the entire clip, so trimming is suggested.\n• Take the first 5 seconds of a clip, and reverse it.\n\nApply Recursive Least Squares algorithm to the first audio stream using the second audio stream.\n\nThis adaptive filter is used to mimic a desired filter by recursively finding the filter coefficients that relate to producing the minimal weighted linear least squares cost function of the error signal (difference between the desired, 2nd input audio stream and the actual signal, the 1st input audio stream).\n\nA description of the accepted options follows.\n\nThis filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThis filter takes two audio streams for input, and outputs first audio stream. Results are in dB per channel at end of either input.\n\nSet the number of samples per each output audio frame.\n\nThe last output packet may contain a different number of samples, as the filter will flush all the remaining samples when the input audio signals its end.\n\nThe filter accepts the following options:\n\nFor example, to set the number of per-frame samples to 1234 and disable padding for the last frame, use:\n\nSet the sample rate without altering the PCM data. This will result in a change of speed and pitch.\n\nThe filter accepts the following options:\n\nShow a line containing various information for each input audio frame. The input audio is not modified.\n\nThe shown line contains a sequence of key/value pairs of the form : .\n\nThe following values are shown in the output:\n\nThis filter takes two audio streams for input, and outputs first audio stream. Results are in dB per channel at end of either input.\n\nSoft clipping is a type of distortion effect where the amplitude of a signal is saturated along a smooth curve, rather than the abrupt shape of hard-clipping.\n\nThis filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nDisplay frequency domain statistical information about the audio channels. Statistics are calculated and stored as metadata for each audio channel and for each audio frame.\n\nIt accepts the following option:\n\nA list of each metadata key follows:\n\nThis filter uses PocketSphinx for speech recognition. To enable compilation of this filter, you need to configure FFmpeg with .\n\nIt accepts the following options:\n\nThe filter exports recognized speech as the frame metadata .\n\nDisplay time domain statistical information about the audio channels. Statistics are calculated and displayed for each audio channel and, where applicable, an overall figure is also given.\n\nIt accepts the following option:\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThis filter allows to set custom, steeper roll off than highpass filter, and thus is able to more attenuate frequency content in stop-band.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThe filter accepts exactly one parameter, the audio tempo. If not specified then the filter will assume nominal 1.0 tempo. Tempo must be in the [0.5, 100.0] range.\n\nNote that tempo greater than 2 will skip some samples rather than blend them in. If for any reason this is a concern it is always possible to daisy-chain several instances of atempo to achieve the desired product tempo.\n• To speed up audio to 300% tempo:\n• To speed up audio to 300% tempo by daisy-chaining two atempo instances:\n\nThis filter supports the following commands:\n\nThis filter apply any spectral roll-off slope over any specified frequency band.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nTrim the input so that the output contains one continuous subpart of the input.\n\nIt accepts the following parameters:\n\n, , and are expressed as time duration specifications; see (ffmpeg-utils)the Time duration section in the ffmpeg-utils(1) manual.\n\nNote that the first two sets of the start/end options and the option look at the frame timestamp, while the _sample options simply count the samples that pass through the filter. So start/end_pts and start/end_sample will give different results when the timestamps are wrong, inexact or do not start at zero. Also note that this filter does not modify the timestamps. If you wish to have the output timestamps start at zero, insert the asetpts filter after the atrim filter.\n\nIf multiple start or end options are set, this filter tries to be greedy and keep all samples that match at least one of the specified constraints. To keep only the part that matches all the constraints at once, chain multiple atrim filters.\n\nThe defaults are such that all the input is kept. So it is possible to set e.g. just the end values to keep everything before the specified time.\n• Drop everything except the second minute of input:\n• Keep only the first 1000 samples:\n\nResulted samples are always between -1 and 1 inclusive. If result is 1 it means two input samples are highly correlated in that selected segment. Result 0 means they are not correlated at all. If result is -1 it means two input samples are out of phase, which means they cancel each other.\n\nThe filter accepts the following options:\n\nApply a two-pole Butterworth band-pass filter with central frequency , and (3dB-point) band-width width. The option selects a constant skirt gain (peak gain = Q) instead of the default: constant 0dB peak gain. The filter roll off at 6dB per octave (20dB per decade).\n\nThe filter accepts the following options:\n\nThis filter supports the following commands:\n\nApply a two-pole Butterworth band-reject filter with central frequency , and (3dB-point) band-width . The filter roll off at 6dB per octave (20dB per decade).\n\nThe filter accepts the following options:\n\nThis filter supports the following commands:\n\nBoost or cut the bass (lower) frequencies of the audio using a two-pole shelving filter with a response similar to that of a standard hi-fi’s tone-controls. This is also known as shelving equalisation (EQ).\n\nThe filter accepts the following options:\n\nThis filter supports the following commands:\n\nApply a biquad IIR filter with the given coefficients. Where , , and , , are the numerator and denominator coefficients respectively. and , specify which channels to filter, by default all available are filtered.\n\nThis filter supports the following commands:\n\nBauer stereo to binaural transformation, which improves headphone listening of stereo audio records.\n\nTo enable compilation of this filter you need to configure FFmpeg with .\n\nIt accepts the following parameters:\n\nIt accepts the following parameters:\n\nIf no mapping is present, the filter will implicitly map input channels to output channels, preserving indices.\n• For example, assuming a 5.1+downmix input MOV file, will create an output WAV file tagged as stereo from the downmix channels of the input.\n\nSplit each channel from an input audio stream into a separate output stream.\n\nIt accepts the following parameters:\n• For example, assuming a stereo input MP3 file, will create an output Matroska file with two audio streams, one containing only the left channel and the other the right channel.\n\nCan make a single vocal sound like a chorus, but can also be applied to instrumentation.\n\nChorus resembles an echo effect with a short delay, but whereas with echo the delay is constant, with chorus, it is varied using using sinusoidal or triangular modulation. The modulation depth defines the range the modulated delay is played before or after the delay. Hence the delayed sound will sound slower or faster, that is the delayed sound tuned around the original one, like in a chorus where some vocals are slightly off key.\n\nIt accepts the following parameters:\n\nIt accepts the following parameters:\n• Make music with both quiet and loud passages suitable for listening to in a noisy environment: Another example for audio with whisper and explosion parts:\n• A noise gate for when the noise is at a lower level than the signal:\n• Here is another noise gate, this time for when the noise is at a higher level than the signal (making it, in some ways, similar to squelch):\n\nCompensation Delay Line is a metric based delay to compensate differing positions of microphones or speakers.\n\nFor example, you have recorded guitar with two microphones placed in different locations. Because the front of sound wave has fixed speed in normal conditions, the phasing of microphones can vary and depends on their location and interposition. The best sound mix can be achieved when these microphones are in phase (synchronized). Note that a distance of ~30 cm between microphones makes one microphone capture the signal in antiphase to the other microphone. That makes the final mix sound moody. This filter helps to solve phasing problems by adding different delays to each microphone track and make them synchronized.\n\nThe best result can be reached when you take one track as base and synchronize other tracks one by one with it. Remember that synchronization/delay tolerance depends on sample rate, too. Higher sample rates will give more tolerance.\n\nThe filter accepts the following parameters:\n\nThis filter supports the all above options as commands.\n\nCrossfeed is the process of blending the left and right channels of stereo audio recording. It is mainly used to reduce extreme stereo separation of low frequencies.\n\nThe intent is to produce more speaker like sound to the listener.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThis filter linearly increases differences between each audio sample.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThis can be useful to remove a DC offset (caused perhaps by a hardware problem in the recording chain) from the audio. The effect of a DC offset is reduced headroom and hence volume. The astats filter can be used to determine if a signal has a DC offset.\n\nThis filter accepts stereo input and produce surround (3.0) channels output. The newly produced front center channel have enhanced speech dialogue originally available in both stereo channels. This filter outputs front left and front right channels same as available in stereo input.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nDR values of 14 and higher is found in very dynamic material. DR of 8 to 13 is found in transition material. And anything less that 8 have very poor dynamics and is very compressed.\n\nThe filter accepts the following options:\n\nThis filter applies a certain amount of gain to the input audio in order to bring its peak magnitude to a target level (e.g. 0 dBFS). However, in contrast to more \"simple\" normalization algorithms, the Dynamic Audio Normalizer *dynamically* re-adjusts the gain factor to the input audio. This allows for applying extra gain to the \"quiet\" sections of the audio while avoiding distortions or clipping the \"loud\" sections. In other words: The Dynamic Audio Normalizer will \"even out\" the volume of quiet and loud sections, in the sense that the volume of each section is brought to the same target level. Note, however, that the Dynamic Audio Normalizer achieves this goal *without* applying \"dynamic range compressing\". It will retain 100% of the dynamic range *within* each section of the audio file.\n\nThis filter supports the all above options as commands.\n\nMake audio easier to listen to on headphones.\n\nThis filter adds ‘cues’ to 44.1kHz stereo (i.e. audio CD format) audio so that when listened to on headphones the stereo image is moved from inside your head (standard for headphones) to outside and in front of the listener (standard for speakers).\n\nApply a two-pole peaking equalisation (EQ) filter. With this filter, the signal-level at and around a selected frequency can be increased or decreased, whilst (unlike bandpass and bandreject filters) that at all other frequencies is unchanged.\n\nIn order to produce complex equalisation curves, this filter can be given several times, each with a different central frequency.\n\nThe filter accepts the following options:\n• Attenuate 10 dB at 1000 Hz, with a bandwidth of 200 Hz:\n• Apply 2 dB gain at 1000 Hz with Q 1 and attenuate 5 dB at 100 Hz with Q 2:\n\nThis filter supports the following commands:\n\nLinearly increases the difference between left and right channels which adds some sort of \"live\" effect to playback.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThe filter accepts the following option:\n• higher delay with zero phase to compensate delay:\n• lowpass on left channel, highpass on right channel:\n\nThe filter accepts the following options:\n\nNote that this makes most sense to apply on mono signals. With this filter applied to mono signals it give some directionality and stretches its stereo image.\n\nThe filter accepts the following options:\n\nDecodes High Definition Compatible Digital (HDCD) data. A 16-bit PCM stream with embedded HDCD codes is expanded into a 20-bit PCM stream.\n\nThe filter supports the Peak Extend and Low-level Gain Adjustment features of HDCD, and detects the Transient Filter flag.\n\nWhen using the filter with wav, note the default encoding for wav is 16-bit, so the resulting 20-bit stream will be truncated back to 16-bit. Use something like after the filter to get 24-bit PCM output.\n\nThe filter accepts the following options:\n\nApply head-related transfer functions (HRTFs) to create virtual loudspeakers around the user for binaural listening via headphones. The HRIRs are provided via additional streams, for each channel one stereo input stream is needed.\n\nThe filter accepts the following options:\n• Full example using wav files as coefficients with amovie filters for 7.1 downmix, each amovie filter use stereo file with IR coefficients as input. The files give coefficients for each position of virtual loudspeaker:\n• Full example using wav files as coefficients with amovie filters for 7.1 downmix, but now in format.\n\nApply a high-pass filter with 3dB point frequency. The filter can be either single-pole, or double-pole (the default). The filter roll off at 6dB per pole per octave (20dB per pole per decade).\n\nThe filter accepts the following options:\n\nThis filter supports the following commands:\n\nIt accepts the following parameters:\n\nThe filter will attempt to guess the mappings when they are not specified explicitly. It does so by first trying to find an unused matching input channel and if that fails it picks the first unused input channel.\n\nTo enable compilation of this filter you need to configure FFmpeg with .\n• List all available plugins within amp (LADSPA example plugin) library:\n• List all available controls and their valid ranges for plugin from library:\n• Add reverberation to the audio using TAP-plugins (Tom’s Audio Processing plugins):\n• Generate 20 bpm clicks using plugin from the (CAPS) library:\n• Increase volume by 20dB using fast lookahead limiter from Steve Harris collection:\n• Reduce stereo image using from the (CAPS) library:\n• Another white noise, now using (CAPS) library:\n\nThis filter supports the following commands:\n\nEBU R128 loudness normalization. Includes both dynamic and linear normalization modes. Support for both single pass (livestreams, files) and double pass (files) modes. This algorithm can target IL, LRA, and maximum true peak. In dynamic mode, to accurately detect true peaks, the audio stream will be upsampled to 192 kHz. Use the option or filter to explicitly set an output sample rate.\n\nThe filter accepts the following options:\n\nApply a low-pass filter with 3dB point frequency. The filter can be either single-pole or double-pole (the default). The filter roll off at 6dB per pole per octave (20dB per pole per decade).\n\nThe filter accepts the following options:\n• Lowpass only LFE channel, it LFE is not present it does nothing:\n\nThis filter supports the following commands:\n\nTo enable compilation of this filter you need to configure FFmpeg with .\n\nThis filter supports all options that are exported by plugin as commands.\n\nThe input audio is divided into bands using 4th order Linkwitz-Riley IIRs. This is akin to the crossover of a loudspeaker, and results in flat frequency response when absent compander action.\n\nIt accepts the following parameters:\n\nMix channels with specific gain levels. The filter accepts the output channel layout followed by a set of channels definitions.\n\nThis filter is also designed to efficiently remap the channels of an audio stream.\n\nThe filter accepts parameters of the form: \" | | |...\"\n\nIf the ‘=’ in a channel specification is replaced by ‘<’, then the gains for that specification will be renormalized so that the total is 1, thus avoiding clipping noise.\n\nFor example, if you want to down-mix from stereo to mono, but with a bigger factor for the left channel:\n\nA customized down-mix to stereo that works automatically for 3-, 4-, 5- and 7-channels surround:\n\nNote that integrates a default down-mix (and up-mix) system that should be preferred (see \"-ac\" option) unless you have very specific needs.\n\nThe channel remapping will be effective if, and only if:\n• gain coefficients are zeroes or ones,\n• only one input per channel output,\n\nIf all these conditions are satisfied, the filter will notify the user (\"Pure channel mapping detected\"), and use an optimized and lossless method to do the remapping.\n\nFor example, if you have a 5.1 source and want a stereo audio stream by dropping the extra channels:\n\nGiven the same source, you can also switch front left and front right channels and keep the input channel layout:\n\nIf the input is a stereo audio stream, you can mute the front left channel (and still keep the stereo channel layout) with:\n\nStill with a stereo audio stream input, you can copy the right channel in both front left and right:\n\nReplayGain scanner filter. This filter takes an audio stream as an input and outputs it unchanged. At end of filtering it displays and .\n\nThe filter accepts the following exported read-only options:\n\nConvert the audio sample format, sample rate and channel layout. It is not meant to be used directly.\n\nTo enable compilation of this filter, you need to configure FFmpeg with .\n\nThe filter accepts the following options:\n\nThis filter supports the following commands:\n\nThis filter acts like normal compressor but has the ability to compress detected signal using second input signal. It needs two input streams and returns one output stream. First input stream will be processed depending on second stream signal. The filtered signal then can be filtered with other filters in later stages of processing. See pan and amerge filter.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n• Full ffmpeg example taking 2 audio inputs, 1st input to be compressed depending on the signal of 2nd input and later compressed signal to be merged with 2nd input:\n\nA sidechain gate acts like a normal (wideband) gate but has the ability to filter the detected signal before sending it to the gain reduction stage. Normally a gate uses the full range signal to detect a level above the threshold. For example: If you cut all lower frequencies from your sidechain signal the gate will decrease the volume of your track only if not enough highs appear. With this technique you are able to reduce the resonation of a natural drum or remove \"rumbling\" of muted strokes from a heavily distorted guitar. It needs two input streams and returns one output stream. First input stream will be processed depending on second stream signal.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThis filter logs a message when it detects that the input audio volume is less or equal to a noise tolerance value for a duration greater or equal to the minimum detected noise duration.\n\nThe printed times and duration are expressed in seconds. The or metadata key is set on the first frame whose timestamp equals or exceeds the detection duration and it contains the timestamp of the first frame of the silence.\n\nThe or and or metadata keys are set on the first frame after the silence. If is enabled, and each channel is evaluated separately, the suffixed keys are used, and corresponds to the channel number.\n\nThe filter accepts the following options:\n• Complete example with to detect silence with 0.0001 noise tolerance in :\n\nRemove silence from the beginning, middle or end of the audio.\n\nThe filter accepts the following options:\n• The following example shows how this filter can be used to start a recording that does not contain the delay at the start which usually occurs between pressing the record button and the start of the performance:\n• Trim all silence encountered from beginning to end where there is more than 1 second of silence in audio:\n• Trim all digital silence samples, using peak detection, from beginning to end where there is more than 0 samples of digital silence in audio and digital silence is detected in all channels at same positions in stream:\n• Trim every 2nd encountered silence period from beginning to end where there is more than 1 second of silence per silence period in audio:\n• Similar as above, but keep maximum of 0.5 seconds of silence from each trimmed period:\n• Similar as above, but keep maximum of 1.5 seconds of silence from start of audio:\n\nThis filter supports some above options as commands.\n\nSOFAlizer uses head-related transfer functions (HRTFs) to create virtual loudspeakers around the user for binaural listening via headphones (audio formats up to 9 channels supported). The HRTFs are stored in SOFA files (see http://www.sofacoustics.org/ for a database). SOFAlizer is developed at the Acoustics Research Institute (ARI) of the Austrian Academy of Sciences.\n\nTo enable compilation of this filter you need to configure FFmpeg with .\n\nThe filter accepts the following options:\n• Using ClubFritz12 sofa file and bigger radius with small rotation:\n• Similar as above but with custom speaker positions for front left, front right, back left and back right and also with custom gain:\n\nThis filter expands or compresses each half-cycle of audio samples (local set of samples all above or all below zero and between two nearest zero crossings) depending on threshold value, so audio reaches target peak value under conditions controlled by below options.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThis filter has some handy utilities to manage stereo signals, for converting M/S stereo recordings to L/R signal while having control over the parameters or spreading the stereo image of master track.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThis filter enhance the stereo effect by suppressing signal common to both channels and by delaying the signal of left into right and vice versa, thereby widening the stereo effect.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options except as commands.\n\nThe filter accepts the following options:\n\nThis filter allows to produce multichannel output from audio stream.\n\nThe filter accepts the following options:\n\nBoost or cut the lower frequencies and cut or boost higher frequencies of the audio using a two-pole shelving filter with a response similar to that of a standard hi-fi’s tone-controls. This is also known as shelving equalisation (EQ).\n\nThe filter accepts the following options:\n\nThis filter supports some options as commands.\n\nBoost or cut treble (upper) frequencies of the audio using a two-pole shelving filter with a response similar to that of a standard hi-fi’s tone-controls. This is also known as shelving equalisation (EQ).\n\nThe filter accepts the following options:\n\nThis filter supports the following commands:\n\nThe filter accepts the following options:\n\nThe filter accepts the following options:\n\nThis filter accepts stereo input and produce stereo with LFE (2.1) channels output. The newly produced LFE channel have enhanced virtual bass originally obtained from both stereo channels. This filter outputs front left and front right channels unchanged as available in stereo input.\n\nThe filter accepts the following options:\n\nIt accepts the following parameters:\n\nThe volume expression can contain the following parameters.\n\nNote that when is set to ‘ ’ only the and variables are available, all other variables will evaluate to NAN.\n\nThis filter supports the following commands:\n• Halve the input audio volume: In all the above example the named key for can be omitted, for example like in:\n• Fade volume after time 10 with an annihilation period of 5 seconds:\n\nDetect the volume of the input video.\n\nThe filter has no parameters. It supports only 16-bit signed integer samples, so the input will be converted when needed. Statistics about the volume will be printed in the log when the input stream end is reached.\n\nIn particular it will show the mean volume (root mean square), maximum volume (on a per-sample basis), and the beginning of a histogram of the registered volume values (from the maximum value to a cumulated 1/1000 of the samples).\n\nAll volumes are in decibels relative to the maximum PCM value.\n\nHere is an excerpt of the output:\n• The mean square energy is approximately -27 dB, or 10^-2.7.\n• The largest sample is at -4 dB, or more precisely between -4 dB and -5 dB.\n• There are 6 samples at -4 dB, 62 at -5 dB, 286 at -6 dB, etc.\n\nIn other words, raising the volume by +4 dB does not cause any clipping, raising it by +5 dB causes clipping for 6 samples, etc.\n\nBelow is a description of the currently available audio sources.\n\nBuffer audio frames, and make them available to the filter chain.\n\nThis source is mainly intended for a programmatic use, in particular through the interface defined in .\n\nIt accepts the following parameters:\n\nwill instruct the source to accept planar 16bit signed stereo at 44100Hz. Since the sample format with name \"s16p\" corresponds to the number 6 and the \"stereo\" channel layout corresponds to the value 0x3, this is equivalent to:\n\nGenerate an audio signal specified by an expression.\n\nThis source accepts in input one or more expressions (one for each channel), which are evaluated and used to generate a corresponding audio signal.\n\nThis source accepts the following options:\n\nEach expression in can contain the following constants:\n• Generate a sin signal with frequency of 440 Hz, set sample rate to 8000 Hz:\n• Generate a two channels signal, specify the channel layout (Front Center + Back Center) explicitly:\n\nThe resulting stream can be used with afir filter for filtering the audio signal.\n\nThe filter accepts the following options:\n\nThe resulting stream can be used with afir filter for filtering the audio signal.\n\nThe filter accepts the following options:\n\nThe resulting stream can be used with afir filter for filtering the audio signal.\n\nThe filter accepts the following options:\n\nThe null audio source, return unprocessed audio frames. It is mainly useful as a template and to be employed in analysis / debugging tools, or as the source for filters which ignore the input data (for example the sox synth filter).\n\nThis source accepts the following options:\n• Set the sample rate to 48000 Hz and the channel layout to AV_CH_LAYOUT_MONO.\n• Do the same operation with a more obvious syntax:\n\nAll the parameters need to be explicitly defined.\n\nTo enable compilation of this filter you need to configure FFmpeg with .\n\nNote that versions of the flite library prior to 2.0 are not thread-safe.\n\nThe filter accepts the following options:\n• Read from file , and synthesize the text using the standard flite voice:\n• Read the specified text selecting the voice: flite=text='So fare thee well, poor devil of a Sub-Sub, whose commentator I am':voice=slt\n• Input text to ffmpeg: ffmpeg -f lavfi -i flite=text='So fare thee well, poor devil of a Sub-Sub, whose commentator I am':voice=slt\n• Make speak the specified text, using and the device: ffplay -f lavfi flite=text='No more be grieved for which that thou hast done.'\n\nFor more information about libflite, check: http://www.festvox.org/flite/\n\nThe filter accepts the following options:\n• Generate 60 seconds of pink noise, with a 44.1 kHz sampling rate and an amplitude of 0.5:\n\nThe resulting stream can be used with afir filter for phase-shifting the signal by 90 degrees.\n\nThis is used in many matrix coding schemes and for analytic signal generation. The process is often written as a multiplication by i (or j), the imaginary unit.\n\nThe filter accepts the following options:\n\nThe resulting stream can be used with afir filter for filtering the audio signal.\n\nThe filter accepts the following options:\n\nGenerate an audio signal made of a sine wave with amplitude 1/8.\n\nThe filter accepts the following options:\n• Generate a 220 Hz sine wave with a 880 Hz beep each second, for 5 seconds:\n\nBelow is a description of the currently available audio sinks.\n\nBuffer audio frames, and make them available to the end of filter chain.\n\nThis sink is mainly intended for programmatic use, in particular through the interface defined in or the options system.\n\nIt accepts a pointer to an AVABufferSinkContext structure, which defines the incoming buffers’ formats, to be passed as the opaque parameter to for initialization.\n\nNull audio sink; do absolutely nothing with the input audio. It is mainly useful as a template and for use in analysis / debugging tools.\n\nWhen you configure your FFmpeg build, you can disable any of the existing filters using . The configure output will show the video filters included in your build.\n\nBelow is a description of the currently available video filters.\n\nThe frame data is passed through unchanged, but metadata is attached to the frame indicating regions of interest which can affect the behaviour of later encoding. Multiple regions can be marked by applying the filter multiple times.\n• Mark the centre quarter of the frame as interesting.\n• Mark the 100-pixel-wide region on the left edge of the frame as very uninteresting (to be encoded at much lower quality than the rest of the frame).\n\nExtract the alpha component from the input as a grayscale video. This is especially useful with the filter.\n\nAdd or replace the alpha component of the primary input with the grayscale value of a second input. This is intended for use with to allow the transmission or storage of frame sequences that have alpha in a format that doesn’t support an alpha channel.\n\nFor example, to reconstruct full frames from a normal YUV-encoded video and a separate video created with , you might use:\n\nAmplify differences between current pixel and pixels of adjacent frames in same pixel location.\n\nThis filter accepts the following options:\n\nThis filter supports the following commands that corresponds to option of same name:\n\nSame as the subtitles filter, except that it doesn’t require libavcodec and libavformat to work. On the other hand, it is limited to ASS (Advanced Substation Alpha) subtitles files.\n\nThis filter accepts the following option in addition to the common options from the subtitles filter:\n\nApply an Adaptive Temporal Averaging Denoiser to the video input.\n\nThe filter accepts the following options:\n\nThis filter supports same commands as options except option . The command accepts the same syntax of the corresponding option.\n\nThe filter accepts the following options:\n\nThis filter supports same commands as options. The command accepts the same syntax of the corresponding option.\n\nIf the specified expression is not valid, it is kept at its current value.\n\nThe filter accepts the following option:\n\nThis filter supports the all above options as commands.\n\nCompute the bounding box for the non-black pixels in the input frame luma plane.\n\nThis filter computes the bounding box containing all the pixels with a luma value greater than the minimum allowed value. The parameters describing the bounding box are printed on the filter log.\n\nThe filter accepts the following option:\n\nThis filter supports the all above options as commands.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThe filter accepts the following options:\n\nDetect video intervals that are (almost) completely black. Can be useful to detect chapter transitions, commercials, or invalid recordings.\n\nThe filter outputs its detection analysis to both the log as well as frame metadata. If a black segment of at least the specified minimum duration is found, a line with the start and end timestamps as well as duration is printed to the log with level . In addition, a log line with level is printed per frame showing the black amount detected for that frame.\n\nThe filter also attaches metadata to the first frame of a black segment with key and to the first frame after the black segment ends with key . The value is the frame’s timestamp. This metadata is added regardless of the minimum duration specified.\n\nThe filter accepts the following options:\n\nThe following example sets the maximum pixel threshold to the minimum value, and detects only black intervals of 2 or more seconds:\n\nDetect frames that are (almost) completely black. Can be useful to detect chapter transitions or commercials. Output lines consist of the frame number of the detected frame, the percentage of blackness, the position in the file if known or -1 and the timestamp in seconds.\n\nIn order to display the output lines, you need to set the loglevel at least to the AV_LOG_INFO value.\n\nThis filter exports frame metadata . The value represents the percentage of pixels in the picture that are below the threshold value.\n\nIt accepts the following parameters:\n\nBlend two video frames into each other.\n\nThe filter takes two input streams and outputs one stream, the first input is the \"top\" layer and second input is \"bottom\" layer. By default, the output terminates when the longest input terminates.\n\nThe (time blend) filter takes two consecutive frames from one single stream, and outputs the result obtained by blending the new frame on top of the old frame.\n\nA description of the accepted options follows.\n\nThe filter also supports the framesync options.\n• Apply transition from bottom layer to top layer in first 10 seconds:\n• Split diagonally video and shows top and bottom layer on each side:\n• Display differences between the current and the previous frame:\n\nThis filter supports same commands as options.\n\nDetermines blockiness of frames without altering the input frames.\n\nBased on Remco Muijs and Ihor Kirenko: \"A no-reference blocking artifact measure for adaptive video processing.\" 2005 13th European signal processing conference.\n\nThe filter accepts the following options:\n• Determine blockiness for the first plane and search for periods within [8,32]:\n\nDetermines blurriness of frames without altering the input frames.\n\nBased on Marziliano, Pina, et al. \"A no-reference perceptual blur metric.\" Allows for a block-based abbreviation.\n\nThe filter accepts the following options:\n• Determine blur for 80% of most significant 32x32 blocks:\n\nThe filter accepts the following options.\n• Same as above, but filtering only luma:\n• Same as above, but with both estimation modes:\n• Same as above, but prefilter with nlmeans filter instead:\n\nIt accepts the following parameters:\n\nA description of the accepted options follows.\n• Apply a boxblur filter with the luma, chroma, and alpha radii set to 2:\n• Set the luma radius to 2, and alpha and chroma radius to 0:\n• Set the luma and chroma radii to a fraction of the video dimension:\n\nMotion adaptive deinterlacing based on yadif with the use of w3fdif and cubic interpolation algorithms. It accepts the following parameters:\n\nThis filter fixes various issues seen with commerical encoders related to upstream malformed CEA-708 payloads, specifically incorrect number of tuples (wrong cc_count for the target FPS), and incorrect ordering of tuples (i.e. the CEA-608 tuples are not at the first entries in the payload).\n\nThe filter accepts the following options:\n\nThis filter supports same commands as options.\n\nRemove all color information for all colors except for certain one.\n\nThe filter accepts the following options:\n\nThis filter supports same commands as options. The command accepts the same syntax of the corresponding option.\n\nIf the specified expression is not valid, it is kept at its current value.\n\nThe filter accepts the following options:\n\nThis filter supports same commands as options. The command accepts the same syntax of the corresponding option.\n\nIf the specified expression is not valid, it is kept at its current value.\n• Make every green pixel in the input image transparent:\n\nThe filter accepts the following options:\n\nThis filter supports same commands as options. The command accepts the same syntax of the corresponding option.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nDisplay CIE color diagram with pixels overlaid onto it.\n\nThe filter accepts the following options:\n\nSome codecs can export information through frames using side-data or other means. For example, some MPEG based codecs export motion vectors through the flag in the codec option.\n\nThe filter accepts the following option:\n• Visualize forward predicted MVs of all frames using :\n• Visualize multi-directionals MVs of P and B-Frames using :\n\nModify intensity of primary colors (red, green and blue) of input frames.\n\nThe filter allows an input frame to be adjusted in the shadows, midtones or highlights regions for the red-cyan, green-magenta or blue-yellow balance.\n\nA positive adjustment value shifts the balance towards the primary color, a negative value towards the complementary color.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nAdjust color white balance selectively for blacks and whites. This filter operates in YUV colorspace.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThis filter modifies a color channel by adding the values associated to the other channels of the same pixels. For example if the value to modify is red, the output value will be:\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nRGB colorspace color keying. This filter operates on 8-bit RGB format frames by setting the alpha component of each pixel which falls within the similarity radius of the key color to 0. The alpha value for pixels outside the similarity radius depends on the value of the blend option.\n\nThe filter accepts the following options:\n• Make every green pixel in the input image transparent:\n\nThis filter supports same commands as options. The command accepts the same syntax of the corresponding option.\n\nIf the specified expression is not valid, it is kept at its current value.\n\nRemove all color information for all RGB colors except for certain one.\n\nThe filter accepts the following options:\n\nThis filter supports same commands as options. The command accepts the same syntax of the corresponding option.\n\nIf the specified expression is not valid, it is kept at its current value.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThis filter needs three input video streams. First stream is video stream that is going to be filtered out. Second and third video stream specify color patches for source color to target color mapping.\n\nThe filter accepts the following options:\n\nThe filter accepts the following options:\n\nFor example to convert from BT.601 to SMPTE-240M, use the command:\n\nConvert colorspace, transfer characteristics or color primaries. Input video needs to have an even size.\n\nThe filter accepts the following options:\n\nThe filter converts the transfer characteristics, color space and color primaries to the specified user values. The output value, if not specified, is set to a default value based on the \"all\" property. If that property is also not specified, the filter will log an error. The output color range and format default to the same value as the input color range and format. The input transfer characteristics, color space, color primaries and color range should be set on the input data. If any of these are missing, the filter will log an error and no conversion will take place.\n\nFor example to convert the input to SMPTE-240M, use the command:\n\nAdjust color temperature in video to simulate variations in ambient color temperature.\n\nThe filter accepts the following options:\n\nThis filter supports same commands as options.\n\nApply convolution of 3x3, 5x5, 7x7 or horizontal/vertical up to 49 elements.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nApply 2D convolution of video stream in frequency domain using second stream as impulse.\n\nThe filter accepts the following options:\n\nThe filter also supports the framesync options.\n\nCopy the input video source unchanged to the output. This is mainly useful for testing purposes.\n\nVideo filtering on GPU using Apple’s CoreImage API on OSX.\n\nHardware acceleration is based on an OpenGL context. Usually, this means it is processed by video hardware. However, software-based OpenGL implementations exist which means there is no guarantee for hardware processing. It depends on the respective OSX.\n\nThere are many filters and image generators provided by Apple that come with a large variety of options. The filter has to be referenced by its name along with its options.\n\nThe coreimage filter accepts the following options:\n\nSeveral filters can be chained for successive processing without GPU-HOST transfers allowing for fast processing of complex filter chains. Currently, only filters with zero (generators) or exactly one (filters) input image and one output image are supported. Also, transition filters are not yet usable as intended.\n\nSome filters generate output images with additional padding depending on the respective filter kernel. The padding is automatically removed to ensure the filter output has the same size as the input image.\n\nFor image generators, the size of the output image is determined by the previous output image of the filter chain or the input image of the whole filterchain, respectively. The generators do not use the pixel information of this image to generate their output. However, the generated output is blended onto this image, resulting in partial or complete coverage of the output image.\n\nThe coreimagesrc video source can be used for generating input images which are directly fed into the filter chain. By using it, providing input images by another video source or an input video is not required.\n• Use the CIBoxBlur filter with default options to blur an image:\n• Use a filter chain with CISepiaTone at default values and CIVignetteEffect with its center at 100x100 and a radius of 50 pixels:\n• Use nullsrc and CIQRCodeGenerator to create a QR code for the FFmpeg homepage, given as complete and escaped command-line for Apple’s standard bash shell:\n\nObtain the correlation between two input videos.\n\nBoth input videos must have the same resolution and pixel format for this filter to work correctly. Also it assumes that both inputs have the same number of frames, which are compared one by one.\n\nThe obtained per component, average, min and max correlation is printed through the logging system.\n\nThe filter stores the calculated correlation of each frame in frame metadata.\n\nThis filter also supports the framesync options.\n\nIn the below example the input file being processed is compared with the reference file .\n\nIt accepts the following options:\n• Cover a rectangular object by the supplied image of a given video using :\n\nCrop the input video to given dimensions.\n\nIt accepts the following parameters:\n\nThe , , , parameters are expressions containing the following constants:\n\nThe expression for may depend on the value of , and the expression for may depend on , but they cannot depend on and , as and are evaluated after and .\n\nThe and parameters specify the expressions for the position of the top-left corner of the output (non-cropped) area. They are evaluated for each frame. If the evaluated value is not valid, it is approximated to the nearest valid value.\n\nThe expression for may depend on , and the expression for may depend on .\n• Crop area with size 100x100 at position (12,34). Using named options, the example above becomes:\n• Crop the central input area with size 2/3 of the input video:\n• Delimit the rectangle with the top-left corner placed at position 100:100 and the right-bottom corner corresponding to the right-bottom corner of the input image.\n• Crop 10 pixels from the left and right borders, and 20 pixels from the top and bottom borders\n• Keep only the bottom right quarter of the input image:\n• Set x depending on the value of y:\n\nThis filter supports the following commands:\n\nIt calculates the necessary cropping parameters and prints the recommended parameters via the logging system. The detected dimensions correspond to the non-black or video area of the input video according to .\n\nIt accepts the following parameters:\n• Find an embedded video area, use motion vectors from decoder:\n\nThis filter supports the following commands:\n\nDelay video filtering until a given wallclock timestamp. The filter first passes on amount of frames, then it buffers at most amount of frames and waits for the cue. After reaching the cue it forwards the buffered frames and also any subsequent frames coming in its input.\n\nThe filter can be used synchronize the output of multiple ffmpeg processes for realtime output devices like decklink. By putting the delay in the filtering chain and pre-buffering frames the process can pass on data to output almost immediately after the target wallclock timestamp is reached.\n\nPerfect frame accuracy cannot be guaranteed, but the result is good enough for some use cases.\n\nThis filter is similar to the Adobe Photoshop and GIMP curves tools. Each component (red, green and blue) has its values defined by key points tied from each other using a smooth curve. The x-axis represents the pixel values from the input frame, and the y-axis the new pixel values to be set for the output frame.\n\nBy default, a component curve is defined by the two points and . This creates a straight line where each original pixel value is \"adjusted\" to its own value, which means no change to the image.\n\nThe filter allows you to redefine these two points and add some more. A new curve will be defined to pass smoothly through all these new coordinates. The new defined points need to be strictly increasing over the x-axis, and their and values must be in the interval. The curve is formed by using a natural or monotonic cubic spline interpolation, depending on the option (default: ). The spline produces a smoother curve in general while the monotonic ( ) spline guarantees the transitions between the specified points to be monotonic. If the computed curves happened to go outside the vector spaces, the values will be clipped accordingly.\n\nThe filter accepts the following options:\n\nTo avoid some filtergraph syntax conflicts, each key points list need to be defined using the following syntax: .\n\nThis filter supports same commands as options.\n• Vintage effect: Here we obtain the following coordinates for each components:\n• The previous example can also be achieved with the associated built-in preset:\n• Use a Photoshop preset and redefine the points of the green component:\n• Check out the curves of the profile using and :\n\nThis filter shows hexadecimal pixel values of part of video.\n\nThe filter accepts the following options:\n\nThis filter supports same commands as options excluding option.\n\nThe filter accepts the following options:\n\nThis filter supports same commands as options. The command accepts the same syntax of the corresponding option.\n\nIf the specified expression is not valid, it is kept at its current value.\n\nThis filter is not designed for real time.\n\nThe filter accepts the following options:\n\nThe same operation can be achieved using the expression system:\n\nRemove banding artifacts from input video. It works by replacing banded pixels with average value of referenced pixels.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThe filter accepts the following options:\n• Deblock using weak filter and block size of 4 pixels.\n• Deblock using strong filter, block size of 4 pixels and custom thresholds for deblocking more edges.\n• Similar as above, but filter only first plane.\n• Similar as above, but filter only second and third plane.\n\nThis filter supports the all above options as commands.\n\nThe filter accepts the following options:\n\nApply 2D deconvolution of video stream in frequency domain using second stream as impulse.\n\nThe filter accepts the following options:\n\nThe filter also supports the framesync options.\n\nIt accepts the following options:\n\nThis filter replaces the pixel by the local(3x3) average by taking into account only values lower than the pixel.\n\nIt accepts the following options:\n\nThis filter supports the all above options as commands.\n\nIt accepts the following options:\n\nJudder can be introduced, for instance, by pullup filter. If the original source was partially telecined content then the output of will have a variable frame rate. May change the recorded frame rate of the container. Aside from that change, this filter will not affect constant frame rate video.\n\nThe option available in this filter is:\n\nSuppress a TV station logo by a simple interpolation of the surrounding pixels. Just set a rectangle covering the logo and watch it disappear (and sometimes something even uglier appear - your mileage may vary).\n\nIt accepts the following parameters:\n• Set a rectangle covering the area with top left corner coordinates 0,0 and size 100x77:\n\nRemove the rain in the input image/video by applying the derain methods based on convolutional neural networks. Supported models:\n\nTraining as well as model generation scripts are provided in the repository at https://github.com/XueweiMeng/derain_filter.git.\n\nThe filter accepts the following options:\n\nTo get full functionality (such as async execution), please use the dnn_processing filter.\n\nAttempt to fix small changes in horizontal and/or vertical shift. This filter helps remove camera shake from hand-holding a camera, bumping a tripod, moving on a vehicle, etc.\n\nThe filter accepts the following options:\n\nRemove unwanted contamination of foreground colors, caused by reflected color of greenscreen or bluescreen.\n\nThis filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nApply an exact inverse of the telecine operation. It requires a predefined pattern specified using the pattern option which must be the same as that passed to the telecine filter.\n\nThis filter accepts the following options:\n\nThis filter replaces the pixel by the local(3x3) maximum.\n\nIt accepts the following options:\n\nThis filter supports the all above options as commands.\n\nDisplace pixels as indicated by second and third input stream.\n\nIt takes three input streams and outputs one stream, the first input is the source, and second and third input are displacement maps.\n\nThe second input specifies how much to displace pixels along the x-axis, while the third input specifies how much to displace pixels along the y-axis. If one of displacement map streams terminates, last frame from that displacement map will be used.\n\nNote that once generated, displacements maps can be reused over and over again.\n\nA description of the accepted options follows.\n\nDo classification with deep neural networks based on bounding boxes.\n\nThe filter accepts the following options:\n\nThe filter accepts the following options:\n\nDo image processing with deep neural networks. It works together with another filter which converts the pixel format of the Frame to what the dnn network requires.\n\nThe filter accepts the following options:\n• Remove rain in rgb24 frame with can.pb (see derain filter):\n• Handle the Y channel with srcnn.pb (see sr filter) for frame with yuv420p (planar YUV formats supported):\n• Handle the Y channel with espcn.pb (see sr filter), which changes frame size, for format yuv420p (planar YUV formats supported), please use tools/python/tf_sess_config.py to get the configs of TensorFlow backend for your system.\n\nIt accepts the following parameters:\n\nThe parameters for , , and and are expressions containing the following constants:\n• Draw a black box around the edge of the input image:\n• Draw a box with color red and an opacity of 50%: The previous example can be specified as:\n\nThis filter supports same commands as options. The command accepts the same syntax of the corresponding option.\n\nIf the specified expression is not valid, it is kept at its current value.\n\nIt accepts the following parameters:\n\nExample using metadata from signalstats filter:\n\nExample using metadata from ebur128 filter:\n\nIt accepts the following parameters:\n\nThe parameters for , , and and are expressions containing the following constants:\n• Draw a grid with cell 100x100 pixels, thickness 2 pixels, with color red and an opacity of 50%:\n• Draw a white 3x3 grid with an opacity of 50%:\n\nThis filter supports same commands as options. The command accepts the same syntax of the corresponding option.\n\nIf the specified expression is not valid, it is kept at its current value.\n\nDraw a text string or text from a specified file on top of a video, using the libfreetype library.\n\nTo enable compilation of this filter, you need to configure FFmpeg with and . To enable default font fallback and the option you need to configure FFmpeg with . To enable the option, you need to configure FFmpeg with .\n\nIt accepts the following parameters:\n\nThe parameters for and are expressions containing the following constants and functions:\n\nIf is set to , the filter recognizes sequences accepted by the C function in the provided text and expands them accordingly. Check the documentation of . This feature is deprecated in favor of expansion with the or expansion functions.\n\nIf is set to , the text is printed verbatim.\n\nIf is set to (which is the default), the following expansion mechanism is used.\n\nThe backslash character ‘ ’, followed by any character, always expands to the second character.\n\nSequences of the form are expanded. The text between the braces is a function name, possibly followed by arguments separated by ’:’. If the arguments contain special characters or delimiters (’:’ or ’}’), they should be escaped.\n\nNote that they probably must also be escaped as the value for the option in the filter argument string and as the filter argument in the filtergraph description, and possibly also for the shell, that makes up to four levels of escaping; using a text file with the option avoids these problems.\n\nThe following functions are available:\n\nThe following options are also supported as commands:\n• Draw \"Test Text\" with font FreeSerif, using the default values for the optional parameters.\n• Draw ’Test Text’ with font FreeSerif of size 24 at position x=100 and y=50 (counting from the top-left corner of the screen), text is yellow with a red box around it. Both the text and the box have an opacity of 20%. Note that the double quotes are not necessary if spaces are not used within the parameter list.\n• Show the text at the center of the video frame:\n• Show the text at a random position, switching to a new position every 30 seconds:\n• Show a text line sliding from right to left in the last row of the video frame. The file is assumed to contain a single line with no newlines.\n• Show the content of file off the bottom of the frame and scroll up.\n• Draw a single green letter \"g\", at the center of the input video. The glyph baseline is placed at half screen height.\n• Show text for 1 second every 3 seconds:\n• Use fontconfig to set the font. Note that the colons need to be escaped.\n• Draw \"Test Text\" with font size dependent on height of the video.\n• Print the date of a real-time encoding (see documentation for the C function):\n• Show text fading in and out (appearing/disappearing):\n• Horizontally align multiple separate texts. Note that and the value are included in the offset.\n• Plot special metadata onto each frame if such metadata exists. Otherwise, plot the string \"NA\". Note that image2 demuxer must have option for the special metadata fields to be available for filters.\n\nFor more information about libfreetype, check: http://www.freetype.org/.\n\nFor more information about fontconfig, check: http://freedesktop.org/software/fontconfig/fontconfig-user.html.\n\nFor more information about libfribidi, check: http://fribidi.org/.\n\nFor more information about libharfbuzz, check: https://github.com/harfbuzz/harfbuzz.\n\nDetect and draw edges. The filter uses the Canny Edge Detection algorithm.\n\nThe filter accepts the following options:\n• Standard edge detection with custom values for the hysteresis thresholding:\n\nFor each input image, the filter will compute the optimal mapping from the input to the output given the codebook length, that is the number of distinct output colors.\n\nThis filter accepts the following options.\n\nMeasure graylevel entropy in histogram of color channels of video frames.\n\nIt accepts the following parameters:\n\nApply the EPX magnification filter which is designed for pixel art.\n\nIt accepts the following option:\n\nThe filter accepts the following options:\n\nThe expressions accept the following parameters:\n\nThe filter supports the following commands:\n\nThis filter replaces the pixel by the local(3x3) minimum.\n\nIt accepts the following options:\n\nThis filter supports the all above options as commands.\n\nSpatial only filter that uses edge slope tracing algorithm to interpolate missing lines. It accepts the following parameters:\n\nThis filter supports same commands as options.\n\nThe filter accepts the following options:\n\nThis filter supports same commands as options.\n\nThe filter accepts the following option:\n• Extract luma, u and v color channel component from input video frame into 3 grayscale outputs:\n\nIt accepts the following parameters:\n• Fade in the first 30 frames of video: The command above is equivalent to:\n• Fade out the last 45 frames of a 200-frame video:\n• Fade in the first 25 frames and fade out the last 25 frames of a 1000-frame video:\n• Make the first 5 frames yellow, then fade in from frame 5-24:\n• Fade in alpha over first 25 frames of video:\n• Make the first 5.5 seconds black, then fade in for 0.5 seconds:\n\nThis filter pass cropped input frames to 2nd output. From there it can be filtered with other video filters. After filter receives frame from 2nd input, that frame is combined on top of original frame from 1st input and passed to 1st output.\n\nThe typical usage is filter only part of frame.\n\nThe filter accepts the following options:\n• Blur only top left rectangular part of video frame size 100x100 with gblur filter.\n• Draw black box on top left part of video frame of size 100x100 with drawbox filter.\n• Pixelize rectangular part of video frame of size 100x100 with pixelize filter.\n\nThe filter accepts the following options:\n\nExtract a single field from an interlaced image using stride arithmetic to avoid wasting CPU time. The output frames are marked as non-interlaced.\n\nThe filter accepts the following options:\n\nCreate new frames by copying the top and bottom fields from surrounding frames supplied as numbers by the hint file.\n\nExample of first several lines of file for mode:\n\nField matching filter for inverse telecine. It is meant to reconstruct the progressive frames from a telecined stream. The filter does not drop duplicated frames, so to achieve a complete inverse telecine needs to be followed by a decimation filter such as decimate in the filtergraph.\n\nThe separation of the field matching and the decimation is notably motivated by the possibility of inserting a de-interlacing filter fallback between the two. If the source has mixed telecined and real interlaced content, will not be able to match fields for the interlaced parts. But these remaining combed frames will be marked as interlaced, and thus can be de-interlaced by a later filter such as yadif before decimation.\n\nIn addition to the various configuration options, can take an optional second stream, activated through the option. If enabled, the frames reconstruction will be based on the fields and frames from this second stream. This allows the first input to be pre-processed in order to help the various algorithms of the filter, while keeping the output lossless (assuming the fields are matched properly). Typically, a field-aware denoiser, or brightness/contrast adjustments can help.\n\nNote that this filter uses the same algorithms as TIVTC/TFM (AviSynth project) and VIVTC/VFM (VapourSynth project). The later is a light clone of TFM from which is based on. While the semantic and usage are very close, some behaviour and options names can differ.\n\nThe decimate filter currently only works for constant frame rate input. If your input has mixed telecined (30fps) and progressive content with a lower framerate like 24fps use the following filterchain to produce the necessary cfr stream: .\n\nThe filter accepts the following options:\n\nWe assume the following telecined stream:\n\nThe numbers correspond to the progressive frame the fields relate to. Here, the first two frames are progressive, the 3rd and 4th are combed, and so on.\n\nWhen is configured to run a matching from bottom ( = ) this is how this input stream get transformed:\n\nAs a result of the field matching, we can see that some frames get duplicated. To perform a complete inverse telecine, you need to rely on a decimation filter after this operation. See for instance the decimate filter.\n\nThe same operation now matching from top fields ( = ) looks like this:\n\nIn these examples, we can see what , and mean; basically, they refer to the frame and field of the opposite parity:\n• matches the field of the opposite parity in the previous frame\n• matches the field of the opposite parity in the current frame\n• matches the field of the opposite parity in the next frame\n\nThe and matching are a bit special in the sense that they match from the opposite parity flag. In the following examples, we assume that we are currently matching the 2nd frame (Top:2, bottom:2). According to the match, a ’x’ is placed above and below each matched fields.\n\nAdvanced IVTC, with fallback on yadif for still combed frames:\n\nTransform the field order of the input video.\n\nIt accepts the following parameters:\n\nThe default value is ‘ ’.\n\nThe transformation is done by shifting the picture content up or down by one line, and filling the remaining line with appropriate picture content. This method is consistent with most broadcast field order converters.\n\nIf the input video is not flagged as being interlaced, or it is already flagged as being of the required output field order, then this filter does not alter the incoming video.\n\nIt is very useful when converting to or from PAL DV material, which is bottom field first.\n\nFill borders of the input video, without changing video stream dimensions. Sometimes video can have garbage at the four edges and you may not want to crop video input to keep size multiple of some number.\n\nThis filter accepts the following options:\n\nThis filter supports same commands as options. The command accepts the same syntax of the corresponding option.\n\nIf the specified expression is not valid, it is kept at its current value.\n\nThe object to search for must be specified as a gray8 image specified with the option.\n\nFor each possible match, a score is computed. If the score reaches the specified threshold, the object is considered found.\n\nIf the input video contains multiple instances of the object, the filter will find only one of them.\n\nWhen an object is found, the following metadata entries are set in the matching frame:\n\nIt accepts the following options:\n• Cover a rectangular object by the supplied image of a given video using :\n• Find the position of an object in each frame using and write it to a log file:\n\nFlood area with values of same pixel components with another values.\n\nIt accepts the following options:\n\nConvert the input video to one of the specified pixel formats. Libavfilter will try to pick one that is suitable as input to the next filter.\n\nIt accepts the following parameters:\n• Convert the input video to the format Convert the input video to any of the formats in the list\n\nConvert the video to specified constant frame rate by duplicating or dropping frames as necessary.\n\nIt accepts the following parameters:\n\nAlternatively, the options can be specified as a flat string: [: [: ]].\n\nSee also the setpts filter.\n• A typical usage in order to set the fps to 25:\n• Sets the fps to 24, using abbreviation and rounding method to round to nearest:\n\nPack two different video streams into a stereoscopic video, setting proper metadata on supported codecs. The two views should have the same size and framerate and processing will stop when the shorter video ends. Please note that you may conveniently adjust view properties with the scale and fps filters.\n\nIt accepts the following parameters:\n\nChange the frame rate by interpolating new video output frames from the source frames.\n\nThis filter is not designed to function correctly with interlaced media. If you wish to change the frame rate of interlaced media then you are required to deinterlace before this filter and re-interlace after this filter.\n\nA description of the accepted options follows.\n\nThis filter accepts the following option:\n\nThis filter logs a message and sets frame metadata when it detects that the input video has no significant change in content during a specified duration. Video freeze detection calculates the mean average absolute difference of all the components of video frames and compares it to a noise floor.\n\nThe printed times and duration are expressed in seconds. The metadata key is set on the first frame whose timestamp equals or exceeds the detection duration and it contains the timestamp of the first frame of the freeze. The and metadata keys are set on the first frame after the freeze.\n\nThe filter accepts the following options:\n\nThis filter freezes video frames using frame from 2nd input.\n\nThe filter accepts the following options:\n\nTo enable the compilation of this filter, you need to install the frei0r header and configure FFmpeg with .\n\nIt accepts the following parameters:\n\nA frei0r effect parameter can be a boolean (its value is either \"y\" or \"n\"), a double, a color (specified as / / , where , , and are floating point numbers between 0.0 and 1.0, inclusive) or a color description as specified in the (ffmpeg-utils)\"Color\" section in the ffmpeg-utils manual, a position (specified as / , where and are floating point numbers) and/or a string.\n\nThe number and types of parameters depend on the loaded effect. If an effect parameter is not specified, the default value is set.\n• Apply the distort0r effect, setting the first two double parameters:\n• Apply the colordistance effect, taking a color as the first parameter:\n• Apply the perspective effect, specifying the top left and top right image positions:\n\nFor more information, see http://frei0r.dyne.org\n\nThis filter supports the option as commands.\n\nApply fast and simple postprocessing. It is a faster version of spp.\n\nIt splits (I)DCT into horizontal/vertical passes. Unlike the simple post- processing filter, one of them is performed once per block, not per pixel. This allows for much higher speed.\n\nThe filter accepts the following options:\n\nSynchronize video frames with an external mapping from a file.\n\nFor each input PTS given in the map file it either drops or creates as many frames as necessary to recreate the sequence of output frames given in the map file.\n\nThis filter is useful to recreate the output frames of a framerate conversion by the fps filter, recorded into a map file using the ffmpeg option , and do further processing to the corresponding frames e.g. quality comparison.\n\nEach line of the map file must contain three items per input frame, the input PTS (decimal), the output PTS (decimal) and the output TIMEBASE (decimal/decimal), seperated by a space. This file format corresponds to the output of .\n\nThe filter assumes the map file is sorted by increasing input PTS.\n\nThe filter accepts the following options:\n\nThe filter accepts the following options:\n\nThis filter supports same commands as options. The command accepts the same syntax of the corresponding option.\n\nIf the specified expression is not valid, it is kept at its current value.\n\nThe filter accepts the following options:\n\nThe colorspace is selected according to the specified options. If one of the , , or options is specified, the filter will automatically select a YCbCr colorspace. If one of the , , or options is specified, it will select an RGB colorspace.\n\nIf one of the chrominance expression is not defined, it falls back on the other one. If no alpha expression is specified it will evaluate to opaque value. If none of chrominance expressions are specified, they will evaluate to the luma expression.\n\nThe expressions can use the following variables and functions:\n\nFor functions, if and are outside the area, the value will be automatically clipped to the closer edge.\n\nPlease note that this filter can use multiple threads in which case each slice will have its own expression state. If you want to use only a single expression state because your expressions depend on previous state then you should limit the number of filter threads to 1.\n• Generate a bidimensional sine wave, with angle and a wavelength of 100 pixels:\n• Create a radial gradient that is the same size as the input (also see the vignette filter):\n\nFix the banding artifacts that are sometimes introduced into nearly flat regions by truncation to 8-bit color depth. Interpolate the gradients that should go where the bands are, and dither them.\n\nIt is designed for playback only. Do not use it prior to lossy compression, because compression tends to lose the dither and bring back the bands.\n\nIt accepts the following parameters:\n\nAlternatively, the options can be specified as a flat string: [: ]\n• Apply the filter with a strength and radius of :\n• Specify radius, omitting the strength (which will fall-back to the default value):\n\nWith this filter one can debug complete filtergraph. Especially issues with links filling with queued frames.\n\nThe filter accepts the following options:\n\nA color constancy filter that applies color correction based on the grayworld assumption\n\nThe algorithm uses linear light, so input data should be linearized beforehand (and possibly correctly tagged).\n\nA color constancy variation filter which estimates scene illumination via grey edge algorithm and corrects the scene colors accordingly.\n\nThe filter accepts the following options:\n\nApply guided filter for edge-preserving smoothing, dehazing and so on.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n• Dehazing, structure-transferring filtering, detail enhancement with guided filter. For the generation of guidance image, refer to paper \"Guided Image Filtering\". See: http://kaiminghe.com/publications/pami12guidedfilter.pdf.\n\nFirst input is the video stream to process, and second one is the Hald CLUT. The Hald CLUT input can be a simple picture or a complete video stream.\n\nThe filter accepts the following options:\n\nalso has the same interpolation options as lut3d (both filters share the same internals).\n\nThis filter also supports the framesync options.\n\nMore information about the Hald CLUT can be found on Eskil Steenberg’s website (Hald CLUT author) at http://www.quelsolaar.com/technology/clut.html.\n\nThis filter supports the option as commands.\n\nGenerate an identity Hald CLUT stream altered with various effects:\n\nNote: make sure you use a lossless codec.\n\nThen use it with to apply it on some random stream:\n\nThe Hald CLUT will be applied to the 10 first seconds (duration of ), then the latest picture of that CLUT stream will be applied to the remaining frames of the stream.\n\nA Hald CLUT is supposed to be a squared image of by pixels. For a given Hald CLUT, FFmpeg will select the biggest possible square starting at the top left of the picture. The remaining padding pixels (bottom or right) will be ignored. This area can be used to add a preview of the Hald CLUT.\n\nTypically, the following generated Hald CLUT will be supported by the filter:\n\nIt contains the original and a preview of the effect of the CLUT: SMPTE color bars are displayed on the right-top, and below the same color bars processed by the color changes.\n\nThen, the effect of this Hald CLUT can be visualized with:\n\nFor example, to horizontally flip the input video with :\n\nIt can be used to correct video that has a compressed range of pixel intensities. The filter redistributes the pixel intensities to equalize their distribution across the intensity range. It may be viewed as an \"automatically adjusting contrast filter\". This filter is useful only for correcting degraded or poorly captured source video.\n\nThe filter accepts the following options:\n\nCompute and draw a color distribution histogram for the input video.\n\nThe computed histogram is a representation of the color component distribution in an image.\n\nStandard histogram displays the color components distribution in an image. Displays color graph for each color component. Shows distribution of the Y, U, V, A or R, G, B components, depending on input format, in the current frame. Below each graph a color component scale meter is shown.\n\nThe filter accepts the following options:\n\nThis is a high precision/quality 3d denoise filter. It aims to reduce image noise, producing smooth images and making still images really still. It should enhance compressibility.\n\nIt accepts the following optional parameters:\n\nThis filter supports same commands as options. The command accepts the same syntax of the corresponding option.\n\nIf the specified expression is not valid, it is kept at its current value.\n\nThe input must be in hardware frames, and the output a non-hardware format. Not all formats will be supported on the output - it may be necessary to insert an additional filter immediately following in the graph to get the output in a supported format.\n\nMap hardware frames to system memory or to another device.\n\nThis filter has several different modes of operation; which one is used depends on the input and output formats:\n• Hardware frame input, normal frame output Map the input frames to system memory and pass them to the output. If the original hardware frame is later required (for example, after overlaying something else on part of it), the filter can be used again in the next mode to retrieve it.\n• Normal frame input, hardware frame output If the input is actually a software-mapped hardware frame, then unmap it - that is, return the original hardware frame. Otherwise, a device must be provided. Create new hardware surfaces on that device for the output, then map them back to the software format at the input and give those frames to the preceding filter. This will then act like the filter, but may be able to avoid an additional copy when the input is already in a compatible format.\n• Hardware frame input and output A device must be supplied for the output, either directly or with the option. The input and output devices must be of different types and compatible - the exact meaning of this is system-dependent, but typically it means that they must refer to the same underlying hardware context (for example, refer to the same graphics card). If the input frames were originally created on the output device, then unmap to retrieve the original frames. Otherwise, map the frames to the output device - create new hardware frames on the output corresponding to the frames on the input.\n\nThe following additional parameters are accepted:\n\nThe device to upload to must be supplied when the filter is initialised. If using ffmpeg, select the appropriate device with the option or with the option. The input and output devices must be of different types and compatible - the exact meaning of this is system-dependent, but typically it means that they must refer to the same underlying hardware context (for example, refer to the same graphics card).\n\nThe following additional parameters are accepted:\n\nIt accepts the following optional parameters:\n\nApply a high-quality magnification filter designed for pixel art. This filter was originally created by Maxim Stepin.\n\nIt accepts the following option:\n\nAll streams must be of same pixel format and of same height.\n\nNote that this filter is faster than using overlay and pad filter to create same output.\n\nThe filter accepts the following option:\n\nThis filter measures color difference between set HSV color in options and ones measured in video stream. Depending on options, output colors can be changed to be gray or not.\n\nThe filter accepts the following options:\n\nThis filter measures color difference between set HSV color in options and ones measured in video stream. Depending on options, output colors can be changed to transparent by adding alpha channel.\n\nThe filter accepts the following options:\n\nModify the hue and/or the saturation of the input.\n\nIt accepts the following parameters:\n\nand are mutually exclusive, and can’t be specified at the same time.\n\nThe , , and option values are expressions containing the following constants:\n• Set the hue to 90 degrees and the saturation to 1.0:\n• Same command but expressing the hue in radians:\n• Rotate hue and make the saturation swing between 0 and 2 over a period of 1 second:\n• Apply a 3 seconds saturation fade-in effect starting at 0: The general fade-in expression can be written as:\n• Apply a 3 seconds saturation fade-out effect starting at 5 seconds: The general fade-out expression can be written as:\n\nThis filter supports the following commands:\n\nThis filter accepts the following options:\n\nGrow first stream into second stream by connecting components. This makes it possible to build more robust edge masks.\n\nThis filter accepts the following options:\n\nThe filter also supports the framesync options.\n\nDetect the colorspace from an embedded ICC profile (if present), and update the frame’s tags accordingly.\n\nThis filter accepts the following options:\n\nGenerate ICC profiles and attach them to frames.\n\nThis filter accepts the following options:\n\nObtain the identity score between two input videos.\n\nBoth input videos must have the same resolution and pixel format for this filter to work correctly. Also it assumes that both inputs have the same number of frames, which are compared one by one.\n\nThe obtained per component, average, min and max identity score is printed through the logging system.\n\nThe filter stores the calculated identity scores of each frame in frame metadata.\n\nThis filter also supports the framesync options.\n\nIn the below example the input file being processed is compared with the reference file .\n\nThis filter tries to detect if the input frames are interlaced, progressive, top or bottom field first. It will also try to detect fields that are repeated between adjacent frames (a sign of telecine).\n\nSingle frame detection considers only immediately adjacent frames when classifying each frame. Multiple frame detection incorporates the classification history of previous frames.\n\nThe filter will log these metadata values:\n\nThe filter accepts the following options:\n\nInspect the field order of the first 360 frames in a video, in verbose detail:\n\nThe idet filter will add analysis metadata to each frame, which will then be discarded. At the end, the filter will also print a final report with statistics.\n\nThis filter allows one to process interlaced images fields without deinterlacing them. Deinterleaving splits the input frame into 2 fields (so called half pictures). Odd lines are moved to the top half of the output image, even lines to the bottom half. You can process (filter) them independently and then re-interleave them.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThis filter replaces the pixel by the local(3x3) average by taking into account only values higher than the pixel.\n\nIt accepts the following options:\n\nThis filter supports the all above options as commands.\n\nSimple interlacing filter from progressive contents. This interleaves upper (or lower) lines from odd frames with lower (or upper) lines from even frames, halving the frame rate and preserving image height.\n\nIt accepts the following optional parameters:\n\nDeinterlace input video by applying Donald Graft’s adaptive kernel deinterling. Work on interlaced parts of a video to produce progressive frames.\n\nThe description of the accepted parameters follows.\n\nThe filter accepts the following option:\n\nThis filter supports the all above options as commands.\n\nThis filter makes short flashes of light appear longer. This filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThis filter can be used to correct for radial distortion as can result from the use of wide angle lenses, and thereby re-rectify the image. To find the right parameters one can use tools available for example as part of opencv or simply trial-and-error. To use opencv use the calibration sample (under samples/cpp) from the opencv sources and extract the k1 and k2 coefficients from the resulting matrix.\n\nNote that effectively the same filter is available in the open-source tools Krita and Digikam from the KDE project.\n\nIn contrast to the vignette filter, which can also be used to compensate lens errors, this filter corrects the distortion of the image, whereas vignette corrects the brightness distribution, so you may want to use both filters together in certain cases, though you will have to take care of ordering, i.e. whether vignetting should be applied before or after lens correction.\n\nThe filter accepts the following options:\n\nThe formula that generates the correction is:\n\nwhere is halve of the image diagonal and and are the distances from the focal point in the source and target images, respectively.\n\nThis filter supports the all above options as commands.\n\nThe filter requires the camera make, camera model, and lens model to apply the lens correction. The filter will load the lensfun database and query it to find the corresponding camera and lens entries in the database. As long as these entries can be found with the given options, the filter can perform corrections on frames. Note that incomplete strings will result in the filter choosing the best match with the given options, and the filter will output the chosen camera and lens models (logged with level \"info\"). You must provide the make, camera model, and lens model as they are required.\n\nTo obtain a list of available makes and models, leave out one or both of and options. The filter will send the full list to the log with level . The first column is the make and the second column is the model. To obtain a list of available lenses, set any values for make and model and leave out the option. The filter will send the full list of lenses in the log with level . The ffmpeg tool will exit after the list is printed.\n\nThe filter accepts the following options:\n• Apply lens correction with make \"Canon\", camera model \"Canon EOS 100D\", and lens model \"Canon EF-S 18-55mm f/3.5-5.6 IS STM\" with focal length of \"18\" and aperture of \"8.0\".\n• Apply the same as before, but only for the first 5 seconds of video.\n\nThe options for this filter are divided into the following sections:\n\nThese options control the overall output mode. By default, libplacebo will try to preserve the source colorimetry and size as best as it can, but it will apply any embedded film grain, dolby vision metadata or anamorphic SAR present in source frames.\n\nIn addition to the expression constants documented for the scale filter, the , , , , , , and options can also contain the following constants:\n\nThe options in this section control how libplacebo performs upscaling and (if necessary) downscaling. Note that libplacebo will always internally operate on 4:4:4 content, so any sub-sampled chroma formats such as will necessarily be upsampled and downsampled as part of the rendering process. That means scaling might be in effect even if the source and destination resolution are the same.\n\nDeinterlacing is automatically supported when frames are tagged as interlaced, however frames are not deinterlaced unless a deinterlacing algorithm is chosen.\n\nLibplacebo comes with a built-in debanding filter that is good at counteracting many common sources of banding and blocking. Turning this on is highly recommended whenever quality is desired.\n\nA collection of subjective color controls. Not very rigorous, so the exact effect will vary somewhat depending on the input primaries and colorspace.\n\nTo help deal with sources that only have static HDR10 metadata (or no tagging whatsoever), libplacebo uses its own internal frame analysis compute shader to analyze source frames and adapt the tone mapping function in realtime. If this is too slow, or if exactly reproducible frame-perfect results are needed, it’s recommended to turn this feature off.\n\nThe options in this section control how libplacebo performs tone-mapping and gamut-mapping when dealing with mismatches between wide-gamut or HDR content. In general, libplacebo relies on accurate source tagging and mastering display gamut information to produce the best results.\n\nBy default, libplacebo will dither whenever necessary, which includes rendering to any integer format below 16-bit precision. It’s recommended to always leave this on, since not doing so may result in visible banding in the output, even if the filter is enabled. If maximum performance is needed, use instead of disabling dithering.\n\nlibplacebo supports a number of custom shaders based on the mpv .hook GLSL syntax. A collection of such shaders can be found here: https://github.com/mpv-player/mpv/wiki/User-Scripts#user-shaders\n\nA full description of the mpv shader format is beyond the scope of this section, but a summary can be found here: https://mpv.io/manual/master/#options-glsl-shader\n\nAll of the options in this section default off. They may be of assistance when attempting to squeeze the maximum performance at the cost of quality.\n\nThis filter supports almost all of the above options as commands.\n• Rescale input to fit into standard 1080p, with high quality scaling:\n• Run this filter on the CPU, on systems with Mesa installed (and with the most expensive options disabled):\n• Suppress CPU-based AV1/H.274 film grain application in the decoder, in favor of doing it with this filter. Note that this is only a gain if the frames are either already on the GPU, or if you’re using libplacebo for other purposes, since otherwise the VRAM roundtrip will more than offset any expected speedup.\n• Interop with VAAPI hwdec to avoid round-tripping through RAM:\n\nCalculate the VMAF (Video Multi-Method Assessment Fusion) score for a reference/distorted pair of input videos.\n\nThe first input is the distorted video, and the second input is the reference video.\n\nThe obtained VMAF score is printed through the logging system.\n\nIt requires Netflix’s vmaf library (libvmaf) as a pre-requisite. After installing the library it can be enabled using: .\n\nThe filter has following options:\n\nThis filter also supports the framesync options.\n• In the examples below, a distorted video is compared with a reference file .\n• Example with options and different containers:\n\nThis is the CUDA variant of the libvmaf filter. It only accepts CUDA frames.\n\nIt requires Netflix’s vmaf library (libvmaf) as a pre-requisite. After installing the library it can be enabled using: .\n\nApply limited difference filter using second and optionally third video stream.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands except option ‘ ’.\n\nLimits the pixel components values to the specified range [min, max].\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThe filter accepts the following options:\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThe filter accepts the following options:\n\nThis filter supports the option as commands.\n\nThe filter accepts the following options:\n\nThis filter supports same commands as options. The command accepts the same syntax of the corresponding option.\n\nIf the specified expression is not valid, it is kept at its current value.\n\nCompute a look-up table for binding each pixel component input value to an output value, and apply it to the input video.\n\napplies a lookup table to a YUV input video, to an RGB input video.\n\nThese filters accept the following parameters:\n\nEach of them specifies the expression to use for computing the lookup table for the corresponding pixel component values.\n\nThe exact component associated to each of the options depends on the format in input.\n\nThe filter requires either YUV or RGB pixel formats in input, requires RGB pixel formats in input, and requires YUV.\n\nThe expressions can contain the following constants and functions:\n\nThis filter supports same commands as options.\n• Negate input video: The above is the same as:\n\nThe filter takes two input streams and outputs one stream.\n\nThe (time lut2) filter takes two consecutive frames from one single stream.\n\nThis filter accepts the following parameters:\n\nThe filter also supports the framesync options.\n\nEach of them specifies the expression to use for computing the lookup table for the corresponding pixel component values.\n\nThe exact component associated to each of the options depends on the format in inputs.\n\nThe expressions can contain the following constants:\n\nThis filter supports the all above options as commands except option .\n\nClamp the first input stream with the second input and third input stream.\n\nReturns the value of first stream to be between second input stream - and third input stream + .\n\nThis filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nMerge the second and third input stream into output stream using absolute differences between second input stream and first input stream and absolute difference between third input stream and first input stream. The picked value will be from second input stream if second absolute difference is greater than first one or from third input stream otherwise.\n\nThis filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nMerge the first input stream with the second input stream using per pixel weights in the third input stream.\n\nA value of 0 in the third stream pixel component means that pixel component from first stream is returned unchanged, while maximum value (eg. 255 for 8-bit videos) means that pixel component from second stream is returned unchanged. Intermediate values define the amount of merging between both input stream’s pixel components.\n\nThis filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nMerge the second and third input stream into output stream using absolute differences between second input stream and first input stream and absolute difference between third input stream and first input stream. The picked value will be from second input stream if second absolute difference is less than first one or from third input stream otherwise.\n\nThis filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nPick pixels comparing absolute difference of two video streams with fixed threshold.\n\nIf absolute difference between pixel component of first and second video stream is equal or lower than user supplied threshold than pixel component from first video stream is picked, otherwise pixel component from second video stream is picked.\n\nThis filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nFor example it is useful to create motion masks after filter.\n\nThis filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nIt needs one field per frame as input and must thus be used together with yadif=1/3 or equivalent.\n\nThis filter accepts the following options:\n\nPick median pixel from certain rectangle defined by radius.\n\nThis filter accepts the following options:\n\nThis filter supports same commands as options. The command accepts the same syntax of the corresponding option.\n\nIf the specified expression is not valid, it is kept at its current value.\n\nThe filter accepts up to 4 input streams, and merge selected input planes to the output video.\n\nThis filter accepts the following options:\n• Merge three gray video streams of same width and height into single video stream:\n\nEstimate and export motion vectors using block matching algorithms. Motion vectors are stored in frame side data to be used by other filters.\n\nThis filter accepts the following options:\n\nMidway Image Equalization adjusts a pair of images to have the same histogram, while maintaining their dynamics as much as possible. It’s useful for e.g. matching exposures from a pair of stereo cameras.\n\nThis filter has two inputs and one output, which must be of same pixel format, but may be of different sizes. The output of filter is first input adjusted with midway histogram of both inputs.\n\nThis filter accepts the following option:\n\nConvert the video to specified frame rate using motion interpolation.\n\nThis filter accepts the following options:\n\nMix several video input streams into one video stream.\n\nA description of the accepted options follows.\n\nThis filter supports the following commands:\n\nA description of the accepted options follows.\n\nThis filter supports the all above options as commands.\n\nThis filter allows to apply main morphological grayscale transforms, erode and dilate with arbitrary structures set in second input stream.\n\nUnlike naive implementation and much slower performance in erosion and dilation filters, when speed is critical filter should be used instead.\n\nThe filter also supports the framesync options.\n\nThis filter supports same commands as options.\n\nDrop frames that do not differ greatly from the previous frame in order to reduce frame rate.\n\nThe main use of this filter is for very-low-bitrate encoding (e.g. streaming over dialup modem), but it could in theory be used for fixing movies that were inverse-telecined incorrectly.\n\nA description of the accepted options follows.\n\nObtain the MSAD (Mean Sum of Absolute Differences) between two input videos.\n\nBoth input videos must have the same resolution and pixel format for this filter to work correctly. Also it assumes that both inputs have the same number of frames, which are compared one by one.\n\nThe obtained per component, average, min and max MSAD is printed through the logging system.\n\nThe filter stores the calculated MSAD of each frame in frame metadata.\n\nThis filter also supports the framesync options.\n\nIn the below example the input file being processed is compared with the reference file .\n\nMultiply first video stream pixels values with second video stream pixels values.\n\nThe filter accepts the following options:\n\nThis filter supports same commands as options.\n\nIt accepts the following option:\n\nThis filter supports same commands as options.\n\nEach pixel is adjusted by looking for other pixels with similar contexts. This context similarity is defined by comparing their surrounding patches of size x . Patches are searched in an area of x around the pixel.\n\nNote that the research area defines centers for patches, which means some patches will be made of pixels outside that research area.\n\nThe filter accepts the following options.\n\nThis filter accepts the following options:\n\nThis filter supports same commands as options, excluding option.\n\nForce libavfilter not to use any of the specified pixel formats for the input to the next filter.\n\nIt accepts the following parameters:\n• Force libavfilter to use a format different from for the input to the vflip filter:\n• Convert the input video to any of the formats not contained in the list:\n\nThe filter accepts the following options:\n\nFor each channel of each frame, the filter computes the input range and maps it linearly to the user-specified output range. The output range defaults to the full dynamic range from pure black to pure white.\n\nTemporal smoothing can be used on the input range to reduce flickering (rapid changes in brightness) caused when small dark or bright objects enter or leave the scene. This is similar to the auto-exposure (automatic gain control) on a video camera, and, like a video camera, it may cause a period of over- or under-exposure of the video.\n\nThe R,G,B channels can be normalized independently, which may cause some color shifting, or linked together as a single channel, which prevents color shifting. Linked normalization preserves hue. Independent normalization does not, so it can be used to remove some color casts. Independent and linked normalization can be combined in any ratio.\n\nThe normalize filter accepts the following options:\n\nThis filter supports same commands as options, excluding option. The command accepts the same syntax of the corresponding option.\n\nIf the specified expression is not valid, it is kept at its current value.\n\nStretch video contrast to use the full dynamic range, with no temporal smoothing; may flicker depending on the source content:\n\nAs above, but with 50 frames of temporal smoothing; flicker should be reduced, depending on the source content:\n\nAs above, but with hue-preserving linked channel normalization:\n\nAs above, but with half strength:\n\nMap the darkest input color to red, the brightest input color to cyan:\n\nPass the video source unchanged to the output.\n\nThis filter uses Tesseract for optical character recognition. To enable compilation of this filter, you need to configure FFmpeg with .\n\nIt accepts the following options:\n\nThe filter exports recognized text as the frame metadata . The filter exports confidence of recognized words as the frame metadata .\n\nTo enable this filter, install the libopencv library and headers and configure FFmpeg with .\n\nIt accepts the following parameters:\n\nRefer to the official libopencv documentation for more precise information: http://docs.opencv.org/master/modules/imgproc/doc/filtering.html\n\nSeveral libopencv filters are supported; see the following subsections.\n\nDilate an image by using a specific structuring element. It corresponds to the libopencv function .\n\nrepresents a structuring element, and has the syntax: x + x /\n\nand represent the number of columns and rows of the structuring element, and the anchor point, and the shape for the structuring element. must be \"rect\", \"cross\", \"ellipse\", or \"custom\".\n\nIf the value for is \"custom\", it must be followed by a string of the form \"= \". The file with name is assumed to represent a binary image, with each printable character corresponding to a bright pixel. When a custom is used, and are ignored, the number or columns and rows of the read file are assumed instead.\n\nThe default value for is \"3x3+0x0/rect\".\n\nspecifies the number of times the transform is applied to the image, and defaults to 1.\n\nErode an image by using a specific structuring element. It corresponds to the libopencv function .\n\nIt accepts the parameters: : , with the same syntax and semantics as the dilate filter.\n\nThe filter takes the following parameters: | | | | .\n\nis the type of smooth filter to apply, and must be one of the following values: \"blur\", \"blur_no_scale\", \"median\", \"gaussian\", or \"bilateral\". The default value is \"gaussian\".\n\nThe meaning of , , , and depends on the smooth type. and accept integer positive values or 0. and accept floating point values.\n\nThe default value for is 3. The default value for the other parameters is 0.\n\nThese parameters correspond to the parameters assigned to the libopencv function .\n\nUseful to measure spatial impulse, step responses, chroma delays, etc.\n\nIt accepts the following parameters:\n\nThis filter supports same commands as options. The command accepts the same syntax of the corresponding option.\n\nIf the specified expression is not valid, it is kept at its current value.\n\nOverlay one video on top of another.\n\nIt takes two inputs and has one output. The first input is the \"main\" video on which the second input is overlaid.\n\nIt accepts the following parameters:\n\nA description of the accepted options follows.\n\nThe , and expressions can contain the following parameters.\n\nThis filter also supports the framesync options.\n\nNote that the , variables are available only when evaluation is done per frame, and will evaluate to NAN when is set to ‘ ’.\n\nBe aware that frames are taken from each input video in timestamp order, hence, if their initial timestamps differ, it is a good idea to pass the two inputs through a filter to have them begin in the same zero timestamp, as the example for the filter does.\n\nYou can chain together more overlays but you should test the efficiency of such approach.\n\nThis filter supports the following commands:\n• Draw the overlay at 10 pixels from the bottom right corner of the main video: Using named options the example above becomes:\n• Insert a transparent PNG logo in the bottom left corner of the input, using the tool with the option:\n• Insert 2 different transparent PNG logos (second logo on bottom right corner) using the tool:\n• Add a transparent color layer on top of the main video; must specify the size of the main input to the overlay filter:\n• Play an original video and a filtered version (here with the deshake filter) side by side using the tool: The above command is the same as:\n• Make a sliding overlay appearing from the left to the right top part of the screen starting since time 2:\n• Compose output by putting two input videos side to side:\n• Mask 10-20 seconds of a video by applying the delogo filter to a section\n\nThe filter accepts the following options:\n\nAdd paddings to the input image, and place the original input at the provided , coordinates.\n\nIt accepts the following parameters:\n\nThe value for the , , , and options are expressions containing the following constants:\n• Add paddings with the color \"violet\" to the input video. The output video size is 640x480, and the top-left corner of the input video is placed at column 0, row 40 The example above is equivalent to the following command:\n• Pad the input to get an output with dimensions increased by 3/2, and put the input video at the center of the padded area:\n• Pad the input to get a squared output with size equal to the maximum value between the input width and height, and put the input video at the center of the padded area:\n• Pad the input to get a final w/h ratio of 16:9:\n• In case of anamorphic video, in order to set the output display aspect correctly, it is necessary to use in the expression, according to the relation: Thus the previous example needs to be modified to:\n• Double the output size and put the input video in the bottom-right corner of the output padded area:\n\nGenerate one palette for a whole video stream.\n\nIt accepts the following options:\n\nThe filter also exports the frame metadata ( ) which you can use to evaluate the degree of color quantization of the palette. This information is also visible at logging level.\n• Generate a representative palette of a given video using :\n\nUse a palette to downsample an input video stream.\n\nThe filter takes two inputs: one video stream and a palette. The palette must be a 256 pixels image.\n\nIt accepts the following options:\n• Use a palette (generated for example with palettegen) to encode a GIF using :\n\nCorrect perspective of video not recorded perpendicular to the screen.\n\nA description of the accepted parameters follows.\n\nDelay interlaced video by one field time so that the field order changes.\n\nThe intended use is to fix PAL movies that have been captured with the opposite field order to the film-to-video transfer.\n\nA description of the accepted parameters follows.\n\nThis filter supports the all above options as commands.\n\nReduce various flashes in video, so to help users with epilepsy.\n\nIt accepts the following options:\n\nPixel format descriptor test filter, mainly useful for internal testing. The output video should be equal to the input video.\n\ncan be used to test the monowhite pixel format descriptor definition.\n\nThe filter accepts the following options:\n\nThis filter supports all options as commands.\n\nDisplay sample values of color channels. Mainly useful for checking color and levels. Minimum supported resolution is 640x480.\n\nThe filters accept the following options:\n\nThis filter supports same commands as options.\n\nEnable the specified chain of postprocessing subfilters using libpostproc. This library should be automatically selected with a GPL build ( ). Subfilters must be separated by ’/’ and can be disabled by prepending a ’-’. Each subfilter and some options have a short and a long name that can be used interchangeably, i.e. dr/dering are the same.\n\nThe filters accept the following options:\n\nAll subfilters share common options to determine their scope:\n\nThese options can be appended after the subfilter name, separated by a ’|’.\n\nThe horizontal and vertical deblocking filters share the difference and flatness values so you cannot set different horizontal and vertical thresholds.\n• Apply deblocking on luma only, and switch vertical deblocking on or off automatically depending on available CPU time:\n\nApply Postprocessing filter 7. It is variant of the spp filter, similar to spp = 6 with 7 point DCT, where only the center sample is used after IDCT.\n\nThe filter accepts the following options:\n\nApply alpha premultiply effect to input video stream using first plane of second stream as alpha.\n\nBoth streams must have same dimensions and same pixel format.\n\nThe filter accepts the following option:\n\nThe filter accepts the following option:\n\nThis filter supports the all above options as commands.\n\nThis filter accepts the following options:\n\nEach of the expression options specifies the expression to use for computing the lookup table for the corresponding pixel component values.\n\nThe expressions can contain the following constants and functions:\n\nThis filter supports the all above options as commands.\n\nObtain the average, maximum and minimum PSNR (Peak Signal to Noise Ratio) between two input videos.\n\nThis filter takes in input two input videos, the first input is considered the \"main\" source and is passed unchanged to the output. The second input is used as a \"reference\" video for computing the PSNR.\n\nBoth video inputs must have the same resolution and pixel format for this filter to work correctly. Also it assumes that both inputs have the same number of frames, which are compared one by one.\n\nThe obtained average PSNR is printed through the logging system.\n\nThe filter stores the accumulated MSE (mean squared error) of each frame, and at the end of the processing it is averaged across all frames equally, and the following formula is applied to obtain the PSNR:\n\nWhere MAX is the average of the maximum values of each component of the image.\n\nThe description of the accepted parameters follows.\n\nThis filter also supports the framesync options.\n\nThe file printed if is selected, contains a sequence of key/value pairs of the form : for each compared couple of frames.\n\nIf a greater than 1 is specified, a header line precedes the list of per-frame-pair stats, with key value pairs following the frame format with the following parameters:\n\nA description of each shown per-frame-pair parameter follows:\n• For example: On this example the input file being processed is compared with the reference file . The PSNR of each individual frame is stored in .\n• Another example with different containers:\n\nThe pullup filter is designed to take advantage of future context in making its decisions. This filter is stateless in the sense that it does not lock onto a pattern to follow, but it instead looks forward to the following fields in order to identify matches and rebuild progressive frames.\n\nTo produce content with an even framerate, insert the fps filter after pullup, use if the input frame rate is 29.97fps, for 30fps and the (rare) telecined 25fps input.\n\nThe filter accepts the following options:\n\nFor best results (without duplicated frames in the output file) it is necessary to change the output frame rate. For example, to inverse telecine NTSC input:\n\nThe filter accepts the following option:\n\nThe expression is evaluated through the eval API and can contain, among others, the following constants:\n\nGenerate a QR code using the libqrencode library (see https://fukuchi.org/works/qrencode/), and overlay it on top of the current frame.\n\nTo enable the compilation of this filter, you need to configure FFmpeg with .\n\nThe QR code is generated from the provided text or text pattern. The corresponding QR code is scaled and overlayed into the video output according to the specified options.\n\nIn case no text is specified, no QR code is overlaied.\n\nThis filter accepts the following options:\n\nThe expressions set by the options contain the following constants and functions.\n\nIf is set to , the text is printed verbatim.\n\nIf is set to (which is the default), the following expansion mechanism is used.\n\nThe backslash character ‘ ’, followed by any character, always expands to the second character.\n\nSequences of the form are expanded. The text between the braces is a function name, possibly followed by arguments separated by ’:’. If the arguments contain special characters or delimiters (’:’ or ’}’), they should be escaped.\n\nNote that they probably must also be escaped as the value for the option in the filter argument string and as the filter argument in the filtergraph description, and possibly also for the shell, that makes up to four levels of escaping; using a text file with the option avoids these problems.\n\nThe following functions are available:\n• Generate a QR code encoding the specified text with the default size, overalaid in the top left corner of the input video, with the default size:\n• Same as below, but select blue on pink colors:\n• Place the QR code in the bottom right corner of the input video:\n• Generate a QR code with width of 200 pixels and padding, making the padded width 4/3 of the QR code width:\n• Generate a QR code with padded width of 200 pixels and padding, making the QR code width 3/4 of the padded width:\n• Make the QR code a fraction of the input video width:\n\nIdentify and decode a QR code using the libquirc library (see https://github.com/dlbeer/quirc/), and print the identified QR codes positions and payload as metadata.\n\nTo enable the compilation of this filter, you need to configure FFmpeg with .\n\nFor each found QR code in the input video, some metadata entries are added with the prefix , where is the index, starting from 0, associated to the QR code.\n\nA description of each metadata value follows:\n\nFlush video frames from internal cache of frames into a random order. No frame is discarded. Inspired by frei0r nervous filter.\n\nRead closed captioning (EIA-608) information from the top lines of a video frame.\n\nThis filter adds frame metadata for and , where is the number of the identified line with EIA-608 data (starting from 0). A description of each metadata value follows:\n\nThis filter accepts the following options:\n\nThis filter supports the all above options as commands.\n• Output a csv with presentation time and the first two lines of identified EIA-608 captioning data.\n\nRead vertical interval timecode (VITC) information from the top lines of a video frame.\n\nThe filter adds frame metadata key with the timecode value, if a valid timecode has been detected. Further metadata key is set to 0/1 depending on whether timecode data has been found or not.\n\nThis filter accepts the following options:\n• Detect and draw VITC data onto the video frame; if no valid VITC is detected, draw as a placeholder:\n\nDestination pixel at position (X, Y) will be picked from source (x, y) position where x = Xmap(X, Y) and y = Ymap(X, Y). If mapping values are out of range, zero value for pixel will be used for destination pixel.\n\nXmap and Ymap input video streams must be of same dimensions. Output video stream will have Xmap/Ymap video stream dimensions. Xmap and Ymap input video streams are 16bit depth, single channel.\n\nThe removegrain filter is a spatial denoiser for progressive video.\n\nRange of mode is from 0 to 24. Description of each mode follows:\n\nSuppress a TV station logo, using an image file to determine which pixels comprise the logo. It works by filling in the pixels that comprise the logo with neighboring pixels.\n\nThe filter accepts the following options:\n\nPixels in the provided bitmap image with a value of zero are not considered part of the logo, non-zero pixels are considered part of the logo. If you use white (255) for the logo and black (0) for the rest, you will be safe. For making the filter bitmap, it is recommended to take a screen capture of a black frame with the logo visible, and then using a threshold filter followed by the erode filter once or twice.\n\nIf needed, little splotches can be fixed manually. Remember that if logo pixels are not covered, the filter quality will be much reduced. Marking too many pixels as part of the logo does not hurt as much, but it will increase the amount of blurring needed to cover over the image and will destroy more information than necessary, and extra pixels will slow things down on a large logo.\n\nThis filter uses the repeat_field flag from the Video ES headers and hard repeats fields based on its value.\n\nWarning: This filter requires memory to buffer the entire clip, so trimming is suggested.\n• Take the first 5 seconds of a clip, and reverse it.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThe filter accepts the following option:\n\nThis filter supports the all above options as commands.\n\nRotate video by an arbitrary angle expressed in radians.\n\nThe filter accepts the following options:\n\nA description of the optional parameters follows.\n\nThe expressions for the angle and the output size can contain the following constants and functions:\n• Apply a constant rotation with period T, starting from an angle of PI/3:\n• Make the input video rotation oscillating with a period of T seconds and an amplitude of A radians:\n• Rotate the video, output size is chosen so that the whole rotating input video is always completely contained in the output:\n• Rotate the video, reduce the output size so that no background is ever shown:\n\nThe filter supports the following commands:\n\nThe filter accepts the following options:\n\nEach chroma option value, if not explicitly specified, is set to the corresponding luma option value.\n\nScale (resize) the input video, using the libswscale library.\n\nThe scale filter forces the output display aspect ratio to be the same of the input, by changing the output sample aspect ratio.\n\nIf the input image format is different from the format requested by the next filter, the scale filter will convert the input to the requested format.\n\nThe filter accepts the following options, any of the options supported by the libswscale scaler, as well as any of the framesync options.\n\nSee (ffmpeg-scaler)the ffmpeg-scaler manual for the complete list of scaler options.\n\nThe values of the and options are expressions containing the following constants:\n• Scale the input video to a size of 200x100 This is equivalent to:\n• Specify a size abbreviation for the output size: which can also be written as:\n• The above is the same as:\n• Scale the input to 2x with forced interlaced scaling:\n• Increase the width, and set the height to the same size:\n• Increase the height, and set the width to 3/2 of the height:\n• Increase the size, making the size a multiple of the chroma subsample values:\n• Increase the width to a maximum of 500 pixels, keeping the same aspect ratio as the input:\n• Make pixels square using reset_sar, making sure the resulting resolution is even (required by some codecs):\n• Scale to target exactly, however reset SAR to 1:\n• Scale to even dimensions that fit within 400x300, preserving input SAR:\n• Scale to produce square pixels with even dimensions that fit within 400x300:\n• Scale a subtitle stream (sub) to match the main video (main) in size before overlaying. (\"scale2ref\")\n• Scale a logo to 1/10th the height of a video, while preserving its display aspect ratio.\n\nThis filter supports the following commands:\n\nScale and convert the color parameters using VTPixelTransferSession.\n\nThe filter accepts the following options:\n\nThe filter accepts the following option:\n\nThis filter supports the all above options as commands.\n\nThe filter accepts the following options:\n\nThis filter supports the following commands:\n\nThis filter sets frame metadata with mafd between frame, the scene score, and forward the frame to the next filter, so they can use these metadata to detect scene change or others.\n\nIn addition, this filter logs a message and sets frame metadata when it detects a scene change by .\n\nmetadata keys are set with mafd for every frame.\n\nmetadata keys are set with scene change score for every frame to detect scene change.\n\nmetadata keys are set with current filtered frame time which detect scene change with .\n\nThe filter accepts the following options:\n\nAdjust cyan, magenta, yellow and black (CMYK) to certain ranges of colors (such as \"reds\", \"yellows\", \"greens\", \"cyans\", ...). The adjustment range is defined by the \"purity\" of the color (that is, how saturated it already is).\n\nThis filter is similar to the Adobe Photoshop Selective Color tool.\n\nThe filter accepts the following options:\n\nAll the adjustment settings ( , , ...) accept up to 4 space separated floating point adjustment values in the [-1,1] range, respectively to adjust the amount of cyan, magenta, yellow and black for the pixels of its range.\n• Increase cyan by 50% and reduce yellow by 33% in every green areas, and increase magenta by 27% in blue areas:\n\nThe takes a frame-based video input and splits each frame into its components fields, producing a new half height clip with twice the frame rate and twice the frame count.\n\nThis filter use field-dominance information in frame to decide which of each pair of fields to place first in the output. If it gets it wrong use setfield filter before filter.\n\nThe filter sets the Display Aspect Ratio for the filter output video.\n\nThis is done by changing the specified Sample (aka Pixel) Aspect Ratio, according to the following equation:\n\nKeep in mind that the filter does not modify the pixel dimensions of the video frame. Also, the display aspect ratio set by this filter may be changed by later filters in the filterchain, e.g. in case of scaling or if another \"setdar\" or a \"setsar\" filter is applied.\n\nThe filter sets the Sample (aka Pixel) Aspect Ratio for the filter output video.\n\nNote that as a consequence of the application of this filter, the output display aspect ratio will change according to the equation above.\n\nKeep in mind that the sample aspect ratio set by the filter may be changed by later filters in the filterchain, e.g. if another \"setsar\" or a \"setdar\" filter is applied.\n\nIt accepts the following parameters:\n\nThe parameter is an expression containing the following constants:\n• To change the display aspect ratio to 16:9, specify one of the following:\n• To change the sample aspect ratio to 10:11, specify:\n• To set a display aspect ratio of 16:9, and specify a maximum integer value of 1000 in the aspect ratio reduction, use the command:\n\nThe filter marks the interlace type field for the output frames. It does not change the input frame, but only sets the corresponding property, which affects how the frame is treated by following filters (e.g. or ).\n\nThe filter accepts the following options:\n\nThe filter marks interlace and color range for the output frames. It does not change the input frame, but only sets the corresponding property, which affects how the frame is treated by filters/encoders.\n\nThis filter supports the following options:\n\nThis filter supports the all above options as commands.\n\nShow a line containing various information for each input video frame. The input video is not modified.\n\nThis filter supports the following options:\n\nThe shown line contains a sequence of key/value pairs of the form : .\n\nThe following values are shown in the output:\n\nDisplays the 256 colors palette of each frame. This filter is only relevant for pixel format frames.\n\nIt accepts the following option:\n\nIt accepts the following parameters:\n\nThe first frame has the index 0. The default is to keep the input unchanged.\n• Swap second and third frame of every three frames of the input:\n• Swap 10th and 1st frame of every ten frames of the input:\n\nThis filter accepts the following options:\n\nIt accepts the following parameters:\n\nThe first plane has the index 0. The default is to keep the input unchanged.\n• Swap the second and third planes of the input:\n\nEvaluate various visual metrics that assist in determining issues associated with the digitization of analog video media.\n\nBy default the filter will log these metadata values:\n\nThe filter accepts the following options:\n• Output specific data about the minimum and maximum values of the Y plane per frame:\n• Playback video while highlighting pixels that are outside of broadcast range in red.\n• Playback video with signalstats metadata drawn over the frame. The contents of signalstat_drawtext.txt used in the command are:\n\nCalculates the MPEG-7 Video Signature. The filter can handle more than one input. In this case the matching between the inputs can be calculated additionally. The filter always passes through the first input. The signature of each stream can be written into a file.\n\nIt accepts the following options:\n• To calculate the signature of an input video and store it in signature.bin:\n• To detect whether two videos match and store the signatures in XML format in signature0.xml and signature1.xml:\n\nCalculate Spatial Information (SI) and Temporal Information (TI) scores for a video, as defined in ITU-T Rec. P.910 (11/21): Subjective video quality assessment methods for multimedia applications. Available PDF at https://www.itu.int/rec/T-REC-P.910-202111-S/en. Note that this is a legacy implementation that corresponds to a superseded recommendation. Refer to ITU-T Rec. P.910 (07/22) for the latest version: https://www.itu.int/rec/T-REC-P.910-202207-I/en\n\nIt accepts the following option:\n\nBlur the input video without impacting the outlines.\n\nIt accepts the following options:\n\nIf a chroma or alpha option is not explicitly set, the corresponding luma value is set.\n\nThe filter accepts the following option:\n\nThis filter supports the all above options as commands.\n\nApply a simple postprocessing filter that compresses and decompresses the image at several (or - in the case of level - all) shifts and average the results.\n\nThe filter accepts the following options:\n\nThis filter supports the following commands:\n\nScale the input by applying one of the super-resolution methods based on convolutional neural networks. Supported models:\n\nTraining scripts as well as scripts for model file (.pb) saving can be found at https://github.com/XueweiMeng/sr/tree/sr_dnn_native. Original repository is at https://github.com/HighVoltageRocknRoll/sr.git.\n\nThe filter accepts the following options:\n\nTo get full functionality (such as async execution), please use the dnn_processing filter.\n\nUpscale (size increasing) for the input video using AMD Advanced Media Framework library for hardware acceleration. Use advanced algorithms for upscaling with higher output quality. Setting the output width and height works in the same way as for the scale filter.\n\nThe filter accepts the following options:\n• Scale input to 720p, keeping aspect ratio and ensuring the output is yuv420p.\n\nObtain the SSIM (Structural SImilarity Metric) between two input videos.\n\nThis filter takes in input two input videos, the first input is considered the \"main\" source and is passed unchanged to the output. The second input is used as a \"reference\" video for computing the SSIM.\n\nBoth video inputs must have the same resolution and pixel format for this filter to work correctly. Also it assumes that both inputs have the same number of frames, which are compared one by one.\n\nThe filter stores the calculated SSIM of each frame.\n\nThe description of the accepted parameters follows.\n\nThe file printed if is selected, contains a sequence of key/value pairs of the form : for each compared couple of frames.\n\nA description of each shown parameter follows:\n\nThis filter also supports the framesync options.\n• For example: On this example the input file being processed is compared with the reference file . The SSIM of each individual frame is stored in .\n• Another example with both psnr and ssim at same time:\n• Another example with different containers:\n\nThe filters accept the following options:\n• Convert input video from side by side parallel to anaglyph yellow/blue dubois:\n• Convert input video from above below (left eye above, right eye below) to side by side crosseye.\n\nThe filter accepts the following options:\n\nThe and filter supports the following commands:\n• Select first 5 seconds 1st stream and rest of time 2nd stream:\n• Same as above, but for audio:\n\nDraw subtitles on top of input video using the libass library.\n\nTo enable compilation of this filter you need to configure FFmpeg with . This filter also requires a build with libavcodec and libavformat to convert the passed subtitles file to ASS (Advanced Substation Alpha) subtitles format.\n\nThe filter accepts the following options:\n\nIf the first key is not specified, it is assumed that the first value specifies the .\n\nFor example, to render the file on top of the input video, use the command:\n\nwhich is equivalent to:\n\nTo render the default subtitles stream from file , use:\n\nTo render the second subtitles stream from that file, use:\n\nTo make the subtitles stream from appear in 80% transparent blue , use:\n\nScale the input by 2x and smooth using the Super2xSaI (Scale and Interpolate) pixel art scaling algorithm.\n\nUseful for enlarging pixel art images without reducing sharpness.\n\nThis filter accepts the following options:\n\nThe all options are expressions containing the following constants:\n\nThis filter supports the all above options as commands.\n\nThis filter accepts the following options:\n\nCompute and draw a color distribution histogram for the input video across time.\n\nUnlike histogram video filter which only shows histogram of single input frame at certain time, this filter shows also past histograms of number of frames defined by option.\n\nThe computed histogram is a representation of the color component distribution in an image.\n\nThe filter accepts the following options:\n\nThis filter needs four video streams to perform thresholding. First stream is stream we are filtering. Second stream is holding threshold values, third stream is holding min values, and last, fourth stream is holding max values.\n\nThe filter accepts the following option:\n\nFor example if first stream pixel’s component value is less then threshold value of pixel component from 2nd threshold stream, third stream value will picked, otherwise fourth stream pixel component value will be picked.\n\nUsing color source filter one can perform various types of thresholding:\n\nThis filter supports the all options as commands.\n• Threshold to zero, using gray color as threshold:\n• Inverted threshold to zero, using gray color as threshold:\n\nSelect the most representative frame in a given sequence of consecutive frames.\n\nThe filter accepts the following options:\n\nSince the filter keeps track of the whole frames sequence, a bigger value will result in a higher memory usage, so a high value is not recommended.\n• Complete example of a thumbnail creation with :\n\nThe untile filter can do the reverse.\n\nThe filter accepts the following options:\n• Produce 8x8 PNG tiles of all keyframes ( ) in a movie: The is necessary to prevent from duplicating each output frame to accommodate the originally detected frame rate.\n• Display pictures in an area of frames, with pixels between them, and pixels of initial margin, using mixed flat and named options:\n\nWhat happens when you invert time and space?\n\nNormally a video is composed of several frames that represent a different instant of time and shows a scene that evolves in the space captured by the frame. This filter is the antipode of that concept, taking inspiration from tilt and shift photography.\n\nA filtered frame contains the whole timeline of events composing the sequence, and this is obtained by placing a slice of pixels from each frame into a single one. However, since there are no infinite-width frames, this is done up the width of the input frame, and a video is recomposed by shifting away one column for each subsequent frame. In order to map space to time, the filter tilts each input frame as well, so that motion is preserved. This is accomplished by progressively selecting a different column from each input frame.\n\nThe end result is a sort of inverted parallax, so that far away objects move much faster that the ones in the front. The ideal conditions for this video effect are when there is either very little motion and the backgroud is static, or when there is a lot of motion and a very wide depth of field (e.g. wide panorama, while moving on a train).\n\nThe filter accepts the following parameters:\n\nNormally the filter shifts and tilts from the very first frame, and stops when the last one is received. However, before filtering starts, normal video may be preseved, so that the effect is slowly shifted in its place. Similarly, the last video frame may be reconstructed at the end. Alternatively it is possible to just start and end with black.\n\nFrames are counted starting from 1, so the first input frame is considered odd.\n\nThe filter accepts the following options:\n\nThe filter accepts the following options:\n\nThis filter supports all above options as commands, excluding option .\n\nMidway Video Equalization adjusts a sequence of video frames to have the same histograms, while maintaining their dynamics as much as possible. It’s useful for e.g. matching exposures from a video frames sequence.\n\nThis filter accepts the following option:\n\nA description of the accepted options follows.\n• Similar as above but only showing temporal differences:\n\nThis filter supports the following commands:\n\nThis filter expects data in single precision floating point, as it needs to operate on (and can output) out-of-range values. Another filter, such as zscale, is needed to convert the resulting frame to a usable format.\n\nThe tonemapping algorithms implemented only work on linear light, so input data should be linearized beforehand (and possibly correctly tagged).\n\nThe filter accepts the following options.\n\nThe filter accepts the following options:\n\nTranspose rows with columns in the input video and optionally flip it.\n\nIt accepts the following parameters:\n\nFor example to rotate by 90 degrees clockwise and preserve portrait layout:\n\nThe command above can also be specified as:\n\nTrim the input so that the output contains one continuous subpart of the input.\n\nIt accepts the following parameters:\n\n, , and are expressed as time duration specifications; see (ffmpeg-utils)the Time duration section in the ffmpeg-utils(1) manual for the accepted syntax.\n\nNote that the first two sets of the start/end options and the option look at the frame timestamp, while the _frame variants simply count the frames that pass through the filter. Also note that this filter does not modify the timestamps. If you wish for the output timestamps to start at zero, insert a setpts filter after the trim filter.\n\nIf multiple start or end options are set, this filter tries to be greedy and keep all the frames that match at least one of the specified constraints. To keep only the part that matches all the constraints at once, chain multiple trim filters.\n\nThe defaults are such that all the input is kept. So it is possible to set e.g. just the end values to keep everything before the specified time.\n• Drop everything except the second minute of input:\n• Keep only the first second:\n\nApply alpha unpremultiply effect to input video stream using first plane of second stream as alpha.\n\nBoth streams must have same dimensions and same pixel format.\n\nThe filter accepts the following option:\n\nIt accepts the following parameters:\n\nAll parameters are optional and default to the equivalent of the string ’5:5:1.0:5:5:0.0’.\n• Apply a strong blur of both luma and chroma parameters:\n\nDecompose a video made of tiled images into the individual images.\n\nThe frame rate of the output video is the frame rate of the input video multiplied by the number of tiles.\n\nThis filter does the reverse of tile.\n\nThe filter accepts the following options:\n• Produce a 1-second video from a still image file made of 25 frames stacked vertically, like an analogic film reel:\n\nApply ultra slow/simple postprocessing filter that compresses and decompresses the image at several (or - in the case of level - all) shifts and average the results.\n\nThe way this differs from the behavior of spp is that uspp actually encodes & decodes each case with libavcodec Snow, whereas spp uses a simplified intra only 8x8 DCT similar to MJPEG.\n\nThis filter is not available in ffmpeg versions between 5.0 and 6.0.\n\nThe filter accepts the following options:\n\nThe filter accepts the following options:\n• Convert equirectangular video to cubemap with 3x2 layout and 1% padding using bicubic interpolation:\n• Convert transposed and horizontally flipped Equi-Angular Cubemap in side-by-side stereo format to equirectangular top-bottom stereo format:\n\nThis filter supports subset of above options as commands.\n\nIt transforms each frame from the video input into the wavelet domain, using Cohen-Daubechies-Feauveau 9/7. Then it applies some filtering to the obtained coefficients. It does an inverse wavelet transform after. Due to wavelet properties, it should give a nice smoothed result, and reduced noise, without blurring picture features.\n\nThis filter accepts the following options:\n\nApply variable blur filter by using 2nd video stream to set blur radius. The 2nd stream must have the same dimensions.\n\nThis filter accepts the following options:\n\nThe filter also supports the framesync options.\n\nThis filter supports all the above options as commands.\n\nDisplay 2 color component values in the two dimensional graph (which is called a vectorscope).\n\nThis filter accepts the following options:\n\nAnalyze video stabilization/deshaking. Perform pass 1 of 2, see vidstabtransform for pass 2.\n\nThis filter generates a file with relative translation and rotation transform information about subsequent frames, which is then used by the vidstabtransform filter.\n\nTo enable compilation of this filter you need to configure FFmpeg with .\n\nThis filter accepts the following options:\n• Analyze strongly shaky movie and put the results in file :\n• Visualize the result of internal transformations in the resulting video:\n\nVideo stabilization/deshaking: pass 2 of 2, see vidstabdetect for pass 1.\n\nRead a file with transform information for each frame and apply/compensate them. Together with the vidstabdetect filter this can be used to deshake videos. See also http://public.hronopik.de/vid.stab. It is important to also use the unsharp filter, see below.\n\nTo enable compilation of this filter you need to configure FFmpeg with .\n• Use for a typical stabilization with default values: Note the use of the unsharp filter which is always recommended.\n• Zoom in a bit more and load transform data from a given file:\n• Smoothen the video even more:\n\nFor example, to vertically flip a video with :\n\nThis filter tries to detect if the input is variable or constant frame rate.\n\nAt end it will output number of frames detected as having variable delta pts, and ones with constant delta pts. If there was frames with variable delta, than it will also show min, max and average delta encountered.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nObtain the average VIF (Visual Information Fidelity) between two input videos.\n\nBoth input videos must have the same resolution and pixel format for this filter to work correctly. Also it assumes that both inputs have the same number of frames, which are compared one by one.\n\nThe obtained average VIF score is printed through the logging system.\n\nThe filter stores the calculated VIF score of each frame.\n\nThis filter also supports the framesync options.\n\nIn the below example the input file being processed is compared with the reference file .\n\nThe filter accepts the following options:\n\nThe , and expressions can contain the following parameters.\n\nObtain the average VMAF motion score of a video. It is one of the component metrics of VMAF.\n\nThe obtained average motion score is printed through the logging system.\n\nThe filter accepts the following options:\n\nScale (resize) and convert colorspace, transfer characteristics or color primaries for the input video, using AMD Advanced Media Framework library for hardware acceleration. Setting the output width and height works in the same way as for the scale filter.\n\nThe filter accepts the following options:\n• Scale input to 720p, keeping aspect ratio and ensuring the output is yuv420p.\n• Upscale to 4K and change color profile to bt2020.\n\nAll streams must be of same pixel format and of same width.\n\nNote that this filter is faster than using overlay and pad filter to create same output.\n\nThe filter accepts the following options:\n\nBased on the process described by Martin Weston for BBC R&D, and implemented based on the de-interlace algorithm written by Jim Easterbrook for BBC R&D, the Weston 3 field deinterlacing filter uses filter coefficients calculated by BBC R&D.\n\nThis filter uses field-dominance information in frame to decide which of each pair of fields to place first in the output. If it gets it wrong use setfield filter before filter.\n\nThere are two sets of filter coefficients, so called \"simple\" and \"complex\". Which set of filter coefficients is used can be set by passing an optional parameter:\n\nThis filter supports same commands as options.\n\nThe waveform monitor plots color component intensity. By default luma only. Each column of the waveform corresponds to a column of pixels in the source video.\n\nIt accepts the following options:\n\nThe takes a field-based video input and join each two sequential fields into single frame, producing a new double height clip with half the frame rate and half the frame count.\n\nThe works same as but without halving frame rate and frame count.\n\nIt accepts the following option:\n\nApply the xBR high-quality magnification filter which is designed for pixel art. It follows a set of edge-detection rules, see https://forums.libretro.com/t/xbr-algorithm-tutorial/123.\n\nIt accepts the following option:\n\nApply normalized cross-correlation between first and second input video stream.\n\nSecond input video stream dimensions must be lower than first input video stream.\n\nThe filter accepts the following options:\n\nThe filter also supports the framesync options.\n\nApply cross fade from one input video stream to another input video stream. The cross fade is applied for specified duration.\n\nBoth inputs must be constant frame-rate and have the same resolution, pixel format, frame rate and timebase.\n\nThe filter accepts the following options:\n• Cross fade from one input video to another input video, with fade transition and duration of transition of 2 seconds starting at offset of 5 seconds:\n\nThe filter accepts the following options:\n\nThis filter supports all above options as commands, excluding option .\n\nObtain the average (across all input frames) and minimum (across all color plane averages) eXtended Perceptually weighted peak Signal-to-Noise Ratio (XPSNR) between two input videos.\n\nThe XPSNR is a low-complexity psychovisually motivated distortion measurement algorithm for assessing the difference between two video streams or images. This is especially useful for objectively quantifying the distortions caused by video and image codecs, as an alternative to a formal subjective test. The logarithmic XPSNR output values are in a similar range as those of traditional psnr assessments but better reflect human impressions of visual coding quality. More details on the XPSNR measure, which essentially represents a blockwise weighted variant of the PSNR measure, can be found in the following freely available papers:\n• C. R. Helmrich, M. Siekmann, S. Becker, S. Bosse, D. Marpe, and T. Wiegand, \"XPSNR: A Low-Complexity Extension of the Perceptually Weighted Peak Signal-to-Noise Ratio for High-Resolution Video Quality Assessment,\" in Proc. IEEE Int. Conf. Acoustics, Speech, Sig. Process. (ICASSP), virt./online, May 2020. www.ecodis.de/xpsnr.htm\n• C. R. Helmrich, S. Bosse, H. Schwarz, D. Marpe, and T. Wiegand, \"A Study of the Extended Perceptually Weighted Peak Signal-to-Noise Ratio (XPSNR) for Video Compression with Different Resolutions and Bit Depths,\" ITU Journal: ICT Discoveries, vol. 3, no. 1, pp. 65 - 72, May 2020. http://handle.itu.int/11.1002/pub/8153d78b-en\n\nWhen publishing the results of XPSNR assessments obtained using, e.g., this FFmpeg filter, a reference to the above papers as a means of documentation is strongly encouraged. The filter requires two input videos. The first input is considered a (usually not distorted) reference source and is passed unchanged to the output, whereas the second input is a (distorted) test signal. Except for the bit depth, these two video inputs must have the same pixel format. In addition, for best performance, both compared input videos should be in YCbCr color format.\n\nThe obtained overall XPSNR values mentioned above are printed through the logging system. In case of input with multiple color planes, we suggest reporting of the minimum XPSNR average.\n\nThe following parameter, which behaves like the one for the psnr filter, is accepted:\n\nThis filter also supports the framesync options.\n• XPSNR analysis of two 1080p HD videos, ref_source.yuv and test_video.yuv, both at 24 frames per second, with color format 4:2:0, bit depth 8, and output of a logfile named \"xpsnr.log\":\n• XPSNR analysis of two 2160p UHD videos, ref_source.yuv with bit depth 8 and test_video.yuv with bit depth 10, both at 60 frames per second with color format 4:2:0, no logfile output:\n\nAll streams must be of same pixel format.\n\nThe filter accepts the following options:\n• Display 4 inputs into 2x2 grid. Note that if inputs are of different sizes, gaps or overlaps may occur.\n• Display 4 inputs into 1x4 grid. Note that if inputs are of different widths, unused space will appear.\n• Display 9 inputs into 3x3 grid. Note that if inputs are of different sizes, gaps or overlaps may occur.\n• Display 16 inputs into 4x4 grid. Note that if inputs are of different sizes, gaps or overlaps may occur.\n\nDeinterlace the input video (\"yadif\" means \"yet another deinterlacing filter\").\n\nIt accepts the following parameters:\n\nApply blur filter while preserving edges (\"yaepblur\" means \"yet another edge preserving blur filter\"). The algorithm is described in \"J. S. Lee, Digital image enhancement and noise filtering by use of local statistics, IEEE Trans. Pattern Anal. Mach. Intell. PAMI-2, 1980.\"\n\nIt accepts the following parameters:\n\nThis filter supports same commands as options.\n\nThis filter accepts the following options:\n\nEach expression can contain the following constants:\n• Zoom in up to 1.5x and pan at same time to some spot near center of picture:\n• Zoom in up to 1.5x and pan always at center of picture:\n• Same as above but without pausing:\n• Zoom in 2x into center of picture only for the first second of the input video:\n\nScale (resize) the input video, using the z.lib library: https://github.com/sekrit-twc/zimg. To enable compilation of this filter, you need to configure FFmpeg with .\n\nThe zscale filter forces the output display aspect ratio to be the same as the input, by changing the output sample aspect ratio.\n\nIf the input image format is different from the format requested by the next filter, the zscale filter will convert the input to the requested format.\n\nThe filter accepts the following options.\n\nThe values of the and options are expressions containing the following constants:\n\nThis filter supports the following commands:\n\nTo enable CUDA and/or NPP filters please refer to configuration guidelines for CUDA and for CUDA NPP filters.\n\nRunning CUDA filters requires you to initialize a hardware device and to pass that device to all filters in any filter graph.\n\nFor more detailed information see https://www.ffmpeg.org/ffmpeg.html#Advanced-Video-options\n• Example of initializing second CUDA device on the system and running scale_cuda and bilateral_cuda filters.\n\nSince CUDA filters operate exclusively on GPU memory, frame data must sometimes be uploaded (hwupload) to hardware surfaces associated with the appropriate CUDA device before processing, and downloaded (hwdownload) back to normal memory afterward, if required. Whether hwupload or hwdownload is necessary depends on the specific workflow:\n• If the input frames are already in GPU memory (e.g., when using or ), explicit use of hwupload is not needed, as the data is already in the appropriate memory space.\n• If the input frames are in CPU memory (e.g., software-decoded frames or frames processed by CPU-based filters), it is necessary to use hwupload to transfer the data to GPU memory for CUDA processing.\n• If the output of the CUDA filters needs to be further processed by software-based filters or saved in a format not supported by GPU-based encoders, hwdownload is required to transfer the data back to CPU memory.\n\nNote that hwupload uploads data to a surface with the same layout as the software frame, so it may be necessary to add a format filter immediately before hwupload to ensure the input is in the correct format. Similarly, hwdownload may not support all output formats, so an additional format filter may need to be inserted immediately after hwdownload in the filter graph to ensure compatibility.\n\nBelow is a description of the currently available Nvidia CUDA video filters.\n\nNote: If FFmpeg detects the Nvidia CUDA Toolkit during configuration, it will enable CUDA filters automatically without requiring any additional flags. If you want to explicitly enable them, use the following options:\n• Configure FFmpeg with . Additional requirement: lib must be installed.\n\nCUDA accelerated bilateral filter, an edge preserving filter. This filter is mathematically accurate thanks to the use of GPU acceleration. For best output quality, use one to one chroma subsampling, i.e. yuv444p format.\n\nThe filter accepts the following options:\n\nDeinterlace the input video using the bwdif algorithm, but implemented in CUDA so that it can work as part of a GPU accelerated pipeline with nvdec and/or nvenc.\n\nIt accepts the following parameters:\n\nThis filter works like normal chromakey filter but operates on CUDA frames. for more details and parameters see chromakey.\n• Make all the green pixels in the input video transparent and use it as an overlay for another video:\n\nIt is by no means feature complete compared to the software colorspace filter, and at the current time only supports color range conversion between jpeg/full and mpeg/limited range.\n\nThe filter accepts the following options:\n\nOverlay one video on top of another.\n\nThis is the CUDA variant of the overlay filter. It only accepts CUDA frames. The underlying input pixel formats have to match.\n\nIt takes two inputs and has one output. The first input is the \"main\" video on which the second input is overlaid.\n\nIt accepts the following parameters:\n\nThis filter also supports the framesync options.\n\nScale (resize) and convert (pixel format) the input video, using accelerated CUDA kernels. Setting the output width and height works in the same way as for the scale filter.\n\nThe filter accepts the following options:\n• Scale input to 720p, keeping aspect ratio and ensuring the output is yuv420p.\n• Don’t do any conversion or scaling, but copy all input frames into newly allocated ones. This can be useful to deal with a filter and encode chain that otherwise exhausts the decoders frame pool.\n\nDeinterlace the input video using the yadif algorithm, but implemented in CUDA so that it can work as part of a GPU accelerated pipeline with nvdec and/or nvenc.\n\nIt accepts the following parameters:\n\nBelow is a description of the currently available NVIDIA Performance Primitives (libnpp) video filters.\n\nUse the NVIDIA Performance Primitives (libnpp) to perform scaling and/or pixel format conversion on CUDA video frames. Setting the output width and height works in the same way as for the filter.\n\nThe following additional options are accepted:\n\nThe values of the and options are expressions containing the following constants:\n\nUse the NVIDIA Performance Primitives (libnpp) to scale (resize) the input video, based on a reference video.\n\nSee the scale_npp filter for available options, scale2ref_npp supports the same but uses the reference video instead of the main input as basis. scale2ref_npp also supports the following additional constants for the and options:\n• Scale a subtitle stream (b) to match the main video (a) in size before overlaying\n• Scale a logo to 1/10th the height of a video, while preserving its display aspect ratio.\n\nUse the NVIDIA Performance Primitives (libnpp) to perform image sharpening with border control.\n\nThe following additional options are accepted:\n\nTranspose rows with columns in the input video and optionally flip it. For more in depth examples see the transpose video filter, which shares mostly the same options.\n\nIt accepts the following parameters:\n\nBelow is a description of the currently available OpenCL video filters.\n\nTo enable compilation of these filters you need to configure FFmpeg with .\n\nRunning OpenCL filters requires you to initialize a hardware device and to pass that device to all filters in any filter graph.\n\nFor more detailed information see https://www.ffmpeg.org/ffmpeg.html#Advanced-Video-options\n• Example of choosing the first device on the second platform and running avgblur_opencl filter with default parameters on it.\n\nSince OpenCL filters are not able to access frame data in normal memory, all frame data needs to be uploaded(hwupload) to hardware surfaces connected to the appropriate device before being used and then downloaded(hwdownload) back to normal memory. Note that hwupload will upload to a surface with the same layout as the software frame, so it may be necessary to add a format filter immediately before to get the input into the right format and hwdownload does not support all formats on the output - it may be necessary to insert an additional format filter immediately following in the graph to get the output in a supported format.\n\nThe filter accepts the following options:\n• Apply average blur filter with horizontal and vertical size of 3, setting each pixel of the output to the average value of the 7x7 region centered on it in the input. For pixels on the edges of the image, the region does not extend beyond the image boundaries, and so out-of-range coordinates are not used in the calculations.\n\nIt accepts the following parameters:\n\nA description of the accepted options follows.\n\nApply boxblur filter, setting each pixel of the output to the average value of box-radiuses , , for each plane respectively. The filter will apply , , times onto the corresponding plane. For pixels on the edges of the image, the radius does not extend beyond the image boundaries, and so out-of-range coordinates are not used in the calculations.\n• Apply a boxblur filter with the luma, chroma, and alpha radius set to 2 and luma, chroma, and alpha power set to 3. The filter will run 3 times with box-radius set to 2 for every plane of the image.\n• Apply a boxblur filter with luma radius set to 2, luma_power to 1, chroma_radius to 4, chroma_power to 5, alpha_radius to 3 and alpha_power to 7. For the luma plane, a 2x2 box radius will be run once. For the chroma plane, a 4x4 box radius will be run 5 times. For the alpha plane, a 3x3 box radius will be run 7 times.\n\nThe filter accepts the following options:\n• Make every semi-green pixel in the input transparent with some slight blending:\n\nThe filter accepts the following options:\n\nThis filter replaces the pixel by the local(3x3) minimum.\n\nIt accepts the following options:\n• Apply erosion filter with threshold0 set to 30, threshold1 set 40, threshold2 set to 50 and coordinates set to 231, setting each pixel of the output to the local minimum between pixels: 1, 2, 3, 6, 7, 8 of the 3x3 region centered on it in the input. If the difference between input pixel and local minimum is more then threshold of the corresponding plane, output pixel will be set to input pixel - threshold of corresponding plane.\n\nThe filter accepts the following options:\n• Stabilize a video with debugging (both in console and in rendered video):\n\nThis filter replaces the pixel by the local(3x3) maximum.\n\nIt accepts the following options:\n• Apply dilation filter with threshold0 set to 30, threshold1 set 40, threshold2 set to 50 and coordinates set to 231, setting each pixel of the output to the local maximum between pixels: 1, 2, 3, 6, 7, 8 of the 3x3 region centered on it in the input. If the difference between input pixel and local maximum is more then threshold of the corresponding plane, output pixel will be set to input pixel + threshold of corresponding plane.\n\nNon-local Means denoise filter through OpenCL, this filter accepts same options as nlmeans.\n\nOverlay one video on top of another.\n\nIt takes two inputs and has one output. The first input is the \"main\" video on which the second input is overlaid. This filter requires same memory layout for all the inputs. So, format conversion may be needed.\n\nThe filter accepts the following options:\n• Overlay an image LOGO at the top-left corner of the INPUT video. Both inputs are yuv420p format.\n• The inputs have same memory layout for color channels , the overlay has additional alpha plane, like INPUT is yuv420p, and the LOGO is yuva420p.\n\nAdd paddings to the input image, and place the original input at the provided , coordinates.\n\nIt accepts the following options:\n\nThe value for the , , , and options are expressions containing the following constants:\n\nThe filter accepts the following option:\n• Apply the Prewitt operator with scale set to 2 and delta set to 10.\n\nThe filter also supports the framesync options.\n\nThe program source file must contain a kernel function with the given name, which will be run once for each plane of the output. Each run on a plane gets enqueued as a separate 2D global NDRange with one work-item for each pixel to be generated. The global ID offset for each work-item is therefore the coordinates of a pixel in the destination image.\n\nThe kernel function needs to take the following arguments:\n• Destination image, . This image will become the output; the kernel should write all of it.\n• Frame index, . This is a counter starting from zero and increasing by one for each frame.\n• Source images, . These are the most recent images on each input. The kernel may read from them to generate the output, but they can’t be written to.\n• Copy the input to the output (output must be the same size as the input).\n• Apply a simple transformation, rotating the input by an amount increasing with the index counter. Pixel values are linearly interpolated by the sampler, and the output need not have the same dimensions as the input.\n• Blend two inputs together, with the amount of each input used varying with the index counter.\n\nDestination pixel at position (X, Y) will be picked from source (x, y) position where x = Xmap(X, Y) and y = Ymap(X, Y). If mapping values are out of range, zero value for pixel will be used for destination pixel.\n\nXmap and Ymap input video streams must be of same dimensions. Output video stream will have Xmap/Ymap video stream dimensions. Xmap and Ymap input video streams are 32bit float pixel format, single channel.\n\nThe filter accepts the following option:\n• Apply the Roberts cross operator with scale set to 2 and delta set to 10\n\nThe filter accepts the following option:\n• Apply sobel operator with scale set to 2 and delta set to 10\n\nIt accepts the following parameters:\n\nIt accepts the following parameters:\n\nAll parameters are optional and default to the equivalent of the string ’5:5:1.0:5:5:0.0’.\n• Apply a strong blur of both luma and chroma parameters:\n\nCross fade two videos with custom transition effect by using OpenCL.\n\nIt accepts the following options:\n\nThe program source file must contain a kernel function with the given name, which will be run once for each plane of the output. Each run on a plane gets enqueued as a separate 2D global NDRange with one work-item for each pixel to be generated. The global ID offset for each work-item is therefore the coordinates of a pixel in the destination image.\n\nThe kernel function needs to take the following arguments:\n• Destination image, . This image will become the output; the kernel should write all of it.\n• First Source image, . Second Source image, . These are the most recent images on each input. The kernel may read from them to generate the output, but they can’t be written to.\n• Transition progress, . This value is always between 0 and 1 inclusive.\n\nVAAPI Video filters are usually used with VAAPI decoder and VAAPI encoder. Below is a description of VAAPI video filters.\n\nTo enable compilation of these filters you need to configure FFmpeg with .\n\nTo use vaapi filters, you need to setup the vaapi device correctly. For more information, please read https://trac.ffmpeg.org/wiki/Hardware/VAAPI\n\nOverlay one video on the top of another.\n\nIt takes two inputs and has one output. The first input is the \"main\" video on which the second input is overlaid.\n\nThe filter accepts the following options:\n\nThis filter also supports the framesync options.\n• Overlay an image LOGO at the top-left corner of the INPUT video. Both inputs for this filter are yuv420p format.\n• Overlay an image LOGO at the offset (200, 100) from the top-left corner of the INPUT video. The inputs have same memory layout for color channels, the overlay has additional alpha plane, like INPUT is yuv420p, and the LOGO is yuva420p.\n\nPerform HDR-to-SDR or HDR-to-HDR tone-mapping. It currently only accepts HDR10 as input.\n\nIt accepts the following parameters:\n\nThis is the VA-API variant of the hstack filter, each input stream may have different height, this filter will scale down/up each input stream while keeping the original aspect.\n\nIt accepts the following options:\n\nThis is the VA-API variant of the vstack filter, each input stream may have different width, this filter will scale down/up each input stream while keeping the original aspect.\n\nIt accepts the following options:\n\nThis is the VA-API variant of the xstack filter, each input stream may have different size, this filter will scale down/up each input stream to the given output size, or the size of the first input stream.\n\nIt accepts the following options:\n\nAdd paddings to the input image, and place the original input at the provided , coordinates.\n\nIt accepts the following options:\n\nThe value for the , , , and options are expressions containing the following constants:\n\nIt accepts the following parameters:\n\nThe parameters for , , and and are expressions containing the following constants:\n• Draw a black box around the edge of the input image:\n• Draw a box with color red and an opacity of 50%: The previous example can be specified as:\n\nBelow is a description of the currently available Vulkan video filters.\n\nTo enable compilation of these filters you need to configure FFmpeg with and either or .\n\nRunning Vulkan filters requires you to initialize a hardware device and to pass that device to all filters in any filter graph.\n\nFor more detailed information see https://www.ffmpeg.org/ffmpeg.html#Advanced-Video-options\n• Example of choosing the first device and running nlmeans_vulkan filter with default parameters on it.\n\nAs Vulkan filters are not able to access frame data in normal memory, all frame data needs to be uploaded (hwupload) to hardware surfaces connected to the appropriate device before being used and then downloaded (hwdownload) back to normal memory. Note that hwupload will upload to a frame with the same layout as the software frame, so it may be necessary to add a format filter immediately before to get the input into the right format and hwdownload does not support all formats on the output - it is usually necessary to insert an additional format filter immediately following in the graph to get the output in a supported format.\n\nApply an average blur filter, implemented on the GPU using Vulkan.\n\nThe filter accepts the following options:\n\nBlend two Vulkan frames into each other.\n\nThe filter takes two input streams and outputs one stream, the first input is the \"top\" layer and second input is \"bottom\" layer. By default, the output terminates when the longest input terminates.\n\nA description of the accepted options follows.\n\nDeinterlacer using bwdif, the \"Bob Weaver Deinterlacing Filter\" algorithm, implemented on the GPU using Vulkan.\n\nIt accepts the following parameters:\n\nApply an effect that emulates chromatic aberration. Works best with RGB inputs, but provides a similar effect with YCbCr inputs too.\n\nVideo source that creates a Vulkan frame of a solid color. Useful for benchmarking, or overlaying.\n\nIt accepts the following parameters:\n\nFlips an image along both the vertical and horizontal axis.\n\nThe filter accepts the following options:\n\nDenoise frames using Non-Local Means algorithm, implemented on the GPU using Vulkan. Supports more pixel formats than nlmeans or nlmeans_opencl, including alpha channel support.\n\nThe filter accepts the following options.\n\nOverlay one video on top of another.\n\nIt takes two inputs and has one output. The first input is the \"main\" video on which the second input is overlaid. This filter requires all inputs to use the same pixel format. So, format conversion may be needed.\n\nThe filter accepts the following options:\n\nTranspose rows with columns in the input video and optionally flip it. For more in depth examples see the transpose video filter, which shares mostly the same options.\n\nIt accepts the following parameters:\n\nTranspose rows with columns in the input video and optionally flip it. For more in depth examples see the transpose video filter, which shares mostly the same options.\n\nIt accepts the following parameters:\n\nBelow is a description of the currently available QSV video filters.\n\nTo enable compilation of these filters you need to configure FFmpeg with or .\n\nTo use QSV filters, you need to setup the QSV device correctly. For more information, please read https://trac.ffmpeg.org/wiki/Hardware/QuickSync\n\nThis is the QSV variant of the hstack filter, each input stream may have different height, this filter will scale down/up each input stream while keeping the original aspect.\n\nIt accepts the following options:\n\nThis is the QSV variant of the vstack filter, each input stream may have different width, this filter will scale down/up each input stream while keeping the original aspect.\n\nIt accepts the following options:\n\nThis is the QSV variant of the xstack filter.\n\nIt accepts the following options:\n\nBelow is a description of the currently available video sources.\n\nBuffer video frames, and make them available to the filter chain.\n\nThis source is mainly intended for a programmatic use, in particular through the interface defined in .\n\nIt accepts the following parameters:\n\nwill instruct the source to accept video frames with size 320x240 and with format \"yuv410p\", assuming 1/24 as the timestamps timebase and square pixels (1:1 sample aspect ratio). Since the pixel format with name \"yuv410p\" corresponds to the number 6 (check the enum AVPixelFormat definition in ), this example corresponds to:\n\nAlternatively, the options can be specified as a flat string, but this syntax is deprecated:\n\nThe initial state of the cellular automaton can be defined through the and options. If such options are not specified an initial state is created randomly.\n\nAt each new frame a new row in the video is filled with the result of the cellular automaton next generation. The behavior when the whole frame is filled is defined by the option.\n\nThis source accepts the following options:\n• Read the initial state from , and specify an output of size 200x400.\n• Generate a random initial row with a width of 200 cells, with a fill ratio of 2/3:\n• Create a pattern generated by rule 18 starting by a single alive cell centered on an initial row with width 100:\n\nVideo source generated on GPU using Apple’s CoreImage API on OSX.\n\nThis video source is a specialized version of the coreimage video filter. Use a core image generator at the beginning of the applied filterchain to generate the content.\n\nThe coreimagesrc video source accepts the following options:\n\nAdditionally, all options of the coreimage video filter are accepted. A complete filterchain can be used for further processing of the generated input without CPU-HOST transfer. See coreimage documentation and examples for details.\n• Use CIQRCodeGenerator to create a QR code for the FFmpeg homepage, given as complete and escaped command-line for Apple’s standard bash shell: This example is equivalent to the QRCode example of coreimage without the need for a nullsrc video source.\n\nThe filter exclusively returns D3D11 Hardware Frames, for on-gpu encoding or processing. So an explicit hwdownload is needed for any kind of software processing.\n\nIt accepts the following options:\n\nYou can also skip the lavfi device and directly use the filter. Also demonstrates downloading the frame and encoding with libx264. Explicit output format specification is required in this case:\n\nIf you want to capture only a subsection of the desktop, this can be achieved by specifying a smaller size and its offsets into the screen:\n\nThis source supports the some above options as commands.\n\nGenerate a Mandelbrot set fractal, and progressively zoom towards the point specified with and .\n\nThis source accepts the following options:\n\nGenerate various test patterns, as generated by the MPlayer test filter.\n\nThe size of the generated video is fixed, and is 256x256. This source is useful in particular for testing encoding features.\n\nThis source accepts the following options:\n\nTo enable compilation of this filter you need to install the frei0r header and configure FFmpeg with .\n\nThis source accepts the following parameters:\n\nFor example, to generate a frei0r partik0l source with size 200x200 and frame rate 10 which is overlaid on the overlay filter main input:\n\nThis source is based on a generalization of John Conway’s life game.\n\nThe sourced input represents a life grid, each pixel represents a cell which can be in one of two possible states, alive or dead. Every cell interacts with its eight neighbours, which are the cells that are horizontally, vertically, or diagonally adjacent.\n\nAt each interaction the grid evolves according to the adopted rule, which specifies the number of neighbor alive cells which will make a cell stay alive or born. The option allows one to specify the rule to adopt.\n\nThis source accepts the following options:\n• Read a grid from , and center it on a grid of size 300x300 pixels:\n• Generate a random grid of size 200x200, with a fill ratio of 2/3:\n• Full example with slow death effect (mold) using :\n\nPerlin noise is a kind of noise with local continuity in space. This can be used to generate patterns with continuity in space and time, e.g. to simulate smoke, fluids, or terrain.\n\nIn case more than one octave is specified through the option, Perlin noise is generated as a sum of components, each one with doubled frequency. In this case the option specify the ratio of the amplitude with respect to the previous component. More octave components enable to specify more high frequency details in the generated noise (e.g. small size variations due to boulders in a generated terrain).\n• Use Perlin noise with 7 components, each one with a halved contribution to total amplitude:\n• Chain Perlin noise with the lutyuv to generate a black&white effect:\n• Stretch noise along the y axis, and convert gray level to red-only signal:\n\nGenerate a QR code using the libqrencode library (see https://fukuchi.org/works/qrencode/).\n\nTo enable the compilation of this source, you need to configure FFmpeg with .\n\nThe QR code is generated from the provided text or text pattern. The corresponding QR code is scaled and put in the video output according to the specified output size options.\n\nIn case no text is specified, the QR code is not generated, but an empty colored output is returned instead.\n\nThis source accepts the following options:\n• Generate a QR code encoding the specified text with the default size:\n• Same as below, but select blue on pink colors:\n• Generate a QR code with width of 200 pixels and padding, making the padded width 4/3 of the QR code width:\n• Generate a QR code with padded width of 200 pixels and padding, making the QR code width 3/4 of the padded width:\n\nThe source returns frames of size 4096x4096 of all rgb colors.\n\nThe source returns frames of size 4096x4096 of all yuv colors.\n\nThe source provides an uniformly colored input.\n\nThe source provides an identity Hald CLUT. See also haldclut filter.\n\nThe source returns unprocessed video frames. It is mainly useful to be employed in analysis / debugging tools, or as the source for filters which ignore the input data.\n\nThe source generates a color bars pattern, based on EBU PAL recommendations with 75% color levels.\n\nThe source generates a color bars pattern, based on EBU PAL recommendations with 100% color levels.\n\nThe source generates an RGB test pattern useful for detecting RGB vs BGR issues. You should see a red, green and blue stripe from top to bottom.\n\nThe source generates a color bars pattern, based on the SMPTE Engineering Guideline EG 1-1990.\n\nThe source generates a color bars pattern, based on the SMPTE RP 219-2002.\n\nThe source generates a test video pattern, showing a color pattern, a scrolling gradient and a timestamp. This is mainly intended for testing purposes.\n\nThe source is similar to testsrc, but supports more pixel formats instead of just . This allows using it as an input for other tests without requiring a format conversion.\n\nThe source generates an YUV test pattern. You should see a y, cb and cr stripe from top to bottom.\n\nThe sources accept the following parameters:\n• Generate a video with a duration of 5.3 seconds, with size 176x144 and a frame rate of 10 frames per second:\n• The following graph description will generate a red source with an opacity of 0.2, with size \"qcif\" and a frame rate of 10 frames per second:\n• If the input content is to be ignored, can be used. The following command generates noise in the luma plane by employing the filter:\n\nThe source supports the following commands:\n\nFor details of how the program loading works, see the program_opencl filter.\n• Generate a colour ramp by setting pixel values from the position of the pixel in the output image. (Note that this will work with all pixel formats, but the generated output will not be the same.)\n• Generate a Sierpinski carpet pattern, panning by a single pixel each frame. __kernel void sierpinski_carpet(__write_only image2d_t dst, unsigned int index) { int2 loc = (int2)(get_global_id(0), get_global_id(1)); float4 value = 0.0f; int x = loc.x + index; int y = loc.y + index; while (x > 0 || y > 0) { if (x % 3 == 1 && y % 3 == 1) { value = 1.0f; break; } x /= 3; y /= 3; } write_imagef(dst, loc, value); }\n\nThis source accepts the following options:\n\nThis source accepts the following options:\n\nThis source supports the some above options as commands.\n\nBelow is a description of the currently available video sinks.\n\nBuffer video frames, and make them available to the end of the filter graph.\n\nThis sink is mainly intended for programmatic use, in particular through the interface defined in or the options system.\n\nIt accepts a pointer to an AVBufferSinkContext structure, which defines the incoming buffers’ formats, to be passed as the opaque parameter to for initialization.\n\nNull video sink: do absolutely nothing with the input video. It is mainly useful as a template and for use in analysis / debugging tools.\n\nBelow is a description of the currently available multimedia filters.\n\nThe filter accepts the following options:\n\nFilter supports the some above options as commands.\n\nThe filter accepts the following options:\n\nThe filter accepts the following options:\n\nMeasures phase of input audio, which is exported as metadata , representing mean phase of current audio frame. A video output can also be produced and is enabled by default. The audio is passed through as first output.\n\nAudio will be rematrixed to stereo if it has a different channel layout. Phase value is in range where means left and right channels are completely out of phase and means channels are in phase.\n\nThe filter accepts the following options, all related to its video output:\n\nThe filter also detects out of phase and mono sequences in stereo streams. It logs the sequence start, end and duration when it lasts longer or as long as the minimum set.\n\nThe filter accepts the following options for this detection:\n• Complete example with to detect 1 second of mono with 0.001 phase tolerance:\n\nThe filter is used to measure the difference between channels of stereo audio stream. A monaural signal, consisting of identical left and right signal, results in straight vertical line. Any stereo separation is visible as a deviation from this line, creating a Lissajous figure. If the straight (or deviation from it) but horizontal line appears this indicates that the left and right channels are out of phase.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands except options and .\n\nThe filter accepts the following options:\n\nConcatenate audio and video streams, joining them together one after the other.\n\nThe filter works on segments of synchronized video and audio streams. All segments must have the same number of streams of each type, and that will also be the number of streams at output.\n\nThe filter accepts the following options:\n\nThe filter has + outputs: first video outputs, then audio outputs.\n\nThere are x( + ) inputs: first the inputs for the first segment, in the same order as the outputs, then the inputs for the second segment, etc.\n\nRelated streams do not always have exactly the same duration, for various reasons including codec frame size or sloppy authoring. For that reason, related synchronized streams (e.g. a video and its audio track) should be concatenated at once. The concat filter will use the duration of the longest stream in each segment (except the last one), and if necessary pad shorter audio streams with silence.\n\nFor this filter to work correctly, all segments must start at timestamp 0.\n\nAll corresponding streams must have the same parameters in all segments; the filtering system will automatically select a common pixel format for video streams, and a common sample format, sample rate and channel layout for audio streams, but other settings, such as resolution, must be converted explicitly by the user.\n\nDifferent frame rates are acceptable but will result in variable frame rate at output; be sure to configure the output file to handle it.\n• Concatenate an opening, an episode and an ending, all in bilingual version (video in stream 0, audio in streams 1 and 2):\n• Concatenate two parts, handling audio and video separately, using the (a)movie sources, and adjusting the resolution: Note that a desync will happen at the stitch if the audio and video streams do not have exactly the same duration in the first file.\n\nThis filter supports the following commands:\n\nEBU R128 scanner filter. This filter takes an audio stream and analyzes its loudness level. By default, it logs a message at a frequency of 10Hz with the Momentary loudness (identified by ), Short-term loudness ( ), Integrated loudness ( ) and Loudness Range ( ).\n\nThe filter can only analyze streams which have sample format is double-precision floating point. The input stream will be converted to this specification, if needed. Users may need to insert aformat and/or aresample filters after this filter to obtain the original parameters.\n\nThe filter also has a video output (see the option) with a real time graph to observe the loudness evolution. The graphic contains the logged message mentioned above, so it is not printed anymore when this option is set, unless the verbose logging is set. The main graphing area contains the short-term loudness (3 seconds of analysis), and the gauge on the right is for the momentary loudness (400 milliseconds), but can optionally be configured to instead display short-term loudness (see ).\n\nThe green area marks a +/- 1LU target range around the target loudness (-23LUFS by default, unless modified through ).\n\nMore information about the Loudness Recommendation EBU R128 on http://tech.ebu.ch/loudness.\n\nThe filter accepts the following options:\n\nThese filters read frames from several inputs and send the oldest queued frame to the output.\n\nInput streams must have well defined, monotonically increasing frame timestamp values.\n\nIn order to submit one frame to output, these filters need to enqueue at least one frame for each input, so they cannot work in case one input is not yet terminated and will not receive incoming frames.\n\nFor example consider the case when one input is a filter which always drops input frames. The filter will keep reading from that input, but it will never be able to send new frames to output until the input sends an end-of-stream signal.\n\nAlso, depending on inputs synchronization, the filters will drop frames in case one input receives more frames than the other ones, and the queue is already filled.\n\nThese filters accept the following options:\n• Interleave frames belonging to different streams using :\n\nReport previous filter filtering latency, delay in number of audio samples for audio filters or number of video frames for video filters.\n\nOn end of input stream, filter will report min and max measured latency for previous running filter in filtergraph.\n\nThis filter accepts the following options:\n• Print all metadata values for frames with key with values between 0 and 1.\n• Direct all metadata to a pipe with file descriptor 4.\n\nThese filters are mainly aimed at developers to test direct path in the following filter in the filtergraph.\n\nThe filters accept the following options:\n\nNote: in case of auto-inserted filter between the permission filter and the following one, the permission might not be received as expected in that following filter. Inserting a format or aformat filter before the perms/aperms filter can avoid this problem.\n\nThese filters will pause the filtering for a variable amount of time to match the output rate with the input timestamps. They are similar to the option to .\n\nThey accept the following options:\n\nBoth filters supports the all above options as commands.\n\nThis filter does opposite of concat filters.\n\nThis filter accepts the following options:\n\nIn all cases, prefixing an each segment with ’+’ will make it relative to the previous segment.\n• Split input audio stream into three output audio streams, starting at start of input audio stream and storing that in 1st output audio stream, then following at 60th second and storing than in 2nd output audio stream, and last after 150th second of input audio stream store in 3rd output audio stream:\n\nThis filter accepts the following options:\n\nThe expression can contain the following constants:\n\nThe default value of the select expression is \"1\".\n• Select all frames in input: The example above is the same as:\n• Select only frames contained in the 10-20 time interval:\n• Select only I-frames contained in the 10-20 time interval:\n• Use aselect to select only audio frames with samples number > 100:\n• Create a mosaic of the first scenes: Comparing against a value between 0.3 and 0.5 is generally a sane choice.\n• Send even and odd frames to separate outputs, and compose them:\n• Select useful frames from an ffconcat file which is using inpoints and outpoints but where the source files are not intra frame only.\n\nSend commands to filters in the filtergraph.\n\nThese filters read commands to be sent to other filters in the filtergraph.\n\nmust be inserted between two video filters, must be inserted between two audio filters, but apart from that they act the same way.\n\nThe specification of commands can be provided in the filter arguments with the option, or in a file specified by the option.\n\nThese filters accept the following options:\n\nA commands description consists of a sequence of interval specifications, comprising a list of commands to be executed when a particular event related to that interval occurs. The occurring event is typically the current frame time entering or leaving a given time interval.\n\nAn interval is specified by the following syntax:\n\nThe time interval is specified by the and times. is optional and defaults to the maximum time.\n\nThe current frame time is considered within the specified interval if it is included in the interval [ , ), that is when the time is greater or equal to and is lesser than .\n\nconsists of a sequence of one or more command specifications, separated by \",\", relating to that interval. The syntax of a command specification is given by:\n\nis optional and specifies the type of events relating to the time interval which enable sending the specified command, and must be a non-null sequence of identifier flags separated by \"+\" or \"|\" and enclosed between \"[\" and \"]\".\n\nThe following flags are recognized:\n\nIf is not specified, a default value of is assumed.\n\nspecifies the target of the command, usually the name of the filter class or a specific filter instance name.\n\nspecifies the name of the command for the target filter.\n\nis optional and specifies the optional list of argument for the given .\n\nBetween one interval specification and another, whitespaces, or sequences of characters starting with until the end of line, are ignored and can be used to annotate comments.\n\nA simplified BNF description of the commands specification syntax follows:\n• Specify audio tempo change at second 4:\n• Specify a list of drawtext and hue commands in a file. # show text in the interval 5-10 5.0-10.0 [enter] drawtext reinit 'fontfile=FreeSerif.ttf:text=hello world', [leave] drawtext reinit 'fontfile=FreeSerif.ttf:text='; # desaturate the image in the interval 15-20 15.0-20.0 [enter] hue s 0, [enter] drawtext reinit 'fontfile=FreeSerif.ttf:text=nocolor', [leave] hue s 1, [leave] drawtext reinit 'fontfile=FreeSerif.ttf:text=color'; # apply an exponential saturation fade-out effect, starting from time 25 25 [enter] hue s exp(25-t) A filtergraph allowing to read and process the above command list stored in a file , can be specified with:\n\nChange the PTS (presentation timestamp) of the input frames.\n\nThis filter accepts the following options:\n\nThe expression is evaluated through the eval API and can contain the following constants:\n• Set fixed rate of 25 frames per second:\n• Apply an offset of 10 seconds to the input PTS:\n• Generate timestamps from a \"live source\" and rebase onto the current timebase:\n\nBoth filters support all above options as commands.\n\nThe filter marks the color range property for the output frames. It does not change the input frame, but only sets the corresponding property, which affects how the frame is treated by following filters.\n\nThe filter accepts the following options:\n\nSet the timebase to use for the output frames timestamps. It is mainly useful for testing timebase configuration.\n\nIt accepts the following parameters:\n\nThe value for is an arithmetic expression representing a rational. The expression can contain the constants \"AVTB\" (the default timebase), \"intb\" (the input timebase) and \"sr\" (the sample rate, audio only). Default value is \"intb\".\n\nConvert input audio to a video output representing frequency spectrum logarithmically using Brown-Puckette constant Q transform algorithm with direct frequency domain coefficient calculation (but the transform itself is not really constant Q, instead the Q factor is actually variable/clamped), with musical tone scale, from E0 to D#10.\n\nThe filter accepts the following options:\n• Same as above, but with frame rate 30 fps:\n• Same as above, but with more accuracy in frequency domain:\n• Custom gamma, now spectrum is linear to the amplitude.\n• Custom fontcolor and fontfile, C-note is colored green, others are colored blue:\n\nConvert input audio to video output representing frequency spectrum using Continuous Wavelet Transform and Morlet wavelet.\n\nThe filter accepts the following options:\n\nConvert input audio to video output representing the audio power spectrum. Audio amplitude is on Y-axis while frequency is on X-axis.\n\nThe filter accepts the following options:\n\nConvert stereo input audio to a video output, representing the spatial relationship between two channels.\n\nThe filter accepts the following options:\n\nThe filter accepts the following options:\n\nThe usage is very similar to the showwaves filter; see the examples in that section.\n• Complete example for a colored and sliding spectrum per channel using :\n\nThe filter accepts the following options:\n• Extract an audio spectrogram of a whole audio track in a 1024x1024 picture using :\n\nThe filter accepts the following options:\n\nThe filter accepts the following options:\n• Output the input file audio and the corresponding video representation at the same time:\n• Create a synthetic signal and show it with showwaves, forcing a frame rate of 30 frames per second:\n\nThe filter accepts the following options:\n• Extract a channel split representation of the wave form of a whole audio track in a 1024x800 picture using :\n\nDelete frame side data, or select frames based on it.\n\nThis filter accepts the following options:\n\nSynthesize audio from 2 input video spectrums, first input stream represents magnitude across time and second represents phase across time. The filter will transform from frequency domain as displayed in videos back to time domain as presented in audio output.\n\nThis filter is primarily created for reversing processed showspectrum filter outputs, but can synthesize sound from other spectrograms too. But in such case results are going to be poor if the phase data is not available, because in such cases phase data need to be recreated, usually it’s just recreated from random noise. For best results use gray only output ( color mode in showspectrum filter) and scale for magnitude video and scale for phase video. To produce phase, for 2nd video, use option. Inputs videos should generally use slide mode as that saves resources needed for decoding video.\n\nThe filter accepts the following options:\n• First create magnitude and phase videos from audio, assuming audio is stereo with 44100 sample rate, then resynthesize videos back to audio with spectrumsynth:\n\nThe filter accepts a single parameter which specifies the number of outputs. If unspecified, it defaults to 2.\n• Create two separate outputs from the same input:\n• To create 3 or more outputs, you need to specify the number of outputs, like in:\n• Create two separate outputs from the same input, one cropped and one padded:\n• Create 5 copies of the input audio with :\n\nReceive commands sent through a libzmq client, and forward them to filters in the filtergraph.\n\nand work as a pass-through filters. must be inserted between two video filters, between two audio filters. Both are capable to send messages to any filter type.\n\nTo enable these filters you need to install the libzmq library and headers and configure FFmpeg with .\n\nFor more information about libzmq see: http://www.zeromq.org/\n\nThe and filters work as a libzmq server, which receives messages sent through a network interface defined by the (or the abbreviation \" \") option. Default value of this option is . You may want to alter this value to your needs, but do not forget to escape any ’:’ signs (see filtergraph escaping).\n\nThe received message must be in the form:\n\nspecifies the target of the command, usually the name of the filter class or a specific filter instance name. The default filter instance name uses the pattern ‘ ’, but you can override this by using the ‘ ’ syntax (see Filtergraph syntax).\n\nspecifies the name of the command for the target filter.\n\nis optional and specifies the optional argument list for the given .\n\nUpon reception, the message is processed and the corresponding command is injected into the filtergraph. Depending on the result, the filter will send a reply to the client, adopting the format:\n\nLook at for an example of a zmq client which can be used to send commands processed by these filters.\n\nConsider the following filtergraph generated by . In this example the last overlay filter has an instance name. All other filters will have default instance names.\n\nTo change the color of the left side of the video, the following command can be used:\n\nTo change the right side:\n\nTo change the position of the right side:\n\nBelow is a description of the currently available multimedia sources.\n\nThis is the same as movie source, except it selects an audio stream by default.\n\nGenerated stream periodically shows flash video frame and emits beep in audio. Useful to inspect A/V sync issues.\n\nIt accepts the following options:\n\nThis source supports the some above options as commands.\n\nIt accepts the following parameters:\n\nIt allows overlaying a second video on top of the main input of a filtergraph, as shown in this graph:\n• Skip 3.2 seconds from the start of the AVI file in.avi, and overlay it on top of the input labelled \"in\": movie=in.avi:seek_point=3.2, scale=180:-1, setpts=PTS-STARTPTS [over]; [in] setpts=PTS-STARTPTS [main]; [main][over] overlay=16:16 [out]\n• Read from a video4linux2 device, and overlay it on top of the input labelled \"in\": movie=/dev/video0:f=video4linux2, scale=180:-1, setpts=PTS-STARTPTS [over]; [in] setpts=PTS-STARTPTS [main]; [main][over] overlay=16:16 [out]\n• Read the first video stream and the audio stream with id 0x81 from dvd.vob; the video is connected to the pad named \"video\" and the audio is connected to the pad named \"audio\":\n\nBoth movie and amovie support the following commands:\n\nFor details about the authorship, see the Git history of the project (https://git.ffmpeg.org/ffmpeg), e.g. by typing the command in the FFmpeg source directory, or browsing the online repository at https://git.ffmpeg.org/ffmpeg.\n\nMaintainers for the specific components are listed in the file in the source code tree.\n\nThis document was generated on March 23, 2025 using makeinfo."
    },
    {
        "link": "https://ffmpeg.org/ffmpeg-all.html",
        "document": "is a universal media converter. It can read a wide variety of inputs - including live grabbing/recording devices - filter, and transcode them into a plethora of output formats.\n\nreads from an arbitrary number of inputs (which can be regular files, pipes, network streams, grabbing devices, etc.), specified by the option, and writes to an arbitrary number of outputs, which are specified by a plain output url. Anything found on the command line which cannot be interpreted as an option is considered to be an output url.\n\nEach input or output can, in principle, contain any number of elementary streams of different types (video/audio/subtitle/attachment/data), though the allowed stream counts and/or types may be limited by the container format. Selecting which streams from which inputs will go into which output is either done automatically or with the option (see the Stream selection chapter).\n\nTo refer to inputs/outputs in options, you must use their indices (0-based). E.g. the first input is , the second is , etc. Similarly, streams within an input/output are referred to by their indices. E.g. refers to the fourth stream in the third input or output. Also see the Stream specifiers chapter.\n\nAs a general rule, options are applied to the next specified file. Therefore, order is important, and you can have the same option on the command line multiple times. Each occurrence is then applied to the next input or output file. Exceptions from this rule are the global options (e.g. verbosity level), which should be specified first.\n\nDo not mix input and output files – first specify all input files, then all output files. Also do not mix options which belong to different files. All options apply ONLY to the next input or output file and are reset between files.\n• Convert an input media file to a different format, by re-encoding media streams:\n• Set the video bitrate of the output file to 64 kbit/s:\n• Force the frame rate of the output file to 24 fps:\n• Force the frame rate of the input file (valid for raw formats only) to 1 fps and the frame rate of the output file to 24 fps:\n\nThe format option may be needed for raw input files.\n\nbuilds a transcoding pipeline out of the components listed below. The program’s operation then consists of input data chunks flowing from the sources down the pipes towards the sinks, while being transformed by the components they encounter along the way.\n\nThe following kinds of components are available:\n• Demuxers (short for \"demultiplexers\") read an input source in order to extract\n• global properties such as metadata or chapters;\n• list of input elementary streams and their properties One demuxer instance is created for each option, and sends encoded packets to decoders or muxers. In other literature, demuxers are sometimes called splitters, because their main function is splitting a file into elementary streams (though some files only contain one elementary stream). A schematic representation of a demuxer looks like this: ┌──────────┬───────────────────────┐ │ demuxer │ │ packets for stream 0 ╞══════════╡ elementary stream 0 ├──────────────────────► │ │ │ │ global ├───────────────────────┤ │properties│ │ packets for stream 1 │ and │ elementary stream 1 ├──────────────────────► │ metadata │ │ │ ├───────────────────────┤ │ │ │ │ │ ........... │ │ │ │ │ ├───────────────────────┤ │ │ │ packets for stream N │ │ elementary stream N ├──────────────────────► │ │ │ └──────────┴───────────────────────┘ ▲ │ │ read from file, network stream, │ grabbing device, etc. │\n• Decoders receive encoded (compressed) packets for an audio, video, or subtitle elementary stream, and decode them into raw frames (arrays of pixels for video, PCM for audio). A decoder is typically associated with (and receives its input from) an elementary stream in a demuxer, but sometimes may also exist on its own (see Loopback decoders). A schematic representation of a decoder looks like this:\n• Filtergraphs process and transform raw audio or video frames. A filtergraph consists of one or more individual filters linked into a graph. Filtergraphs come in two flavors - simple and complex, configured with the and options, respectively. A simple filtergraph is associated with an output elementary stream; it receives the input to be filtered from a decoder and sends filtered output to that output stream’s encoder. A simple video filtergraph that performs deinterlacing (using the deinterlacer) followed by resizing (using the filter) can look like this: ┌────────────────────────┐ │ simple filtergraph │ frames from ╞════════════════════════╡ frames for a decoder │ ┌───────┐ ┌───────┐ │ an encoder ────────────►├─►│ yadif ├─►│ scale ├─►│────────────► │ └───────┘ └───────┘ │ └────────────────────────┘ A complex filtergraph is standalone and not associated with any specific stream. It may have multiple (or zero) inputs, potentially of different types (audio or video), each of which receiving data either from a decoder or another complex filtergraph’s output. It also has one or more outputs that feed either an encoder or another complex filtergraph’s input. The following example diagram represents a complex filtergraph with 3 inputs and 2 outputs (all video): Frames from second input are overlaid over those from the first. Frames from the third input are rescaled, then the duplicated into two identical streams. One of them is overlaid over the combined first two inputs, with the result exposed as the filtergraph’s first output. The other duplicate ends up being the filtergraph’s second output.\n• Encoders receive raw audio, video, or subtitle frames and encode them into encoded packets. The encoding (compression) process is typically lossy - it degrades stream quality to make the output smaller; some encoders are lossless, but at the cost of much higher output size. A video or audio encoder receives its input from some filtergraph’s output, subtitle encoders receive input from a decoder (since subtitle filtering is not supported yet). Every encoder is associated with some muxer’s output elementary stream and sends its output to that muxer. A schematic representation of an encoder looks like this:\n• Muxers (short for \"multiplexers\") receive encoded packets for their elementary streams from encoders (the transcoding path) or directly from demuxers (the streamcopy path), interleave them (when there is more than one elementary stream), and write the resulting bytes into the output file (or pipe, network stream, etc.). A schematic representation of a muxer looks like this: ┌──────────────────────┬───────────┐ packets for stream 0 │ │ muxer │ ──────────────────────►│ elementary stream 0 ╞═══════════╡ │ │ │ ├──────────────────────┤ global │ packets for stream 1 │ │properties │ ──────────────────────►│ elementary stream 1 │ and │ │ │ metadata │ ├──────────────────────┤ │ │ │ │ │ ........... │ │ │ │ │ ├──────────────────────┤ │ packets for stream N │ │ │ ──────────────────────►│ elementary stream N │ │ │ │ │ └──────────────────────┴─────┬─────┘ │ write to file, network stream, │ grabbing device, etc. │ │ ▼\n\nThe simplest pipeline in is single-stream streamcopy, that is copying one input elementary stream’s packets without decoding, filtering, or encoding them. As an example, consider an input file called with 3 elementary streams, from which we take the second and write it to file . A schematic representation of such a pipeline looks like this:\n\nThe above pipeline can be constructed with the following commandline:\n• there are no input options for this input;\n• there are two output options for this output:\n• selects the input stream to be used - from input with index 0 (i.e. the first one) the stream with index 1 (i.e. the second one);\n• selects the encoder, i.e. streamcopy with no decoding or encoding.\n\nStreamcopy is useful for changing the elementary stream count, container format, or modifying container-level metadata. Since there is no decoding or encoding, it is very fast and there is no quality loss. However, it might not work in some cases because of a variety of factors (e.g. certain information required by the target container is not available in the source). Applying filters is obviously also impossible, since filters work on decoded frames.\n\nMore complex streamcopy scenarios can be constructed - e.g. combining streams from two input files into a single output:\n\nthat can be built by the commandline\n\nThe output option is used twice here, creating two streams in the output file - one fed by the first input and one by the second. The single instance of the option selects streamcopy for both of those streams. You could also use multiple instances of this option together with Stream specifiers to apply different values to each stream, as will be demonstrated in following sections.\n\nA converse scenario is splitting multiple streams from a single input into multiple outputs:\n\nNote how a separate instance of the option is needed for every output file even though their values are the same. This is because non-global options (which is most of them) only apply in the context of the file before which they are placed.\n\nThese examples can of course be further generalized into arbitrary remappings of any number of inputs into any number of outputs.\n\nTranscoding is the process of decoding a stream and then encoding it again. Since encoding tends to be computationally expensive and in most cases degrades the stream quality (i.e. it is lossy), you should only transcode when you need to and perform streamcopy otherwise. Typical reasons to transcode are:\n• you want to feed the stream to something that cannot decode the original codec.\n\nNote that will transcode all audio, video, and subtitle streams unless you specify for them.\n\nConsider an example pipeline that reads an input file with one audio and one video stream, transcodes the video and copies the audio into a single output file. This can be schematically represented as follows\n\nand implemented with the following commandline:\n\nNote how it uses stream specifiers and to select input streams and apply different values of the option to them; see the Stream specifiers section for more details.\n\nWhen transcoding, audio and video streams can be filtered before encoding, with either a simple or complex filtergraph.\n\nSimple filtergraphs are those that have exactly one input and output, both of the same type (audio or video). They are configured with the per-stream option (with and aliases for (video) and (audio) respectively). Note that simple filtergraphs are tied to their output stream, so e.g. if you have multiple audio streams, will create a separate filtergraph for each one.\n\nTaking the trancoding example from above, adding filtering (and omitting audio, for clarity) makes it look like this:\n\nComplex filtergraphs are those which cannot be described as simply a linear processing chain applied to one stream. This is the case, for example, when the graph has more than one input and/or output, or when output stream type is different from input. Complex filtergraphs are configured with the option. Note that this option is global, since a complex filtergraph, by its nature, cannot be unambiguously associated with a single stream or file. Each instance of creates a new complex filtergraph, and there can be any number of them.\n\nA trivial example of a complex filtergraph is the filter, which has two video inputs and one video output, containing one video overlaid on top of the other. Its audio counterpart is the filter.\n\nWhile decoders are normally associated with demuxer streams, it is also possible to create \"loopback\" decoders that decode the output from some encoder and allow it to be fed back to complex filtergraphs. This is done with the directive, which takes as a parameter the index of the output stream that should be decoded. Every such directive creates a new loopback decoder, indexed with successive integers starting at zero. These indices should then be used to refer to loopback decoders in complex filtergraph link labels, as described in the documentation for .\n\nDecoding AVOptions can be passed to loopback decoders by placing them before , analogously to input/output options.\n\nE.g. the following example:\n• (line 2) encodes it with at low quality;\n• (line 4) places decoded video side by side with the original input video;\n• (line 5) combined video is then losslessly encoded and written into .\n\nSuch a transcoding pipeline can be represented with the following diagram:\n\nprovides the option for manual control of stream selection in each output file. Users can skip and let ffmpeg perform automatic stream selection as described below. The options can be used to skip inclusion of video, audio, subtitle and data streams respectively, whether manually mapped or automatically selected, except for those streams which are outputs of complex filtergraphs.\n\nThe sub-sections that follow describe the various rules that are involved in stream selection. The examples that follow next show how these rules are applied in practice.\n\nWhile every effort is made to accurately reflect the behavior of the program, FFmpeg is under continuous development and the code may have changed since the time of this writing.\n\nIn the absence of any map options for a particular output file, ffmpeg inspects the output format to check which type of streams can be included in it, viz. video, audio and/or subtitles. For each acceptable stream type, ffmpeg will pick one stream, when available, from among all the inputs.\n\nIt will select that stream based upon the following criteria:\n• for video, it is the stream with the highest resolution,\n• for audio, it is the stream with the most channels,\n• for subtitles, it is the first subtitle stream found but there’s a caveat. The output format’s default subtitle encoder can be either text-based or image-based, and only a subtitle stream of the same type will be chosen.\n\nIn the case where several streams of the same type rate equally, the stream with the lowest index is chosen.\n\nData or attachment streams are not automatically selected and can only be included using .\n\nWhen is used, only user-mapped streams are included in that output file, with one possible exception for filtergraph outputs described below.\n\nIf there are any complex filtergraph output streams with unlabeled pads, they will be added to the first output file. This will lead to a fatal error if the stream type is not supported by the output format. In the absence of the map option, the inclusion of these streams leads to the automatic stream selection of their types being skipped. If map options are present, these filtergraph streams are included in addition to the mapped streams.\n\nComplex filtergraph output streams with labeled pads must be mapped once and exactly once.\n\nStream handling is independent of stream selection, with an exception for subtitles described below. Stream handling is set via the option addressed to streams within a specific output file. In particular, codec options are applied by ffmpeg after the stream selection process and thus do not influence the latter. If no option is specified for a stream type, ffmpeg will select the default encoder registered by the output file muxer.\n\nAn exception exists for subtitles. If a subtitle encoder is specified for an output file, the first subtitle stream found of any type, text or image, will be included. ffmpeg does not validate if the specified encoder can convert the selected stream or if the converted stream is acceptable within the output format. This applies generally as well: when the user sets an encoder manually, the stream selection process cannot check if the encoded stream can be muxed into the output file. If it cannot, ffmpeg will abort and all output files will fail to be processed.\n\nThe following examples illustrate the behavior, quirks and limitations of ffmpeg’s stream selection methods.\n\nThey assume the following three input files.\n\nThere are three output files specified, and for the first two, no options are set, so ffmpeg will select streams for these two files automatically.\n\nis a Matroska container file and accepts video, audio and subtitle streams, so ffmpeg will try to select one of each type.\n\n For video, it will select from , which has the highest resolution among all the input video streams.\n\n For audio, it will select from , since it has the greatest number of channels.\n\n For subtitles, it will select from , which is the first subtitle stream from among and .\n\naccepts only audio streams, so only from is selected.\n\nFor , since a option is set, no automatic stream selection will occur. The option will select all audio streams from the second input . No other streams will be included in this output file.\n\nFor the first two outputs, all included streams will be transcoded. The encoders chosen will be the default ones registered by each output format, which may not match the codec of the selected input streams.\n\nFor the third output, codec option for audio streams has been set to , so no decoding-filtering-encoding operations will occur, or can occur. Packets of selected streams shall be conveyed from the input file and muxed within the output file.\n\nAlthough is a Matroska container file which accepts subtitle streams, only a video and audio stream shall be selected. The subtitle stream of is image-based and the default subtitle encoder of the Matroska muxer is text-based, so a transcode operation for the subtitles is expected to fail and hence the stream isn’t selected. However, in , a subtitle encoder is specified in the command and so, the subtitle stream is selected, in addition to the video stream. The presence of disables audio stream selection for .\n\nA filtergraph is setup here using the option and consists of a single video filter. The filter requires exactly two video inputs, but none are specified, so the first two available video streams are used, those of and . The output pad of the filter has no label and so is sent to the first output file . Due to this, automatic selection of the video stream is skipped, which would have selected the stream in . The audio stream with most channels viz. in , is chosen automatically. No subtitle stream is chosen however, since the MP4 format has no default subtitle encoder registered, and the user hasn’t specified a subtitle encoder.\n\nThe 2nd output file, , only accepts text-based subtitle streams. So, even though the first subtitle stream available belongs to , it is image-based and hence skipped. The selected stream, in , is the first text-based subtitle stream.\n\nThe above command will fail, as the output pad labelled has been mapped twice. None of the output files shall be processed.\n\nThis command above will also fail as the hue filter output has a label, , and hasn’t been mapped anywhere.\n\nThe command should be modified as follows,\n\nThe video stream from is sent to the hue filter, whose output is cloned once using the split filter, and both outputs labelled. Then a copy each is mapped to the first and third output files.\n\nThe overlay filter, requiring two video inputs, uses the first two unused video streams. Those are the streams from and . The overlay output isn’t labelled, so it is sent to the first output file , regardless of the presence of the option.\n\nThe aresample filter is sent the first unused audio stream, that of . Since this filter output is also unlabelled, it too is mapped to the first output file. The presence of only suppresses automatic or manual stream selection of audio streams, not outputs sent from filtergraphs. Both these mapped streams shall be ordered before the mapped stream in .\n\nThe video, audio and subtitle streams mapped to are entirely determined by automatic stream selection.\n\nconsists of the cloned video output from the hue filter and the first audio stream from . \n\n\n\nAll the numerical options, if not specified otherwise, accept a string representing a number as input, which may be followed by one of the SI unit prefixes, for example: ’K’, ’M’, or ’G’.\n\nIf ’i’ is appended to the SI unit prefix, the complete prefix will be interpreted as a unit prefix for binary multiples, which are based on powers of 1024 instead of powers of 1000. Appending ’B’ to the SI unit prefix multiplies the value by 8. This allows using, for example: ’KB’, ’MiB’, ’G’ and ’B’ as number suffixes.\n\nOptions which do not take arguments are boolean options, and set the corresponding value to true. They can be set to false by prefixing the option name with \"no\". For example using \"-nofoo\" will set the boolean option with name \"foo\" to false.\n\nOptions that take arguments support a special syntax where the argument given on the command line is interpreted as a path to the file from which the actual argument value is loaded. To use this feature, add a forward slash ’/’ immediately before the option name (after the leading dash). E.g.\n\nwill load a filtergraph description from the file named .\n\nSome options are applied per-stream, e.g. bitrate or codec. Stream specifiers are used to precisely specify which stream(s) a given option belongs to.\n\nA stream specifier is a string generally appended to the option name and separated from it by a colon. E.g. contains the stream specifier, which matches the second audio stream. Therefore, it would select the ac3 codec for the second audio stream.\n\nA stream specifier can match several streams, so that the option is applied to all of them. E.g. the stream specifier in matches all audio streams.\n\nAn empty stream specifier matches all streams. For example, or would copy all the streams without reencoding.\n\nPossible forms of stream specifiers are:\n\nThese options are shared amongst the ff* tools.\n\nThese options are provided directly by the libavformat, libavdevice and libavcodec libraries. To see the list of available AVOptions, use the option. They are separated into two categories:\n\nFor example to write an ID3v2.3 header instead of a default ID3v2.4 to an MP3 file, use the private option of the MP3 muxer:\n\nAll codec AVOptions are per-stream, and thus a stream specifier should be attached to them:\n\nIn the above example, a multichannel audio stream is mapped twice for output. The first instance is encoded with codec ac3 and bitrate 640k. The second instance is downmixed to 2 channels and encoded with codec aac. A bitrate of 128k is specified for it using absolute index of the output stream.\n\nNote: the syntax cannot be used for boolean AVOptions, use / .\n\nNote: the old undocumented way of specifying per-stream AVOptions by prepending v/a/s to the options name is now obsolete and will be removed soon.\n\nA preset file contains a sequence of = pairs, one for each line, specifying a sequence of options which would be awkward to specify on the command line. Lines starting with the hash (’#’) character are ignored and are used to provide comments. Check the directory in the FFmpeg source tree for examples.\n\nThere are two types of preset files: ffpreset and avpreset files.\n\nffpreset files are specified with the , , , and options. The option takes the filename of the preset instead of a preset name as input and can be used for any kind of codec. For the , , and options, the options specified in a preset file are applied to the currently selected codec of the same type as the preset option.\n\nThe argument passed to the , , and preset options identifies the preset file to use according to the following rules:\n\nFirst ffmpeg searches for a file named .ffpreset in the directories (if set), and , and in the datadir defined at configuration time (usually ) or in a folder along the executable on win32, in that order. For example, if the argument is , it will search for the file .\n\nIf no such file is found, then ffmpeg will search for a file named - .ffpreset in the above-mentioned directories, where is the name of the codec to which the preset file options will be applied. For example, if you select the video codec with and use , then it will search for the file .\n\navpreset files are specified with the option. They work similar to ffpreset files, but they only allow encoder- specific options. Therefore, an = pair specifying an encoder cannot be used.\n\nWhen the option is specified, ffmpeg will look for files with the suffix .avpreset in the directories (if set), and , and in the datadir defined at configuration time (usually ), in that order.\n\nFirst ffmpeg searches for a file named - .avpreset in the above-mentioned directories, where is the name of the codec to which the preset file options will be applied. For example, if you select the video codec with and use , then it will search for the file .\n\nIf no such file is found, then ffmpeg will search for a file named .avpreset in the same directories.\n\nThe and options enable generation of a file containing statistics about the generated video outputs.\n\nThe option controls the format version of the generated file.\n\nWith version the format is:\n\nWith version the format is:\n\nThe value corresponding to each key is described below:\n\nSee also the -stats_enc options for an alternative way to show encoding statistics.\n\nIf you specify the input format and device then ffmpeg can grab video and audio directly.\n\nOr with an ALSA audio source (mono input, card id 1) instead of OSS:\n\nNote that you must activate the right video source and channel before launching ffmpeg with any TV viewer such as xawtv by Gerd Knorr. You also have to set the audio recording levels correctly with a standard mixer.\n\nGrab the X11 display with ffmpeg via\n\n0.0 is display.screen number of your X11 server, same as the DISPLAY environment variable.\n\n0.0 is display.screen number of your X11 server, same as the DISPLAY environment variable. 10 is the x-offset and 20 the y-offset for the grabbing.\n\nAny supported file format and protocol can serve as input to ffmpeg:\n• You can use YUV files as input: It will use the files: The Y files use twice the resolution of the U and V files. They are raw files, without header. They can be generated by all decent video decoders. You must specify the size of the image with the option if ffmpeg cannot guess it.\n• You can input from a raw YUV420P file: test.yuv is a file containing raw YUV planar data. Each frame is composed of the Y plane followed by the U and V planes at half vertical and horizontal resolution.\n• You can output to a raw YUV420P file:\n• You can set several input files and output files: Converts the audio file a.wav and the raw YUV video file a.yuv to MPEG file a.mpg.\n• You can also do audio and video conversions at the same time:\n• You can encode to several formats at the same time and define a mapping from input stream to output streams: Converts a.wav to a.mp2 at 64 kbits and to b.mp2 at 128 kbits. ’-map file:index’ specifies which input stream is used for each output stream, in the order of the definition of output streams.\n• You can transcode decrypted VOBs: This is a typical DVD ripping example; the input is a VOB file, the output an AVI file with MPEG-4 video and MP3 audio. Note that in this command we use B-frames so the MPEG-4 stream is DivX5 compatible, and GOP size is 300 which means one intra frame every 10 seconds for 29.97fps input video. Furthermore, the audio stream is MP3-encoded so you need to enable LAME support by passing to configure. The mapping is particularly useful for DVD transcoding to get the desired audio language. NOTE: To see the supported input formats, use .\n• You can extract images from a video, or create a video from many images: This will extract one video frame per second from the video and will output them in files named , , etc. Images will be rescaled to fit the new WxH values. If you want to extract just a limited number of frames, you can use the above command in combination with the or option, or in combination with -ss to start extracting from a certain point in time. For creating a video from many images: The syntax specifies to use a decimal number composed of three digits padded with zeroes to express the sequence number. It is the same syntax supported by the C printf function, but only formats accepting a normal integer are suitable. When importing an image sequence, -i also supports expanding shell-like wildcard patterns (globbing) internally, by selecting the image2-specific option. For example, for creating a video from filenames matching the glob pattern :\n• You can put many streams of the same type in the output: The resulting output file will contain the first four streams from the input files in reverse order.\n• The four options lmin, lmax, mblmin and mblmax use ’lambda’ units, but you may use the QP2LAMBDA constant to easily convert from ’q’ units:\n\nThis section documents the syntax and formats employed by the FFmpeg libraries and tools.\n\nFFmpeg adopts the following quoting and escaping mechanism, unless explicitly specified. The following rules are applied:\n• ‘ ’ and ‘ ’ are special characters (respectively used for quoting and escaping). In addition to them, there might be other special characters depending on the specific syntax where the escaping and quoting are employed.\n• A special character is escaped by prefixing it with a ‘ ’.\n• All characters enclosed between ‘ ’ are included literally in the parsed string. The quote character ‘ ’ itself cannot be quoted, so you may need to close the quote and escape it.\n• Leading and trailing whitespaces, unless escaped or quoted, are removed from the parsed string.\n\nNote that you may need to add a second level of escaping when using the command line or a script, which depends on the syntax of the adopted shell language.\n\nThe function defined in can be used to parse a token quoted or escaped according to the rules defined above.\n\nThe tool in the FFmpeg source tree can be used to automatically quote or escape a string in a script.\n• Escape the string containing the special character:\n• The string above contains a quote, so the needs to be escaped when quoting it:\n• Include leading or trailing whitespaces using quoting: ' this string starts and ends with whitespaces '\n• Escaping and quoting can be mixed together:\n• To include a literal ‘ ’ you can use either escaping or quoting: 'c:\\foo' can be written as c:\\\\foo\n\nIf the value is \"now\" it takes the current time.\n\nTime is local time unless Z is appended, in which case it is interpreted as UTC. If the year-month-day part is not specified it takes the current year-month-day.\n\nThere are two accepted syntaxes for expressing time duration.\n\nexpresses the number of hours, the number of minutes for a maximum of 2 digits, and the number of seconds for a maximum of 2 digits. The at the end expresses decimal value for .\n\nexpresses the number of seconds, with the optional decimal part . The optional literal suffixes ‘ ’, ‘ ’ or ‘ ’ indicate to interpret the value as seconds, milliseconds or microseconds, respectively.\n\nIn both expressions, the optional ‘ ’ indicates negative duration.\n\nThe following examples are all valid time duration:\n\nSpecify the size of the sourced video, it may be a string of the form x , or the name of a size abbreviation.\n\nThe following abbreviations are recognized:\n\nSpecify the frame rate of a video, expressed as the number of frames generated per second. It has to be a string in the format / , an integer number, a float number or a valid video frame rate abbreviation.\n\nThe following abbreviations are recognized:\n\nA ratio can be expressed as an expression, or in the form : .\n\nNote that a ratio with infinite (1/0) or negative value is considered valid, so you should check on the returned value if you want to exclude those values.\n\nThe undefined value can be expressed using the \"0:0\" string.\n\nIt can be the name of a color as defined below (case insensitive match) or a sequence, possibly followed by @ and a string representing the alpha component.\n\nThe alpha component may be a string composed by \"0x\" followed by an hexadecimal number or a decimal number between 0.0 and 1.0, which represents the opacity value (‘ ’ or ‘ ’ means completely transparent, ‘ ’ or ‘ ’ completely opaque). If the alpha component is not specified then ‘ ’ is assumed.\n\nThe string ‘ ’ will result in a random color.\n\nThe following names of colors are recognized:\n\nA channel layout specifies the spatial disposition of the channels in a multi-channel audio stream. To specify a channel layout, FFmpeg makes use of a special syntax.\n\nIndividual channels are identified by an id, as given by the table below:\n\nStandard channel layout compositions can be specified by using the following identifiers:\n\nA custom channel layout can be specified as a sequence of terms, separated by ’+’. Each term can be:\n• the name of a single channel (e.g. ‘ ’, ‘ ’, ‘ ’, ‘ ’, etc.), each optionally containing a custom name after a ’@’, (e.g. ‘ ’, ‘ ’, ‘ ’, ‘ ’, etc.)\n\nA standard channel layout can be specified by the following:\n• the name of a single channel (e.g. ‘ ’, ‘ ’, ‘ ’, ‘ ’, etc.)\n• the name of a standard channel layout (e.g. ‘ ’, ‘ ’, ‘ ’, ‘ ’, ‘ ’, etc.)\n• a number of channels, in decimal, followed by ’c’, yielding the default channel layout for that number of channels (see the function ). Note that not all channel counts have a default layout.\n• a number of channels, in decimal, followed by ’C’, yielding an unknown channel layout with the specified number of channels. Note that not all channel layout specification strings support unknown channel layouts.\n• a channel layout mask, in hexadecimal starting with \"0x\" (see the macros in .\n\nBefore libavutil version 53 the trailing character \"c\" to specify a number of channels was optional, but now it is required, while a channel layout mask can also be specified as a decimal number (if and only if not followed by \"c\" or \"C\").\n\nSee also the function defined in .\n\nWhen evaluating an arithmetic expression, FFmpeg uses an internal formula evaluator, implemented through the interface.\n\nAn expression may contain unary, binary operators, constants, and functions.\n\nTwo expressions and can be combined to form another expression \" ; \". and are evaluated in turn, and the new expression evaluates to the value of .\n\nThe following binary operators are available: , , , , .\n\nThe following unary operators are available: , .\n\nSome internal variables can be used to store and load intermediary results. They can be accessed using the and functions with an index argument varying from 0 to 9 to specify which internal variable to access.\n\nThe following functions are available:\n\nThe following constants are available:\n\nAssuming that an expression is considered \"true\" if it has a non-zero value, note that:\n\nFor example the construct:\n\nIn your C code, you can extend the list of unary and binary functions, and define recognized constants, so that they are available for your expressions.\n\nThe evaluator also recognizes the International System unit prefixes. If ’i’ is appended after the prefix, binary prefixes are used, which are based on powers of 1024 instead of powers of 1000. The ’B’ postfix multiplies the value by 8, and can be appended after a unit prefix or used alone. This allows using for example ’KB’, ’MiB’, ’G’ and ’B’ as number postfix.\n\nThe list of available International System prefixes follows, with indication of the corresponding powers of 10 and of 2.\n\nlibavcodec provides some generic global options, which can be set on all the encoders and decoders. In addition, each codec may support so-called private options, which are specific for a given codec.\n\nSometimes, a global option may only affect a specific kind of codec, and may be nonsensical or ignored by another, so you need to be aware of the meaning of the specified options. Also some options are meant only for decoding or encoding.\n\nOptions may be set by specifying - in the FFmpeg tools, or by setting the value explicitly in the options or using the API for programmatic use.\n\nDecoders are configured elements in FFmpeg which allow the decoding of multimedia streams.\n\nWhen you configure your FFmpeg build, all the supported native decoders are enabled by default. Decoders requiring an external library must be enabled manually via the corresponding option. You can list all available decoders using the configure option .\n\nYou can disable all the decoders with the configure option and selectively enable / disable single decoders with the options / .\n\nThe option of the ff* tools will display the list of enabled decoders.\n\nA description of some of the currently available video decoders follows.\n\nThe decoder supports MV-HEVC multiview streams with at most two views. Views to be output are selected by supplying a list of view IDs to the decoder (the option). This option may be set either statically before decoder init, or from the callback - useful for the case when the view count or IDs change dynamically during decoding.\n\nOnly the base layer is decoded by default.\n\nNote that if you are using the CLI tool, you should be using view specifiers as documented in its manual, rather than the options documented here.\n\nlibdav1d allows libavcodec to decode the AOMedia Video 1 (AV1) codec. Requires the presence of the libdav1d headers and library during configuration. You need to explicitly configure the build with .\n\nThe following options are supported by the libdav1d wrapper.\n\nThis decoder allows libavcodec to decode AVS2 streams with davs2 library.\n\nlibuavs3d allows libavcodec to decode AVS3 streams. Requires the presence of the libuavs3d headers and library during configuration. You need to explicitly configure the build with .\n\nThe following option is supported by the libuavs3d wrapper.\n\nThis decoder requires the presence of the libxevd headers and library during configuration. You need to explicitly configure the build with .\n\nThe xevd project website is at https://github.com/mpeg5/xevd.\n\nThe following options are supported by the libxevd wrapper. The xevd-equivalent options or values are listed in parentheses for easy migration.\n\nTo get a more accurate and extensive documentation of the libxevd options, invoke the command or consult the libxevd documentation.\n\nThe following options are supported by all qsv decoders.\n\nA description of some of the currently available audio decoders follows.\n\nThis decoder implements part of ATSC A/52:2010 and ETSI TS 102 366, as well as the undocumented RealAudio 3 (a.k.a. dnet).\n\nThis decoder aims to implement the complete FLAC specification from Xiph.\n\nThis decoder generates wave patterns according to predefined sequences. Its use is purely internal and the format of the data it accepts is not publicly documented.\n\nlibcelt allows libavcodec to decode the Xiph CELT ultra-low delay audio codec. Requires the presence of the libcelt headers and library during configuration. You need to explicitly configure the build with .\n\nlibgsm allows libavcodec to decode the GSM full rate audio codec. Requires the presence of the libgsm headers and library during configuration. You need to explicitly configure the build with .\n\nThis decoder supports both the ordinary GSM and the Microsoft variant.\n\nlibilbc allows libavcodec to decode the Internet Low Bitrate Codec (iLBC) audio codec. Requires the presence of the libilbc headers and library during configuration. You need to explicitly configure the build with .\n\nThe following option is supported by the libilbc wrapper.\n\nlibopencore-amrnb allows libavcodec to decode the Adaptive Multi-Rate Narrowband audio codec. Using it requires the presence of the libopencore-amrnb headers and library during configuration. You need to explicitly configure the build with .\n\nAn FFmpeg native decoder for AMR-NB exists, so users can decode AMR-NB without this library.\n\nlibopencore-amrwb allows libavcodec to decode the Adaptive Multi-Rate Wideband audio codec. Using it requires the presence of the libopencore-amrwb headers and library during configuration. You need to explicitly configure the build with .\n\nAn FFmpeg native decoder for AMR-WB exists, so users can decode AMR-WB without this library.\n\nlibopus allows libavcodec to decode the Opus Interactive Audio Codec. Requires the presence of the libopus headers and library during configuration. You need to explicitly configure the build with .\n\nAn FFmpeg native decoder for Opus exists, so users can decode Opus without this library.\n\nImplements profiles A and C of the ARIB STD-B24 standard.\n\nYet another ARIB STD-B24 caption decoder using external libaribcaption library.\n\nImplements profiles A and C of the Japanse ARIB STD-B24 standard, Brazilian ABNT NBR 15606-1, and Philippines version of ISDB-T.\n\nRequires the presence of the libaribcaption headers and library (https://github.com/xqq/libaribcaption) during configuration. You need to explicitly configure the build with . If both libaribb24 and libaribcaption are enabled, libaribcaption decoder precedes.\n\nThis codec decodes the bitmap subtitles used in DVDs; the same subtitles can also be found in VobSub file pairs and in some Matroska files.\n\nLibzvbi allows libavcodec to decode DVB teletext pages and DVB teletext subtitles. Requires the presence of the libzvbi headers and library during configuration. You need to explicitly configure the build with .\n\nEncoders are configured elements in FFmpeg which allow the encoding of multimedia streams.\n\nWhen you configure your FFmpeg build, all the supported native encoders are enabled by default. Encoders requiring an external library must be enabled manually via the corresponding option. You can list all available encoders using the configure option .\n\nYou can disable all the encoders with the configure option and selectively enable / disable single encoders with the options / .\n\nThe option of the ff* tools will display the list of enabled encoders.\n\nA description of some of the currently available audio encoders follows.\n\nThis encoder is the default AAC encoder, natively implemented into FFmpeg.\n\nThese encoders implement part of ATSC A/52:2010 and ETSI TS 102 366.\n\nThe encoder uses floating-point math, while the encoder only uses fixed-point integer math. This does not mean that one is always faster, just that one or the other may be better suited to a particular system. The encoder is not the default codec for any of the output formats, so it must be specified explicitly using the option in order to use it.\n\nThe AC-3 metadata options are used to set parameters that describe the audio, but in most cases do not affect the audio encoding itself. Some of the options do directly affect or influence the decoding and playback of the resulting bitstream, while others are just for informational purposes. A few of the options will add bits to the output stream that could otherwise be used for audio data, and will thus affect the quality of the output. Those will be indicated accordingly with a note in the option list below.\n\nThese parameters are described in detail in several publicly-available documents.\n• A/54 - Guide to the Use of the ATSC Digital Television Standard\n\nAudio Production Information is optional information describing the mixing environment. Either none or both of the fields are written to the bitstream.\n\nThe extended bitstream options are part of the Alternate Bit Stream Syntax as specified in Annex D of the A/52:2010 standard. It is grouped into 2 parts. If any one parameter in a group is specified, all values in that group will be written to the bitstream. Default values are used for those that are written but have not been specified. If the mixing levels are written, the decoder will use these values instead of the ones specified in the and options if it supports the Alternate Bit Stream Syntax.\n\nThese options are only valid for the floating-point encoder and do not exist for the fixed-point encoder due to the corresponding features not being implemented in fixed-point.\n\nThe following options are supported by FFmpeg’s FFv1 encoder.\n\nThe following options are supported by FFmpeg’s flac encoder.\n\nThis is a native FFmpeg encoder for the Opus format. Currently, it’s in development and only implements the CELT part of the codec. Its quality is usually worse and at best is equal to the libopus encoder.\n\nThe libfdk-aac library is based on the Fraunhofer FDK AAC code from the Android project.\n\nRequires the presence of the libfdk-aac headers and library during configuration. You need to explicitly configure the build with . The library is also incompatible with GPL, so if you allow the use of GPL, you should configure with .\n\nThis encoder has support for the AAC-HE profiles.\n\nVBR encoding, enabled through the or options, is experimental and only works with some combinations of parameters.\n\nSupport for encoding 7.1 audio is only available with libfdk-aac 0.1.3 or higher.\n\nFor more information see the fdk-aac project at http://sourceforge.net/p/opencore-amr/fdk-aac/.\n\nThe following options are mapped on the shared FFmpeg codec options.\n\nThe following are private options of the libfdk_aac encoder.\n• Use to convert an audio file to VBR AAC in an M4A (MP4) container:\n• Use to convert an audio file to CBR 64k kbps AAC, using the High-Efficiency AAC profile:\n\nRequires the presence of the liblc3 headers and library during configuration. You need to explicitly configure the build with .\n\nThis encoder has support for the Bluetooth SIG LC3 codec for the LE Audio protocol, and the following features of LC3plus:\n\nFor more information see the liblc3 project at https://github.com/google/liblc3.\n\nThe following options are mapped on the shared FFmpeg codec options.\n\nRequires the presence of the libmp3lame headers and library during configuration. You need to explicitly configure the build with .\n\nSee libshine for a fixed-point MP3 encoder, although with a lower quality.\n\nThe following options are supported by the libmp3lame wrapper. The -equivalent of the options are listed in parentheses.\n\nRequires the presence of the libopencore-amrnb headers and library during configuration. You need to explicitly configure the build with .\n\nThis is a mono-only encoder. Officially it only supports 8000Hz sample rate, but you can override it by setting to ‘ ’ or lower.\n\nRequires the presence of the libopus headers and library during configuration. You need to explicitly configure the build with .\n\nMost libopus options are modelled after the utility from opus-tools. The following is an option mapping chart describing options supported by the libopus wrapper, and their -equivalent in parentheses.\n\nShine is a fixed-point MP3 encoder. It has a far better performance on platforms without an FPU, e.g. armel CPUs, and some phones and tablets. However, as it is more targeted on performance than quality, it is not on par with LAME and other production-grade encoders quality-wise. Also, according to the project’s homepage, this encoder may not be free of bugs as the code was written a long time ago and the project was dead for at least 5 years.\n\nThis encoder only supports stereo and mono input. This is also CBR-only.\n\nThe original project (last updated in early 2007) is at http://sourceforge.net/projects/libshine-fxp/. We only support the updated fork by the Savonet/Liquidsoap project at https://github.com/savonet/shine.\n\nRequires the presence of the libshine headers and library during configuration. You need to explicitly configure the build with .\n\nThe following options are supported by the libshine wrapper. The -equivalent of the options are listed in parentheses.\n\nRequires the presence of the libtwolame headers and library during configuration. You need to explicitly configure the build with .\n\nThe following options are supported by the libtwolame wrapper. The -equivalent options follow the FFmpeg ones and are in parentheses.\n\nRequires the presence of the libvo-amrwbenc headers and library during configuration. You need to explicitly configure the build with .\n\nThis is a mono-only encoder. Officially it only supports 16000Hz sample rate, but you can override it by setting to ‘ ’ or lower.\n\nRequires the presence of the libvorbisenc headers and library during configuration. You need to explicitly configure the build with .\n\nThe following options are supported by the libvorbis wrapper. The -equivalent of the options are listed in parentheses.\n\nTo get a more accurate and extensive documentation of the libvorbis options, consult the libvorbisenc’s and ’s documentations. See http://xiph.org/vorbis/, http://wiki.xiph.org/Vorbis-tools, and oggenc(1).\n\nThe equivalent options for command line utility are listed in parentheses.\n\nThe following shared options are effective for this encoder. Only special notes about this particular encoder will be documented here. For the general meaning of the options, see the Codec Options chapter.\n\nA description of some of the currently available video encoders follows.\n\nThe native jpeg 2000 encoder is lossy by default, the option can be used to set the encoding quality. Lossless encoding can be selected with .\n\nRequires the presence of the rav1e headers and library during configuration. You need to explicitly configure the build with .\n\nRequires the presence of the libaom headers and library during configuration. You need to explicitly configure the build with .\n\nThe wrapper supports the following standard libavcodec options:\n\nThe wrapper also has some specific options:\n\nRequires the presence of the SVT-AV1 headers and library during configuration. You need to explicitly configure the build with .\n\nRequires the presence of the libjxl headers and library during configuration. You need to explicitly configure the build with .\n\nThe libjxl wrapper supports the following options:\n\nRequires the presence of the libkvazaar headers and library during configuration. You need to explicitly configure the build with .\n\nThis encoder requires the presence of the libopenh264 headers and library during configuration. You need to explicitly configure the build with . The library is detected using .\n\nFor more information about the library see http://www.openh264.org.\n\nThe following FFmpeg global options affect the configurations of the libopenh264 encoder.\n\nRequires the presence of the libtheora headers and library during configuration. You need to explicitly configure the build with .\n\nFor more information about the libtheora project see http://www.theora.org/.\n\nThe following global options are mapped to internal libtheora options which affect the quality and the bitrate of the encoded stream.\n\nRequires the presence of the libvpx headers and library during configuration. You need to explicitly configure the build with .\n\nThe following options are supported by the libvpx wrapper. The -equivalent options or values are listed in parentheses for easy migration.\n\nTo reduce the duplication of documentation, only the private options and some others requiring special attention are documented here. For the documentation of the undocumented generic options, see the Codec Options chapter.\n\nTo get more documentation of the libvpx options, invoke the command , or . Further information is available in the libvpx API documentation.\n\nFor more information about libvpx see: http://www.webmproject.org/\n\nThis encoder requires the presence of the libvvenc headers and library during configuration. You need to explicitly configure the build with .\n\nThe VVenC project website is at https://github.com/fraunhoferhhi/vvenc.\n\nVVenC supports only 10-bit color spaces as input. But the internal (encoded) bit depth can be set to 8-bit or 10-bit at runtime.\n\nlibwebp is Google’s official encoder for WebP images. It can encode in either lossy or lossless mode. Lossy images are essentially a wrapper around a VP8 frame. Lossless images are a separate codec developed by Google.\n\nCurrently, libwebp only supports YUV420 for lossy and RGB for lossless due to limitations of the format and libwebp. Alpha is supported for either mode. Because of API limitations, if RGB is passed in when encoding lossy or YUV is passed in for encoding lossless, the pixel format will automatically be converted using functions from libwebp. This is not ideal and is done only for convenience.\n\nThis encoder requires the presence of the libx264 headers and library during configuration. You need to explicitly configure the build with .\n\nlibx264 supports an impressive number of features, including 8x8 and 4x4 adaptive spatial transform, adaptive B-frame placement, CAVLC/CABAC entropy coding, interlacing (MBAFF), lossless mode, psy optimizations for detail retention (adaptive quantization, psy-RD, psy-trellis).\n\nMany libx264 encoder options are mapped to FFmpeg global codec options, while unique encoder options are provided through private options. Additionally the and private options allows one to pass a list of key=value tuples as accepted by the libx264 function.\n\nThe x264 project website is at http://www.videolan.org/developers/x264.html.\n\nThe libx264rgb encoder is the same as libx264, except it accepts packed RGB pixel formats as input instead of YUV.\n\nx264 supports 8- to 10-bit color spaces. The exact bit depth is controlled at x264’s configure time.\n\nThe following options are supported by the libx264 wrapper. The -equivalent options or values are listed in parentheses for easy migration.\n\nTo reduce the duplication of documentation, only the private options and some others requiring special attention are documented here. For the documentation of the undocumented generic options, see the Codec Options chapter.\n\nTo get a more accurate and extensive documentation of the libx264 options, invoke the command or consult the libx264 documentation.\n\nIn the list below, note that the option name is shown in parentheses after the libavcodec corresponding name, in case there is a direct mapping.\n\nEncoding ffpresets for common usages are provided so they can be used with the general presets system (e.g. passing the option).\n\nThis encoder requires the presence of the libx265 headers and library during configuration. You need to explicitly configure the build with .\n\nThis encoder requires the presence of the libxavs2 headers and library during configuration. You need to explicitly configure the build with .\n\nThe following standard libavcodec options are used:\n\nThe encoder also has its own specific options:\n\neXtra-fast Essential Video Encoder (XEVE) MPEG-5 EVC encoder wrapper. The xeve-equivalent options or values are listed in parentheses for easy migration.\n\nThis encoder requires the presence of the libxeve headers and library during configuration. You need to explicitly configure the build with .\n\nThe xeve project website is at https://github.com/mpeg5/xeve.\n\nThe following options are supported by the libxeve wrapper. The xeve-equivalent options or values are listed in parentheses for easy migration.\n\nThis encoder requires the presence of the libxvidcore headers and library during configuration. You need to explicitly configure the build with .\n\nThe native encoder supports the MPEG-4 Part 2 format, so users can encode to this format without this library.\n\nThe following options are supported by the libxvid wrapper. Some of the following options are listed but are not documented, and correspond to shared codec options. See the Codec Options chapter for their documentation. The other shared options which are not listed have no effect for the libxvid encoder.\n\nThis provides wrappers to encoders (both audio and video) in the MediaFoundation framework. It can access both SW and HW encoders. Video encoders can take input in either of nv12 or yuv420p form (some encoders support both, some support only either - in practice, nv12 is the safer choice, especially among HW encoders).\n\nMicrosoft RLE aka MSRLE encoder. Only 8-bit palette mode supported. Compatible with Windows 3.1 and Windows 95.\n\nFFmpeg contains 2 ProRes encoders, the prores-aw and prores-ks encoder. The used encoder can be chosen with the option.\n\nIn the default mode of operation the encoder has to honor frame constraints (i.e. not produce frames with size bigger than requested) while still making output picture as good as possible. A frame containing a lot of small details is harder to compress and the encoder would spend more time searching for appropriate quantizers for each slice.\n\nFor the fastest encoding speed set the parameter (4 is the recommended value) and do not set a size constraint.\n\nThe ratecontrol method is selected as follows:\n• When is specified, a quality-based mode is used. Specifically this means either\n• - - constant quantizer scale, when the codec flag is also set (the ffmpeg option).\n• - - intelligent constant quality with lookahead, when the option is also set.\n• - – intelligent constant quality otherwise. For the ICQ modes, global quality range is 1 to 51, with 1 being the best quality.\n• Otherwise when the desired average bitrate is specified with the option, a bitrate-based mode is used.\n• - - VBR with lookahead, when the option is specified.\n• - - video conferencing mode, when the option is set.\n• - - constant bitrate, when is specified and equal to the average bitrate.\n• - - variable bitrate, when is specified, but is higher than the average bitrate.\n• - - average VBR mode, when is not specified, both and are set to non-zero. This mode is available for H264 and HEVC on Windows.\n• Otherwise the default ratecontrol method is used.\n\nNote that depending on your system, a different mode than the one you specified may be selected by the encoder. Set the verbosity level to or higher to see the actual settings used by the QSV runtime.\n\nAdditional libavcodec global options are mapped to MSDK options as follows:\n• For the mode, the and set the difference between and , and and respectively.\n• Setting the option to the value will make the H.264 encoder use CAVLC instead of CABAC.\n\nFollowing options are used by all qsv encoders.\n\nFollowing options can be used durning qsv encoding.\n\nThese options are used by h264_qsv\n\nThese options are used by hevc_qsv\n\nThese options are used by mpeg2_qsv\n\nThese options are used by vp9_qsv\n\nThese options are used by av1_qsv (requires libvpl).\n\nThese encoders only accept input in VAAPI hardware surfaces. If you have input in software frames, use the filter to upload them to the GPU.\n\nThe following standard libavcodec options are used:\n• If not set, this will be determined automatically from the format of the input frames and the profiles supported by the driver.\n\nAll encoders support the following options:\n\nEach encoder also has its own specific options:\n\nThis format is used by the broadcast vendor Vizrt for quick texture streaming. Advanced features of the format such as LZW compression of texture data or generation of mipmaps are not supported.\n\nSMPTE VC-2 (previously BBC Dirac Pro). This codec was primarily aimed at professional broadcasting but since it supports yuv420, yuv422 and yuv444 at 8 (limited range or full range), 10 or 12 bits, this makes it suitable for other tasks which require low overhead and low compression (like screen recording).\n\nThis codec encodes the bitmap subtitle format that is used in DVDs. Typically they are stored in VOBSUB file pairs (*.idx + *.sub), and they can also be used in Matroska files.\n\nWhen you configure your FFmpeg build, all the supported bitstream filters are enabled by default. You can list all available ones using the configure option .\n\nYou can disable all the bitstream filters using the configure option , and selectively enable any bitstream filter using the option , or you can disable a particular bitstream filter using the option .\n\nThe option of the ff* tools will display the list of all the supported bitstream filters included in your build.\n\nThe ff* tools have a -bsf option applied per stream, taking a comma-separated list of filters, whose parameters follow the filter name after a ’=’.\n\nBelow is a description of the currently available bitstream filters, with their parameters, if any.\n\nThis filter creates an MPEG-4 AudioSpecificConfig from an MPEG-2/4 ADTS header and removes the ADTS header.\n\nThis filter is required for example when copying an AAC stream from a raw ADTS AAC or an MPEG-TS container to MP4A-LATM, to an FLV file, or to MOV/MP4 files and related formats such as 3GP or M4A. Please note that it is auto-inserted for MP4A-LATM and MOV/MP4 and related formats.\n\nRemove zero padding at the end of a packet.\n\nExtract the core from a DCA/DTS stream, dropping extensions such as DTS-HD.\n\nAdd extradata to the beginning of the filtered packets except when said packets already exactly begin with the extradata that is intended to be added.\n\nIf not specified it is assumed ‘ ’.\n\nFor example the following command forces a global header (thus disabling individual packet headers) in the H.264 packets generated by the encoder, but corrects them by adding the header stored in extradata to the key packets:\n\nBlocks in DV which are marked as damaged are replaced by blocks of the specified color.\n\nCertain codecs allow the long-term headers (e.g. MPEG-2 sequence headers, or H.264/HEVC (VPS/)SPS/PPS) to be transmitted either \"in-band\" (i.e. as a part of the bitstream containing the coded frames) or \"out of band\" (e.g. on the container level). This latter form is called \"extradata\" in FFmpeg terminology.\n\nThis bitstream filter detects the in-band headers and makes them available as extradata.\n\nRemove units with types in or not in a given set from the stream.\n\nThe types used by pass_types and remove_types correspond to NAL unit types (nal_unit_type) in H.264, HEVC and H.266 (see Table 7-1 in the H.264 and HEVC specifications or Table 5 in the H.266 specification), to marker values for JPEG (without 0xFF prefix) and to start codes without start code prefix (i.e. the byte following the 0x000001) for MPEG-2. For VP8 and VP9, every unit has type zero.\n\nExtradata is unchanged by this transformation, but note that if the stream contains inline parameter sets then the output may be unusable if they are removed.\n\nFor example, to remove all non-VCL NAL units from an H.264 stream:\n\nTo remove all AUDs, SEI and filler from an H.265 stream:\n\nTo remove all user data from a MPEG-2 stream, including Closed Captions:\n\nTo remove all SEI from a H264 stream, including Closed Captions:\n\nTo remove all prefix and suffix SEI from a HEVC stream, including Closed Captions and dynamic HDR:\n\nExtract Rgb or Alpha part of an HAPQA file, without recompression, in order to create an HAPQ or an HAPAlphaOnly file.\n\nConvert an H.264 bitstream from length prefixed mode to start code prefixed mode (as defined in the Annex B of the ITU-T H.264 specification).\n\nThis is required by some streaming formats, typically the MPEG-2 transport stream format (muxer ).\n\nFor example to remux an MP4 file containing an H.264 stream to mpegts format with , you can use the command:\n\nPlease note that this filter is auto-inserted for MPEG-TS (muxer ) and raw H.264 (muxer ) output formats.\n\nThis applies a specific fixup to some Blu-ray BDMV H264 streams which contain redundant PPSs. The PPSs modify irrelevant parameters of the stream, confusing other transformations which require the correct extradata.\n\nThe encoder used on these impacted streams adds extra PPSs throughout the stream, varying the initial QP and whether weighted prediction was enabled. This causes issues after copying the stream into a global header container, as the starting PPS is not suitable for the rest of the stream. One side effect, for example, is seeking will return garbled output until a new PPS appears.\n\nThis BSF removes the extra PPSs and rewrites the slice headers such that the stream uses a single leading PPS in the global header, which resolves the issue.\n\nConvert an HEVC/H.265 bitstream from length prefixed mode to start code prefixed mode (as defined in the Annex B of the ITU-T H.265 specification).\n\nThis is required by some streaming formats, typically the MPEG-2 transport stream format (muxer ).\n\nFor example to remux an MP4 file containing an HEVC stream to mpegts format with , you can use the command:\n\nPlease note that this filter is auto-inserted for MPEG-TS (muxer ) and raw HEVC/H.265 (muxer or ) output formats.\n\nModifies the bitstream to fit in MOV and to be usable by the Final Cut Pro decoder. This filter only applies to the mpeg2video codec, and is likely not needed for Final Cut Pro 7 and newer with the appropriate .\n\nFor example, to remux 30 MB/sec NTSC IMX to MOV:\n\nMJPEG is a video codec wherein each video frame is essentially a JPEG image. The individual frames can be extracted without loss, e.g. by\n\nUnfortunately, these chunks are incomplete JPEG images, because they lack the DHT segment required for decoding. Quoting from http://www.digitalpreservation.gov/formats/fdd/fdd000063.shtml:\n\nAvery Lee, writing in the rec.video.desktop newsgroup in 2001, commented that \"MJPEG, or at least the MJPEG in AVIs having the MJPG fourcc, is restricted JPEG with a fixed – and *omitted* – Huffman table. The JPEG must be YCbCr colorspace, it must be 4:2:2, and it must use basic Huffman encoding, not arithmetic or progressive. . . . You can indeed extract the MJPEG frames and decode them with a regular JPEG decoder, but you have to prepend the DHT segment to them, or else the decoder won’t have any idea how to decompress the data. The exact table necessary is given in the OpenDML spec.\"\n\nThis bitstream filter patches the header of frames extracted from an MJPEG stream (carrying the AVI1 header ID and lacking a DHT segment) to produce fully qualified JPEG images.\n\nAdd an MJPEG A header to the bitstream, to enable decoding by Quicktime.\n\nExtract a representable text file from MOV subtitles, stripping the metadata header from each subtitle packet.\n\nSee also the text2movsub filter.\n\nDivX-style packed B-frames are not valid MPEG-4 and were only a workaround for the broken Video for Windows subsystem. They use more space, can cause minor AV sync issues, require more CPU power to decode (unless the player has some decoded picture queue to compensate the 2,0,2,0 frame per packet style) and cause trouble if copied into a standard container like mp4 or mpeg-ps/ts, because MPEG-4 decoders may not be able to decode them, since they are not valid MPEG-4.\n\nFor example to fix an AVI file containing an MPEG-4 stream with DivX-style packed B-frames using , you can use the command:\n\nDamages the contents of packets or simply drops them without damaging the container. Can be used for fuzzing or testing error resilience/concealment.\n\nBoth and accept expressions containing the following variables:\n\nApply modification to every byte but don’t drop any packets.\n\nDrop every video packet not marked as a keyframe after timestamp 30s but do not modify any of the remaining packets.\n\nDrop one second of audio every 10 seconds and add some random noise to the rest.\n\nThis bitstream filter passes the packets through unchanged.\n\nRepacketize PCM audio to a fixed number of samples per packet or a fixed packet rate per second. This is similar to the (ffmpeg-filters)asetnsamples audio filter but works on audio packets instead of audio frames.\n\nYou can generate the well known 1602-1601-1602-1601-1602 pattern of 48kHz audio for NTSC frame rate using the option.\n\nMerge a sequence of PGS Subtitle segments ending with an \"end of display set\" segment into a single packet.\n\nThis is required by some containers that support PGS subtitles (muxer ).\n\nSet Rec709 colorspace for each frame of the file\n\nSet Hybrid Log-Gamma parameters for each frame of the file\n\nIt accepts the following parameter:\n\nIt accepts the following parameters:\n\nThe expressions are evaluated through the eval API and can contain the following constants:\n\nFor example, to set PTS equal to DTS (not recommended if B-frames are involved):\n\nLog basic packet information. Mainly useful for testing, debugging, and development.\n\nConvert text subtitles to MOV subtitles (as used by the codec) with metadata headers.\n\nSee also the mov2textsub filter.\n\nLog trace output containing all syntax elements in the coded stream headers (everything above the level of individual coded blocks). This can be useful for debugging low-level stream issues.\n\nSupports AV1, H.264, H.265, (M)JPEG, MPEG-2 and VP9, but depending on the build only a subset of these may be available.\n\nMerge VP9 invisible (alt-ref) frames back into VP9 superframes. This fixes merging of split/segmented VP9 streams where the alt-ref frame was split from its visible counterpart.\n\nGiven a VP9 stream with correct timestamps but possibly out of order, insert additional show-existing-frame packets to correct the ordering.\n\nThe libavformat library provides some generic global options, which can be set on all the muxers and demuxers. In addition each muxer or demuxer may support so-called private options, which are specific for that component.\n\nOptions may be set by specifying - in the FFmpeg tools, or by setting the value explicitly in the options or using the API for programmatic use.\n\nThe list of supported options follows:\n\nFormat stream specifiers allow selection of one or more streams that match specific properties.\n\nThe exact semantics of stream specifiers is defined by the function declared in the header and documented in the (ffmpeg)Stream specifiers section in the ffmpeg(1) manual.\n\nDemuxers are configured elements in FFmpeg that can read the multimedia streams from a particular type of file.\n\nWhen you configure your FFmpeg build, all the supported demuxers are enabled by default. You can list all available ones using the configure option .\n\nYou can disable all the demuxers using the configure option , and selectively enable a single demuxer with the option , or disable it with the option .\n\nThe option of the ff* tools will display the list of enabled demuxers. Use to view a combined list of enabled demuxers and muxers.\n\nThe description of some of the currently available demuxers follows.\n\nThis demuxer is used to demux Audible Format 2, 3, and 4 (.aa) files.\n\nThis demuxer is used to demux an ADTS input containing a single AAC stream alongwith any ID3v1/2 or APE tags in it.\n\nThis demuxer is used to demux APNG files. All headers, but the PNG signature, up to (but not including) the first fcTL chunk are transmitted as extradata. Frames are then split as being all the chunks between two fcTL ones, or between the last fcTL and IEND chunks.\n\nThis demuxer is used to demux ASF files and MMS network streams.\n\nThis demuxer reads a list of files and other directives from a text file and demuxes them one after the other, as if all their packets had been muxed together.\n\nThe timestamps in the files are adjusted so that the first file starts at 0 and each next file starts where the previous one finishes. Note that it is done globally and may cause gaps if all streams do not have exactly the same length.\n\nAll files must have the same streams (same codecs, same time base, etc.).\n\nThe duration of each file is used to adjust the timestamps of the next file: if the duration is incorrect (because it was computed using the bit-rate or because the file is truncated, for example), it can cause artifacts. The directive can be used to override the duration stored in each file.\n\nThe script is a text file in extended-ASCII, with one directive per line. Empty lines, leading spaces and lines starting with ’#’ are ignored. The following directive is recognized:\n\nThis demuxer accepts the following option:\n• Use absolute filenames and include some comments: # my first filename file /mnt/share/file-1.wav # my second filename including whitespace file '/mnt/share/file 2.wav' # my third filename including whitespace plus single quote file '/mnt/share/file 3'\\''.wav'\n• Allow for input format auto-probing, use safe filenames and set the duration of the first file:\n\nThis demuxer presents all AVStreams found in the manifest. By setting the discard flags on AVStreams the caller can decide which streams to actually receive. Each stream mirrors the and properties from the as metadata keys named \"id\" and \"variant_bitrate\" respectively.\n\nThis demuxer accepts the following option:\n\nCan directly ingest DVD titles, specifically sequential PGCs, into a conversion pipeline. Menu assets, such as background video or audio, can also be demuxed given the menu’s coordinates (at best effort).\n\nBlock devices (DVD drives), ISO files, and directory structures are accepted. Activate with in front of one of these inputs.\n\nThis demuxer does NOT have decryption code of any kind. You are on your own working with encrypted DVDs, and should not expect support on the matter.\n\nUnderlying playback is handled by libdvdnav, and structure parsing by libdvdread. FFmpeg must be built with GPL library support available as well as the configure switches and .\n\nYou will need to provide either the desired \"title number\" or exact PGC/PG coordinates. Many open-source DVD players and tools can aid in providing this information. If not specified, the demuxer will default to title 1 which works for many discs. However, due to the flexibility of the format, it is recommended to check manually. There are many discs that are authored strangely or with invalid headers.\n\nIf the input is a real DVD drive, please note that there are some drives which may silently fail on reading bad sectors from the disc, returning random bits instead which is effectively corrupt data. This is especially prominent on aging or rotting discs. A second pass and integrity checks would be needed to detect the corruption. This is not an FFmpeg issue.\n\nDVD-Video is not a directly accessible, linear container format in the traditional sense. Instead, it allows for complex and programmatic playback of carefully muxed MPEG-PS streams that are stored in headerless VOB files. To the end-user, these streams are known simply as \"titles\", but the actual logical playback sequence is defined by one or more \"PGCs\", or Program Group Chains, within the title. The PGC is in turn comprised of multiple \"PGs\", or Programs\", which are the actual video segments (and for a typical video feature, sequentially ordered). The PGC structure, along with stream layout and metadata, are stored in IFO files that need to be parsed. PGCs can be thought of as playlists in easier terms.\n\nAn actual DVD player relies on user GUI interaction via menus and an internal VM to drive the direction of demuxing. Generally, the user would either navigate (via menus) or automatically be redirected to the PGC of their choice. During this process and the subsequent playback, the DVD player’s internal VM also maintains a state and executes instructions that can create jumps to different sectors during playback. This is why libdvdnav is involved, as a linear read of the MPEG-PS blobs on the disc (VOBs) is not enough to produce the right sequence in many cases.\n\nThere are many other DVD structures (a long subject) that will not be discussed here. NAV packets, in particular, are handled by this demuxer to build accurate timing but not emitted as a stream. For a good high-level understanding, refer to: https://code.videolan.org/videolan/libdvdnav/-/blob/master/doc/dvd_structures\n\nThis demuxer accepts the following options:\n• Open chapters 3-6 from title 1 from a given DVD structure:\n• Open only chapter 5 from title 1 from a given DVD structure:\n• Demux menu with language 1 from VTS 1, PGC 1, starting at PG 1:\n\nThis format is used by various Electronic Arts games.\n\nThis demuxer presents audio and video streams found in an IMF Composition, as specified in SMPTE ST 2067-2.\n\nIf is not specified, the demuxer looks for a file called in the same directory as the CPL.\n\nThis demuxer is used to demux FLV files and RTMP network streams. In case of live network streams, if you force format, you may use live_flv option instead of flv to survive timestamp discontinuities. KUX is a flv variant used on the Youku platform.\n\nIt accepts the following options:\n\nFor example, with the overlay filter, place an infinitely looping GIF over another video:\n\nNote that in the above example the shortest option for overlay filter is used to end the output video at the length of the shortest input file, which in this case is as the GIF in this example loops infinitely.\n\nThis demuxer presents all AVStreams from all variant streams. The id field is set to the bitrate variant index number. By setting the discard flags on AVStreams (by pressing ’a’ or ’v’ in ffplay), the caller can decide which variant streams to actually receive. The total bitrate of the variant that the stream belongs to is available in a metadata key named \"variant_bitrate\".\n\nIt accepts the following options:\n\nThis demuxer reads from a list of image files specified by a pattern. The syntax and meaning of the pattern is specified by the option .\n\nThe pattern may contain a suffix which is used to automatically determine the format of the images contained in the files.\n\nThe size, the pixel format, and the format of each image must be the same for all the files in the sequence.\n\nThis demuxer accepts the following options:\n• Use for creating a video from the images in the file sequence , , ..., assuming an input frame rate of 10 frames per second:\n• As above, but start by reading from a file with index 100 in the sequence:\n• Read images matching the \"*.png\" glob pattern , that is all the files terminating with the \".png\" suffix:\n\nThe Game Music Emu library is a collection of video game music file emulators.\n\nSee https://bitbucket.org/mpyne/game-music-emu/overview for more information.\n\nIt accepts the following options:\n\nIt will export one 2-channel 16-bit 44.1 kHz audio stream. Optionally, a 16-color video stream can be exported with or without printed metadata.\n\nIt accepts the following options:\n\nSee https://lib.openmpt.org/libopenmpt/ for more information.\n\nSome files have multiple subsongs (tracks) this can be set with the option.\n\nIt accepts the following options:\n\nDemuxer for Quicktime File Format & ISO/IEC Base Media File Format (ISO/IEC 14496-12 or MPEG-4 Part 12, ISO/IEC 15444-12 or JPEG 2000 Part 12).\n\nThis demuxer accepts the following options:\n\nAudible AAX files are encrypted M4B files, and they can be decrypted by specifying a 4 byte activation secret.\n\nThis demuxer accepts the following options:\n\nThis demuxer allows reading of MJPEG, where each frame is represented as a part of multipart/x-mixed-replace stream.\n\nThis demuxer allows one to read raw video data. Since there is no header specifying the assumed video parameters, the user must specify them in order to be able to decode the data correctly.\n\nThis demuxer accepts the following options:\n\nFor example to read a rawvideo file with , assuming a pixel format of , a video size of , and a frame rate of 10 images per second, use the command:\n\nRCWT (Raw Captions With Time) is a format native to ccextractor, a commonly used open source tool for processing 608/708 Closed Captions (CC) sources. For more information on the format, see (ffmpeg-formats)rcwtenc.\n\nThis demuxer implements the specification as of March 2024, which has been stable and unchanged since April 2014.\n• Render CC to ASS using the built-in decoder: Note that if your output appears to be empty, you may have to manually set the decoder’s option to pick the desired CC substream.\n• Convert an RCWT backup to Scenarist (SCC) format: Note that the SCC format does not support all of the possible CC extensions that can be stored in RCWT (such as EIA-708).\n\nThis demuxer reads the script language used by SBaGen http://uazu.net/sbagen/ to generate binaural beats sessions. A SBG script looks like that:\n\nA SBG script can mix absolute and relative timestamps. If the script uses either only absolute timestamps (including the script start time) or only relative ones, then its layout is fixed, and the conversion is straightforward. On the other hand, if the script mixes both kind of timestamps, then the reference for relative timestamps will be taken from the current time of day at the time the script is read, and the script layout will be frozen according to that reference. That means that if the script is directly played, the actual times will match the absolute timestamps up to the sound controller’s clock accuracy, but if the user somehow pauses the playback or seeks, all times will be shifted accordingly.\n\nTED does not provide links to the captions, but they can be guessed from the page. The file from the FFmpeg source tree contains a bookmarklet to expose them.\n\nThis demuxer accepts the following option:\n\nExample: convert the captions to a format most players understand:\n\nDue to security concerns, Vapoursynth scripts will not be autodetected so the input format has to be forced. For ff* CLI tools, add before the input .\n\nThis demuxer accepts the following option:\n\nThis demuxer accepts the following options:\n\nThis demuxer accepts the following options:\n\nMuxers are configured elements in FFmpeg which allow writing multimedia streams to a particular type of file.\n\nWhen you configure your FFmpeg build, all the supported muxers are enabled by default. You can list all available muxers using the configure option .\n\nYou can disable all the muxers with the configure option and selectively enable / disable single muxers with the options / .\n\nThe option of the ff* tools will display the list of enabled muxers. Use to view a combined list of enabled demuxers and muxers.\n\nA description of some of the currently available muxers follows.\n\nThis section covers raw muxers. They accept a single stream matching the designated codec. They do not store timestamps or metadata. The recognized extension is the same as the muxer name unless indicated otherwise.\n\nIt comprises the following muxers. The media type and the eventual extensions used to automatically selects the muxer from the output extensions are also shown.\n• Store raw video frames with the ‘ ’ muxer using : Since the rawvideo muxer do not store the information related to size and format, this information must be provided when demuxing the file:\n\nThey accept a single stream matching the designated codec. They do not store timestamps or metadata. The recognized extension is the same as the muxer name.\n\nIt comprises the following muxers. The optional additional extension used to automatically select the muxer from the output extension is also shown in parentheses.\n\nThis section covers formats belonging to the MPEG-1 and MPEG-2 Systems family.\n\nThe MPEG-1 Systems format (also known as ISO/IEEC 11172-1 or MPEG-1 program stream) has been adopted for the format of media track stored in VCD (Video Compact Disc).\n\nThe MPEG-2 Systems standard (also known as ISO/IEEC 13818-1) covers two containers formats, one known as transport stream and one known as program stream; only the latter is covered here.\n\nThe MPEG-2 program stream format (also known as VOB due to the corresponding file extension) is an extension of MPEG-1 program stream: in addition to support different codecs for the audio and video streams, it also stores subtitles and navigation metadata. MPEG-2 program stream has been adopted for storing media streams in SVCD and DVD storage devices.\n\nThis section comprises the following muxers.\n\nThis section covers formats belonging to the QuickTime / MOV family, including the MPEG-4 Part 14 format and ISO base media file format (ISOBMFF). These formats share a common structure based on the ISO base media file format (ISOBMFF).\n\nThe MOV format was originally developed for use with Apple QuickTime. It was later used as the basis for the MPEG-4 Part 1 (later Part 14) format, also known as ISO/IEC 14496-1. That format was then generalized into ISOBMFF, also named MPEG-4 Part 12 format, ISO/IEC 14496-12, or ISO/IEC 15444-12.\n\nIt comprises the following muxers.\n\nThe ‘ ’, ‘ ’, and ‘ ’ muxers support fragmentation. Normally, a MOV/MP4 file has all the metadata about all packets stored in one location.\n\nThis data is usually written at the end of the file, but it can be moved to the start for better playback by adding to the , or using the tool).\n\nA fragmented file consists of a number of fragments, where packets and metadata about these packets are stored together. Writing a fragmented file has the advantage that the file is decodable even if the writing is interrupted (while a normal MOV/MP4 is undecodable if it is not properly finished), and it requires less memory when writing very long files (since writing normal MOV/MP4 files stores info about every single packet in memory until the file is closed). The downside is that it is less compatible with other applications.\n\nFragmentation is enabled by setting one of the options that define how to cut the file into fragments:\n\nIf more than one condition is specified, fragments are cut when one of the specified conditions is fulfilled. The exception to this is the option , which has to be fulfilled for any of the other conditions to apply.\n• Push Smooth Streaming content in real time to a publishing point on IIS with the ‘ ’ muxer using :\n\nThis muxer accepts a single ATRAC1 audio stream with either one or two channels and a sample rate of 44100Hz.\n\nAs AEA supports storing the track title, this muxer will also write the title from stream’s metadata to the container.\n\nIt accepts a single ADPCM_IMA_ALP stream with no more than 2 channels and a sample rate not greater than 44100 Hz.\n\nIt accepts a single audio stream containing an AMR NB stream.\n• Use to generate an APNG output with 2 repetitions, and with a delay of half a second after the first repetition:\n\nThe and options set the corresponding flags in the header which can be later retrieved to process the audio stream accordingly.\n\nThe ‘ ’ variant should be selected for streaming.\n\nNote that Windows Media Audio (wma) and Windows Media Video (wmv) use this muxer too.\n\nThis format is used to play audio on some Nintendo Wii games.\n\nThe and options can be used to define a section of the file to loop for players honoring such options.\n\nAVI is a proprietary format developed by Microsoft, and later formally specified through the Open DML specification.\n\nBecause of differences in players implementations, it might be required to set some options to make sure that the generated output can be correctly played by the target player.\n\nThis muxers stores images encoded using the AV1 codec.\n\nIt accepts one or two video streams. In case two video streams are provided, the second one shall contain a single plane storing the alpha mask.\n\nIn case more than one image is provided, the generated output is considered an animated AVIF and the number of loops can be specified with the option.\n\nThis is based on the specification by Alliance for Open Media at url https://aomediacodec.github.io/av1-avif.\n\nIt accepts one audio stream, one video stream, or both.\n\nTo enable compilation of this filter you need to configure FFmpeg with .\n\nThis muxer feeds audio data to the Chromaprint library, which generates a fingerprint for the provided audio data. See: https://acoustid.org/chromaprint\n\nIt takes a single signed native-endian 16-bit raw audio stream of at most 2 channels.\n\nThis muxer computes and prints the Adler-32 CRC of all the input audio and video frames. By default audio frames are converted to signed 16-bit raw audio and video frames to raw video before computing the CRC.\n\nThe output of the muxer consists of a single line of the form: CRC=0x , where is a hexadecimal number 0-padded to 8 digits containing the CRC for all the decoded input frames.\n\nSee also the framecrc muxer.\n• Use to compute the CRC of the input, and store it in the file :\n• Use to print the CRC to stdout with the command:\n• You can select the output format of each frame with by specifying the audio and video codec and format. For example, to compute the CRC of the input audio converted to PCM unsigned 8-bit and the input video converted to MPEG-2 video, use the command:\n\nThis muxer creates segments and manifest files according to the MPEG-DASH standard ISO/IEC 23009-1:2014 and following standard updates.\n\nFor more information see:\n\nThis muxer creates an MPD (Media Presentation Description) manifest file and segment files for each stream. Segment files are placed in the same directory of the MPD manifest file.\n\nThe segment filename might contain pre-defined identifiers used in the manifest section as defined in section 5.3.9.4.4 of the standard.\n\nAvailable identifiers are , , , and . In addition to the standard identifiers, an ffmpeg-specific identifier is also supported. When specified, will replace in the file name with muxing format’s extensions such as , etc.\n\nGenerate a DASH output reading from an input source in realtime using .\n\nTwo multimedia streams are generated from the input file, both containing a video stream encoded through ‘ ’, and an audio stream encoded with ‘ ’. The first multimedia stream contains video with a bitrate of 800k and audio at the default rate, the second with video scaled to 320x170 pixels at 300k and audio resampled at 22005 Hz.\n\nThe option keeps only the latest 5 segments with the default duration of 5 seconds.\n\nIt accepts a single 6-channels audio stream resampled at 96000 Hz encoded with the ‘ ’ codec.\n\nUse to mux input audio to a ‘ ’ channel layout resampled at 96000Hz:\n\nFor ffmpeg versions before 7.0 you might have to use the ‘ ’ filter to limit the muxed packet size, because this format does not support muxing packets larger than 65535 bytes (3640 samples). For newer ffmpeg versions audio is automatically packetized to 36000 byte (2000 sample) packets.\n\nIt accepts exactly one ‘ ’ video stream and at most two ‘ ’ audio streams. More constraints are defined by the property of the video, which must correspond to a DV video supported profile, and on the framerate.\n\nUse to convert the input:\n\nThis muxer writes the streams metadata in the ‘ ’ format.\n\nSee (ffmpeg-formats)the Metadata chapter for information about the format.\n\nUse to extract metadata from an input file to a file in ‘ ’ format:\n\nThe ‘ ’ pseudo-muxer allows the separation of encoding and muxing by using a first-in-first-out queue and running the actual muxer in a separate thread.\n\nThis is especially useful in combination with the tee muxer and can be used to send data to several destinations with different reliability/writing speed/latency.\n\nThe target muxer is either selected from the output name or specified through the option.\n\nThe behavior of the ‘ ’ muxer if the queue fills up or if the output fails (e.g. if a packet cannot be written to the output) is selectable:\n• Output can be transparently restarted with configurable delay between retries based on real time or time of the processed stream.\n• Encoding can be blocked during temporary failure, or continue transparently dropping packets in case the FIFO queue fills up.\n\nAPI users should be aware that callback functions ( , and ) used within its must be thread-safe.\n\nUse to stream to an RTMP server, continue processing the stream at real-time rate even in case of temporary failure (network outage) and attempt to recover streaming every second indefinitely:\n\nThis format was used as internal format for several Sega games.\n\nFor more information regarding the Sega film file format, visit http://wiki.multimedia.cx/index.php?title=Sega_FILM.\n\nIt accepts at maximum one ‘ ’ or raw video stream, and at maximum one audio stream.\n\nThis format is used by several Adobe tools to store a generated filmstrip export. It accepts a single raw video stream.\n\nThis image format is used to store astronomical data.\n\nFor more information regarding the format, visit https://fits.gsfc.nasa.gov.\n\nThis muxer accepts exactly one FLAC audio stream. Additionally, it is possible to add images with disposition ‘ ’.\n\nUse to store the audio stream from an input file, together with several pictures used with ‘ ’ disposition:\n\nThis muxer computes and prints the Adler-32 CRC for each audio and video packet. By default audio frames are converted to signed 16-bit raw audio and video frames to raw video before computing the CRC.\n\nThe output of the muxer consists of a line for each audio and video packet of the form:\n\nis a hexadecimal number 0-padded to 8 digits containing the CRC of the packet.\n\nFor example to compute the CRC of the audio and video frames in , converted to raw audio and video packets, and store it in the file :\n\nTo print the information to stdout, use the command:\n\nWith , you can select the output format to which the audio and video frames are encoded before computing the CRC for each packet by specifying the audio and video codec. For example, to compute the CRC of each decoded input audio frame converted to PCM unsigned 8-bit and of each decoded input video frame converted to MPEG-2 video, use the command:\n\nSee also the crc muxer.\n\nThis muxer computes and prints a cryptographic hash for each audio and video packet. This can be used for packet-by-packet equality checks without having to individually do a binary comparison on each.\n\nBy default audio frames are converted to signed 16-bit raw audio and video frames to raw video before computing the hash, but the output of explicit conversions to other codecs can also be used. It uses the SHA-256 cryptographic hash function by default, but supports several other algorithms.\n\nThe output of the muxer consists of a line for each audio and video packet of the form:\n\nis a hexadecimal number representing the computed hash for the packet.\n\nTo compute the SHA-256 hash of the audio and video frames in , converted to raw audio and video packets, and store it in the file :\n\nTo print the information to stdout, using the MD5 hash function, use the command:\n\nSee also the hash muxer.\n\nThis is a variant of the framehash muxer. Unlike that muxer, it defaults to using the MD5 hash function.\n\nTo compute the MD5 hash of the audio and video frames in , converted to raw audio and video packets, and store it in the file :\n\nTo print the information to stdout, use the command:\n\nSee also the framehash and md5 muxers.\n\nNote that the GIF format has a very large time base: the delay between two frames can therefore not be smaller than one centi second.\n\nEncode a gif looping 10 times, with a 5 seconds delay between the loops:\n\nNote 1: if you wish to extract the frames into separate GIF files, you need to force the image2 muxer:\n\nGXF was developed by Grass Valley Group, then standardized by SMPTE as SMPTE 360M and was extended in SMPTE RDD 14-2007 to include high-definition video resolutions.\n\nIt accepts at most one video stream with codec ‘ ’, or ‘ ’, or ‘ ’, or ‘ ’ with resolution ‘ ’ or ‘ ’, and several audio streams with rate 48000Hz and codec ‘ ’.\n\nThis muxer computes and prints a cryptographic hash of all the input audio and video frames. This can be used for equality checks without having to do a complete binary comparison.\n\nBy default audio frames are converted to signed 16-bit raw audio and video frames to raw video before computing the hash, but the output of explicit conversions to other codecs can also be used. Timestamps are ignored. It uses the SHA-256 cryptographic hash function by default, but supports several other algorithms.\n\nThe output of the muxer consists of a single line of the form: = , where is a short string representing the hash function used, and is a hexadecimal number representing the computed hash.\n\nTo compute the SHA-256 hash of the input converted to raw audio and video, and store it in the file :\n\nTo print an MD5 hash to stdout use the command:\n\nSee also the framehash muxer.\n\nHTTP dynamic streaming, or HDS, is an adaptive bitrate streaming method developed by Adobe. HDS delivers MP4 video content over HTTP connections. HDS can be used for on-demand streaming or live streaming.\n\nThis muxer creates an .f4m (Adobe Flash Media Manifest File) manifest, an .abst (Adobe Bootstrap File) for each stream, and segment files in a directory specified as the output.\n\nThese needs to be accessed by an HDS player throuhg HTTPS for it to be able to perform playback on the generated stream.\n\nUse to generate HDS files to the directory in real-time rate:\n\nApple HTTP Live Streaming muxer that segments MPEG-TS according to the HTTP Live Streaming (HLS) specification.\n\nIt creates a playlist file, and one or more segment files. The output filename specifies the playlist filename.\n\nBy default, the muxer creates a file for each segment produced. These files have the same name as the playlist, followed by a sequential number and a .ts extension.\n\nMake sure to require a closed GOP when encoding and to set the GOP size to fit your segment time constraint.\n\nFor example, to convert an input file with :\n\nThis example will produce the playlist, , and segment files: , , , etc.\n\nSee also the segment muxer, which provides a more generic and flexible implementation of a segmenter, and can be used to perform HLS segmentation.\n\nIAMF is used to provide immersive audio content for presentation on a wide range of devices in both streaming and offline applications. These applications include internet audio streaming, multicasting/broadcasting services, file download, gaming, communication, virtual and augmented reality, and others. In these applications, audio may be played back on a wide range of devices, e.g., headphones, mobile phones, tablets, TVs, sound bars, home theater systems, and big screens.\n\nThis format was promoted and desgined by Alliance for Open Media.\n\nFor more information about this format, see https://aomedia.org/iamf/.\n\nMicrosoft’s icon file format (ICO) has some strict limitations that should be noted:\n• Size cannot exceed 256 pixels in any dimension\n• Only BMP and PNG images can be stored\n• If a BMP image is used, it must be one of the following pixel formats:\n• If a BMP image is used, it must use the BITMAPINFOHEADER DIB header\n• If a PNG image is used, it must use the rgba pixel format\n\nThe output filenames are specified by a pattern, which can be used to produce sequentially numbered series of files. The pattern may contain the string \"%d\" or \"%0 d\", this string specifies the position of the characters representing a numbering in the filenames. If the form \"%0 d\" is used, the string representing the number in each filename is 0-padded to digits. The literal character ’%’ can be specified in the pattern with the string \"%%\".\n\nIf the pattern contains \"%d\" or \"%0 d\", the first filename of the file list specified will contain the number 1, all the following numbers will be sequential.\n\nThe pattern may contain a suffix which is used to automatically determine the format of the image files to write.\n\nFor example the pattern \"img-%03d.bmp\" will specify a sequence of filenames of the form , , ..., , etc. The pattern \"img%%-%d.jpg\" will specify a sequence of filenames of the form , , ..., , etc.\n\nThe image muxer supports the .Y.U.V image file format. This format is special in that each image frame consists of three files, for each of the YUV420P components. To read or write this image file format, specify the name of the ’.Y’ file. The muxer will automatically open the ’.U’ and ’.V’ files as required.\n\nThe ‘ ’ muxer accepts the same options as the ‘ ’ muxer, but ignores the pattern verification and expansion, as it is supposed to write to the command output rather than to an actual stored file.\n• Use for creating a sequence of files , , ..., taking one image every second from the input video: Note that with , if the format is not specified with the option and the output filename specifies an image file format, the image2 muxer is automatically selected, so the previous command can be written as: Note also that the pattern must not necessarily contain \"%d\" or \"%0 d\", for example to create a single image file from the start of the input video you can employ the command:\n• The option allows you to expand the filename with date and time information. Check the documentation of the function for the syntax. To generate image files from the \"%Y-%m-%d_%H-%M-%S\" pattern, the following command can be used:\n• Set the file name with current frame’s PTS:\n• Publish contents of your desktop directly to a WebDAV server every second:\n\nThe Berkeley/IRCAM/CARL Sound Format, developed in the 1980s, is a result of the merging of several different earlier sound file formats and systems including the csound system developed by Dr Gareth Loy at the Computer Audio Research Lab (CARL) at UC San Diego, the IRCAM sound file system developed by Rob Gross and Dan Timis at the Institut de Recherche et Coordination Acoustique / Musique in Paris and the Berkeley Fast Filesystem.\n\nIt was developed initially as part of the Berkeley/IRCAM/CARL Sound Filesystem, a suite of programs designed to implement a filesystem for audio applications running under Berkeley UNIX. It was particularly popular in academic music research centres, and was used a number of times in the creation of early computer-generated compositions.\n\nIVF was developed by On2 Technologies (formerly known as Duck Corporation), to store internally developed codecs.\n\nFor more information about the format, see http://unicorn.us.com/jacosub/jscripts.html.\n\nThis custom VAG container is used by some Simon & Schuster Interactive games such as \"Real War\", and \"Real War: Rogue States\".\n\nLRC (short for LyRiCs) is a computer file format that synchronizes song lyrics with an audio file, such as MP3, Vorbis, or MIDI.\n\nThe following metadata tags are converted to the format corresponding metadata:\n\nIf ‘ ’ is not explicitly set, it is automatically set to the libavformat version.\n\nThis muxer implements the matroska and webm container specs.\n\nThe recognized metadata settings in this muxer are:\n\nFor example a 3D WebM clip can be created using the following command line:\n\nThis is a variant of the hash muxer. Unlike that muxer, it defaults to using the MD5 hash function.\n\nSee also the hash and framemd5 muxers.\n• To compute the MD5 hash of the input converted to raw audio and video, and store it in the file :\n• To print the MD5 hash to stdout:\n\nSMAF is a music data format specified by Yamaha for portable electronic devices, such as mobile phones and personal digital assistants.\n\nThe MP3 muxer writes a raw MP3 stream with the following optional features:\n• An ID3v2 metadata header at the beginning (enabled by default). Versions 2.3 and 2.4 are supported, the private option controls which one is used (3 or 4). Setting to 0 disables the ID3v2 header completely. The muxer supports writing attached pictures (APIC frames) to the ID3v2 header. The pictures are supplied to the muxer in form of a video stream with a single packet. There can be any number of those streams, each will correspond to a single APIC frame. The stream metadata tags and map to APIC and respectively. See http://id3.org/id3v2.4.0-frames for allowed picture types. Note that the APIC frames must be written at the beginning, so the muxer will buffer the audio frames until it gets all the pictures. It is therefore advised to provide the pictures as soon as possible to avoid excessive buffering.\n• A Xing/LAME frame right after the ID3v2 header (if present). It is enabled by default, but will be written only if the output is seekable. The private option can be used to disable it. The frame contains various information that may be useful to the decoder, like the audio duration or encoder delay.\n• A legacy ID3v1 tag at the end of the file (disabled by default). It may be enabled with the private option, but as its capabilities are very limited, its usage is not recommended.\n\nWrite an mp3 with an ID3v2.3 header and an ID3v1 footer:\n\nTo attach a picture to an mp3 file select both the audio and the picture stream with :\n\nThis muxer implements ISO 13818-1 and part of ETSI EN 300 468.\n\nThe recognized metadata settings in mpegts muxer are and . If they are not set the default for is ‘ ’ and the default for is ‘ ’.\n\nThis muxer does not generate any output file, it is mainly useful for testing or benchmarking purposes.\n\nFor example to benchmark decoding with you can use the command:\n\nNote that the above command does not read or write the file, but specifying the output file is required by the syntax.\n\nAlternatively you can write the command as:\n\nRCWT (Raw Captions With Time) is a format native to ccextractor, a commonly used open source tool for processing 608/708 Closed Captions (CC) sources. It can be used to archive the original extracted CC bitstream and to produce a source file for later processing or conversion. The format allows for interoperability between ccextractor and FFmpeg, is simple to parse, and can be used to create a backup of the CC presentation.\n\nThis muxer implements the specification as of March 2024, which has been stable and unchanged since April 2014.\n\nThis muxer will have some nuances from the way that ccextractor muxes RCWT. No compatibility issues when processing the output with ccextractor have been observed as a result of this so far, but mileage may vary and outputs will not be a bit-exact match.\n\nA free specification of RCWT can be found here: https://github.com/CCExtractor/ccextractor/blob/master/docs/BINARY_FILE_FORMAT.TXT\n\nThis muxer outputs streams to a number of separate files of nearly fixed duration. Output filename pattern can be set in a fashion similar to image2, or by using a template if the option is enabled.\n\nis a variant of the muxer used to write to streaming output formats, i.e. which do not require global headers, and is recommended for outputting e.g. to MPEG transport stream segments. is a shorter alias for .\n\nEvery segment starts with a keyframe of the selected reference stream, which is set through the option.\n\nNote that if you want accurate splitting for a video file, you need to make the input key frames correspond to the exact splitting times expected by the segmenter, or the segment muxer will start the new segment with the key frame found next after the specified start time.\n\nThe segment muxer works best with a single constant frame rate video.\n\nOptionally it can generate a list of the created segments, by setting the option . The list type is specified by the option. The entry filenames in the segment list are set by default to the basename of the corresponding segment files.\n\nSee also the hls muxer, which provides a more specific implementation for HLS segmentation.\n\nThe segment muxer supports the following options:\n\nMake sure to require a closed GOP when encoding and to set the GOP size to fit your segment time constraint.\n• Remux the content of file to a list of segments , , etc., and write the list of generated segments to :\n• Segment input and set output format options for the output segments:\n• Segment the input file according to the split points specified by the option:\n• Use the option to force key frames in the input at the specified location, together with the segment option to account for possible roundings operated when setting key frame times. In order to force key frames on the input file, transcoding is required.\n• Segment the input file by splitting the input file according to the frame numbers sequence specified with the option:\n• Convert the to TS segments using the and encoders:\n• Segment the input file, and create an M3U8 live playlist (can be used as live HLS source):\n\nSmooth Streaming muxer generates a set of files (Manifest, chunks) suitable for serving with conventional web server.\n\nThis muxer computes and prints a cryptographic hash of all the input frames, on a per-stream basis. This can be used for equality checks without having to do a complete binary comparison.\n\nBy default audio frames are converted to signed 16-bit raw audio and video frames to raw video before computing the hash, but the output of explicit conversions to other codecs can also be used. Timestamps are ignored. It uses the SHA-256 cryptographic hash function by default, but supports several other algorithms.\n\nThe output of the muxer consists of one line per stream of the form: , , = , where is the index of the mapped stream, is a single character indicating the type of stream, is a short string representing the hash function used, and is a hexadecimal number representing the computed hash.\n\nTo compute the SHA-256 hash of the input converted to raw audio and video, and store it in the file :\n\nTo print an MD5 hash to stdout use the command:\n\nSee also the hash and framehash muxers.\n\nThe tee muxer can be used to write the same data to several outputs, such as files or streams. It can be used, for example, to stream a video over a network and save it to disk at the same time.\n\nIt is different from specifying several outputs to the command-line tool. With the tee muxer, the audio and video data will be encoded only once. With conventional multiple outputs, multiple encoding operations in parallel are initiated, which can be a very expensive process. The tee muxer is not useful when using the libavformat API directly because it is then possible to feed the same packets to several muxers directly.\n\nSince the tee muxer does not represent any particular output format, ffmpeg cannot auto-select output streams. So all streams intended for output must be specified using . See the examples below.\n\nSome encoders may need different options depending on the output format; the auto-detection of this can not work with the tee muxer, so they need to be explicitly specified. The main example is the flag.\n\nThe slave outputs are specified in the file name given to the muxer, separated by ’|’. If any of the slave name contains the ’|’ separator, leading or trailing spaces or any special character, those must be escaped (see (ffmpeg-utils)the \"Quoting and escaping\" section in the ffmpeg-utils(1) manual).\n\nMuxer options can be specified for each slave by prepending them as a list of = pairs separated by ’:’, between square brackets. If the options values contain a special character or the ’:’ separator, they must be escaped; note that this is a second level escaping.\n\nThe following special options are also recognized:\n• Encode something and both archive it in a WebM file and stream it as MPEG-TS over UDP:\n• As above, but continue streaming even if output to local file fails (for example local drive fills up):\n• Use to encode the input, and send the output to three different destinations. The bitstream filter is used to add extradata information to all the output video keyframes packets, as requested by the MPEG-TS format. The select option is applied to in order to make it contain only audio packets.\n• As above, but select only stream for the audio output. Note that a second level escaping must be performed, as \":\" is a special character used to separate options.\n\nThis muxer writes out WebM headers and chunks as separate files which can be consumed by clients that support WebM Live streams via DASH.\n\nThis muxer supports the following options:\n\nThis muxer implements the WebM DASH Manifest specification to generate the DASH manifest XML. It also supports manifest generation for DASH live streams.\n\nFor more information see:\n\nThis muxer supports the following options:\n\nFFmpeg is able to dump metadata from media files into a simple UTF-8-encoded INI-like text file and then load it back using the metadata muxer/demuxer.\n\nThe file format is as follows:\n• A file consists of a header and a number of metadata tags divided into sections, each on its own line.\n• The header is a ‘ ’ string, followed by a version number (now 1).\n• Metadata tags are of the form ‘ ’\n• After global metadata there may be sections with per-stream/per-chapter metadata.\n• A section starts with the section name in uppercase (i.e. STREAM or CHAPTER) in brackets (‘ ’, ‘ ’) and ends with next section or end of file.\n• At the beginning of a chapter section there may be an optional timebase to be used for start/end values. It must be in form ‘ ’, where and are integers. If the timebase is missing then start/end times are assumed to be in nanoseconds. Next a chapter section must contain chapter start and end times in form ‘ ’, ‘ ’, where is a positive integer.\n• Empty lines and lines starting with ‘ ’ or ‘ ’ are ignored.\n• Metadata keys or values containing special characters (‘ ’, ‘ ’, ‘ ’, ‘ ’ and a newline) must be escaped with a backslash ‘ ’.\n• Note that whitespace in metadata (e.g. ‘ ’) is considered to be a part of the tag (in the example above key is ‘ ’, value is ‘ ’).\n\nA ffmetadata file might look like this:\n\nBy using the ffmetadata muxer and demuxer it is possible to extract metadata from an input file to an ffmetadata file, and then transcode the file into an output file with the edited ffmetadata file.\n\nExtracting an ffmetadata file with goes as follows:\n\nReinserting edited metadata information from the FFMETADATAFILE file can be done as:\n\nThe libavformat library provides some generic global options, which can be set on all the protocols. In addition each protocol may support so-called private options, which are specific for that component.\n\nOptions may be set by specifying - in the FFmpeg tools, or by setting the value explicitly in the options or using the API for programmatic use.\n\nThe list of supported options follows:\n\nProtocols are configured elements in FFmpeg that enable access to resources that require specific protocols.\n\nWhen you configure your FFmpeg build, all the supported protocols are enabled by default. You can list all available ones using the configure option \"–list-protocols\".\n\nYou can disable all the protocols using the configure option \"–disable-protocols\", and selectively enable a protocol using the option \"–enable-protocol= \", or you can disable a particular protocol using the option \"–disable-protocol= \".\n\nThe option \"-protocols\" of the ff* tools will display the list of supported protocols.\n\nAll protocols accept the following options:\n\nA description of the currently available protocols follows.\n\nFFmpeg must be compiled with –enable-librabbitmq to support AMQP. A separate AMQP broker must also be run. An example open-source AMQP broker is RabbitMQ.\n\nAfter starting the broker, an FFmpeg client may stream data to the broker using the command:\n\nWhere hostname and port (default is 5672) is the address of the broker. The client may also set a user/password for authentication. The default for both fields is \"guest\". Name of virtual host on broker can be set with vhost. The default value is \"/\".\n\nMuliple subscribers may stream from the broker using the command:\n\nIn RabbitMQ all data published to the broker flows through a specific exchange, and each subscribing client has an assigned queue/buffer. When a packet arrives at an exchange, it may be copied to a client’s queue depending on the exchange and routing_key fields.\n\nThe following options are supported:\n\nFill data in a background thread, to decouple I/O operation from demux thread.\n\nRead angle 2 of playlist 4 from BluRay mounted to /mnt/bluray, start from chapter 2:\n\nCache the input stream to temporary file. It brings seeking capability to live streams.\n\nRead and seek from many resources in sequence as if they were a unique resource.\n\nA URL accepted by this protocol has the syntax:\n\nwhere , , ..., are the urls of the resource to be concatenated, each one possibly specifying a distinct protocol.\n\nFor example to read a sequence of files , , with use the command:\n\nNote that you may need to escape the character \"|\" which is special for many shells.\n\nRead and seek from many resources in sequence as if they were a unique resource.\n\nA URL accepted by this protocol has the syntax:\n\nwhere is the url containing a line break delimited list of resources to be concatenated, each one possibly specifying a distinct protocol. Special characters must be escaped with backslash or single quotes. See (ffmpeg-utils)the \"Quoting and escaping\" section in the ffmpeg-utils(1) manual.\n\nFor example to read a sequence of files , , listed in separate lines within a file with use the command:\n\nWhere contains the lines:\n\nData in-line in the URI. See http://en.wikipedia.org/wiki/Data_URI_scheme.\n\nFor example, to convert a GIF file given inline with :\n\nIf is not specified, by default the stdout file descriptor will be used for writing, stdin for reading. Unlike the pipe protocol, fd protocol has seek support if it corresponding to a regular file. fd protocol doesn’t support pass file descriptor via URL for security.\n\nThis protocol accepts the following options:\n\nRead from or write to a file.\n\nA file URL can have the form:\n\nwhere is the path of the file to read.\n\nAn URL that does not have a protocol prefix will be assumed to be a file URL. Depending on the build, an URL that looks like a Windows path with the drive letter at the beginning will also be assumed to be a file URL (usually not the case in builds for unix-like systems).\n\nFor example to read from a file with use the command:\n\nThis protocol accepts the following options:\n\nRead from or write to remote resources using FTP protocol.\n\nThis protocol accepts the following options.\n\nNOTE: Protocol can be used as output, but it is recommended to not do it, unless special care is taken (tests, customized server configuration etc.). Different FTP servers behave in different way during seek operation. ff* tools may produce incomplete content due to server limitations.\n\nRead Apple HTTP Live Streaming compliant segmented stream as a uniform one. The M3U8 playlists describing the segments can be remote HTTP resources or local files, accessed using the standard file protocol. The nested protocol is declared by specifying \"+ \" after the hls URI scheme name, where is either \"file\" or \"http\".\n\nUsing this protocol is discouraged - the hls demuxer should work just as well (if not, please report the issues) and is more complete. To use the hls demuxer instead, simply use the direct URLs to the m3u8 files.\n\nThis protocol accepts the following options:\n\nSome HTTP requests will be denied unless cookie values are passed in with the request. The option allows these cookies to be specified. At the very least, each cookie must specify a value along with a path and domain. HTTP requests that match both the domain and path will automatically include the cookie value in the HTTP Cookie header field. Multiple cookies can be delimited by a newline.\n\nThe required syntax to play a stream specifying a cookie is:\n\nThis protocol accepts the following options:\n\nInterPlanetary File System (IPFS) protocol support. One can access files stored on the IPFS network through so-called gateways. These are http(s) endpoints. This protocol wraps the IPFS native protocols (ipfs:// and ipns://) to be sent to such a gateway. Users can (and should) host their own node which means this protocol will use one’s local gateway to access files on the IPFS network.\n\nThis protocol accepts the following options:\n\nOne can use this protocol in 2 ways. Using IPFS:\n\nOr the IPNS protocol (IPNS is mutable IPFS):\n\nComputes the MD5 hash of the data to be written, and on close writes this to the designated output or stdout if none is specified. It can be used to test muxers without writing an actual file.\n\nNote that some formats (typically MOV) require the output protocol to be seekable, so they will fail with the MD5 output protocol.\n\nIf isn’t specified, is the number corresponding to the file descriptor of the pipe (e.g. 0 for stdin, 1 for stdout, 2 for stderr). If is not specified, by default the stdout file descriptor will be used for writing, stdin for reading.\n\nFor example to read from stdin with :\n\nFor writing to stdout with :\n\nThis protocol accepts the following options:\n\nNote that some formats (typically MOV), require the output protocol to be seekable, so they will fail with the pipe output protocol.\n\nThe Pro-MPEG CoP#3 FEC is a 2D parity-check forward error correction mechanism for MPEG-2 Transport Streams sent over RTP.\n\nThis protocol must be used in conjunction with the muxer and the protocol.\n\nThe destination UDP ports are for the column FEC stream and for the row FEC stream.\n\nThis protocol accepts the following options:\n\nThe Real-Time Messaging Protocol (RTMP) is used for streaming multimedia content across a TCP/IP network.\n\nAdditionally, the following parameters can be set via command line options (or in code via s):\n\nFor example to read with a multimedia resource named \"sample\" from the application \"vod\" from an RTMP server \"myserver\":\n\nTo publish to a password protected server, passing the playpath and app names separately:\n\nThe Encrypted Real-Time Messaging Protocol (RTMPE) is used for streaming multimedia content within standard cryptographic primitives, consisting of Diffie-Hellman key exchange and HMACSHA256, generating a pair of RC4 keys.\n\nThe Real-Time Messaging Protocol (RTMPS) is used for streaming multimedia content across an encrypted connection.\n\nThe Real-Time Messaging Protocol tunneled through HTTP (RTMPT) is used for streaming multimedia content within HTTP requests to traverse firewalls.\n\nThe Encrypted Real-Time Messaging Protocol tunneled through HTTP (RTMPTE) is used for streaming multimedia content within HTTP requests to traverse firewalls.\n\nThe Real-Time Messaging Protocol tunneled through HTTPS (RTMPTS) is used for streaming multimedia content within HTTPS requests to traverse firewalls.\n\nThis protocol accepts the following options.\n\nFor more information see: http://www.samba.org/.\n\nRead from or write to remote resources using SFTP protocol.\n\nThis protocol accepts the following options.\n\nReal-Time Messaging Protocol and its variants supported through librtmp.\n\nRequires the presence of the librtmp headers and library during configuration. You need to explicitly configure the build with \"–enable-librtmp\". If enabled this will replace the native RTMP protocol.\n\nThis protocol provides most client functions and a few server functions needed to support RTMP, RTMP tunneled in HTTP (RTMPT), encrypted RTMP (RTMPE), RTMP over SSL/TLS (RTMPS) and tunneled variants of these encrypted types (RTMPTE, RTMPTS).\n\nwhere is one of the strings \"rtmp\", \"rtmpt\", \"rtmpe\", \"rtmps\", \"rtmpte\", \"rtmpts\" corresponding to each RTMP variant, and , , and have the same meaning as specified for the RTMP native protocol. contains a list of space-separated options of the form = .\n\nSee the librtmp manual page (man 3 librtmp) for more information.\n\nFor example, to stream a file in real-time to an RTMP server using :\n\nTo play the same stream using :\n\nThe required syntax for an RTP URL is:\n\nspecifies the RTP port to use.\n\ncontains a list of &-separated options of the form = .\n\nThe following URL options are supported:\n• If is not set the RTCP port will be set to the RTP port value plus 1.\n• If (the local RTP port) is not set any available port will be used for the local RTP and RTCP ports.\n• If (the local RTCP port) is not set it will be set to the local RTP port value plus 1.\n\nRTSP is not technically a protocol handler in libavformat, it is a demuxer and muxer. The demuxer supports both normal RTSP (with data transferred over RTP; this is used by e.g. Apple and Microsoft) and Real-RTSP (with data transferred over RDT).\n\nThe muxer can be used to send a stream using RTSP ANNOUNCE to a server supporting it (currently Darwin Streaming Server and Mischa Spiegelmock’s RTSP server).\n\nThe required syntax for a RTSP url is:\n\nOptions can be set on the / command line, or set in code via s or in .\n\nThe following options are supported.\n\nThe following options are supported.\n\nWhen receiving data over UDP, the demuxer tries to reorder received packets (since they may arrive out of order, or packets may get lost totally). This can be disabled by setting the maximum demuxing delay to zero (via the field of AVFormatContext).\n\nWhen watching multi-bitrate Real-RTSP streams with , the streams to display can be chosen with and for video and audio respectively, and can be switched on the fly by pressing and .\n\nThe following examples all make use of the and tools.\n• Watch a stream over UDP, with a max reordering delay of 0.5 seconds:\n• Send a stream in realtime to a RTSP server, for others to watch:\n\nSession Announcement Protocol (RFC 2974). This is not technically a protocol handler in libavformat, it is a muxer and demuxer. It is used for signalling of RTP streams, by announcing the SDP for the streams regularly on a separate port.\n\nThe syntax for a SAP url given to the muxer is:\n\nThe RTP packets are sent to on port , or to port 5004 if no port is specified. is a -separated list. The following options are supported:\n\nTo broadcast a stream on the local subnet, for watching in VLC:\n\nAnd for watching in , over IPv6:\n\nThe syntax for a SAP url given to the demuxer is:\n\nis the multicast address to listen for announcements on, if omitted, the default 224.2.127.254 (sap.mcast.net) is used. is the port that is listened on, 9875 if omitted.\n\nThe demuxers listens for announcements on the given address and port. Once an announcement is received, it tries to receive that particular stream.\n\nTo play back the first stream announced on the normal SAP multicast address:\n\nTo play back the first stream announced on one the default IPv6 SAP multicast address:\n\nThe protocol accepts the following options:\n\nThe supported syntax for a SRT URL is:\n\ncontains a list of &-separated options of the form = .\n\nThis protocol accepts the following options.\n\nFor more information see: https://github.com/Haivision/srt.\n\nVirtually extract a segment of a file or another stream. The underlying stream must be seekable.\n\nExtract a chapter from a DVD VOB file (start and end sectors obtained externally and multiplied by 2048):\n\nWrites the output to multiple protocols. The individual outputs are separated by |\n\nThe required syntax for a TCP url is:\n\ncontains a list of &-separated options of the form = .\n\nThe list of supported options follows.\n\nThe following example shows how to setup a listening TCP connection with , which is then accessed with :\n\nThe required syntax for a TLS/SSL url is:\n\nThe following parameters can be set via command line options (or in code via s):\n\nTo create a TLS/SSL server that serves an input stream.\n\nTo play back a stream from the TLS/SSL server using :\n\nThe required syntax for an UDP URL is:\n\ncontains a list of &-separated options of the form = .\n\nIn case threading is enabled on the system, a circular buffer is used to store the incoming data, which allows one to reduce loss of data due to UDP socket buffer overruns. The and options are related to this buffer.\n\nThe list of supported options follows.\n• Use to stream over UDP to a remote endpoint:\n• Use to stream in mpegts format over UDP using 188 sized UDP packets, using a large input buffer:\n• Use to receive over UDP from a remote endpoint:\n\nThe required syntax for a Unix socket URL is:\n\nThe following parameters can be set via command line options (or in code via s):\n\nThis library supports unicast streaming to multiple clients without relying on an external server.\n\nThe required syntax for streaming or connecting to a stream is:\n\nMultiple clients may connect to the stream using:\n\nStreaming to multiple clients is implemented using a ZeroMQ Pub-Sub pattern. The server side binds to a port and publishes data. Clients connect to the server (via IP address/port) and subscribe to the stream. The order in which the server and client start generally does not matter.\n\nffmpeg must be compiled with the –enable-libzmq option to support this protocol.\n\nOptions can be set on the / command line. The following options are supported:\n\nThe libavdevice library provides the same interface as libavformat. Namely, an input device is considered like a demuxer, and an output device like a muxer, and the interface and generic device options are the same provided by libavformat (see the ffmpeg-formats manual).\n\nIn addition each input or output device may support so-called private options, which are specific for that component.\n\nOptions may be set by specifying - in the FFmpeg tools, or by setting the value explicitly in the device options or using the API for programmatic use.\n\nInput devices are configured elements in FFmpeg which enable accessing the data coming from a multimedia device attached to your system.\n\nWhen you configure your FFmpeg build, all the supported input devices are enabled by default. You can list all available ones using the configure option \"–list-indevs\".\n\nYou can disable all the input devices using the configure option \"–disable-indevs\", and selectively enable an input device using the option \"–enable-indev= \", or you can disable a particular input device using the option \"–disable-indev= \".\n\nThe option \"-devices\" of the ff* tools will display the list of supported input devices.\n\nA description of the currently available input devices follows.\n\nTo enable this input device during configuration you need libasound installed on your system.\n\nThis device allows capturing from an ALSA device. The name of the device to capture has to be an ALSA card identifier.\n\nAn ALSA identifier has the syntax:\n\nwhere the and components are optional.\n\nThe three arguments (in order: , , ) specify card number or identifier, device number and subdevice number (-1 means any).\n\nTo see the list of cards currently recognized by your system check the files and .\n\nFor example to capture with from an ALSA device with card id 0, you may run the command:\n\nFor more information see: http://www.alsa-project.org/alsa-doc/alsa-lib/pcm.html\n\nThis input devices uses the Android Camera2 NDK API which is available on devices with API level 24+. The availability of android_camera is autodetected during configuration.\n\nThis device allows capturing from all cameras on an Android device, which are integrated into the Camera2 NDK API.\n\nThe available cameras are enumerated internally and can be selected with the parameter. The input file string is discarded.\n\nGenerally the back facing camera has index 0 while the front facing camera has index 1.\n\nAVFoundation is the currently recommended framework by Apple for streamgrabbing on OSX >= 10.7 as well as on iOS.\n\nThe input filename has to be given in the following syntax:\n\nThe first entry selects the video input while the latter selects the audio input. The stream has to be specified by the device name or the device index as shown by the device list. Alternatively, the video and/or audio input device can be chosen by index using the and/or , overriding any device name or index given in the input filename.\n\nAll available devices can be enumerated by using , listing all device names and corresponding indices.\n\nThere are two device name aliases:\n• Print the list of AVFoundation supported devices and exit:\n• Record video from video device 0 and audio from audio device 0 into out.avi:\n• Record video from video device 2 and audio from audio device 1 into out.avi:\n• Record video from the system default video device using the pixel format bgr0 and do not record any audio into out.avi:\n• Record raw DV data from a suitable input device and write the output into out.dv:\n\nBSD video input device. Deprecated and will be removed - please contact the developers if you are interested in maintaining it.\n\nThe decklink input device provides capture capabilities for Blackmagic DeckLink devices.\n\nTo enable this input device, you need the Blackmagic DeckLink SDK and you need to configure with the appropriate and . On Windows, you need to run the IDL files through .\n\nDeckLink is very picky about the formats it supports. Pixel format of the input can be set with . Framerate and video size must be determined for your device with . Audio sample rate is always 48 kHz and the number of channels can be 2, 8 or 16. Note that all audio channels are bundled in one single audio track.\n\nDirectShow support is enabled when FFmpeg is built with the mingw-w64 project. Currently only audio and video devices are supported.\n\nMultiple devices may be opened as separate inputs, but they may also be opened on the same input, which should improve synchronism between them.\n\nThe input name should be in the format:\n\nwhere can be either or , and is the device’s name or alternative name..\n\nIf no options are specified, the device’s defaults are used. If the device does not support the requested options, it will fail to open.\n• Print the list of DirectShow supported devices and exit:\n• Open second video device with name :\n• Print the list of supported options in selected device and exit:\n• Specify pin names to capture by name or alternative name, specify alternative device name:\n• Configure a crossbar device, specifying crossbar pins, allow user to adjust video capture properties at startup:\n\nThe Linux framebuffer is a graphic hardware-independent abstraction layer to show graphics on a computer monitor, typically on the console. It is accessed through a file device node, usually .\n\nFor more detailed information read the file Documentation/fb/framebuffer.txt included in the Linux source tree.\n\nSee also http://linux-fbdev.sourceforge.net/, and fbset(1).\n\nTo record from the framebuffer device with :\n\nYou can take a single screenshot image with the command:\n\nThis device allows you to capture a region of the display on Windows.\n\nAmongst options for the imput filenames are such elements as:\n\nThe first option will capture the entire desktop, or a fixed region of the desktop. The second and third options will instead capture the contents of a single window, regardless of its position on the screen.\n\nFor example, to grab the entire desktop using :\n\nGrab the contents of the window named \"Calculator\"\n\nTo enable this input device, you need libiec61883, libraw1394 and libavc1394 installed on your system. Use the configure option to compile with the device enabled.\n\nThe iec61883 capture device supports capturing from a video device connected via IEEE1394 (FireWire), using libiec61883 and the new Linux FireWire stack (juju). This is the default DV/HDV input method in Linux Kernel 2.6.37 and later, since the old FireWire stack was removed.\n\nSpecify the FireWire port to be used as input file, or \"auto\" to choose the first port connected.\n• Grab and show the input of a FireWire DV/HDV device.\n• Grab and record the input of a FireWire DV/HDV device, using a packet buffer of 100000 packets if the source is HDV.\n\nTo enable this input device during configuration you need libjack installed on your system.\n\nA JACK input device creates one or more JACK writable clients, one for each audio channel, with name :input_ , where is the name provided by the application, and is a number which identifies the channel. Each writable client will send the acquired data to the FFmpeg input device.\n\nOnce you have created one or more JACK readable clients, you need to connect them to one or more JACK writable clients.\n\nTo connect or disconnect JACK clients you can use the and programs, or do it through a graphical interface, for example with .\n\nTo list the JACK clients and their properties you can invoke the command .\n\nFollows an example which shows how to capture a JACK readable client with .\n\nCaptures the KMS scanout framebuffer associated with a specified CRTC or plane as a DRM object that can be passed to other hardware functions.\n\nRequires either DRM master or CAP_SYS_ADMIN to run.\n\nIf you don’t understand what all of that means, you probably don’t want this. Look at instead.\n• Capture from the first active plane, download the result to normal frames and encode. This will only work if the framebuffer is both linear and mappable - if not, the result may be scrambled or fail to download.\n• Capture from CRTC ID 42 at 60fps, map the result to VAAPI, convert to NV12 and encode as H.264.\n• To capture only part of a plane the output can be cropped - this can be used to capture a single window, as long as it has a known absolute position and size. For example, to capture and encode the middle quarter of a 1920x1080 plane:\n\nThis input device reads data from the open output pads of a libavfilter filtergraph.\n\nFor each filtergraph open output, the input device will create a corresponding stream which is mapped to the generated output. The filtergraph is specified through the option .\n• Create a color video stream and play it back with :\n• As the previous example, but use filename for specifying the graph description, and omit the \"out0\" label:\n• Create three different video test filtered sources and play them:\n• Read an audio stream from a file using the amovie source and play it back with :\n• Read an audio stream and a video stream and play it back with :\n• Dump decoded frames to images and Closed Captions to an RCWT backup:\n\nTo enable this input device during configuration you need libcdio installed on your system. It requires the configure option .\n\nThis device allows playing and grabbing from an Audio-CD.\n\nFor example to copy with the entire Audio-CD in , you may run the command:\n\nThe OpenAL input device provides audio capture on all systems with a working OpenAL 1.1 implementation.\n\nTo enable this input device during configuration, you need OpenAL headers and libraries installed on your system, and need to configure FFmpeg with .\n\nOpenAL headers and libraries should be provided as part of your OpenAL implementation, or as an additional download (an SDK). Depending on your installation you may need to specify additional flags via the and for allowing the build system to locate the OpenAL headers and libraries.\n\nAn incomplete list of OpenAL implementations follows:\n\nThis device allows one to capture from an audio input device handled through OpenAL.\n\nYou need to specify the name of the device to capture in the provided filename. If the empty string is provided, the device will automatically select the default device. You can get the list of the supported devices by using the option .\n\nPrint the list of OpenAL supported devices and exit:\n\nCapture from the default device (note the empty string ” as filename):\n\nCapture from two devices simultaneously, writing to two different files, within the same command:\n\nNote: not all OpenAL implementations support multiple simultaneous capture - try the latest OpenAL Soft if the above does not work.\n\nThe filename to provide to the input device is the device node representing the OSS input device, and is usually set to .\n\nFor example to grab from using use the command:\n\nFor more information about OSS see: http://manuals.opensound.com/usersguide/dsp.html\n\nTo enable this output device you need to configure FFmpeg with .\n\nThe filename to provide to the input device is a source device or the string \"default\"\n\nTo list the PulseAudio source devices and their properties you can invoke the command .\n\nMore information about PulseAudio can be found on http://www.pulseaudio.org.\n\nTo enable this input device during configuration you need libsndio installed on your system.\n\nThe filename to provide to the input device is the device node representing the sndio input device, and is usually set to .\n\nFor example to grab from using use the command:\n\n\"v4l2\" can be used as alias for \"video4linux2\".\n\nIf FFmpeg is built with v4l-utils support (by using the configure option), it is possible to use it with the input device option.\n\nThe name of the device to grab is a file device node, usually Linux systems tend to automatically create such nodes when the device (e.g. an USB webcam) is plugged into the system, and has a name of the kind , where is a number associated to the device.\n\nVideo4Linux2 devices usually support a limited set of x sizes and frame rates. You can check which are supported using for Video4Linux2 devices. Some devices, like TV cards, support one or more standards. It is possible to list all the supported standards using .\n\nThe time base for the timestamps is 1 microsecond. Depending on the kernel version and configuration, the timestamps may be derived from the real time clock (origin at the Unix Epoch) or the monotonic clock (origin usually at boot time, unaffected by NTP or manual changes to the clock). The or option can be used to force conversion into the real time clock.\n\nSome usage examples of the video4linux2 device with and :\n• Grab and show the input of a video4linux2 device:\n• Grab and record the input of a video4linux2 device, leave the frame rate and size as previously set:\n\nFor more information about Video4Linux, check http://linuxtv.org/.\n\nThe filename passed as input is the capture driver number, ranging from 0 to 9. You may use \"list\" as filename to print a list of drivers. Any other filename will be interpreted as device number 0.\n\nTo enable this input device during configuration you need libxcb installed on your system. It will be automatically detected during configuration.\n\nThis device allows one to capture a region of an X11 display.\n\nThe filename passed as input has the syntax:\n\n: . specifies the X11 display name of the screen to grab from. can be omitted, and defaults to \"localhost\". The environment variable contains the default display name.\n\nand specify the offsets of the grabbed area with respect to the top-left border of the X11 screen. They default to 0.\n\nCheck the X11 documentation (e.g. ) for more detailed information.\n\nUse the program for getting basic information about the properties of your X11 display (e.g. grep for \"name\" or \"dimensions\").\n\nFor example to grab from using :\n\nOutput devices are configured elements in FFmpeg that can write multimedia data to an output device attached to your system.\n\nWhen you configure your FFmpeg build, all the supported output devices are enabled by default. You can list all available ones using the configure option \"–list-outdevs\".\n\nYou can disable all the output devices using the configure option \"–disable-outdevs\", and selectively enable an output device using the option \"–enable-outdev= \", or you can disable a particular input device using the option \"–disable-outdev= \".\n\nThe option \"-devices\" of the ff* tools will display the list of enabled output devices.\n\nA description of the currently available output devices follows.\n\nAllows native output to CoreAudio devices on OSX.\n\nThe output filename can be empty (or ) to refer to the default system output device or a number that refers to the device index as shown using: .\n\nAlternatively, the audio input device can be chosen by index using the , overriding any device name or index given in the input filename.\n\nAll available devices can be enumerated by using , listing all device names, UIDs and corresponding indices.\n• Print the list of supported devices and output a sine wave to the default device:\n• Output a sine wave to the device with the index 2, overriding any output filename:\n\nThis output device allows one to show a video stream in CACA window. Only one CACA window is allowed per application, so you can have only one instance of this output device in an application.\n\nTo enable this output device you need to configure FFmpeg with . libcaca is a graphics library that outputs text instead of pixels.\n\nFor more information about libcaca, check: http://caca.zoy.org/wiki/libcaca\n• The following command shows the output is an CACA window, forcing its size to 80x25:\n• Show the list of available drivers and exit:\n• Show the list of available dither colors and exit:\n\nThe decklink output device provides playback capabilities for Blackmagic DeckLink devices.\n\nTo enable this output device, you need the Blackmagic DeckLink SDK and you need to configure with the appropriate and . On Windows, you need to run the IDL files through .\n\nDeckLink is very picky about the formats it supports. Pixel format is always uyvy422, framerate, field order and video size must be determined for your device with . Audio sample rate is always 48 kHz.\n\nThe Linux framebuffer is a graphic hardware-independent abstraction layer to show graphics on a computer monitor, typically on the console. It is accessed through a file device node, usually .\n\nFor more detailed information read the file included in the Linux source tree.\n\nSee also http://linux-fbdev.sourceforge.net/, and fbset(1).\n\nOpenGL output device. Deprecated and will be removed.\n\nTo enable this output device you need to configure FFmpeg with .\n\nThis output device allows one to render to OpenGL context. Context may be provided by application or default SDL window is created.\n\nWhen device renders to external context, application must implement handlers for following messages: - create OpenGL context on current thread. - make OpenGL context current. - swap buffers. - destroy OpenGL context. Application is also required to inform a device about current resolution by sending message.\n\nTo enable this output device you need to configure FFmpeg with .\n\nMore information about PulseAudio can be found on http://www.pulseaudio.org\n\nSDL (Simple DirectMedia Layer) output device. Deprecated and will be removed.\n\nFor monitoring purposes in FFmpeg, pipes and a video player such as ffplay can be used:\n\n\"sdl2\" can be used as alias for \"sdl\".\n\nThis output device allows one to show a video stream in an SDL window. Only one SDL window is allowed per application, so you can have only one instance of this output device in an application.\n\nTo enable this output device you need libsdl installed on your system when configuring your build.\n\nFor more information about SDL, check: http://www.libsdl.org/\n\nThe window created by the device can be controlled through the following interactive commands.\n\nThe following command shows the output is an SDL window, forcing its size to the qcif format:\n\nThis output device allows one to show a video stream in a X Window System window.\n\nFor more information about XVideo see http://www.x.org/.\n• Decode, display and encode video input with at the same time:\n• Decode and display the input video to multiple X11 windows:\n\nThe audio resampler supports the following named options.\n\nOptions may be set by specifying - in the FFmpeg tools, = for the aresample filter, by setting the value explicitly in the options or using the API for programmatic use.\n\nThe video scaler supports the following named options.\n\nOptions may be set by specifying - in the FFmpeg tools, with a few API-only exceptions noted below. For programmatic use, they can be set explicitly in the options or through the API.\n\nFiltering in FFmpeg is enabled through the libavfilter library.\n\nIn libavfilter, a filter can have multiple inputs and multiple outputs. To illustrate the sorts of things that are possible, we consider the following filtergraph.\n\nThis filtergraph splits the input stream in two streams, then sends one stream through the crop filter and the vflip filter, before merging it back with the other stream by overlaying it on top. You can use the following command to achieve this:\n\nThe result will be that the top half of the video is mirrored onto the bottom half of the output video.\n\nFilters in the same linear chain are separated by commas, and distinct linear chains of filters are separated by semicolons. In our example, are in one linear chain, and are separately in another. The points where the linear chains join are labelled by names enclosed in square brackets. In the example, the split filter generates two outputs that are associated to the labels and .\n\nThe stream sent to the second output of , labelled as , is processed through the filter, which crops away the lower half part of the video, and then vertically flipped. The filter takes in input the first unchanged output of the split filter (which was labelled as ), and overlay on its lower half the output generated by the filterchain.\n\nSome filters take in input a list of parameters: they are specified after the filter name and an equal sign, and are separated from each other by a colon.\n\nThere exist so-called that do not have an audio/video input, and that will not have audio/video output.\n\nThe program included in the FFmpeg directory can be used to parse a filtergraph description and issue a corresponding textual representation in the dot language.\n\nto see how to use .\n\nYou can then pass the dot description to the program (from the graphviz suite of programs) and obtain a graphical representation of the filtergraph.\n\nFor example the sequence of commands:\n\ncan be used to create and display an image representing the graph described by the string. Note that this string must be a complete self-contained graph, with its inputs and outputs explicitly defined. For example if your command line is of the form:\n\nyour string will need to be of the form:\n\nyou may also need to set the parameters and add a filter in order to simulate a specific input file.\n\nA filtergraph is a directed graph of connected filters. It can contain cycles, and there can be multiple links between a pair of filters. Each link has one input pad on one side connecting it to one filter from which it takes its input, and one output pad on the other side connecting it to one filter accepting its output.\n\nEach filter in a filtergraph is an instance of a filter class registered in the application, which defines the features and the number of input and output pads of the filter.\n\nA filter with no input pads is called a \"source\", and a filter with no output pads is called a \"sink\".\n\nA filtergraph has a textual representation, which is recognized by the / / and options in and / in , and by the function defined in .\n\nA filterchain consists of a sequence of connected filters, each one connected to the previous one in the sequence. A filterchain is represented by a list of \",\"-separated filter descriptions.\n\nA filtergraph consists of a sequence of filterchains. A sequence of filterchains is represented by a list of \";\"-separated filterchain descriptions.\n\nA filter is represented by a string of the form: [ ]...[ ] @ = [ ]...[ ]\n\nis the name of the filter class of which the described filter is an instance of, and has to be the name of one of the filter classes registered in the program optionally followed by \"@ \". The name of the filter class is optionally followed by a string \"= \".\n\nis a string which contains the parameters used to initialize the filter instance. It may have one of two forms:\n• A ’:’-separated list of . In this case, the keys are assumed to be the option names in the order they are declared. E.g. the filter declares three options in this order – , and . Then the parameter list means that the value is assigned to the option , to and to .\n• A ’:’-separated list of mixed direct and long pairs. The direct must precede the pairs, and follow the same constraints order of the previous point. The following pairs can be set in any preferred order.\n\nIf the option value itself is a list of items (e.g. the filter takes a list of pixel formats), the items in the list are usually separated by ‘ ’.\n\nThe list of arguments can be quoted using the character ‘ ’ as initial and ending mark, and the character ‘ ’ for escaping the characters within the quoted text; otherwise the argument string is considered terminated when the next special character (belonging to the set ‘ ’) is encountered.\n\nA special syntax implemented in the CLI tool allows loading option values from files. This is done be prepending a slash ’/’ to the option name, then the supplied value is interpreted as a path from which the actual value is loaded. E.g.\n\nwill load the text to be drawn from . API users wishing to implement a similar feature should use the functions together with custom IO code.\n\nThe name and arguments of the filter are optionally preceded and followed by a list of link labels. A link label allows one to name a link and associate it to a filter output or input pad. The preceding labels ... , are associated to the filter input pads, the following labels ... , are associated to the output pads.\n\nWhen two link labels with the same name are found in the filtergraph, a link between the corresponding input and output pad is created.\n\nIf an output pad is not labelled, it is linked by default to the first unlabelled input pad of the next filter in the filterchain. For example in the filterchain\n\nthe split filter instance has two output pads, and the overlay filter instance two input pads. The first output pad of split is labelled \"L1\", the first input pad of overlay is labelled \"L2\", and the second output pad of split is linked to the second input pad of overlay, which are both unlabelled.\n\nIn a filter description, if the input label of the first filter is not specified, \"in\" is assumed; if the output label of the last filter is not specified, \"out\" is assumed.\n\nIn a complete filterchain all the unlabelled filter input and output pads must be connected. A filtergraph is considered valid if all the filter input and output pads of all the filterchains are connected.\n\nLeading and trailing whitespaces (space, tabs, or line feeds) separating tokens in the filtergraph specification are ignored. This means that the filtergraph can be expressed using empty lines and spaces to improve redability.\n\nFor example, the filtergraph:\n\ncan be represented as:\n\nLibavfilter will automatically insert scale filters where format conversion is required. It is possible to specify swscale flags for those automatically inserted scalers by prepending to the filtergraph description.\n\nHere is a BNF description of the filtergraph syntax:\n\nFiltergraph description composition entails several levels of escaping. See (ffmpeg-utils)the \"Quoting and escaping\" section in the ffmpeg-utils(1) manual for more information about the employed escaping procedure.\n\nA first level escaping affects the content of each filter option value, which may contain the special character used to separate values, or one of the escaping characters .\n\nA second level escaping affects the whole filter description, which may contain the escaping characters or the special characters used by the filtergraph description.\n\nFinally, when you specify a filtergraph on a shell commandline, you need to perform a third level escaping for the shell special characters contained within it.\n\nFor example, consider the following string to be embedded in the drawtext filter description value:\n\nThis string contains the special escaping character, and the special character, so it needs to be escaped in this way:\n\nA second level of escaping is required when embedding the filter description in a filtergraph description, in order to escape all the filtergraph special characters. Thus the example above becomes:\n\n(note that in addition to the escaping special characters, also needs to be escaped).\n\nFinally an additional level of escaping is needed when writing the filtergraph description in a shell command, which depends on the escaping rules of the adopted shell. For example, assuming that is special and needs to be escaped with another , the previous string will finally result in:\n\nIn order to avoid cumbersome escaping when using a commandline tool accepting a filter specification as input, it is advisable to avoid direct inclusion of the filter or options specification in the shell.\n\nFor example, in case of the drawtext filter, you might prefer to use the option in place of to specify the text to render.\n\nSome filters support a generic option. For the filters supporting timeline editing, this option can be set to an expression which is evaluated before sending a frame to the filter. If the evaluation is non-zero, the filter will be enabled, otherwise the frame will be sent unchanged to the next filter in the filtergraph.\n\nThe expression accepts the following values:\n\nAdditionally, these filters support an command that can be used to re-define the expression.\n\nLike any other filtering option, the option follows the same rules.\n\nFor example, to enable a blur filter (smartblur) from 10 seconds to 3 minutes, and a curves filter starting at 3 seconds:\n\nSee to view which filters have timeline support.\n\nSome options can be changed during the operation of the filter using a command. These options are marked ’T’ on the output of . The name of the command is the name of the option and the argument is the new value.\n\n35 Options for filters with several inputs (framesync)\n\nSome filters with several inputs support a common set of options. These options can only be set by name, not with the short notation.\n\nWhen you configure your FFmpeg build, you can disable any of the existing filters using . The configure output will show the audio filters included in your build.\n\nBelow is a description of the currently available audio filters.\n\nApply Affine Projection algorithm to the first audio stream using the second audio stream.\n\nThis adaptive filter is used to estimate unknown audio based on multiple input audio samples. Affine projection algorithm can make trade-offs between computation complexity with convergence speed.\n\nA description of the accepted options follows.\n\nA compressor is mainly used to reduce the dynamic range of a signal. Especially modern music is mostly compressed at a high ratio to improve the overall loudness. It’s done to get the highest attention of a listener, \"fatten\" the sound and bring more \"power\" to the track. If a signal is compressed too much it may sound dull or \"dead\" afterwards or it may start to \"pump\" (which could be a powerful effect but can also destroy a track completely). The right compression is the key to reach a professional sound and is the high art of mixing and mastering. Because of its complex settings it may take a long time to get the right feeling for this kind of effect.\n\nCompression is done by detecting the volume above a chosen level and dividing it by the factor set with . So if you set the threshold to -12dB and your signal reaches -6dB a ratio of 2:1 will result in a signal at -9dB. Because an exact manipulation of the signal would cause distortion of the waveform the reduction can be levelled over the time. This is done by setting \"Attack\" and \"Release\". determines how long the signal has to rise above the threshold before any reduction will occur and sets the time the signal has to fall below the threshold to reduce the reduction again. Shorter signals than the chosen attack time will be left untouched. The overall reduction of the signal can be made up afterwards with the setting. So compressing the peaks of a signal about 6dB and raising the makeup to this level results in a signal twice as loud than the source. To gain a softer entry in the compression the flattens the hard edge at the threshold in the range of the chosen decibels.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThe filter accepts the following options:\n\nCopy the input audio source unchanged to the output. This is mainly useful for testing purposes.\n\nApply cross fade from one input audio stream to another input audio stream. The cross fade is applied for specified duration near the end of first stream.\n\nThe filter accepts the following options:\n• Cross fade from one input to another:\n• Cross fade from one input to another but without overlapping:\n\nThis filter splits audio stream into two or more frequency ranges. Summing all streams back will give flat output.\n\nThe filter accepts the following options:\n• Split input audio stream into two bands (low and high) with split frequency of 1500 Hz, each band will be in separate stream:\n• Same as above, but with higher filter order:\n• Same as above, but also with additional middle band (frequencies between 1500 and 8000):\n\nThis filter is bit crusher with enhanced functionality. A bit crusher is used to audibly reduce number of bits an audio signal is sampled with. This doesn’t change the bit depth at all, it just produces the effect. Material reduced in bit depth sounds more harsh and \"digital\". This filter is able to even round to continuous values instead of discrete bit depths. Additionally it has a D/C offset which results in different crushing of the lower and the upper half of the signal. An Anti-Aliasing setting is able to produce \"softer\" crushing sounds.\n\nAnother feature of this filter is the logarithmic mode. This setting switches from linear distances between bits to logarithmic ones. The result is a much more \"natural\" sounding crusher which doesn’t gate low signals for example. The human ear has a logarithmic perception, so this kind of crushing is much more pleasant. Logarithmic crushing is also able to get anti-aliased.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nDelay audio filtering until a given wallclock timestamp. See the cue filter.\n\nSamples detected as impulsive noise are replaced by interpolated samples using autoregressive modelling.\n\nSamples detected as clipped are replaced by interpolated samples using autoregressive modelling.\n\nThe filter accepts the following options:\n\nDelay one or more audio channels.\n\nSamples in delayed channel are filled with silence.\n\nThe filter accepts the following option:\n• Delay first channel by 1.5 seconds, the third channel by 0.5 seconds and leave the second channel (and any other channels that may be present) unchanged.\n• Delay second channel by 500 samples, the third channel by 700 samples and leave the first channel (and any other channels that may be present) unchanged.\n• Delay all channels by same number of samples:\n\nThis filter shall be placed before any filter that can produce denormals.\n\nA description of the accepted parameters follows.\n\nThis filter supports the all above options as commands.\n\nApplying both filters one after another produces original audio.\n\nA description of the accepted options follows.\n\nThis filter supports the all above options as commands.\n• Apply spectral compression to all frequencies with threshold of -50 dB and 1:6 ratio:\n• Similar to above but with 1:2 ratio and filtering only front center channel:\n• Apply spectral noise gate to all frequencies with threshold of -85 dB and with short attack time and short release time:\n• Apply spectral expansion to all frequencies with threshold of -10 dB and 1:2 ratio:\n• Apply limiter to max -60 dB to all frequencies, with attack of 2 ms and release of 10 ms:\n\nA description of the accepted options follows.\n\nThis filter supports the all above options as commands.\n\nA description of the accepted options follows.\n\nThis filter supports the all above options as commands.\n\nEchoes are reflected sound and can occur naturally amongst mountains (and sometimes large buildings) when talking or shouting; digital echo effects emulate this behaviour and are often used to help fill out the sound of a single instrument or vocal. The time difference between the original signal and the reflection is the , and the loudness of the reflected signal is the . Multiple echoes can have different delays and decays.\n\nA description of the accepted parameters follows.\n• Make it sound as if there are twice as many instruments as are actually playing:\n• If delay is very short, then it sounds like a (metallic) robot playing music:\n• A longer delay will sound like an open air concert in the mountains:\n• Same as above but with one more mountain:\n\nAudio emphasis filter creates or restores material directly taken from LPs or emphased CDs with different filter curves. E.g. to store music on vinyl the signal has to be altered by a filter first to even out the disadvantages of this recording medium. Once the material is played back the inverse filter has to be applied to restore the distortion of the frequency response.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nModify an audio signal according to the specified expressions.\n\nThis filter accepts one or more expressions (one for each channel), which are evaluated and used to modify a corresponding audio signal.\n\nIt accepts the following parameters:\n\nEach expression in can contain the following constants and functions:\n\nNote: this filter is slow. For faster processing you should use a dedicated filter.\n• Invert phase of the second channel:\n\nAn exciter is used to produce high sound that is not present in the original signal. This is done by creating harmonic distortions of the signal which are restricted in range and added to the original signal. An Exciter raises the upper end of an audio signal without simply raising the higher frequencies like an equalizer would do to create a more \"crisp\" or \"brilliant\" sound.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nA description of the accepted parameters follows.\n\nThis filter supports the all above options as commands.\n• Fade in first 15 seconds of audio:\n• Fade out last 25 seconds of a 900 seconds audio:\n\nA description of the accepted parameters follows.\n\nThis filter supports the some above mentioned options as commands.\n• Reduce white noise by 10dB, and use previously measured noise floor of -40dB:\n• Reduce white noise by 10dB, also set initial noise floor to -80dB and enable automatic tracking of noise floor so noise floor will gradually change during processing:\n• Reduce noise by 20dB, using noise floor of -40dB and using commands to take noise profile of first 0.4 seconds of input audio:\n• Leave almost only low frequencies in audio:\n\nThis filter is designed for applying long FIR filters, up to 60 seconds long.\n\nIt can be used as component for digital crossover filters, room equalization, cross talk cancellation, wavefield synthesis, auralization, ambiophonics, ambisonics and spatialization.\n\nThis filter uses the streams higher than first one as FIR coefficients. If the non-first stream holds a single channel, it will be used for all input channels in the first stream, otherwise the number of channels in the non-first stream must be same as the number of channels in the first stream.\n\nIt accepts the following parameters:\n• Apply reverb to stream using mono IR file as second input, complete command using ffmpeg:\n• Apply true stereo processing given input stereo stream, and two stereo impulse responses for left and right channel, the impulse response files are files with names l_ir.wav and r_ir.wav, and setting irnorm option value:\n• Similar to above example, but with explicitly set to estimated value and with disabled:\n\nSet output format constraints for the input audio. The framework will negotiate the most appropriate format to minimize conversions.\n\nIt accepts the following parameters:\n\nIf a parameter is omitted, all values are allowed.\n\nForce the output to either unsigned 8-bit or signed 16-bit stereo\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nA description of the accepted options follows.\n\nThis filter supports the all above options as commands.\n\nA gate is mainly used to reduce lower parts of a signal. This kind of signal processing reduces disturbing noise between useful signals.\n\nGating is done by detecting the volume below a chosen level and dividing it by the factor set with . The bottom of the noise floor is set via . Because an exact manipulation of the signal would cause distortion of the waveform the reduction can be levelled over time. This is done by setting and .\n\ndetermines how long the signal has to fall below the threshold before any reduction will occur and sets the time the signal has to rise above the threshold to reduce the reduction again. Shorter signals than the chosen attack time will be left untouched.\n\nThis filter supports the all above options as commands.\n\nIt accepts the following parameters:\n\nCoefficients in and format are separated by spaces and are in ascending order.\n\nCoefficients in format are separated by spaces and order of coefficients doesn’t matter. Coefficients in format are complex numbers with imaginary unit.\n\nDifferent coefficients and gains can be provided for every channel, in such case use ’|’ to separate coefficients or gains. Last provided coefficients will be used for all remaining channels.\n• Apply 2 pole elliptic notch at around 5000Hz for 48000 Hz sample rate:\n• Same as above but in format:\n\nThe limiter prevents an input signal from rising over a desired threshold. This limiter uses lookahead technology to prevent your signal from distorting. It means that there is a small delay after the signal is processed. Keep in mind that the delay it produces is the attack time you set.\n\nThe filter accepts the following options:\n\nDepending on picked setting it is recommended to upsample input 2x or 4x times with aresample before applying this filter.\n\nApply a two-pole all-pass filter with central frequency (in Hz) , and filter-width . An all-pass filter changes the audio’s frequency to phase relationship without changing its frequency to amplitude relationship.\n\nThe filter accepts the following options:\n\nThis filter supports the following commands:\n\nThe filter accepts the following options:\n\nMerge two or more audio streams into a single multi-channel stream.\n\nThe filter accepts the following options:\n\nIf the channel layouts of the inputs are disjoint, and therefore compatible, the channel layout of the output will be set accordingly and the channels will be reordered as necessary. If the channel layouts of the inputs are not disjoint, the output will have all the channels of the first input then all the channels of the second input, in that order, and the channel layout of the output will be the default value corresponding to the total number of channels.\n\nFor example, if the first input is in 2.1 (FL+FR+LF) and the second input is FC+BL+BR, then the output will be in 5.1, with the channels in the following order: a1, a2, b1, a3, b2, b3 (a1 is the first channel of the first input, b1 is the first channel of the second input).\n\nOn the other hand, if both input are in stereo, the output channels will be in the default order: a1, a2, b1, b2, and the channel layout will be arbitrarily set to 4.0, which may or may not be the expected value.\n\nAll inputs must have the same sample rate, and format.\n\nIf inputs do not have the same duration, the output will stop with the shortest.\n\nNote that this filter only supports float samples (the and audio filters support many formats). If the input has integer samples then aresample will be automatically inserted to perform the conversion to float samples.\n\nIt accepts the following parameters:\n• This will mix 3 input audio streams to a single output with the same duration as the first input and a dropout transition time of 3 seconds:\n• This will mix one vocal and one music input audio stream to a single output with the same duration as the longest input. The music will have quarter the weight as the vocals, and the inputs are not normalized:\n\nThis filter supports the following commands:\n\nMultiply first audio stream with second audio stream and store result in output audio stream. Multiplication is done by multiplying each sample from first stream with sample at same position from second stream.\n\nWith this element-wise multiplication one can create amplitude fades and amplitude modulations.\n\nIt accepts the following parameters:\n• Lower gain by 10 of central frequency 200Hz and width 100 Hz for first 2 channels using Chebyshev type 1 filter:\n\nThis filter supports the following commands:\n\nEach sample is adjusted by looking for other samples with similar contexts. This context similarity is defined by comparing their surrounding patches of size . Patches are searched in an area of around the sample.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nApply Normalized Least-Mean-(Squares|Fourth) algorithm to the first audio stream using the second audio stream.\n\nThis adaptive filter is used to mimic a desired filter by finding the filter coefficients that relate to producing the least mean square of the error signal (difference between the desired, 2nd input audio stream and the actual signal, the 1st input audio stream).\n\nA description of the accepted options follows.\n• One of many usages of this filter is noise reduction, input audio is filtered with same samples that are delayed by fixed amount, one such example for stereo audio is:\n\nThis filter supports the same commands as options, excluding option .\n\nPass the audio source unchanged to the output.\n\nPad the end of an audio stream with silence.\n\nThis can be used together with to extend audio streams to the same length as the video stream.\n\nA description of the accepted options follows.\n\nIf neither the nor the nor nor option is set, the filter will add silence to the end of the input stream indefinitely.\n\nNote that for ffmpeg 4.4 and earlier a zero or also caused the filter to add silence indefinitely.\n• Add 1024 samples of silence to the end of the input:\n• Make sure the audio output will contain at least 10000 samples, pad the input with silence if required:\n• Use to pad the audio input with silence, so that the video stream will always result the shortest and will be converted until the end in the output file when using the option:\n\nA phaser filter creates series of peaks and troughs in the frequency spectrum. The position of the peaks and troughs are modulated so that they vary over time, creating a sweeping effect.\n\nA description of the accepted parameters follows.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThis filter takes two audio streams for input, and outputs first audio stream. Results are in dB per channel at end of either input.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nAudio pulsator is something between an autopanner and a tremolo. But it can produce funny stereo effects as well. Pulsator changes the volume of the left and right channel based on a LFO (low frequency oscillator) with different waveforms and shifted phases. This filter have the ability to define an offset between left and right channel. An offset of 0 means that both LFO shapes match each other. The left and right channel are altered equally - a conventional tremolo. An offset of 50% means that the shape of the right channel is exactly shifted in phase (or moved backwards about half of the frequency) - pulsator acts as an autopanner. At 1 both curves match again. Every setting in between moves the phase shift gapless between all stages and produces some \"bypassing\" sounds with sine and triangle waveforms. The more you set the offset near 1 (starting from the 0.5) the faster the signal passes from the left to the right speaker.\n\nThe filter accepts the following options:\n\nResample the input audio to the specified parameters, using the libswresample library. If none are specified then the filter will automatically convert between its input and output.\n\nThis filter is also able to stretch/squeeze the audio data to make it match the timestamps or to inject silence / cut out audio to make it match the timestamps, do a combination of both or do neither.\n\nThe filter accepts the syntax [ :] , where expresses a sample rate and is a list of = pairs, separated by \":\". See the (ffmpeg-resampler)\"Resampler Options\" section in the ffmpeg-resampler(1) manual for the complete list of supported options.\n• Stretch/squeeze samples to the given timestamps, with a maximum of 1000 samples per second compensation:\n\nWarning: This filter requires memory to buffer the entire clip, so trimming is suggested.\n• Take the first 5 seconds of a clip, and reverse it.\n\nApply Recursive Least Squares algorithm to the first audio stream using the second audio stream.\n\nThis adaptive filter is used to mimic a desired filter by recursively finding the filter coefficients that relate to producing the minimal weighted linear least squares cost function of the error signal (difference between the desired, 2nd input audio stream and the actual signal, the 1st input audio stream).\n\nA description of the accepted options follows.\n\nThis filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThis filter takes two audio streams for input, and outputs first audio stream. Results are in dB per channel at end of either input.\n\nSet the number of samples per each output audio frame.\n\nThe last output packet may contain a different number of samples, as the filter will flush all the remaining samples when the input audio signals its end.\n\nThe filter accepts the following options:\n\nFor example, to set the number of per-frame samples to 1234 and disable padding for the last frame, use:\n\nSet the sample rate without altering the PCM data. This will result in a change of speed and pitch.\n\nThe filter accepts the following options:\n\nShow a line containing various information for each input audio frame. The input audio is not modified.\n\nThe shown line contains a sequence of key/value pairs of the form : .\n\nThe following values are shown in the output:\n\nThis filter takes two audio streams for input, and outputs first audio stream. Results are in dB per channel at end of either input.\n\nSoft clipping is a type of distortion effect where the amplitude of a signal is saturated along a smooth curve, rather than the abrupt shape of hard-clipping.\n\nThis filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nDisplay frequency domain statistical information about the audio channels. Statistics are calculated and stored as metadata for each audio channel and for each audio frame.\n\nIt accepts the following option:\n\nA list of each metadata key follows:\n\nThis filter uses PocketSphinx for speech recognition. To enable compilation of this filter, you need to configure FFmpeg with .\n\nIt accepts the following options:\n\nThe filter exports recognized speech as the frame metadata .\n\nDisplay time domain statistical information about the audio channels. Statistics are calculated and displayed for each audio channel and, where applicable, an overall figure is also given.\n\nIt accepts the following option:\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThis filter allows to set custom, steeper roll off than highpass filter, and thus is able to more attenuate frequency content in stop-band.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThe filter accepts exactly one parameter, the audio tempo. If not specified then the filter will assume nominal 1.0 tempo. Tempo must be in the [0.5, 100.0] range.\n\nNote that tempo greater than 2 will skip some samples rather than blend them in. If for any reason this is a concern it is always possible to daisy-chain several instances of atempo to achieve the desired product tempo.\n• To speed up audio to 300% tempo:\n• To speed up audio to 300% tempo by daisy-chaining two atempo instances:\n\nThis filter supports the following commands:\n\nThis filter apply any spectral roll-off slope over any specified frequency band.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nTrim the input so that the output contains one continuous subpart of the input.\n\nIt accepts the following parameters:\n\n, , and are expressed as time duration specifications; see (ffmpeg-utils)the Time duration section in the ffmpeg-utils(1) manual.\n\nNote that the first two sets of the start/end options and the option look at the frame timestamp, while the _sample options simply count the samples that pass through the filter. So start/end_pts and start/end_sample will give different results when the timestamps are wrong, inexact or do not start at zero. Also note that this filter does not modify the timestamps. If you wish to have the output timestamps start at zero, insert the asetpts filter after the atrim filter.\n\nIf multiple start or end options are set, this filter tries to be greedy and keep all samples that match at least one of the specified constraints. To keep only the part that matches all the constraints at once, chain multiple atrim filters.\n\nThe defaults are such that all the input is kept. So it is possible to set e.g. just the end values to keep everything before the specified time.\n• Drop everything except the second minute of input:\n• Keep only the first 1000 samples:\n\nResulted samples are always between -1 and 1 inclusive. If result is 1 it means two input samples are highly correlated in that selected segment. Result 0 means they are not correlated at all. If result is -1 it means two input samples are out of phase, which means they cancel each other.\n\nThe filter accepts the following options:\n\nApply a two-pole Butterworth band-pass filter with central frequency , and (3dB-point) band-width width. The option selects a constant skirt gain (peak gain = Q) instead of the default: constant 0dB peak gain. The filter roll off at 6dB per octave (20dB per decade).\n\nThe filter accepts the following options:\n\nThis filter supports the following commands:\n\nApply a two-pole Butterworth band-reject filter with central frequency , and (3dB-point) band-width . The filter roll off at 6dB per octave (20dB per decade).\n\nThe filter accepts the following options:\n\nThis filter supports the following commands:\n\nBoost or cut the bass (lower) frequencies of the audio using a two-pole shelving filter with a response similar to that of a standard hi-fi’s tone-controls. This is also known as shelving equalisation (EQ).\n\nThe filter accepts the following options:\n\nThis filter supports the following commands:\n\nApply a biquad IIR filter with the given coefficients. Where , , and , , are the numerator and denominator coefficients respectively. and , specify which channels to filter, by default all available are filtered.\n\nThis filter supports the following commands:\n\nBauer stereo to binaural transformation, which improves headphone listening of stereo audio records.\n\nTo enable compilation of this filter you need to configure FFmpeg with .\n\nIt accepts the following parameters:\n\nIt accepts the following parameters:\n\nIf no mapping is present, the filter will implicitly map input channels to output channels, preserving indices.\n• For example, assuming a 5.1+downmix input MOV file, will create an output WAV file tagged as stereo from the downmix channels of the input.\n\nSplit each channel from an input audio stream into a separate output stream.\n\nIt accepts the following parameters:\n• For example, assuming a stereo input MP3 file, will create an output Matroska file with two audio streams, one containing only the left channel and the other the right channel.\n\nCan make a single vocal sound like a chorus, but can also be applied to instrumentation.\n\nChorus resembles an echo effect with a short delay, but whereas with echo the delay is constant, with chorus, it is varied using using sinusoidal or triangular modulation. The modulation depth defines the range the modulated delay is played before or after the delay. Hence the delayed sound will sound slower or faster, that is the delayed sound tuned around the original one, like in a chorus where some vocals are slightly off key.\n\nIt accepts the following parameters:\n\nIt accepts the following parameters:\n• Make music with both quiet and loud passages suitable for listening to in a noisy environment: Another example for audio with whisper and explosion parts:\n• A noise gate for when the noise is at a lower level than the signal:\n• Here is another noise gate, this time for when the noise is at a higher level than the signal (making it, in some ways, similar to squelch):\n\nCompensation Delay Line is a metric based delay to compensate differing positions of microphones or speakers.\n\nFor example, you have recorded guitar with two microphones placed in different locations. Because the front of sound wave has fixed speed in normal conditions, the phasing of microphones can vary and depends on their location and interposition. The best sound mix can be achieved when these microphones are in phase (synchronized). Note that a distance of ~30 cm between microphones makes one microphone capture the signal in antiphase to the other microphone. That makes the final mix sound moody. This filter helps to solve phasing problems by adding different delays to each microphone track and make them synchronized.\n\nThe best result can be reached when you take one track as base and synchronize other tracks one by one with it. Remember that synchronization/delay tolerance depends on sample rate, too. Higher sample rates will give more tolerance.\n\nThe filter accepts the following parameters:\n\nThis filter supports the all above options as commands.\n\nCrossfeed is the process of blending the left and right channels of stereo audio recording. It is mainly used to reduce extreme stereo separation of low frequencies.\n\nThe intent is to produce more speaker like sound to the listener.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThis filter linearly increases differences between each audio sample.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThis can be useful to remove a DC offset (caused perhaps by a hardware problem in the recording chain) from the audio. The effect of a DC offset is reduced headroom and hence volume. The astats filter can be used to determine if a signal has a DC offset.\n\nThis filter accepts stereo input and produce surround (3.0) channels output. The newly produced front center channel have enhanced speech dialogue originally available in both stereo channels. This filter outputs front left and front right channels same as available in stereo input.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nDR values of 14 and higher is found in very dynamic material. DR of 8 to 13 is found in transition material. And anything less that 8 have very poor dynamics and is very compressed.\n\nThe filter accepts the following options:\n\nThis filter applies a certain amount of gain to the input audio in order to bring its peak magnitude to a target level (e.g. 0 dBFS). However, in contrast to more \"simple\" normalization algorithms, the Dynamic Audio Normalizer *dynamically* re-adjusts the gain factor to the input audio. This allows for applying extra gain to the \"quiet\" sections of the audio while avoiding distortions or clipping the \"loud\" sections. In other words: The Dynamic Audio Normalizer will \"even out\" the volume of quiet and loud sections, in the sense that the volume of each section is brought to the same target level. Note, however, that the Dynamic Audio Normalizer achieves this goal *without* applying \"dynamic range compressing\". It will retain 100% of the dynamic range *within* each section of the audio file.\n\nThis filter supports the all above options as commands.\n\nMake audio easier to listen to on headphones.\n\nThis filter adds ‘cues’ to 44.1kHz stereo (i.e. audio CD format) audio so that when listened to on headphones the stereo image is moved from inside your head (standard for headphones) to outside and in front of the listener (standard for speakers).\n\nApply a two-pole peaking equalisation (EQ) filter. With this filter, the signal-level at and around a selected frequency can be increased or decreased, whilst (unlike bandpass and bandreject filters) that at all other frequencies is unchanged.\n\nIn order to produce complex equalisation curves, this filter can be given several times, each with a different central frequency.\n\nThe filter accepts the following options:\n• Attenuate 10 dB at 1000 Hz, with a bandwidth of 200 Hz:\n• Apply 2 dB gain at 1000 Hz with Q 1 and attenuate 5 dB at 100 Hz with Q 2:\n\nThis filter supports the following commands:\n\nLinearly increases the difference between left and right channels which adds some sort of \"live\" effect to playback.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThe filter accepts the following option:\n• higher delay with zero phase to compensate delay:\n• lowpass on left channel, highpass on right channel:\n\nThe filter accepts the following options:\n\nNote that this makes most sense to apply on mono signals. With this filter applied to mono signals it give some directionality and stretches its stereo image.\n\nThe filter accepts the following options:\n\nDecodes High Definition Compatible Digital (HDCD) data. A 16-bit PCM stream with embedded HDCD codes is expanded into a 20-bit PCM stream.\n\nThe filter supports the Peak Extend and Low-level Gain Adjustment features of HDCD, and detects the Transient Filter flag.\n\nWhen using the filter with wav, note the default encoding for wav is 16-bit, so the resulting 20-bit stream will be truncated back to 16-bit. Use something like after the filter to get 24-bit PCM output.\n\nThe filter accepts the following options:\n\nApply head-related transfer functions (HRTFs) to create virtual loudspeakers around the user for binaural listening via headphones. The HRIRs are provided via additional streams, for each channel one stereo input stream is needed.\n\nThe filter accepts the following options:\n• Full example using wav files as coefficients with amovie filters for 7.1 downmix, each amovie filter use stereo file with IR coefficients as input. The files give coefficients for each position of virtual loudspeaker:\n• Full example using wav files as coefficients with amovie filters for 7.1 downmix, but now in format.\n\nApply a high-pass filter with 3dB point frequency. The filter can be either single-pole, or double-pole (the default). The filter roll off at 6dB per pole per octave (20dB per pole per decade).\n\nThe filter accepts the following options:\n\nThis filter supports the following commands:\n\nIt accepts the following parameters:\n\nThe filter will attempt to guess the mappings when they are not specified explicitly. It does so by first trying to find an unused matching input channel and if that fails it picks the first unused input channel.\n\nTo enable compilation of this filter you need to configure FFmpeg with .\n• List all available plugins within amp (LADSPA example plugin) library:\n• List all available controls and their valid ranges for plugin from library:\n• Add reverberation to the audio using TAP-plugins (Tom’s Audio Processing plugins):\n• Generate 20 bpm clicks using plugin from the (CAPS) library:\n• Increase volume by 20dB using fast lookahead limiter from Steve Harris collection:\n• Reduce stereo image using from the (CAPS) library:\n• Another white noise, now using (CAPS) library:\n\nThis filter supports the following commands:\n\nEBU R128 loudness normalization. Includes both dynamic and linear normalization modes. Support for both single pass (livestreams, files) and double pass (files) modes. This algorithm can target IL, LRA, and maximum true peak. In dynamic mode, to accurately detect true peaks, the audio stream will be upsampled to 192 kHz. Use the option or filter to explicitly set an output sample rate.\n\nThe filter accepts the following options:\n\nApply a low-pass filter with 3dB point frequency. The filter can be either single-pole or double-pole (the default). The filter roll off at 6dB per pole per octave (20dB per pole per decade).\n\nThe filter accepts the following options:\n• Lowpass only LFE channel, it LFE is not present it does nothing:\n\nThis filter supports the following commands:\n\nTo enable compilation of this filter you need to configure FFmpeg with .\n\nThis filter supports all options that are exported by plugin as commands.\n\nThe input audio is divided into bands using 4th order Linkwitz-Riley IIRs. This is akin to the crossover of a loudspeaker, and results in flat frequency response when absent compander action.\n\nIt accepts the following parameters:\n\nMix channels with specific gain levels. The filter accepts the output channel layout followed by a set of channels definitions.\n\nThis filter is also designed to efficiently remap the channels of an audio stream.\n\nThe filter accepts parameters of the form: \" | | |...\"\n\nIf the ‘=’ in a channel specification is replaced by ‘<’, then the gains for that specification will be renormalized so that the total is 1, thus avoiding clipping noise.\n\nFor example, if you want to down-mix from stereo to mono, but with a bigger factor for the left channel:\n\nA customized down-mix to stereo that works automatically for 3-, 4-, 5- and 7-channels surround:\n\nNote that integrates a default down-mix (and up-mix) system that should be preferred (see \"-ac\" option) unless you have very specific needs.\n\nThe channel remapping will be effective if, and only if:\n• gain coefficients are zeroes or ones,\n• only one input per channel output,\n\nIf all these conditions are satisfied, the filter will notify the user (\"Pure channel mapping detected\"), and use an optimized and lossless method to do the remapping.\n\nFor example, if you have a 5.1 source and want a stereo audio stream by dropping the extra channels:\n\nGiven the same source, you can also switch front left and front right channels and keep the input channel layout:\n\nIf the input is a stereo audio stream, you can mute the front left channel (and still keep the stereo channel layout) with:\n\nStill with a stereo audio stream input, you can copy the right channel in both front left and right:\n\nReplayGain scanner filter. This filter takes an audio stream as an input and outputs it unchanged. At end of filtering it displays and .\n\nThe filter accepts the following exported read-only options:\n\nConvert the audio sample format, sample rate and channel layout. It is not meant to be used directly.\n\nTo enable compilation of this filter, you need to configure FFmpeg with .\n\nThe filter accepts the following options:\n\nThis filter supports the following commands:\n\nThis filter acts like normal compressor but has the ability to compress detected signal using second input signal. It needs two input streams and returns one output stream. First input stream will be processed depending on second stream signal. The filtered signal then can be filtered with other filters in later stages of processing. See pan and amerge filter.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n• Full ffmpeg example taking 2 audio inputs, 1st input to be compressed depending on the signal of 2nd input and later compressed signal to be merged with 2nd input:\n\nA sidechain gate acts like a normal (wideband) gate but has the ability to filter the detected signal before sending it to the gain reduction stage. Normally a gate uses the full range signal to detect a level above the threshold. For example: If you cut all lower frequencies from your sidechain signal the gate will decrease the volume of your track only if not enough highs appear. With this technique you are able to reduce the resonation of a natural drum or remove \"rumbling\" of muted strokes from a heavily distorted guitar. It needs two input streams and returns one output stream. First input stream will be processed depending on second stream signal.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThis filter logs a message when it detects that the input audio volume is less or equal to a noise tolerance value for a duration greater or equal to the minimum detected noise duration.\n\nThe printed times and duration are expressed in seconds. The or metadata key is set on the first frame whose timestamp equals or exceeds the detection duration and it contains the timestamp of the first frame of the silence.\n\nThe or and or metadata keys are set on the first frame after the silence. If is enabled, and each channel is evaluated separately, the suffixed keys are used, and corresponds to the channel number.\n\nThe filter accepts the following options:\n• Complete example with to detect silence with 0.0001 noise tolerance in :\n\nRemove silence from the beginning, middle or end of the audio.\n\nThe filter accepts the following options:\n• The following example shows how this filter can be used to start a recording that does not contain the delay at the start which usually occurs between pressing the record button and the start of the performance:\n• Trim all silence encountered from beginning to end where there is more than 1 second of silence in audio:\n• Trim all digital silence samples, using peak detection, from beginning to end where there is more than 0 samples of digital silence in audio and digital silence is detected in all channels at same positions in stream:\n• Trim every 2nd encountered silence period from beginning to end where there is more than 1 second of silence per silence period in audio:\n• Similar as above, but keep maximum of 0.5 seconds of silence from each trimmed period:\n• Similar as above, but keep maximum of 1.5 seconds of silence from start of audio:\n\nThis filter supports some above options as commands.\n\nSOFAlizer uses head-related transfer functions (HRTFs) to create virtual loudspeakers around the user for binaural listening via headphones (audio formats up to 9 channels supported). The HRTFs are stored in SOFA files (see http://www.sofacoustics.org/ for a database). SOFAlizer is developed at the Acoustics Research Institute (ARI) of the Austrian Academy of Sciences.\n\nTo enable compilation of this filter you need to configure FFmpeg with .\n\nThe filter accepts the following options:\n• Using ClubFritz12 sofa file and bigger radius with small rotation:\n• Similar as above but with custom speaker positions for front left, front right, back left and back right and also with custom gain:\n\nThis filter expands or compresses each half-cycle of audio samples (local set of samples all above or all below zero and between two nearest zero crossings) depending on threshold value, so audio reaches target peak value under conditions controlled by below options.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThis filter has some handy utilities to manage stereo signals, for converting M/S stereo recordings to L/R signal while having control over the parameters or spreading the stereo image of master track.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThis filter enhance the stereo effect by suppressing signal common to both channels and by delaying the signal of left into right and vice versa, thereby widening the stereo effect.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options except as commands.\n\nThe filter accepts the following options:\n\nThis filter allows to produce multichannel output from audio stream.\n\nThe filter accepts the following options:\n\nBoost or cut the lower frequencies and cut or boost higher frequencies of the audio using a two-pole shelving filter with a response similar to that of a standard hi-fi’s tone-controls. This is also known as shelving equalisation (EQ).\n\nThe filter accepts the following options:\n\nThis filter supports some options as commands.\n\nBoost or cut treble (upper) frequencies of the audio using a two-pole shelving filter with a response similar to that of a standard hi-fi’s tone-controls. This is also known as shelving equalisation (EQ).\n\nThe filter accepts the following options:\n\nThis filter supports the following commands:\n\nThe filter accepts the following options:\n\nThe filter accepts the following options:\n\nThis filter accepts stereo input and produce stereo with LFE (2.1) channels output. The newly produced LFE channel have enhanced virtual bass originally obtained from both stereo channels. This filter outputs front left and front right channels unchanged as available in stereo input.\n\nThe filter accepts the following options:\n\nIt accepts the following parameters:\n\nThe volume expression can contain the following parameters.\n\nNote that when is set to ‘ ’ only the and variables are available, all other variables will evaluate to NAN.\n\nThis filter supports the following commands:\n• Halve the input audio volume: In all the above example the named key for can be omitted, for example like in:\n• Fade volume after time 10 with an annihilation period of 5 seconds:\n\nDetect the volume of the input video.\n\nThe filter has no parameters. It supports only 16-bit signed integer samples, so the input will be converted when needed. Statistics about the volume will be printed in the log when the input stream end is reached.\n\nIn particular it will show the mean volume (root mean square), maximum volume (on a per-sample basis), and the beginning of a histogram of the registered volume values (from the maximum value to a cumulated 1/1000 of the samples).\n\nAll volumes are in decibels relative to the maximum PCM value.\n\nHere is an excerpt of the output:\n• The mean square energy is approximately -27 dB, or 10^-2.7.\n• The largest sample is at -4 dB, or more precisely between -4 dB and -5 dB.\n• There are 6 samples at -4 dB, 62 at -5 dB, 286 at -6 dB, etc.\n\nIn other words, raising the volume by +4 dB does not cause any clipping, raising it by +5 dB causes clipping for 6 samples, etc.\n\nBelow is a description of the currently available audio sources.\n\nBuffer audio frames, and make them available to the filter chain.\n\nThis source is mainly intended for a programmatic use, in particular through the interface defined in .\n\nIt accepts the following parameters:\n\nwill instruct the source to accept planar 16bit signed stereo at 44100Hz. Since the sample format with name \"s16p\" corresponds to the number 6 and the \"stereo\" channel layout corresponds to the value 0x3, this is equivalent to:\n\nGenerate an audio signal specified by an expression.\n\nThis source accepts in input one or more expressions (one for each channel), which are evaluated and used to generate a corresponding audio signal.\n\nThis source accepts the following options:\n\nEach expression in can contain the following constants:\n• Generate a sin signal with frequency of 440 Hz, set sample rate to 8000 Hz:\n• Generate a two channels signal, specify the channel layout (Front Center + Back Center) explicitly:\n\nThe resulting stream can be used with afir filter for filtering the audio signal.\n\nThe filter accepts the following options:\n\nThe resulting stream can be used with afir filter for filtering the audio signal.\n\nThe filter accepts the following options:\n\nThe resulting stream can be used with afir filter for filtering the audio signal.\n\nThe filter accepts the following options:\n\nThe null audio source, return unprocessed audio frames. It is mainly useful as a template and to be employed in analysis / debugging tools, or as the source for filters which ignore the input data (for example the sox synth filter).\n\nThis source accepts the following options:\n• Set the sample rate to 48000 Hz and the channel layout to AV_CH_LAYOUT_MONO.\n• Do the same operation with a more obvious syntax:\n\nAll the parameters need to be explicitly defined.\n\nTo enable compilation of this filter you need to configure FFmpeg with .\n\nNote that versions of the flite library prior to 2.0 are not thread-safe.\n\nThe filter accepts the following options:\n• Read from file , and synthesize the text using the standard flite voice:\n• Read the specified text selecting the voice: flite=text='So fare thee well, poor devil of a Sub-Sub, whose commentator I am':voice=slt\n• Input text to ffmpeg: ffmpeg -f lavfi -i flite=text='So fare thee well, poor devil of a Sub-Sub, whose commentator I am':voice=slt\n• Make speak the specified text, using and the device: ffplay -f lavfi flite=text='No more be grieved for which that thou hast done.'\n\nFor more information about libflite, check: http://www.festvox.org/flite/\n\nThe filter accepts the following options:\n• Generate 60 seconds of pink noise, with a 44.1 kHz sampling rate and an amplitude of 0.5:\n\nThe resulting stream can be used with afir filter for phase-shifting the signal by 90 degrees.\n\nThis is used in many matrix coding schemes and for analytic signal generation. The process is often written as a multiplication by i (or j), the imaginary unit.\n\nThe filter accepts the following options:\n\nThe resulting stream can be used with afir filter for filtering the audio signal.\n\nThe filter accepts the following options:\n\nGenerate an audio signal made of a sine wave with amplitude 1/8.\n\nThe filter accepts the following options:\n• Generate a 220 Hz sine wave with a 880 Hz beep each second, for 5 seconds:\n\nBelow is a description of the currently available audio sinks.\n\nBuffer audio frames, and make them available to the end of filter chain.\n\nThis sink is mainly intended for programmatic use, in particular through the interface defined in or the options system.\n\nIt accepts a pointer to an AVABufferSinkContext structure, which defines the incoming buffers’ formats, to be passed as the opaque parameter to for initialization.\n\nNull audio sink; do absolutely nothing with the input audio. It is mainly useful as a template and for use in analysis / debugging tools.\n\nWhen you configure your FFmpeg build, you can disable any of the existing filters using . The configure output will show the video filters included in your build.\n\nBelow is a description of the currently available video filters.\n\nThe frame data is passed through unchanged, but metadata is attached to the frame indicating regions of interest which can affect the behaviour of later encoding. Multiple regions can be marked by applying the filter multiple times.\n• Mark the centre quarter of the frame as interesting.\n• Mark the 100-pixel-wide region on the left edge of the frame as very uninteresting (to be encoded at much lower quality than the rest of the frame).\n\nExtract the alpha component from the input as a grayscale video. This is especially useful with the filter.\n\nAdd or replace the alpha component of the primary input with the grayscale value of a second input. This is intended for use with to allow the transmission or storage of frame sequences that have alpha in a format that doesn’t support an alpha channel.\n\nFor example, to reconstruct full frames from a normal YUV-encoded video and a separate video created with , you might use:\n\nAmplify differences between current pixel and pixels of adjacent frames in same pixel location.\n\nThis filter accepts the following options:\n\nThis filter supports the following commands that corresponds to option of same name:\n\nSame as the subtitles filter, except that it doesn’t require libavcodec and libavformat to work. On the other hand, it is limited to ASS (Advanced Substation Alpha) subtitles files.\n\nThis filter accepts the following option in addition to the common options from the subtitles filter:\n\nApply an Adaptive Temporal Averaging Denoiser to the video input.\n\nThe filter accepts the following options:\n\nThis filter supports same commands as options except option . The command accepts the same syntax of the corresponding option.\n\nThe filter accepts the following options:\n\nThis filter supports same commands as options. The command accepts the same syntax of the corresponding option.\n\nIf the specified expression is not valid, it is kept at its current value.\n\nThe filter accepts the following option:\n\nThis filter supports the all above options as commands.\n\nCompute the bounding box for the non-black pixels in the input frame luma plane.\n\nThis filter computes the bounding box containing all the pixels with a luma value greater than the minimum allowed value. The parameters describing the bounding box are printed on the filter log.\n\nThe filter accepts the following option:\n\nThis filter supports the all above options as commands.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThe filter accepts the following options:\n\nDetect video intervals that are (almost) completely black. Can be useful to detect chapter transitions, commercials, or invalid recordings.\n\nThe filter outputs its detection analysis to both the log as well as frame metadata. If a black segment of at least the specified minimum duration is found, a line with the start and end timestamps as well as duration is printed to the log with level . In addition, a log line with level is printed per frame showing the black amount detected for that frame.\n\nThe filter also attaches metadata to the first frame of a black segment with key and to the first frame after the black segment ends with key . The value is the frame’s timestamp. This metadata is added regardless of the minimum duration specified.\n\nThe filter accepts the following options:\n\nThe following example sets the maximum pixel threshold to the minimum value, and detects only black intervals of 2 or more seconds:\n\nDetect frames that are (almost) completely black. Can be useful to detect chapter transitions or commercials. Output lines consist of the frame number of the detected frame, the percentage of blackness, the position in the file if known or -1 and the timestamp in seconds.\n\nIn order to display the output lines, you need to set the loglevel at least to the AV_LOG_INFO value.\n\nThis filter exports frame metadata . The value represents the percentage of pixels in the picture that are below the threshold value.\n\nIt accepts the following parameters:\n\nBlend two video frames into each other.\n\nThe filter takes two input streams and outputs one stream, the first input is the \"top\" layer and second input is \"bottom\" layer. By default, the output terminates when the longest input terminates.\n\nThe (time blend) filter takes two consecutive frames from one single stream, and outputs the result obtained by blending the new frame on top of the old frame.\n\nA description of the accepted options follows.\n\nThe filter also supports the framesync options.\n• Apply transition from bottom layer to top layer in first 10 seconds:\n• Split diagonally video and shows top and bottom layer on each side:\n• Display differences between the current and the previous frame:\n\nThis filter supports same commands as options.\n\nDetermines blockiness of frames without altering the input frames.\n\nBased on Remco Muijs and Ihor Kirenko: \"A no-reference blocking artifact measure for adaptive video processing.\" 2005 13th European signal processing conference.\n\nThe filter accepts the following options:\n• Determine blockiness for the first plane and search for periods within [8,32]:\n\nDetermines blurriness of frames without altering the input frames.\n\nBased on Marziliano, Pina, et al. \"A no-reference perceptual blur metric.\" Allows for a block-based abbreviation.\n\nThe filter accepts the following options:\n• Determine blur for 80% of most significant 32x32 blocks:\n\nThe filter accepts the following options.\n• Same as above, but filtering only luma:\n• Same as above, but with both estimation modes:\n• Same as above, but prefilter with nlmeans filter instead:\n\nIt accepts the following parameters:\n\nA description of the accepted options follows.\n• Apply a boxblur filter with the luma, chroma, and alpha radii set to 2:\n• Set the luma radius to 2, and alpha and chroma radius to 0:\n• Set the luma and chroma radii to a fraction of the video dimension:\n\nMotion adaptive deinterlacing based on yadif with the use of w3fdif and cubic interpolation algorithms. It accepts the following parameters:\n\nThis filter fixes various issues seen with commerical encoders related to upstream malformed CEA-708 payloads, specifically incorrect number of tuples (wrong cc_count for the target FPS), and incorrect ordering of tuples (i.e. the CEA-608 tuples are not at the first entries in the payload).\n\nThe filter accepts the following options:\n\nThis filter supports same commands as options.\n\nRemove all color information for all colors except for certain one.\n\nThe filter accepts the following options:\n\nThis filter supports same commands as options. The command accepts the same syntax of the corresponding option.\n\nIf the specified expression is not valid, it is kept at its current value.\n\nThe filter accepts the following options:\n\nThis filter supports same commands as options. The command accepts the same syntax of the corresponding option.\n\nIf the specified expression is not valid, it is kept at its current value.\n• Make every green pixel in the input image transparent:\n\nThe filter accepts the following options:\n\nThis filter supports same commands as options. The command accepts the same syntax of the corresponding option.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nDisplay CIE color diagram with pixels overlaid onto it.\n\nThe filter accepts the following options:\n\nSome codecs can export information through frames using side-data or other means. For example, some MPEG based codecs export motion vectors through the flag in the codec option.\n\nThe filter accepts the following option:\n• Visualize forward predicted MVs of all frames using :\n• Visualize multi-directionals MVs of P and B-Frames using :\n\nModify intensity of primary colors (red, green and blue) of input frames.\n\nThe filter allows an input frame to be adjusted in the shadows, midtones or highlights regions for the red-cyan, green-magenta or blue-yellow balance.\n\nA positive adjustment value shifts the balance towards the primary color, a negative value towards the complementary color.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nAdjust color white balance selectively for blacks and whites. This filter operates in YUV colorspace.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThis filter modifies a color channel by adding the values associated to the other channels of the same pixels. For example if the value to modify is red, the output value will be:\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nRGB colorspace color keying. This filter operates on 8-bit RGB format frames by setting the alpha component of each pixel which falls within the similarity radius of the key color to 0. The alpha value for pixels outside the similarity radius depends on the value of the blend option.\n\nThe filter accepts the following options:\n• Make every green pixel in the input image transparent:\n\nThis filter supports same commands as options. The command accepts the same syntax of the corresponding option.\n\nIf the specified expression is not valid, it is kept at its current value.\n\nRemove all color information for all RGB colors except for certain one.\n\nThe filter accepts the following options:\n\nThis filter supports same commands as options. The command accepts the same syntax of the corresponding option.\n\nIf the specified expression is not valid, it is kept at its current value.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThis filter needs three input video streams. First stream is video stream that is going to be filtered out. Second and third video stream specify color patches for source color to target color mapping.\n\nThe filter accepts the following options:\n\nThe filter accepts the following options:\n\nFor example to convert from BT.601 to SMPTE-240M, use the command:\n\nConvert colorspace, transfer characteristics or color primaries. Input video needs to have an even size.\n\nThe filter accepts the following options:\n\nThe filter converts the transfer characteristics, color space and color primaries to the specified user values. The output value, if not specified, is set to a default value based on the \"all\" property. If that property is also not specified, the filter will log an error. The output color range and format default to the same value as the input color range and format. The input transfer characteristics, color space, color primaries and color range should be set on the input data. If any of these are missing, the filter will log an error and no conversion will take place.\n\nFor example to convert the input to SMPTE-240M, use the command:\n\nAdjust color temperature in video to simulate variations in ambient color temperature.\n\nThe filter accepts the following options:\n\nThis filter supports same commands as options.\n\nApply convolution of 3x3, 5x5, 7x7 or horizontal/vertical up to 49 elements.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nApply 2D convolution of video stream in frequency domain using second stream as impulse.\n\nThe filter accepts the following options:\n\nThe filter also supports the framesync options.\n\nCopy the input video source unchanged to the output. This is mainly useful for testing purposes.\n\nVideo filtering on GPU using Apple’s CoreImage API on OSX.\n\nHardware acceleration is based on an OpenGL context. Usually, this means it is processed by video hardware. However, software-based OpenGL implementations exist which means there is no guarantee for hardware processing. It depends on the respective OSX.\n\nThere are many filters and image generators provided by Apple that come with a large variety of options. The filter has to be referenced by its name along with its options.\n\nThe coreimage filter accepts the following options:\n\nSeveral filters can be chained for successive processing without GPU-HOST transfers allowing for fast processing of complex filter chains. Currently, only filters with zero (generators) or exactly one (filters) input image and one output image are supported. Also, transition filters are not yet usable as intended.\n\nSome filters generate output images with additional padding depending on the respective filter kernel. The padding is automatically removed to ensure the filter output has the same size as the input image.\n\nFor image generators, the size of the output image is determined by the previous output image of the filter chain or the input image of the whole filterchain, respectively. The generators do not use the pixel information of this image to generate their output. However, the generated output is blended onto this image, resulting in partial or complete coverage of the output image.\n\nThe coreimagesrc video source can be used for generating input images which are directly fed into the filter chain. By using it, providing input images by another video source or an input video is not required.\n• Use the CIBoxBlur filter with default options to blur an image:\n• Use a filter chain with CISepiaTone at default values and CIVignetteEffect with its center at 100x100 and a radius of 50 pixels:\n• Use nullsrc and CIQRCodeGenerator to create a QR code for the FFmpeg homepage, given as complete and escaped command-line for Apple’s standard bash shell:\n\nObtain the correlation between two input videos.\n\nBoth input videos must have the same resolution and pixel format for this filter to work correctly. Also it assumes that both inputs have the same number of frames, which are compared one by one.\n\nThe obtained per component, average, min and max correlation is printed through the logging system.\n\nThe filter stores the calculated correlation of each frame in frame metadata.\n\nThis filter also supports the framesync options.\n\nIn the below example the input file being processed is compared with the reference file .\n\nIt accepts the following options:\n• Cover a rectangular object by the supplied image of a given video using :\n\nCrop the input video to given dimensions.\n\nIt accepts the following parameters:\n\nThe , , , parameters are expressions containing the following constants:\n\nThe expression for may depend on the value of , and the expression for may depend on , but they cannot depend on and , as and are evaluated after and .\n\nThe and parameters specify the expressions for the position of the top-left corner of the output (non-cropped) area. They are evaluated for each frame. If the evaluated value is not valid, it is approximated to the nearest valid value.\n\nThe expression for may depend on , and the expression for may depend on .\n• Crop area with size 100x100 at position (12,34). Using named options, the example above becomes:\n• Crop the central input area with size 2/3 of the input video:\n• Delimit the rectangle with the top-left corner placed at position 100:100 and the right-bottom corner corresponding to the right-bottom corner of the input image.\n• Crop 10 pixels from the left and right borders, and 20 pixels from the top and bottom borders\n• Keep only the bottom right quarter of the input image:\n• Set x depending on the value of y:\n\nThis filter supports the following commands:\n\nIt calculates the necessary cropping parameters and prints the recommended parameters via the logging system. The detected dimensions correspond to the non-black or video area of the input video according to .\n\nIt accepts the following parameters:\n• Find an embedded video area, use motion vectors from decoder:\n\nThis filter supports the following commands:\n\nDelay video filtering until a given wallclock timestamp. The filter first passes on amount of frames, then it buffers at most amount of frames and waits for the cue. After reaching the cue it forwards the buffered frames and also any subsequent frames coming in its input.\n\nThe filter can be used synchronize the output of multiple ffmpeg processes for realtime output devices like decklink. By putting the delay in the filtering chain and pre-buffering frames the process can pass on data to output almost immediately after the target wallclock timestamp is reached.\n\nPerfect frame accuracy cannot be guaranteed, but the result is good enough for some use cases.\n\nThis filter is similar to the Adobe Photoshop and GIMP curves tools. Each component (red, green and blue) has its values defined by key points tied from each other using a smooth curve. The x-axis represents the pixel values from the input frame, and the y-axis the new pixel values to be set for the output frame.\n\nBy default, a component curve is defined by the two points and . This creates a straight line where each original pixel value is \"adjusted\" to its own value, which means no change to the image.\n\nThe filter allows you to redefine these two points and add some more. A new curve will be defined to pass smoothly through all these new coordinates. The new defined points need to be strictly increasing over the x-axis, and their and values must be in the interval. The curve is formed by using a natural or monotonic cubic spline interpolation, depending on the option (default: ). The spline produces a smoother curve in general while the monotonic ( ) spline guarantees the transitions between the specified points to be monotonic. If the computed curves happened to go outside the vector spaces, the values will be clipped accordingly.\n\nThe filter accepts the following options:\n\nTo avoid some filtergraph syntax conflicts, each key points list need to be defined using the following syntax: .\n\nThis filter supports same commands as options.\n• Vintage effect: Here we obtain the following coordinates for each components:\n• The previous example can also be achieved with the associated built-in preset:\n• Use a Photoshop preset and redefine the points of the green component:\n• Check out the curves of the profile using and :\n\nThis filter shows hexadecimal pixel values of part of video.\n\nThe filter accepts the following options:\n\nThis filter supports same commands as options excluding option.\n\nThe filter accepts the following options:\n\nThis filter supports same commands as options. The command accepts the same syntax of the corresponding option.\n\nIf the specified expression is not valid, it is kept at its current value.\n\nThis filter is not designed for real time.\n\nThe filter accepts the following options:\n\nThe same operation can be achieved using the expression system:\n\nRemove banding artifacts from input video. It works by replacing banded pixels with average value of referenced pixels.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThe filter accepts the following options:\n• Deblock using weak filter and block size of 4 pixels.\n• Deblock using strong filter, block size of 4 pixels and custom thresholds for deblocking more edges.\n• Similar as above, but filter only first plane.\n• Similar as above, but filter only second and third plane.\n\nThis filter supports the all above options as commands.\n\nThe filter accepts the following options:\n\nApply 2D deconvolution of video stream in frequency domain using second stream as impulse.\n\nThe filter accepts the following options:\n\nThe filter also supports the framesync options.\n\nIt accepts the following options:\n\nThis filter replaces the pixel by the local(3x3) average by taking into account only values lower than the pixel.\n\nIt accepts the following options:\n\nThis filter supports the all above options as commands.\n\nIt accepts the following options:\n\nJudder can be introduced, for instance, by pullup filter. If the original source was partially telecined content then the output of will have a variable frame rate. May change the recorded frame rate of the container. Aside from that change, this filter will not affect constant frame rate video.\n\nThe option available in this filter is:\n\nSuppress a TV station logo by a simple interpolation of the surrounding pixels. Just set a rectangle covering the logo and watch it disappear (and sometimes something even uglier appear - your mileage may vary).\n\nIt accepts the following parameters:\n• Set a rectangle covering the area with top left corner coordinates 0,0 and size 100x77:\n\nRemove the rain in the input image/video by applying the derain methods based on convolutional neural networks. Supported models:\n\nTraining as well as model generation scripts are provided in the repository at https://github.com/XueweiMeng/derain_filter.git.\n\nThe filter accepts the following options:\n\nTo get full functionality (such as async execution), please use the dnn_processing filter.\n\nAttempt to fix small changes in horizontal and/or vertical shift. This filter helps remove camera shake from hand-holding a camera, bumping a tripod, moving on a vehicle, etc.\n\nThe filter accepts the following options:\n\nRemove unwanted contamination of foreground colors, caused by reflected color of greenscreen or bluescreen.\n\nThis filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nApply an exact inverse of the telecine operation. It requires a predefined pattern specified using the pattern option which must be the same as that passed to the telecine filter.\n\nThis filter accepts the following options:\n\nThis filter replaces the pixel by the local(3x3) maximum.\n\nIt accepts the following options:\n\nThis filter supports the all above options as commands.\n\nDisplace pixels as indicated by second and third input stream.\n\nIt takes three input streams and outputs one stream, the first input is the source, and second and third input are displacement maps.\n\nThe second input specifies how much to displace pixels along the x-axis, while the third input specifies how much to displace pixels along the y-axis. If one of displacement map streams terminates, last frame from that displacement map will be used.\n\nNote that once generated, displacements maps can be reused over and over again.\n\nA description of the accepted options follows.\n\nDo classification with deep neural networks based on bounding boxes.\n\nThe filter accepts the following options:\n\nThe filter accepts the following options:\n\nDo image processing with deep neural networks. It works together with another filter which converts the pixel format of the Frame to what the dnn network requires.\n\nThe filter accepts the following options:\n• Remove rain in rgb24 frame with can.pb (see derain filter):\n• Handle the Y channel with srcnn.pb (see sr filter) for frame with yuv420p (planar YUV formats supported):\n• Handle the Y channel with espcn.pb (see sr filter), which changes frame size, for format yuv420p (planar YUV formats supported), please use tools/python/tf_sess_config.py to get the configs of TensorFlow backend for your system.\n\nIt accepts the following parameters:\n\nThe parameters for , , and and are expressions containing the following constants:\n• Draw a black box around the edge of the input image:\n• Draw a box with color red and an opacity of 50%: The previous example can be specified as:\n\nThis filter supports same commands as options. The command accepts the same syntax of the corresponding option.\n\nIf the specified expression is not valid, it is kept at its current value.\n\nIt accepts the following parameters:\n\nExample using metadata from signalstats filter:\n\nExample using metadata from ebur128 filter:\n\nIt accepts the following parameters:\n\nThe parameters for , , and and are expressions containing the following constants:\n• Draw a grid with cell 100x100 pixels, thickness 2 pixels, with color red and an opacity of 50%:\n• Draw a white 3x3 grid with an opacity of 50%:\n\nThis filter supports same commands as options. The command accepts the same syntax of the corresponding option.\n\nIf the specified expression is not valid, it is kept at its current value.\n\nDraw a text string or text from a specified file on top of a video, using the libfreetype library.\n\nTo enable compilation of this filter, you need to configure FFmpeg with and . To enable default font fallback and the option you need to configure FFmpeg with . To enable the option, you need to configure FFmpeg with .\n\nIt accepts the following parameters:\n\nThe parameters for and are expressions containing the following constants and functions:\n\nIf is set to , the filter recognizes sequences accepted by the C function in the provided text and expands them accordingly. Check the documentation of . This feature is deprecated in favor of expansion with the or expansion functions.\n\nIf is set to , the text is printed verbatim.\n\nIf is set to (which is the default), the following expansion mechanism is used.\n\nThe backslash character ‘ ’, followed by any character, always expands to the second character.\n\nSequences of the form are expanded. The text between the braces is a function name, possibly followed by arguments separated by ’:’. If the arguments contain special characters or delimiters (’:’ or ’}’), they should be escaped.\n\nNote that they probably must also be escaped as the value for the option in the filter argument string and as the filter argument in the filtergraph description, and possibly also for the shell, that makes up to four levels of escaping; using a text file with the option avoids these problems.\n\nThe following functions are available:\n\nThe following options are also supported as commands:\n• Draw \"Test Text\" with font FreeSerif, using the default values for the optional parameters.\n• Draw ’Test Text’ with font FreeSerif of size 24 at position x=100 and y=50 (counting from the top-left corner of the screen), text is yellow with a red box around it. Both the text and the box have an opacity of 20%. Note that the double quotes are not necessary if spaces are not used within the parameter list.\n• Show the text at the center of the video frame:\n• Show the text at a random position, switching to a new position every 30 seconds:\n• Show a text line sliding from right to left in the last row of the video frame. The file is assumed to contain a single line with no newlines.\n• Show the content of file off the bottom of the frame and scroll up.\n• Draw a single green letter \"g\", at the center of the input video. The glyph baseline is placed at half screen height.\n• Show text for 1 second every 3 seconds:\n• Use fontconfig to set the font. Note that the colons need to be escaped.\n• Draw \"Test Text\" with font size dependent on height of the video.\n• Print the date of a real-time encoding (see documentation for the C function):\n• Show text fading in and out (appearing/disappearing):\n• Horizontally align multiple separate texts. Note that and the value are included in the offset.\n• Plot special metadata onto each frame if such metadata exists. Otherwise, plot the string \"NA\". Note that image2 demuxer must have option for the special metadata fields to be available for filters.\n\nFor more information about libfreetype, check: http://www.freetype.org/.\n\nFor more information about fontconfig, check: http://freedesktop.org/software/fontconfig/fontconfig-user.html.\n\nFor more information about libfribidi, check: http://fribidi.org/.\n\nFor more information about libharfbuzz, check: https://github.com/harfbuzz/harfbuzz.\n\nDetect and draw edges. The filter uses the Canny Edge Detection algorithm.\n\nThe filter accepts the following options:\n• Standard edge detection with custom values for the hysteresis thresholding:\n\nFor each input image, the filter will compute the optimal mapping from the input to the output given the codebook length, that is the number of distinct output colors.\n\nThis filter accepts the following options.\n\nMeasure graylevel entropy in histogram of color channels of video frames.\n\nIt accepts the following parameters:\n\nApply the EPX magnification filter which is designed for pixel art.\n\nIt accepts the following option:\n\nThe filter accepts the following options:\n\nThe expressions accept the following parameters:\n\nThe filter supports the following commands:\n\nThis filter replaces the pixel by the local(3x3) minimum.\n\nIt accepts the following options:\n\nThis filter supports the all above options as commands.\n\nSpatial only filter that uses edge slope tracing algorithm to interpolate missing lines. It accepts the following parameters:\n\nThis filter supports same commands as options.\n\nThe filter accepts the following options:\n\nThis filter supports same commands as options.\n\nThe filter accepts the following option:\n• Extract luma, u and v color channel component from input video frame into 3 grayscale outputs:\n\nIt accepts the following parameters:\n• Fade in the first 30 frames of video: The command above is equivalent to:\n• Fade out the last 45 frames of a 200-frame video:\n• Fade in the first 25 frames and fade out the last 25 frames of a 1000-frame video:\n• Make the first 5 frames yellow, then fade in from frame 5-24:\n• Fade in alpha over first 25 frames of video:\n• Make the first 5.5 seconds black, then fade in for 0.5 seconds:\n\nThis filter pass cropped input frames to 2nd output. From there it can be filtered with other video filters. After filter receives frame from 2nd input, that frame is combined on top of original frame from 1st input and passed to 1st output.\n\nThe typical usage is filter only part of frame.\n\nThe filter accepts the following options:\n• Blur only top left rectangular part of video frame size 100x100 with gblur filter.\n• Draw black box on top left part of video frame of size 100x100 with drawbox filter.\n• Pixelize rectangular part of video frame of size 100x100 with pixelize filter.\n\nThe filter accepts the following options:\n\nExtract a single field from an interlaced image using stride arithmetic to avoid wasting CPU time. The output frames are marked as non-interlaced.\n\nThe filter accepts the following options:\n\nCreate new frames by copying the top and bottom fields from surrounding frames supplied as numbers by the hint file.\n\nExample of first several lines of file for mode:\n\nField matching filter for inverse telecine. It is meant to reconstruct the progressive frames from a telecined stream. The filter does not drop duplicated frames, so to achieve a complete inverse telecine needs to be followed by a decimation filter such as decimate in the filtergraph.\n\nThe separation of the field matching and the decimation is notably motivated by the possibility of inserting a de-interlacing filter fallback between the two. If the source has mixed telecined and real interlaced content, will not be able to match fields for the interlaced parts. But these remaining combed frames will be marked as interlaced, and thus can be de-interlaced by a later filter such as yadif before decimation.\n\nIn addition to the various configuration options, can take an optional second stream, activated through the option. If enabled, the frames reconstruction will be based on the fields and frames from this second stream. This allows the first input to be pre-processed in order to help the various algorithms of the filter, while keeping the output lossless (assuming the fields are matched properly). Typically, a field-aware denoiser, or brightness/contrast adjustments can help.\n\nNote that this filter uses the same algorithms as TIVTC/TFM (AviSynth project) and VIVTC/VFM (VapourSynth project). The later is a light clone of TFM from which is based on. While the semantic and usage are very close, some behaviour and options names can differ.\n\nThe decimate filter currently only works for constant frame rate input. If your input has mixed telecined (30fps) and progressive content with a lower framerate like 24fps use the following filterchain to produce the necessary cfr stream: .\n\nThe filter accepts the following options:\n\nWe assume the following telecined stream:\n\nThe numbers correspond to the progressive frame the fields relate to. Here, the first two frames are progressive, the 3rd and 4th are combed, and so on.\n\nWhen is configured to run a matching from bottom ( = ) this is how this input stream get transformed:\n\nAs a result of the field matching, we can see that some frames get duplicated. To perform a complete inverse telecine, you need to rely on a decimation filter after this operation. See for instance the decimate filter.\n\nThe same operation now matching from top fields ( = ) looks like this:\n\nIn these examples, we can see what , and mean; basically, they refer to the frame and field of the opposite parity:\n• matches the field of the opposite parity in the previous frame\n• matches the field of the opposite parity in the current frame\n• matches the field of the opposite parity in the next frame\n\nThe and matching are a bit special in the sense that they match from the opposite parity flag. In the following examples, we assume that we are currently matching the 2nd frame (Top:2, bottom:2). According to the match, a ’x’ is placed above and below each matched fields.\n\nAdvanced IVTC, with fallback on yadif for still combed frames:\n\nTransform the field order of the input video.\n\nIt accepts the following parameters:\n\nThe default value is ‘ ’.\n\nThe transformation is done by shifting the picture content up or down by one line, and filling the remaining line with appropriate picture content. This method is consistent with most broadcast field order converters.\n\nIf the input video is not flagged as being interlaced, or it is already flagged as being of the required output field order, then this filter does not alter the incoming video.\n\nIt is very useful when converting to or from PAL DV material, which is bottom field first.\n\nFill borders of the input video, without changing video stream dimensions. Sometimes video can have garbage at the four edges and you may not want to crop video input to keep size multiple of some number.\n\nThis filter accepts the following options:\n\nThis filter supports same commands as options. The command accepts the same syntax of the corresponding option.\n\nIf the specified expression is not valid, it is kept at its current value.\n\nThe object to search for must be specified as a gray8 image specified with the option.\n\nFor each possible match, a score is computed. If the score reaches the specified threshold, the object is considered found.\n\nIf the input video contains multiple instances of the object, the filter will find only one of them.\n\nWhen an object is found, the following metadata entries are set in the matching frame:\n\nIt accepts the following options:\n• Cover a rectangular object by the supplied image of a given video using :\n• Find the position of an object in each frame using and write it to a log file:\n\nFlood area with values of same pixel components with another values.\n\nIt accepts the following options:\n\nConvert the input video to one of the specified pixel formats. Libavfilter will try to pick one that is suitable as input to the next filter.\n\nIt accepts the following parameters:\n• Convert the input video to the format Convert the input video to any of the formats in the list\n\nConvert the video to specified constant frame rate by duplicating or dropping frames as necessary.\n\nIt accepts the following parameters:\n\nAlternatively, the options can be specified as a flat string: [: [: ]].\n\nSee also the setpts filter.\n• A typical usage in order to set the fps to 25:\n• Sets the fps to 24, using abbreviation and rounding method to round to nearest:\n\nPack two different video streams into a stereoscopic video, setting proper metadata on supported codecs. The two views should have the same size and framerate and processing will stop when the shorter video ends. Please note that you may conveniently adjust view properties with the scale and fps filters.\n\nIt accepts the following parameters:\n\nChange the frame rate by interpolating new video output frames from the source frames.\n\nThis filter is not designed to function correctly with interlaced media. If you wish to change the frame rate of interlaced media then you are required to deinterlace before this filter and re-interlace after this filter.\n\nA description of the accepted options follows.\n\nThis filter accepts the following option:\n\nThis filter logs a message and sets frame metadata when it detects that the input video has no significant change in content during a specified duration. Video freeze detection calculates the mean average absolute difference of all the components of video frames and compares it to a noise floor.\n\nThe printed times and duration are expressed in seconds. The metadata key is set on the first frame whose timestamp equals or exceeds the detection duration and it contains the timestamp of the first frame of the freeze. The and metadata keys are set on the first frame after the freeze.\n\nThe filter accepts the following options:\n\nThis filter freezes video frames using frame from 2nd input.\n\nThe filter accepts the following options:\n\nTo enable the compilation of this filter, you need to install the frei0r header and configure FFmpeg with .\n\nIt accepts the following parameters:\n\nA frei0r effect parameter can be a boolean (its value is either \"y\" or \"n\"), a double, a color (specified as / / , where , , and are floating point numbers between 0.0 and 1.0, inclusive) or a color description as specified in the (ffmpeg-utils)\"Color\" section in the ffmpeg-utils manual, a position (specified as / , where and are floating point numbers) and/or a string.\n\nThe number and types of parameters depend on the loaded effect. If an effect parameter is not specified, the default value is set.\n• Apply the distort0r effect, setting the first two double parameters:\n• Apply the colordistance effect, taking a color as the first parameter:\n• Apply the perspective effect, specifying the top left and top right image positions:\n\nFor more information, see http://frei0r.dyne.org\n\nThis filter supports the option as commands.\n\nApply fast and simple postprocessing. It is a faster version of spp.\n\nIt splits (I)DCT into horizontal/vertical passes. Unlike the simple post- processing filter, one of them is performed once per block, not per pixel. This allows for much higher speed.\n\nThe filter accepts the following options:\n\nSynchronize video frames with an external mapping from a file.\n\nFor each input PTS given in the map file it either drops or creates as many frames as necessary to recreate the sequence of output frames given in the map file.\n\nThis filter is useful to recreate the output frames of a framerate conversion by the fps filter, recorded into a map file using the ffmpeg option , and do further processing to the corresponding frames e.g. quality comparison.\n\nEach line of the map file must contain three items per input frame, the input PTS (decimal), the output PTS (decimal) and the output TIMEBASE (decimal/decimal), seperated by a space. This file format corresponds to the output of .\n\nThe filter assumes the map file is sorted by increasing input PTS.\n\nThe filter accepts the following options:\n\nThe filter accepts the following options:\n\nThis filter supports same commands as options. The command accepts the same syntax of the corresponding option.\n\nIf the specified expression is not valid, it is kept at its current value.\n\nThe filter accepts the following options:\n\nThe colorspace is selected according to the specified options. If one of the , , or options is specified, the filter will automatically select a YCbCr colorspace. If one of the , , or options is specified, it will select an RGB colorspace.\n\nIf one of the chrominance expression is not defined, it falls back on the other one. If no alpha expression is specified it will evaluate to opaque value. If none of chrominance expressions are specified, they will evaluate to the luma expression.\n\nThe expressions can use the following variables and functions:\n\nFor functions, if and are outside the area, the value will be automatically clipped to the closer edge.\n\nPlease note that this filter can use multiple threads in which case each slice will have its own expression state. If you want to use only a single expression state because your expressions depend on previous state then you should limit the number of filter threads to 1.\n• Generate a bidimensional sine wave, with angle and a wavelength of 100 pixels:\n• Create a radial gradient that is the same size as the input (also see the vignette filter):\n\nFix the banding artifacts that are sometimes introduced into nearly flat regions by truncation to 8-bit color depth. Interpolate the gradients that should go where the bands are, and dither them.\n\nIt is designed for playback only. Do not use it prior to lossy compression, because compression tends to lose the dither and bring back the bands.\n\nIt accepts the following parameters:\n\nAlternatively, the options can be specified as a flat string: [: ]\n• Apply the filter with a strength and radius of :\n• Specify radius, omitting the strength (which will fall-back to the default value):\n\nWith this filter one can debug complete filtergraph. Especially issues with links filling with queued frames.\n\nThe filter accepts the following options:\n\nA color constancy filter that applies color correction based on the grayworld assumption\n\nThe algorithm uses linear light, so input data should be linearized beforehand (and possibly correctly tagged).\n\nA color constancy variation filter which estimates scene illumination via grey edge algorithm and corrects the scene colors accordingly.\n\nThe filter accepts the following options:\n\nApply guided filter for edge-preserving smoothing, dehazing and so on.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n• Dehazing, structure-transferring filtering, detail enhancement with guided filter. For the generation of guidance image, refer to paper \"Guided Image Filtering\". See: http://kaiminghe.com/publications/pami12guidedfilter.pdf.\n\nFirst input is the video stream to process, and second one is the Hald CLUT. The Hald CLUT input can be a simple picture or a complete video stream.\n\nThe filter accepts the following options:\n\nalso has the same interpolation options as lut3d (both filters share the same internals).\n\nThis filter also supports the framesync options.\n\nMore information about the Hald CLUT can be found on Eskil Steenberg’s website (Hald CLUT author) at http://www.quelsolaar.com/technology/clut.html.\n\nThis filter supports the option as commands.\n\nGenerate an identity Hald CLUT stream altered with various effects:\n\nNote: make sure you use a lossless codec.\n\nThen use it with to apply it on some random stream:\n\nThe Hald CLUT will be applied to the 10 first seconds (duration of ), then the latest picture of that CLUT stream will be applied to the remaining frames of the stream.\n\nA Hald CLUT is supposed to be a squared image of by pixels. For a given Hald CLUT, FFmpeg will select the biggest possible square starting at the top left of the picture. The remaining padding pixels (bottom or right) will be ignored. This area can be used to add a preview of the Hald CLUT.\n\nTypically, the following generated Hald CLUT will be supported by the filter:\n\nIt contains the original and a preview of the effect of the CLUT: SMPTE color bars are displayed on the right-top, and below the same color bars processed by the color changes.\n\nThen, the effect of this Hald CLUT can be visualized with:\n\nFor example, to horizontally flip the input video with :\n\nIt can be used to correct video that has a compressed range of pixel intensities. The filter redistributes the pixel intensities to equalize their distribution across the intensity range. It may be viewed as an \"automatically adjusting contrast filter\". This filter is useful only for correcting degraded or poorly captured source video.\n\nThe filter accepts the following options:\n\nCompute and draw a color distribution histogram for the input video.\n\nThe computed histogram is a representation of the color component distribution in an image.\n\nStandard histogram displays the color components distribution in an image. Displays color graph for each color component. Shows distribution of the Y, U, V, A or R, G, B components, depending on input format, in the current frame. Below each graph a color component scale meter is shown.\n\nThe filter accepts the following options:\n\nThis is a high precision/quality 3d denoise filter. It aims to reduce image noise, producing smooth images and making still images really still. It should enhance compressibility.\n\nIt accepts the following optional parameters:\n\nThis filter supports same commands as options. The command accepts the same syntax of the corresponding option.\n\nIf the specified expression is not valid, it is kept at its current value.\n\nThe input must be in hardware frames, and the output a non-hardware format. Not all formats will be supported on the output - it may be necessary to insert an additional filter immediately following in the graph to get the output in a supported format.\n\nMap hardware frames to system memory or to another device.\n\nThis filter has several different modes of operation; which one is used depends on the input and output formats:\n• Hardware frame input, normal frame output Map the input frames to system memory and pass them to the output. If the original hardware frame is later required (for example, after overlaying something else on part of it), the filter can be used again in the next mode to retrieve it.\n• Normal frame input, hardware frame output If the input is actually a software-mapped hardware frame, then unmap it - that is, return the original hardware frame. Otherwise, a device must be provided. Create new hardware surfaces on that device for the output, then map them back to the software format at the input and give those frames to the preceding filter. This will then act like the filter, but may be able to avoid an additional copy when the input is already in a compatible format.\n• Hardware frame input and output A device must be supplied for the output, either directly or with the option. The input and output devices must be of different types and compatible - the exact meaning of this is system-dependent, but typically it means that they must refer to the same underlying hardware context (for example, refer to the same graphics card). If the input frames were originally created on the output device, then unmap to retrieve the original frames. Otherwise, map the frames to the output device - create new hardware frames on the output corresponding to the frames on the input.\n\nThe following additional parameters are accepted:\n\nThe device to upload to must be supplied when the filter is initialised. If using ffmpeg, select the appropriate device with the option or with the option. The input and output devices must be of different types and compatible - the exact meaning of this is system-dependent, but typically it means that they must refer to the same underlying hardware context (for example, refer to the same graphics card).\n\nThe following additional parameters are accepted:\n\nIt accepts the following optional parameters:\n\nApply a high-quality magnification filter designed for pixel art. This filter was originally created by Maxim Stepin.\n\nIt accepts the following option:\n\nAll streams must be of same pixel format and of same height.\n\nNote that this filter is faster than using overlay and pad filter to create same output.\n\nThe filter accepts the following option:\n\nThis filter measures color difference between set HSV color in options and ones measured in video stream. Depending on options, output colors can be changed to be gray or not.\n\nThe filter accepts the following options:\n\nThis filter measures color difference between set HSV color in options and ones measured in video stream. Depending on options, output colors can be changed to transparent by adding alpha channel.\n\nThe filter accepts the following options:\n\nModify the hue and/or the saturation of the input.\n\nIt accepts the following parameters:\n\nand are mutually exclusive, and can’t be specified at the same time.\n\nThe , , and option values are expressions containing the following constants:\n• Set the hue to 90 degrees and the saturation to 1.0:\n• Same command but expressing the hue in radians:\n• Rotate hue and make the saturation swing between 0 and 2 over a period of 1 second:\n• Apply a 3 seconds saturation fade-in effect starting at 0: The general fade-in expression can be written as:\n• Apply a 3 seconds saturation fade-out effect starting at 5 seconds: The general fade-out expression can be written as:\n\nThis filter supports the following commands:\n\nThis filter accepts the following options:\n\nGrow first stream into second stream by connecting components. This makes it possible to build more robust edge masks.\n\nThis filter accepts the following options:\n\nThe filter also supports the framesync options.\n\nDetect the colorspace from an embedded ICC profile (if present), and update the frame’s tags accordingly.\n\nThis filter accepts the following options:\n\nGenerate ICC profiles and attach them to frames.\n\nThis filter accepts the following options:\n\nObtain the identity score between two input videos.\n\nBoth input videos must have the same resolution and pixel format for this filter to work correctly. Also it assumes that both inputs have the same number of frames, which are compared one by one.\n\nThe obtained per component, average, min and max identity score is printed through the logging system.\n\nThe filter stores the calculated identity scores of each frame in frame metadata.\n\nThis filter also supports the framesync options.\n\nIn the below example the input file being processed is compared with the reference file .\n\nThis filter tries to detect if the input frames are interlaced, progressive, top or bottom field first. It will also try to detect fields that are repeated between adjacent frames (a sign of telecine).\n\nSingle frame detection considers only immediately adjacent frames when classifying each frame. Multiple frame detection incorporates the classification history of previous frames.\n\nThe filter will log these metadata values:\n\nThe filter accepts the following options:\n\nInspect the field order of the first 360 frames in a video, in verbose detail:\n\nThe idet filter will add analysis metadata to each frame, which will then be discarded. At the end, the filter will also print a final report with statistics.\n\nThis filter allows one to process interlaced images fields without deinterlacing them. Deinterleaving splits the input frame into 2 fields (so called half pictures). Odd lines are moved to the top half of the output image, even lines to the bottom half. You can process (filter) them independently and then re-interleave them.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThis filter replaces the pixel by the local(3x3) average by taking into account only values higher than the pixel.\n\nIt accepts the following options:\n\nThis filter supports the all above options as commands.\n\nSimple interlacing filter from progressive contents. This interleaves upper (or lower) lines from odd frames with lower (or upper) lines from even frames, halving the frame rate and preserving image height.\n\nIt accepts the following optional parameters:\n\nDeinterlace input video by applying Donald Graft’s adaptive kernel deinterling. Work on interlaced parts of a video to produce progressive frames.\n\nThe description of the accepted parameters follows.\n\nThe filter accepts the following option:\n\nThis filter supports the all above options as commands.\n\nThis filter makes short flashes of light appear longer. This filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThis filter can be used to correct for radial distortion as can result from the use of wide angle lenses, and thereby re-rectify the image. To find the right parameters one can use tools available for example as part of opencv or simply trial-and-error. To use opencv use the calibration sample (under samples/cpp) from the opencv sources and extract the k1 and k2 coefficients from the resulting matrix.\n\nNote that effectively the same filter is available in the open-source tools Krita and Digikam from the KDE project.\n\nIn contrast to the vignette filter, which can also be used to compensate lens errors, this filter corrects the distortion of the image, whereas vignette corrects the brightness distribution, so you may want to use both filters together in certain cases, though you will have to take care of ordering, i.e. whether vignetting should be applied before or after lens correction.\n\nThe filter accepts the following options:\n\nThe formula that generates the correction is:\n\nwhere is halve of the image diagonal and and are the distances from the focal point in the source and target images, respectively.\n\nThis filter supports the all above options as commands.\n\nThe filter requires the camera make, camera model, and lens model to apply the lens correction. The filter will load the lensfun database and query it to find the corresponding camera and lens entries in the database. As long as these entries can be found with the given options, the filter can perform corrections on frames. Note that incomplete strings will result in the filter choosing the best match with the given options, and the filter will output the chosen camera and lens models (logged with level \"info\"). You must provide the make, camera model, and lens model as they are required.\n\nTo obtain a list of available makes and models, leave out one or both of and options. The filter will send the full list to the log with level . The first column is the make and the second column is the model. To obtain a list of available lenses, set any values for make and model and leave out the option. The filter will send the full list of lenses in the log with level . The ffmpeg tool will exit after the list is printed.\n\nThe filter accepts the following options:\n• Apply lens correction with make \"Canon\", camera model \"Canon EOS 100D\", and lens model \"Canon EF-S 18-55mm f/3.5-5.6 IS STM\" with focal length of \"18\" and aperture of \"8.0\".\n• Apply the same as before, but only for the first 5 seconds of video.\n\nThe options for this filter are divided into the following sections:\n\nThese options control the overall output mode. By default, libplacebo will try to preserve the source colorimetry and size as best as it can, but it will apply any embedded film grain, dolby vision metadata or anamorphic SAR present in source frames.\n\nIn addition to the expression constants documented for the scale filter, the , , , , , , and options can also contain the following constants:\n\nThe options in this section control how libplacebo performs upscaling and (if necessary) downscaling. Note that libplacebo will always internally operate on 4:4:4 content, so any sub-sampled chroma formats such as will necessarily be upsampled and downsampled as part of the rendering process. That means scaling might be in effect even if the source and destination resolution are the same.\n\nDeinterlacing is automatically supported when frames are tagged as interlaced, however frames are not deinterlaced unless a deinterlacing algorithm is chosen.\n\nLibplacebo comes with a built-in debanding filter that is good at counteracting many common sources of banding and blocking. Turning this on is highly recommended whenever quality is desired.\n\nA collection of subjective color controls. Not very rigorous, so the exact effect will vary somewhat depending on the input primaries and colorspace.\n\nTo help deal with sources that only have static HDR10 metadata (or no tagging whatsoever), libplacebo uses its own internal frame analysis compute shader to analyze source frames and adapt the tone mapping function in realtime. If this is too slow, or if exactly reproducible frame-perfect results are needed, it’s recommended to turn this feature off.\n\nThe options in this section control how libplacebo performs tone-mapping and gamut-mapping when dealing with mismatches between wide-gamut or HDR content. In general, libplacebo relies on accurate source tagging and mastering display gamut information to produce the best results.\n\nBy default, libplacebo will dither whenever necessary, which includes rendering to any integer format below 16-bit precision. It’s recommended to always leave this on, since not doing so may result in visible banding in the output, even if the filter is enabled. If maximum performance is needed, use instead of disabling dithering.\n\nlibplacebo supports a number of custom shaders based on the mpv .hook GLSL syntax. A collection of such shaders can be found here: https://github.com/mpv-player/mpv/wiki/User-Scripts#user-shaders\n\nA full description of the mpv shader format is beyond the scope of this section, but a summary can be found here: https://mpv.io/manual/master/#options-glsl-shader\n\nAll of the options in this section default off. They may be of assistance when attempting to squeeze the maximum performance at the cost of quality.\n\nThis filter supports almost all of the above options as commands.\n• Rescale input to fit into standard 1080p, with high quality scaling:\n• Run this filter on the CPU, on systems with Mesa installed (and with the most expensive options disabled):\n• Suppress CPU-based AV1/H.274 film grain application in the decoder, in favor of doing it with this filter. Note that this is only a gain if the frames are either already on the GPU, or if you’re using libplacebo for other purposes, since otherwise the VRAM roundtrip will more than offset any expected speedup.\n• Interop with VAAPI hwdec to avoid round-tripping through RAM:\n\nCalculate the VMAF (Video Multi-Method Assessment Fusion) score for a reference/distorted pair of input videos.\n\nThe first input is the distorted video, and the second input is the reference video.\n\nThe obtained VMAF score is printed through the logging system.\n\nIt requires Netflix’s vmaf library (libvmaf) as a pre-requisite. After installing the library it can be enabled using: .\n\nThe filter has following options:\n\nThis filter also supports the framesync options.\n• In the examples below, a distorted video is compared with a reference file .\n• Example with options and different containers:\n\nThis is the CUDA variant of the libvmaf filter. It only accepts CUDA frames.\n\nIt requires Netflix’s vmaf library (libvmaf) as a pre-requisite. After installing the library it can be enabled using: .\n\nApply limited difference filter using second and optionally third video stream.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands except option ‘ ’.\n\nLimits the pixel components values to the specified range [min, max].\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThe filter accepts the following options:\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThe filter accepts the following options:\n\nThis filter supports the option as commands.\n\nThe filter accepts the following options:\n\nThis filter supports same commands as options. The command accepts the same syntax of the corresponding option.\n\nIf the specified expression is not valid, it is kept at its current value.\n\nCompute a look-up table for binding each pixel component input value to an output value, and apply it to the input video.\n\napplies a lookup table to a YUV input video, to an RGB input video.\n\nThese filters accept the following parameters:\n\nEach of them specifies the expression to use for computing the lookup table for the corresponding pixel component values.\n\nThe exact component associated to each of the options depends on the format in input.\n\nThe filter requires either YUV or RGB pixel formats in input, requires RGB pixel formats in input, and requires YUV.\n\nThe expressions can contain the following constants and functions:\n\nThis filter supports same commands as options.\n• Negate input video: The above is the same as:\n\nThe filter takes two input streams and outputs one stream.\n\nThe (time lut2) filter takes two consecutive frames from one single stream.\n\nThis filter accepts the following parameters:\n\nThe filter also supports the framesync options.\n\nEach of them specifies the expression to use for computing the lookup table for the corresponding pixel component values.\n\nThe exact component associated to each of the options depends on the format in inputs.\n\nThe expressions can contain the following constants:\n\nThis filter supports the all above options as commands except option .\n\nClamp the first input stream with the second input and third input stream.\n\nReturns the value of first stream to be between second input stream - and third input stream + .\n\nThis filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nMerge the second and third input stream into output stream using absolute differences between second input stream and first input stream and absolute difference between third input stream and first input stream. The picked value will be from second input stream if second absolute difference is greater than first one or from third input stream otherwise.\n\nThis filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nMerge the first input stream with the second input stream using per pixel weights in the third input stream.\n\nA value of 0 in the third stream pixel component means that pixel component from first stream is returned unchanged, while maximum value (eg. 255 for 8-bit videos) means that pixel component from second stream is returned unchanged. Intermediate values define the amount of merging between both input stream’s pixel components.\n\nThis filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nMerge the second and third input stream into output stream using absolute differences between second input stream and first input stream and absolute difference between third input stream and first input stream. The picked value will be from second input stream if second absolute difference is less than first one or from third input stream otherwise.\n\nThis filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nPick pixels comparing absolute difference of two video streams with fixed threshold.\n\nIf absolute difference between pixel component of first and second video stream is equal or lower than user supplied threshold than pixel component from first video stream is picked, otherwise pixel component from second video stream is picked.\n\nThis filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nFor example it is useful to create motion masks after filter.\n\nThis filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nIt needs one field per frame as input and must thus be used together with yadif=1/3 or equivalent.\n\nThis filter accepts the following options:\n\nPick median pixel from certain rectangle defined by radius.\n\nThis filter accepts the following options:\n\nThis filter supports same commands as options. The command accepts the same syntax of the corresponding option.\n\nIf the specified expression is not valid, it is kept at its current value.\n\nThe filter accepts up to 4 input streams, and merge selected input planes to the output video.\n\nThis filter accepts the following options:\n• Merge three gray video streams of same width and height into single video stream:\n\nEstimate and export motion vectors using block matching algorithms. Motion vectors are stored in frame side data to be used by other filters.\n\nThis filter accepts the following options:\n\nMidway Image Equalization adjusts a pair of images to have the same histogram, while maintaining their dynamics as much as possible. It’s useful for e.g. matching exposures from a pair of stereo cameras.\n\nThis filter has two inputs and one output, which must be of same pixel format, but may be of different sizes. The output of filter is first input adjusted with midway histogram of both inputs.\n\nThis filter accepts the following option:\n\nConvert the video to specified frame rate using motion interpolation.\n\nThis filter accepts the following options:\n\nMix several video input streams into one video stream.\n\nA description of the accepted options follows.\n\nThis filter supports the following commands:\n\nA description of the accepted options follows.\n\nThis filter supports the all above options as commands.\n\nThis filter allows to apply main morphological grayscale transforms, erode and dilate with arbitrary structures set in second input stream.\n\nUnlike naive implementation and much slower performance in erosion and dilation filters, when speed is critical filter should be used instead.\n\nThe filter also supports the framesync options.\n\nThis filter supports same commands as options.\n\nDrop frames that do not differ greatly from the previous frame in order to reduce frame rate.\n\nThe main use of this filter is for very-low-bitrate encoding (e.g. streaming over dialup modem), but it could in theory be used for fixing movies that were inverse-telecined incorrectly.\n\nA description of the accepted options follows.\n\nObtain the MSAD (Mean Sum of Absolute Differences) between two input videos.\n\nBoth input videos must have the same resolution and pixel format for this filter to work correctly. Also it assumes that both inputs have the same number of frames, which are compared one by one.\n\nThe obtained per component, average, min and max MSAD is printed through the logging system.\n\nThe filter stores the calculated MSAD of each frame in frame metadata.\n\nThis filter also supports the framesync options.\n\nIn the below example the input file being processed is compared with the reference file .\n\nMultiply first video stream pixels values with second video stream pixels values.\n\nThe filter accepts the following options:\n\nThis filter supports same commands as options.\n\nIt accepts the following option:\n\nThis filter supports same commands as options.\n\nEach pixel is adjusted by looking for other pixels with similar contexts. This context similarity is defined by comparing their surrounding patches of size x . Patches are searched in an area of x around the pixel.\n\nNote that the research area defines centers for patches, which means some patches will be made of pixels outside that research area.\n\nThe filter accepts the following options.\n\nThis filter accepts the following options:\n\nThis filter supports same commands as options, excluding option.\n\nForce libavfilter not to use any of the specified pixel formats for the input to the next filter.\n\nIt accepts the following parameters:\n• Force libavfilter to use a format different from for the input to the vflip filter:\n• Convert the input video to any of the formats not contained in the list:\n\nThe filter accepts the following options:\n\nFor each channel of each frame, the filter computes the input range and maps it linearly to the user-specified output range. The output range defaults to the full dynamic range from pure black to pure white.\n\nTemporal smoothing can be used on the input range to reduce flickering (rapid changes in brightness) caused when small dark or bright objects enter or leave the scene. This is similar to the auto-exposure (automatic gain control) on a video camera, and, like a video camera, it may cause a period of over- or under-exposure of the video.\n\nThe R,G,B channels can be normalized independently, which may cause some color shifting, or linked together as a single channel, which prevents color shifting. Linked normalization preserves hue. Independent normalization does not, so it can be used to remove some color casts. Independent and linked normalization can be combined in any ratio.\n\nThe normalize filter accepts the following options:\n\nThis filter supports same commands as options, excluding option. The command accepts the same syntax of the corresponding option.\n\nIf the specified expression is not valid, it is kept at its current value.\n\nStretch video contrast to use the full dynamic range, with no temporal smoothing; may flicker depending on the source content:\n\nAs above, but with 50 frames of temporal smoothing; flicker should be reduced, depending on the source content:\n\nAs above, but with hue-preserving linked channel normalization:\n\nAs above, but with half strength:\n\nMap the darkest input color to red, the brightest input color to cyan:\n\nPass the video source unchanged to the output.\n\nThis filter uses Tesseract for optical character recognition. To enable compilation of this filter, you need to configure FFmpeg with .\n\nIt accepts the following options:\n\nThe filter exports recognized text as the frame metadata . The filter exports confidence of recognized words as the frame metadata .\n\nTo enable this filter, install the libopencv library and headers and configure FFmpeg with .\n\nIt accepts the following parameters:\n\nRefer to the official libopencv documentation for more precise information: http://docs.opencv.org/master/modules/imgproc/doc/filtering.html\n\nSeveral libopencv filters are supported; see the following subsections.\n\nDilate an image by using a specific structuring element. It corresponds to the libopencv function .\n\nrepresents a structuring element, and has the syntax: x + x /\n\nand represent the number of columns and rows of the structuring element, and the anchor point, and the shape for the structuring element. must be \"rect\", \"cross\", \"ellipse\", or \"custom\".\n\nIf the value for is \"custom\", it must be followed by a string of the form \"= \". The file with name is assumed to represent a binary image, with each printable character corresponding to a bright pixel. When a custom is used, and are ignored, the number or columns and rows of the read file are assumed instead.\n\nThe default value for is \"3x3+0x0/rect\".\n\nspecifies the number of times the transform is applied to the image, and defaults to 1.\n\nErode an image by using a specific structuring element. It corresponds to the libopencv function .\n\nIt accepts the parameters: : , with the same syntax and semantics as the dilate filter.\n\nThe filter takes the following parameters: | | | | .\n\nis the type of smooth filter to apply, and must be one of the following values: \"blur\", \"blur_no_scale\", \"median\", \"gaussian\", or \"bilateral\". The default value is \"gaussian\".\n\nThe meaning of , , , and depends on the smooth type. and accept integer positive values or 0. and accept floating point values.\n\nThe default value for is 3. The default value for the other parameters is 0.\n\nThese parameters correspond to the parameters assigned to the libopencv function .\n\nUseful to measure spatial impulse, step responses, chroma delays, etc.\n\nIt accepts the following parameters:\n\nThis filter supports same commands as options. The command accepts the same syntax of the corresponding option.\n\nIf the specified expression is not valid, it is kept at its current value.\n\nOverlay one video on top of another.\n\nIt takes two inputs and has one output. The first input is the \"main\" video on which the second input is overlaid.\n\nIt accepts the following parameters:\n\nA description of the accepted options follows.\n\nThe , and expressions can contain the following parameters.\n\nThis filter also supports the framesync options.\n\nNote that the , variables are available only when evaluation is done per frame, and will evaluate to NAN when is set to ‘ ’.\n\nBe aware that frames are taken from each input video in timestamp order, hence, if their initial timestamps differ, it is a good idea to pass the two inputs through a filter to have them begin in the same zero timestamp, as the example for the filter does.\n\nYou can chain together more overlays but you should test the efficiency of such approach.\n\nThis filter supports the following commands:\n• Draw the overlay at 10 pixels from the bottom right corner of the main video: Using named options the example above becomes:\n• Insert a transparent PNG logo in the bottom left corner of the input, using the tool with the option:\n• Insert 2 different transparent PNG logos (second logo on bottom right corner) using the tool:\n• Add a transparent color layer on top of the main video; must specify the size of the main input to the overlay filter:\n• Play an original video and a filtered version (here with the deshake filter) side by side using the tool: The above command is the same as:\n• Make a sliding overlay appearing from the left to the right top part of the screen starting since time 2:\n• Compose output by putting two input videos side to side:\n• Mask 10-20 seconds of a video by applying the delogo filter to a section\n\nThe filter accepts the following options:\n\nAdd paddings to the input image, and place the original input at the provided , coordinates.\n\nIt accepts the following parameters:\n\nThe value for the , , , and options are expressions containing the following constants:\n• Add paddings with the color \"violet\" to the input video. The output video size is 640x480, and the top-left corner of the input video is placed at column 0, row 40 The example above is equivalent to the following command:\n• Pad the input to get an output with dimensions increased by 3/2, and put the input video at the center of the padded area:\n• Pad the input to get a squared output with size equal to the maximum value between the input width and height, and put the input video at the center of the padded area:\n• Pad the input to get a final w/h ratio of 16:9:\n• In case of anamorphic video, in order to set the output display aspect correctly, it is necessary to use in the expression, according to the relation: Thus the previous example needs to be modified to:\n• Double the output size and put the input video in the bottom-right corner of the output padded area:\n\nGenerate one palette for a whole video stream.\n\nIt accepts the following options:\n\nThe filter also exports the frame metadata ( ) which you can use to evaluate the degree of color quantization of the palette. This information is also visible at logging level.\n• Generate a representative palette of a given video using :\n\nUse a palette to downsample an input video stream.\n\nThe filter takes two inputs: one video stream and a palette. The palette must be a 256 pixels image.\n\nIt accepts the following options:\n• Use a palette (generated for example with palettegen) to encode a GIF using :\n\nCorrect perspective of video not recorded perpendicular to the screen.\n\nA description of the accepted parameters follows.\n\nDelay interlaced video by one field time so that the field order changes.\n\nThe intended use is to fix PAL movies that have been captured with the opposite field order to the film-to-video transfer.\n\nA description of the accepted parameters follows.\n\nThis filter supports the all above options as commands.\n\nReduce various flashes in video, so to help users with epilepsy.\n\nIt accepts the following options:\n\nPixel format descriptor test filter, mainly useful for internal testing. The output video should be equal to the input video.\n\ncan be used to test the monowhite pixel format descriptor definition.\n\nThe filter accepts the following options:\n\nThis filter supports all options as commands.\n\nDisplay sample values of color channels. Mainly useful for checking color and levels. Minimum supported resolution is 640x480.\n\nThe filters accept the following options:\n\nThis filter supports same commands as options.\n\nEnable the specified chain of postprocessing subfilters using libpostproc. This library should be automatically selected with a GPL build ( ). Subfilters must be separated by ’/’ and can be disabled by prepending a ’-’. Each subfilter and some options have a short and a long name that can be used interchangeably, i.e. dr/dering are the same.\n\nThe filters accept the following options:\n\nAll subfilters share common options to determine their scope:\n\nThese options can be appended after the subfilter name, separated by a ’|’.\n\nThe horizontal and vertical deblocking filters share the difference and flatness values so you cannot set different horizontal and vertical thresholds.\n• Apply deblocking on luma only, and switch vertical deblocking on or off automatically depending on available CPU time:\n\nApply Postprocessing filter 7. It is variant of the spp filter, similar to spp = 6 with 7 point DCT, where only the center sample is used after IDCT.\n\nThe filter accepts the following options:\n\nApply alpha premultiply effect to input video stream using first plane of second stream as alpha.\n\nBoth streams must have same dimensions and same pixel format.\n\nThe filter accepts the following option:\n\nThe filter accepts the following option:\n\nThis filter supports the all above options as commands.\n\nThis filter accepts the following options:\n\nEach of the expression options specifies the expression to use for computing the lookup table for the corresponding pixel component values.\n\nThe expressions can contain the following constants and functions:\n\nThis filter supports the all above options as commands.\n\nObtain the average, maximum and minimum PSNR (Peak Signal to Noise Ratio) between two input videos.\n\nThis filter takes in input two input videos, the first input is considered the \"main\" source and is passed unchanged to the output. The second input is used as a \"reference\" video for computing the PSNR.\n\nBoth video inputs must have the same resolution and pixel format for this filter to work correctly. Also it assumes that both inputs have the same number of frames, which are compared one by one.\n\nThe obtained average PSNR is printed through the logging system.\n\nThe filter stores the accumulated MSE (mean squared error) of each frame, and at the end of the processing it is averaged across all frames equally, and the following formula is applied to obtain the PSNR:\n\nWhere MAX is the average of the maximum values of each component of the image.\n\nThe description of the accepted parameters follows.\n\nThis filter also supports the framesync options.\n\nThe file printed if is selected, contains a sequence of key/value pairs of the form : for each compared couple of frames.\n\nIf a greater than 1 is specified, a header line precedes the list of per-frame-pair stats, with key value pairs following the frame format with the following parameters:\n\nA description of each shown per-frame-pair parameter follows:\n• For example: On this example the input file being processed is compared with the reference file . The PSNR of each individual frame is stored in .\n• Another example with different containers:\n\nThe pullup filter is designed to take advantage of future context in making its decisions. This filter is stateless in the sense that it does not lock onto a pattern to follow, but it instead looks forward to the following fields in order to identify matches and rebuild progressive frames.\n\nTo produce content with an even framerate, insert the fps filter after pullup, use if the input frame rate is 29.97fps, for 30fps and the (rare) telecined 25fps input.\n\nThe filter accepts the following options:\n\nFor best results (without duplicated frames in the output file) it is necessary to change the output frame rate. For example, to inverse telecine NTSC input:\n\nThe filter accepts the following option:\n\nThe expression is evaluated through the eval API and can contain, among others, the following constants:\n\nGenerate a QR code using the libqrencode library (see https://fukuchi.org/works/qrencode/), and overlay it on top of the current frame.\n\nTo enable the compilation of this filter, you need to configure FFmpeg with .\n\nThe QR code is generated from the provided text or text pattern. The corresponding QR code is scaled and overlayed into the video output according to the specified options.\n\nIn case no text is specified, no QR code is overlaied.\n\nThis filter accepts the following options:\n\nThe expressions set by the options contain the following constants and functions.\n\nIf is set to , the text is printed verbatim.\n\nIf is set to (which is the default), the following expansion mechanism is used.\n\nThe backslash character ‘ ’, followed by any character, always expands to the second character.\n\nSequences of the form are expanded. The text between the braces is a function name, possibly followed by arguments separated by ’:’. If the arguments contain special characters or delimiters (’:’ or ’}’), they should be escaped.\n\nNote that they probably must also be escaped as the value for the option in the filter argument string and as the filter argument in the filtergraph description, and possibly also for the shell, that makes up to four levels of escaping; using a text file with the option avoids these problems.\n\nThe following functions are available:\n• Generate a QR code encoding the specified text with the default size, overalaid in the top left corner of the input video, with the default size:\n• Same as below, but select blue on pink colors:\n• Place the QR code in the bottom right corner of the input video:\n• Generate a QR code with width of 200 pixels and padding, making the padded width 4/3 of the QR code width:\n• Generate a QR code with padded width of 200 pixels and padding, making the QR code width 3/4 of the padded width:\n• Make the QR code a fraction of the input video width:\n\nIdentify and decode a QR code using the libquirc library (see https://github.com/dlbeer/quirc/), and print the identified QR codes positions and payload as metadata.\n\nTo enable the compilation of this filter, you need to configure FFmpeg with .\n\nFor each found QR code in the input video, some metadata entries are added with the prefix , where is the index, starting from 0, associated to the QR code.\n\nA description of each metadata value follows:\n\nFlush video frames from internal cache of frames into a random order. No frame is discarded. Inspired by frei0r nervous filter.\n\nRead closed captioning (EIA-608) information from the top lines of a video frame.\n\nThis filter adds frame metadata for and , where is the number of the identified line with EIA-608 data (starting from 0). A description of each metadata value follows:\n\nThis filter accepts the following options:\n\nThis filter supports the all above options as commands.\n• Output a csv with presentation time and the first two lines of identified EIA-608 captioning data.\n\nRead vertical interval timecode (VITC) information from the top lines of a video frame.\n\nThe filter adds frame metadata key with the timecode value, if a valid timecode has been detected. Further metadata key is set to 0/1 depending on whether timecode data has been found or not.\n\nThis filter accepts the following options:\n• Detect and draw VITC data onto the video frame; if no valid VITC is detected, draw as a placeholder:\n\nDestination pixel at position (X, Y) will be picked from source (x, y) position where x = Xmap(X, Y) and y = Ymap(X, Y). If mapping values are out of range, zero value for pixel will be used for destination pixel.\n\nXmap and Ymap input video streams must be of same dimensions. Output video stream will have Xmap/Ymap video stream dimensions. Xmap and Ymap input video streams are 16bit depth, single channel.\n\nThe removegrain filter is a spatial denoiser for progressive video.\n\nRange of mode is from 0 to 24. Description of each mode follows:\n\nSuppress a TV station logo, using an image file to determine which pixels comprise the logo. It works by filling in the pixels that comprise the logo with neighboring pixels.\n\nThe filter accepts the following options:\n\nPixels in the provided bitmap image with a value of zero are not considered part of the logo, non-zero pixels are considered part of the logo. If you use white (255) for the logo and black (0) for the rest, you will be safe. For making the filter bitmap, it is recommended to take a screen capture of a black frame with the logo visible, and then using a threshold filter followed by the erode filter once or twice.\n\nIf needed, little splotches can be fixed manually. Remember that if logo pixels are not covered, the filter quality will be much reduced. Marking too many pixels as part of the logo does not hurt as much, but it will increase the amount of blurring needed to cover over the image and will destroy more information than necessary, and extra pixels will slow things down on a large logo.\n\nThis filter uses the repeat_field flag from the Video ES headers and hard repeats fields based on its value.\n\nWarning: This filter requires memory to buffer the entire clip, so trimming is suggested.\n• Take the first 5 seconds of a clip, and reverse it.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThe filter accepts the following option:\n\nThis filter supports the all above options as commands.\n\nRotate video by an arbitrary angle expressed in radians.\n\nThe filter accepts the following options:\n\nA description of the optional parameters follows.\n\nThe expressions for the angle and the output size can contain the following constants and functions:\n• Apply a constant rotation with period T, starting from an angle of PI/3:\n• Make the input video rotation oscillating with a period of T seconds and an amplitude of A radians:\n• Rotate the video, output size is chosen so that the whole rotating input video is always completely contained in the output:\n• Rotate the video, reduce the output size so that no background is ever shown:\n\nThe filter supports the following commands:\n\nThe filter accepts the following options:\n\nEach chroma option value, if not explicitly specified, is set to the corresponding luma option value.\n\nScale (resize) the input video, using the libswscale library.\n\nThe scale filter forces the output display aspect ratio to be the same of the input, by changing the output sample aspect ratio.\n\nIf the input image format is different from the format requested by the next filter, the scale filter will convert the input to the requested format.\n\nThe filter accepts the following options, any of the options supported by the libswscale scaler, as well as any of the framesync options.\n\nSee (ffmpeg-scaler)the ffmpeg-scaler manual for the complete list of scaler options.\n\nThe values of the and options are expressions containing the following constants:\n• Scale the input video to a size of 200x100 This is equivalent to:\n• Specify a size abbreviation for the output size: which can also be written as:\n• The above is the same as:\n• Scale the input to 2x with forced interlaced scaling:\n• Increase the width, and set the height to the same size:\n• Increase the height, and set the width to 3/2 of the height:\n• Increase the size, making the size a multiple of the chroma subsample values:\n• Increase the width to a maximum of 500 pixels, keeping the same aspect ratio as the input:\n• Make pixels square using reset_sar, making sure the resulting resolution is even (required by some codecs):\n• Scale to target exactly, however reset SAR to 1:\n• Scale to even dimensions that fit within 400x300, preserving input SAR:\n• Scale to produce square pixels with even dimensions that fit within 400x300:\n• Scale a subtitle stream (sub) to match the main video (main) in size before overlaying. (\"scale2ref\")\n• Scale a logo to 1/10th the height of a video, while preserving its display aspect ratio.\n\nThis filter supports the following commands:\n\nScale and convert the color parameters using VTPixelTransferSession.\n\nThe filter accepts the following options:\n\nThe filter accepts the following option:\n\nThis filter supports the all above options as commands.\n\nThe filter accepts the following options:\n\nThis filter supports the following commands:\n\nThis filter sets frame metadata with mafd between frame, the scene score, and forward the frame to the next filter, so they can use these metadata to detect scene change or others.\n\nIn addition, this filter logs a message and sets frame metadata when it detects a scene change by .\n\nmetadata keys are set with mafd for every frame.\n\nmetadata keys are set with scene change score for every frame to detect scene change.\n\nmetadata keys are set with current filtered frame time which detect scene change with .\n\nThe filter accepts the following options:\n\nAdjust cyan, magenta, yellow and black (CMYK) to certain ranges of colors (such as \"reds\", \"yellows\", \"greens\", \"cyans\", ...). The adjustment range is defined by the \"purity\" of the color (that is, how saturated it already is).\n\nThis filter is similar to the Adobe Photoshop Selective Color tool.\n\nThe filter accepts the following options:\n\nAll the adjustment settings ( , , ...) accept up to 4 space separated floating point adjustment values in the [-1,1] range, respectively to adjust the amount of cyan, magenta, yellow and black for the pixels of its range.\n• Increase cyan by 50% and reduce yellow by 33% in every green areas, and increase magenta by 27% in blue areas:\n\nThe takes a frame-based video input and splits each frame into its components fields, producing a new half height clip with twice the frame rate and twice the frame count.\n\nThis filter use field-dominance information in frame to decide which of each pair of fields to place first in the output. If it gets it wrong use setfield filter before filter.\n\nThe filter sets the Display Aspect Ratio for the filter output video.\n\nThis is done by changing the specified Sample (aka Pixel) Aspect Ratio, according to the following equation:\n\nKeep in mind that the filter does not modify the pixel dimensions of the video frame. Also, the display aspect ratio set by this filter may be changed by later filters in the filterchain, e.g. in case of scaling or if another \"setdar\" or a \"setsar\" filter is applied.\n\nThe filter sets the Sample (aka Pixel) Aspect Ratio for the filter output video.\n\nNote that as a consequence of the application of this filter, the output display aspect ratio will change according to the equation above.\n\nKeep in mind that the sample aspect ratio set by the filter may be changed by later filters in the filterchain, e.g. if another \"setsar\" or a \"setdar\" filter is applied.\n\nIt accepts the following parameters:\n\nThe parameter is an expression containing the following constants:\n• To change the display aspect ratio to 16:9, specify one of the following:\n• To change the sample aspect ratio to 10:11, specify:\n• To set a display aspect ratio of 16:9, and specify a maximum integer value of 1000 in the aspect ratio reduction, use the command:\n\nThe filter marks the interlace type field for the output frames. It does not change the input frame, but only sets the corresponding property, which affects how the frame is treated by following filters (e.g. or ).\n\nThe filter accepts the following options:\n\nThe filter marks interlace and color range for the output frames. It does not change the input frame, but only sets the corresponding property, which affects how the frame is treated by filters/encoders.\n\nThis filter supports the following options:\n\nThis filter supports the all above options as commands.\n\nShow a line containing various information for each input video frame. The input video is not modified.\n\nThis filter supports the following options:\n\nThe shown line contains a sequence of key/value pairs of the form : .\n\nThe following values are shown in the output:\n\nDisplays the 256 colors palette of each frame. This filter is only relevant for pixel format frames.\n\nIt accepts the following option:\n\nIt accepts the following parameters:\n\nThe first frame has the index 0. The default is to keep the input unchanged.\n• Swap second and third frame of every three frames of the input:\n• Swap 10th and 1st frame of every ten frames of the input:\n\nThis filter accepts the following options:\n\nIt accepts the following parameters:\n\nThe first plane has the index 0. The default is to keep the input unchanged.\n• Swap the second and third planes of the input:\n\nEvaluate various visual metrics that assist in determining issues associated with the digitization of analog video media.\n\nBy default the filter will log these metadata values:\n\nThe filter accepts the following options:\n• Output specific data about the minimum and maximum values of the Y plane per frame:\n• Playback video while highlighting pixels that are outside of broadcast range in red.\n• Playback video with signalstats metadata drawn over the frame. The contents of signalstat_drawtext.txt used in the command are:\n\nCalculates the MPEG-7 Video Signature. The filter can handle more than one input. In this case the matching between the inputs can be calculated additionally. The filter always passes through the first input. The signature of each stream can be written into a file.\n\nIt accepts the following options:\n• To calculate the signature of an input video and store it in signature.bin:\n• To detect whether two videos match and store the signatures in XML format in signature0.xml and signature1.xml:\n\nCalculate Spatial Information (SI) and Temporal Information (TI) scores for a video, as defined in ITU-T Rec. P.910 (11/21): Subjective video quality assessment methods for multimedia applications. Available PDF at https://www.itu.int/rec/T-REC-P.910-202111-S/en. Note that this is a legacy implementation that corresponds to a superseded recommendation. Refer to ITU-T Rec. P.910 (07/22) for the latest version: https://www.itu.int/rec/T-REC-P.910-202207-I/en\n\nIt accepts the following option:\n\nBlur the input video without impacting the outlines.\n\nIt accepts the following options:\n\nIf a chroma or alpha option is not explicitly set, the corresponding luma value is set.\n\nThe filter accepts the following option:\n\nThis filter supports the all above options as commands.\n\nApply a simple postprocessing filter that compresses and decompresses the image at several (or - in the case of level - all) shifts and average the results.\n\nThe filter accepts the following options:\n\nThis filter supports the following commands:\n\nScale the input by applying one of the super-resolution methods based on convolutional neural networks. Supported models:\n\nTraining scripts as well as scripts for model file (.pb) saving can be found at https://github.com/XueweiMeng/sr/tree/sr_dnn_native. Original repository is at https://github.com/HighVoltageRocknRoll/sr.git.\n\nThe filter accepts the following options:\n\nTo get full functionality (such as async execution), please use the dnn_processing filter.\n\nUpscale (size increasing) for the input video using AMD Advanced Media Framework library for hardware acceleration. Use advanced algorithms for upscaling with higher output quality. Setting the output width and height works in the same way as for the scale filter.\n\nThe filter accepts the following options:\n• Scale input to 720p, keeping aspect ratio and ensuring the output is yuv420p.\n\nObtain the SSIM (Structural SImilarity Metric) between two input videos.\n\nThis filter takes in input two input videos, the first input is considered the \"main\" source and is passed unchanged to the output. The second input is used as a \"reference\" video for computing the SSIM.\n\nBoth video inputs must have the same resolution and pixel format for this filter to work correctly. Also it assumes that both inputs have the same number of frames, which are compared one by one.\n\nThe filter stores the calculated SSIM of each frame.\n\nThe description of the accepted parameters follows.\n\nThe file printed if is selected, contains a sequence of key/value pairs of the form : for each compared couple of frames.\n\nA description of each shown parameter follows:\n\nThis filter also supports the framesync options.\n• For example: On this example the input file being processed is compared with the reference file . The SSIM of each individual frame is stored in .\n• Another example with both psnr and ssim at same time:\n• Another example with different containers:\n\nThe filters accept the following options:\n• Convert input video from side by side parallel to anaglyph yellow/blue dubois:\n• Convert input video from above below (left eye above, right eye below) to side by side crosseye.\n\nThe filter accepts the following options:\n\nThe and filter supports the following commands:\n• Select first 5 seconds 1st stream and rest of time 2nd stream:\n• Same as above, but for audio:\n\nDraw subtitles on top of input video using the libass library.\n\nTo enable compilation of this filter you need to configure FFmpeg with . This filter also requires a build with libavcodec and libavformat to convert the passed subtitles file to ASS (Advanced Substation Alpha) subtitles format.\n\nThe filter accepts the following options:\n\nIf the first key is not specified, it is assumed that the first value specifies the .\n\nFor example, to render the file on top of the input video, use the command:\n\nwhich is equivalent to:\n\nTo render the default subtitles stream from file , use:\n\nTo render the second subtitles stream from that file, use:\n\nTo make the subtitles stream from appear in 80% transparent blue , use:\n\nScale the input by 2x and smooth using the Super2xSaI (Scale and Interpolate) pixel art scaling algorithm.\n\nUseful for enlarging pixel art images without reducing sharpness.\n\nThis filter accepts the following options:\n\nThe all options are expressions containing the following constants:\n\nThis filter supports the all above options as commands.\n\nThis filter accepts the following options:\n\nCompute and draw a color distribution histogram for the input video across time.\n\nUnlike histogram video filter which only shows histogram of single input frame at certain time, this filter shows also past histograms of number of frames defined by option.\n\nThe computed histogram is a representation of the color component distribution in an image.\n\nThe filter accepts the following options:\n\nThis filter needs four video streams to perform thresholding. First stream is stream we are filtering. Second stream is holding threshold values, third stream is holding min values, and last, fourth stream is holding max values.\n\nThe filter accepts the following option:\n\nFor example if first stream pixel’s component value is less then threshold value of pixel component from 2nd threshold stream, third stream value will picked, otherwise fourth stream pixel component value will be picked.\n\nUsing color source filter one can perform various types of thresholding:\n\nThis filter supports the all options as commands.\n• Threshold to zero, using gray color as threshold:\n• Inverted threshold to zero, using gray color as threshold:\n\nSelect the most representative frame in a given sequence of consecutive frames.\n\nThe filter accepts the following options:\n\nSince the filter keeps track of the whole frames sequence, a bigger value will result in a higher memory usage, so a high value is not recommended.\n• Complete example of a thumbnail creation with :\n\nThe untile filter can do the reverse.\n\nThe filter accepts the following options:\n• Produce 8x8 PNG tiles of all keyframes ( ) in a movie: The is necessary to prevent from duplicating each output frame to accommodate the originally detected frame rate.\n• Display pictures in an area of frames, with pixels between them, and pixels of initial margin, using mixed flat and named options:\n\nWhat happens when you invert time and space?\n\nNormally a video is composed of several frames that represent a different instant of time and shows a scene that evolves in the space captured by the frame. This filter is the antipode of that concept, taking inspiration from tilt and shift photography.\n\nA filtered frame contains the whole timeline of events composing the sequence, and this is obtained by placing a slice of pixels from each frame into a single one. However, since there are no infinite-width frames, this is done up the width of the input frame, and a video is recomposed by shifting away one column for each subsequent frame. In order to map space to time, the filter tilts each input frame as well, so that motion is preserved. This is accomplished by progressively selecting a different column from each input frame.\n\nThe end result is a sort of inverted parallax, so that far away objects move much faster that the ones in the front. The ideal conditions for this video effect are when there is either very little motion and the backgroud is static, or when there is a lot of motion and a very wide depth of field (e.g. wide panorama, while moving on a train).\n\nThe filter accepts the following parameters:\n\nNormally the filter shifts and tilts from the very first frame, and stops when the last one is received. However, before filtering starts, normal video may be preseved, so that the effect is slowly shifted in its place. Similarly, the last video frame may be reconstructed at the end. Alternatively it is possible to just start and end with black.\n\nFrames are counted starting from 1, so the first input frame is considered odd.\n\nThe filter accepts the following options:\n\nThe filter accepts the following options:\n\nThis filter supports all above options as commands, excluding option .\n\nMidway Video Equalization adjusts a sequence of video frames to have the same histograms, while maintaining their dynamics as much as possible. It’s useful for e.g. matching exposures from a video frames sequence.\n\nThis filter accepts the following option:\n\nA description of the accepted options follows.\n• Similar as above but only showing temporal differences:\n\nThis filter supports the following commands:\n\nThis filter expects data in single precision floating point, as it needs to operate on (and can output) out-of-range values. Another filter, such as zscale, is needed to convert the resulting frame to a usable format.\n\nThe tonemapping algorithms implemented only work on linear light, so input data should be linearized beforehand (and possibly correctly tagged).\n\nThe filter accepts the following options.\n\nThe filter accepts the following options:\n\nTranspose rows with columns in the input video and optionally flip it.\n\nIt accepts the following parameters:\n\nFor example to rotate by 90 degrees clockwise and preserve portrait layout:\n\nThe command above can also be specified as:\n\nTrim the input so that the output contains one continuous subpart of the input.\n\nIt accepts the following parameters:\n\n, , and are expressed as time duration specifications; see (ffmpeg-utils)the Time duration section in the ffmpeg-utils(1) manual for the accepted syntax.\n\nNote that the first two sets of the start/end options and the option look at the frame timestamp, while the _frame variants simply count the frames that pass through the filter. Also note that this filter does not modify the timestamps. If you wish for the output timestamps to start at zero, insert a setpts filter after the trim filter.\n\nIf multiple start or end options are set, this filter tries to be greedy and keep all the frames that match at least one of the specified constraints. To keep only the part that matches all the constraints at once, chain multiple trim filters.\n\nThe defaults are such that all the input is kept. So it is possible to set e.g. just the end values to keep everything before the specified time.\n• Drop everything except the second minute of input:\n• Keep only the first second:\n\nApply alpha unpremultiply effect to input video stream using first plane of second stream as alpha.\n\nBoth streams must have same dimensions and same pixel format.\n\nThe filter accepts the following option:\n\nIt accepts the following parameters:\n\nAll parameters are optional and default to the equivalent of the string ’5:5:1.0:5:5:0.0’.\n• Apply a strong blur of both luma and chroma parameters:\n\nDecompose a video made of tiled images into the individual images.\n\nThe frame rate of the output video is the frame rate of the input video multiplied by the number of tiles.\n\nThis filter does the reverse of tile.\n\nThe filter accepts the following options:\n• Produce a 1-second video from a still image file made of 25 frames stacked vertically, like an analogic film reel:\n\nApply ultra slow/simple postprocessing filter that compresses and decompresses the image at several (or - in the case of level - all) shifts and average the results.\n\nThe way this differs from the behavior of spp is that uspp actually encodes & decodes each case with libavcodec Snow, whereas spp uses a simplified intra only 8x8 DCT similar to MJPEG.\n\nThis filter is not available in ffmpeg versions between 5.0 and 6.0.\n\nThe filter accepts the following options:\n\nThe filter accepts the following options:\n• Convert equirectangular video to cubemap with 3x2 layout and 1% padding using bicubic interpolation:\n• Convert transposed and horizontally flipped Equi-Angular Cubemap in side-by-side stereo format to equirectangular top-bottom stereo format:\n\nThis filter supports subset of above options as commands.\n\nIt transforms each frame from the video input into the wavelet domain, using Cohen-Daubechies-Feauveau 9/7. Then it applies some filtering to the obtained coefficients. It does an inverse wavelet transform after. Due to wavelet properties, it should give a nice smoothed result, and reduced noise, without blurring picture features.\n\nThis filter accepts the following options:\n\nApply variable blur filter by using 2nd video stream to set blur radius. The 2nd stream must have the same dimensions.\n\nThis filter accepts the following options:\n\nThe filter also supports the framesync options.\n\nThis filter supports all the above options as commands.\n\nDisplay 2 color component values in the two dimensional graph (which is called a vectorscope).\n\nThis filter accepts the following options:\n\nAnalyze video stabilization/deshaking. Perform pass 1 of 2, see vidstabtransform for pass 2.\n\nThis filter generates a file with relative translation and rotation transform information about subsequent frames, which is then used by the vidstabtransform filter.\n\nTo enable compilation of this filter you need to configure FFmpeg with .\n\nThis filter accepts the following options:\n• Analyze strongly shaky movie and put the results in file :\n• Visualize the result of internal transformations in the resulting video:\n\nVideo stabilization/deshaking: pass 2 of 2, see vidstabdetect for pass 1.\n\nRead a file with transform information for each frame and apply/compensate them. Together with the vidstabdetect filter this can be used to deshake videos. See also http://public.hronopik.de/vid.stab. It is important to also use the unsharp filter, see below.\n\nTo enable compilation of this filter you need to configure FFmpeg with .\n• Use for a typical stabilization with default values: Note the use of the unsharp filter which is always recommended.\n• Zoom in a bit more and load transform data from a given file:\n• Smoothen the video even more:\n\nFor example, to vertically flip a video with :\n\nThis filter tries to detect if the input is variable or constant frame rate.\n\nAt end it will output number of frames detected as having variable delta pts, and ones with constant delta pts. If there was frames with variable delta, than it will also show min, max and average delta encountered.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nObtain the average VIF (Visual Information Fidelity) between two input videos.\n\nBoth input videos must have the same resolution and pixel format for this filter to work correctly. Also it assumes that both inputs have the same number of frames, which are compared one by one.\n\nThe obtained average VIF score is printed through the logging system.\n\nThe filter stores the calculated VIF score of each frame.\n\nThis filter also supports the framesync options.\n\nIn the below example the input file being processed is compared with the reference file .\n\nThe filter accepts the following options:\n\nThe , and expressions can contain the following parameters.\n\nObtain the average VMAF motion score of a video. It is one of the component metrics of VMAF.\n\nThe obtained average motion score is printed through the logging system.\n\nThe filter accepts the following options:\n\nScale (resize) and convert colorspace, transfer characteristics or color primaries for the input video, using AMD Advanced Media Framework library for hardware acceleration. Setting the output width and height works in the same way as for the scale filter.\n\nThe filter accepts the following options:\n• Scale input to 720p, keeping aspect ratio and ensuring the output is yuv420p.\n• Upscale to 4K and change color profile to bt2020.\n\nAll streams must be of same pixel format and of same width.\n\nNote that this filter is faster than using overlay and pad filter to create same output.\n\nThe filter accepts the following options:\n\nBased on the process described by Martin Weston for BBC R&D, and implemented based on the de-interlace algorithm written by Jim Easterbrook for BBC R&D, the Weston 3 field deinterlacing filter uses filter coefficients calculated by BBC R&D.\n\nThis filter uses field-dominance information in frame to decide which of each pair of fields to place first in the output. If it gets it wrong use setfield filter before filter.\n\nThere are two sets of filter coefficients, so called \"simple\" and \"complex\". Which set of filter coefficients is used can be set by passing an optional parameter:\n\nThis filter supports same commands as options.\n\nThe waveform monitor plots color component intensity. By default luma only. Each column of the waveform corresponds to a column of pixels in the source video.\n\nIt accepts the following options:\n\nThe takes a field-based video input and join each two sequential fields into single frame, producing a new double height clip with half the frame rate and half the frame count.\n\nThe works same as but without halving frame rate and frame count.\n\nIt accepts the following option:\n\nApply the xBR high-quality magnification filter which is designed for pixel art. It follows a set of edge-detection rules, see https://forums.libretro.com/t/xbr-algorithm-tutorial/123.\n\nIt accepts the following option:\n\nApply normalized cross-correlation between first and second input video stream.\n\nSecond input video stream dimensions must be lower than first input video stream.\n\nThe filter accepts the following options:\n\nThe filter also supports the framesync options.\n\nApply cross fade from one input video stream to another input video stream. The cross fade is applied for specified duration.\n\nBoth inputs must be constant frame-rate and have the same resolution, pixel format, frame rate and timebase.\n\nThe filter accepts the following options:\n• Cross fade from one input video to another input video, with fade transition and duration of transition of 2 seconds starting at offset of 5 seconds:\n\nThe filter accepts the following options:\n\nThis filter supports all above options as commands, excluding option .\n\nObtain the average (across all input frames) and minimum (across all color plane averages) eXtended Perceptually weighted peak Signal-to-Noise Ratio (XPSNR) between two input videos.\n\nThe XPSNR is a low-complexity psychovisually motivated distortion measurement algorithm for assessing the difference between two video streams or images. This is especially useful for objectively quantifying the distortions caused by video and image codecs, as an alternative to a formal subjective test. The logarithmic XPSNR output values are in a similar range as those of traditional psnr assessments but better reflect human impressions of visual coding quality. More details on the XPSNR measure, which essentially represents a blockwise weighted variant of the PSNR measure, can be found in the following freely available papers:\n• C. R. Helmrich, M. Siekmann, S. Becker, S. Bosse, D. Marpe, and T. Wiegand, \"XPSNR: A Low-Complexity Extension of the Perceptually Weighted Peak Signal-to-Noise Ratio for High-Resolution Video Quality Assessment,\" in Proc. IEEE Int. Conf. Acoustics, Speech, Sig. Process. (ICASSP), virt./online, May 2020. www.ecodis.de/xpsnr.htm\n• C. R. Helmrich, S. Bosse, H. Schwarz, D. Marpe, and T. Wiegand, \"A Study of the Extended Perceptually Weighted Peak Signal-to-Noise Ratio (XPSNR) for Video Compression with Different Resolutions and Bit Depths,\" ITU Journal: ICT Discoveries, vol. 3, no. 1, pp. 65 - 72, May 2020. http://handle.itu.int/11.1002/pub/8153d78b-en\n\nWhen publishing the results of XPSNR assessments obtained using, e.g., this FFmpeg filter, a reference to the above papers as a means of documentation is strongly encouraged. The filter requires two input videos. The first input is considered a (usually not distorted) reference source and is passed unchanged to the output, whereas the second input is a (distorted) test signal. Except for the bit depth, these two video inputs must have the same pixel format. In addition, for best performance, both compared input videos should be in YCbCr color format.\n\nThe obtained overall XPSNR values mentioned above are printed through the logging system. In case of input with multiple color planes, we suggest reporting of the minimum XPSNR average.\n\nThe following parameter, which behaves like the one for the psnr filter, is accepted:\n\nThis filter also supports the framesync options.\n• XPSNR analysis of two 1080p HD videos, ref_source.yuv and test_video.yuv, both at 24 frames per second, with color format 4:2:0, bit depth 8, and output of a logfile named \"xpsnr.log\":\n• XPSNR analysis of two 2160p UHD videos, ref_source.yuv with bit depth 8 and test_video.yuv with bit depth 10, both at 60 frames per second with color format 4:2:0, no logfile output:\n\nAll streams must be of same pixel format.\n\nThe filter accepts the following options:\n• Display 4 inputs into 2x2 grid. Note that if inputs are of different sizes, gaps or overlaps may occur.\n• Display 4 inputs into 1x4 grid. Note that if inputs are of different widths, unused space will appear.\n• Display 9 inputs into 3x3 grid. Note that if inputs are of different sizes, gaps or overlaps may occur.\n• Display 16 inputs into 4x4 grid. Note that if inputs are of different sizes, gaps or overlaps may occur.\n\nDeinterlace the input video (\"yadif\" means \"yet another deinterlacing filter\").\n\nIt accepts the following parameters:\n\nApply blur filter while preserving edges (\"yaepblur\" means \"yet another edge preserving blur filter\"). The algorithm is described in \"J. S. Lee, Digital image enhancement and noise filtering by use of local statistics, IEEE Trans. Pattern Anal. Mach. Intell. PAMI-2, 1980.\"\n\nIt accepts the following parameters:\n\nThis filter supports same commands as options.\n\nThis filter accepts the following options:\n\nEach expression can contain the following constants:\n• Zoom in up to 1.5x and pan at same time to some spot near center of picture:\n• Zoom in up to 1.5x and pan always at center of picture:\n• Same as above but without pausing:\n• Zoom in 2x into center of picture only for the first second of the input video:\n\nScale (resize) the input video, using the z.lib library: https://github.com/sekrit-twc/zimg. To enable compilation of this filter, you need to configure FFmpeg with .\n\nThe zscale filter forces the output display aspect ratio to be the same as the input, by changing the output sample aspect ratio.\n\nIf the input image format is different from the format requested by the next filter, the zscale filter will convert the input to the requested format.\n\nThe filter accepts the following options.\n\nThe values of the and options are expressions containing the following constants:\n\nThis filter supports the following commands:\n\nTo enable CUDA and/or NPP filters please refer to configuration guidelines for CUDA and for CUDA NPP filters.\n\nRunning CUDA filters requires you to initialize a hardware device and to pass that device to all filters in any filter graph.\n\nFor more detailed information see https://www.ffmpeg.org/ffmpeg.html#Advanced-Video-options\n• Example of initializing second CUDA device on the system and running scale_cuda and bilateral_cuda filters.\n\nSince CUDA filters operate exclusively on GPU memory, frame data must sometimes be uploaded (hwupload) to hardware surfaces associated with the appropriate CUDA device before processing, and downloaded (hwdownload) back to normal memory afterward, if required. Whether hwupload or hwdownload is necessary depends on the specific workflow:\n• If the input frames are already in GPU memory (e.g., when using or ), explicit use of hwupload is not needed, as the data is already in the appropriate memory space.\n• If the input frames are in CPU memory (e.g., software-decoded frames or frames processed by CPU-based filters), it is necessary to use hwupload to transfer the data to GPU memory for CUDA processing.\n• If the output of the CUDA filters needs to be further processed by software-based filters or saved in a format not supported by GPU-based encoders, hwdownload is required to transfer the data back to CPU memory.\n\nNote that hwupload uploads data to a surface with the same layout as the software frame, so it may be necessary to add a format filter immediately before hwupload to ensure the input is in the correct format. Similarly, hwdownload may not support all output formats, so an additional format filter may need to be inserted immediately after hwdownload in the filter graph to ensure compatibility.\n\nBelow is a description of the currently available Nvidia CUDA video filters.\n\nNote: If FFmpeg detects the Nvidia CUDA Toolkit during configuration, it will enable CUDA filters automatically without requiring any additional flags. If you want to explicitly enable them, use the following options:\n• Configure FFmpeg with . Additional requirement: lib must be installed.\n\nCUDA accelerated bilateral filter, an edge preserving filter. This filter is mathematically accurate thanks to the use of GPU acceleration. For best output quality, use one to one chroma subsampling, i.e. yuv444p format.\n\nThe filter accepts the following options:\n\nDeinterlace the input video using the bwdif algorithm, but implemented in CUDA so that it can work as part of a GPU accelerated pipeline with nvdec and/or nvenc.\n\nIt accepts the following parameters:\n\nThis filter works like normal chromakey filter but operates on CUDA frames. for more details and parameters see chromakey.\n• Make all the green pixels in the input video transparent and use it as an overlay for another video:\n\nIt is by no means feature complete compared to the software colorspace filter, and at the current time only supports color range conversion between jpeg/full and mpeg/limited range.\n\nThe filter accepts the following options:\n\nOverlay one video on top of another.\n\nThis is the CUDA variant of the overlay filter. It only accepts CUDA frames. The underlying input pixel formats have to match.\n\nIt takes two inputs and has one output. The first input is the \"main\" video on which the second input is overlaid.\n\nIt accepts the following parameters:\n\nThis filter also supports the framesync options.\n\nScale (resize) and convert (pixel format) the input video, using accelerated CUDA kernels. Setting the output width and height works in the same way as for the scale filter.\n\nThe filter accepts the following options:\n• Scale input to 720p, keeping aspect ratio and ensuring the output is yuv420p.\n• Don’t do any conversion or scaling, but copy all input frames into newly allocated ones. This can be useful to deal with a filter and encode chain that otherwise exhausts the decoders frame pool.\n\nDeinterlace the input video using the yadif algorithm, but implemented in CUDA so that it can work as part of a GPU accelerated pipeline with nvdec and/or nvenc.\n\nIt accepts the following parameters:\n\nBelow is a description of the currently available NVIDIA Performance Primitives (libnpp) video filters.\n\nUse the NVIDIA Performance Primitives (libnpp) to perform scaling and/or pixel format conversion on CUDA video frames. Setting the output width and height works in the same way as for the filter.\n\nThe following additional options are accepted:\n\nThe values of the and options are expressions containing the following constants:\n\nUse the NVIDIA Performance Primitives (libnpp) to scale (resize) the input video, based on a reference video.\n\nSee the scale_npp filter for available options, scale2ref_npp supports the same but uses the reference video instead of the main input as basis. scale2ref_npp also supports the following additional constants for the and options:\n• Scale a subtitle stream (b) to match the main video (a) in size before overlaying\n• Scale a logo to 1/10th the height of a video, while preserving its display aspect ratio.\n\nUse the NVIDIA Performance Primitives (libnpp) to perform image sharpening with border control.\n\nThe following additional options are accepted:\n\nTranspose rows with columns in the input video and optionally flip it. For more in depth examples see the transpose video filter, which shares mostly the same options.\n\nIt accepts the following parameters:\n\nBelow is a description of the currently available OpenCL video filters.\n\nTo enable compilation of these filters you need to configure FFmpeg with .\n\nRunning OpenCL filters requires you to initialize a hardware device and to pass that device to all filters in any filter graph.\n\nFor more detailed information see https://www.ffmpeg.org/ffmpeg.html#Advanced-Video-options\n• Example of choosing the first device on the second platform and running avgblur_opencl filter with default parameters on it.\n\nSince OpenCL filters are not able to access frame data in normal memory, all frame data needs to be uploaded(hwupload) to hardware surfaces connected to the appropriate device before being used and then downloaded(hwdownload) back to normal memory. Note that hwupload will upload to a surface with the same layout as the software frame, so it may be necessary to add a format filter immediately before to get the input into the right format and hwdownload does not support all formats on the output - it may be necessary to insert an additional format filter immediately following in the graph to get the output in a supported format.\n\nThe filter accepts the following options:\n• Apply average blur filter with horizontal and vertical size of 3, setting each pixel of the output to the average value of the 7x7 region centered on it in the input. For pixels on the edges of the image, the region does not extend beyond the image boundaries, and so out-of-range coordinates are not used in the calculations.\n\nIt accepts the following parameters:\n\nA description of the accepted options follows.\n\nApply boxblur filter, setting each pixel of the output to the average value of box-radiuses , , for each plane respectively. The filter will apply , , times onto the corresponding plane. For pixels on the edges of the image, the radius does not extend beyond the image boundaries, and so out-of-range coordinates are not used in the calculations.\n• Apply a boxblur filter with the luma, chroma, and alpha radius set to 2 and luma, chroma, and alpha power set to 3. The filter will run 3 times with box-radius set to 2 for every plane of the image.\n• Apply a boxblur filter with luma radius set to 2, luma_power to 1, chroma_radius to 4, chroma_power to 5, alpha_radius to 3 and alpha_power to 7. For the luma plane, a 2x2 box radius will be run once. For the chroma plane, a 4x4 box radius will be run 5 times. For the alpha plane, a 3x3 box radius will be run 7 times.\n\nThe filter accepts the following options:\n• Make every semi-green pixel in the input transparent with some slight blending:\n\nThe filter accepts the following options:\n\nThis filter replaces the pixel by the local(3x3) minimum.\n\nIt accepts the following options:\n• Apply erosion filter with threshold0 set to 30, threshold1 set 40, threshold2 set to 50 and coordinates set to 231, setting each pixel of the output to the local minimum between pixels: 1, 2, 3, 6, 7, 8 of the 3x3 region centered on it in the input. If the difference between input pixel and local minimum is more then threshold of the corresponding plane, output pixel will be set to input pixel - threshold of corresponding plane.\n\nThe filter accepts the following options:\n• Stabilize a video with debugging (both in console and in rendered video):\n\nThis filter replaces the pixel by the local(3x3) maximum.\n\nIt accepts the following options:\n• Apply dilation filter with threshold0 set to 30, threshold1 set 40, threshold2 set to 50 and coordinates set to 231, setting each pixel of the output to the local maximum between pixels: 1, 2, 3, 6, 7, 8 of the 3x3 region centered on it in the input. If the difference between input pixel and local maximum is more then threshold of the corresponding plane, output pixel will be set to input pixel + threshold of corresponding plane.\n\nNon-local Means denoise filter through OpenCL, this filter accepts same options as nlmeans.\n\nOverlay one video on top of another.\n\nIt takes two inputs and has one output. The first input is the \"main\" video on which the second input is overlaid. This filter requires same memory layout for all the inputs. So, format conversion may be needed.\n\nThe filter accepts the following options:\n• Overlay an image LOGO at the top-left corner of the INPUT video. Both inputs are yuv420p format.\n• The inputs have same memory layout for color channels , the overlay has additional alpha plane, like INPUT is yuv420p, and the LOGO is yuva420p.\n\nAdd paddings to the input image, and place the original input at the provided , coordinates.\n\nIt accepts the following options:\n\nThe value for the , , , and options are expressions containing the following constants:\n\nThe filter accepts the following option:\n• Apply the Prewitt operator with scale set to 2 and delta set to 10.\n\nThe filter also supports the framesync options.\n\nThe program source file must contain a kernel function with the given name, which will be run once for each plane of the output. Each run on a plane gets enqueued as a separate 2D global NDRange with one work-item for each pixel to be generated. The global ID offset for each work-item is therefore the coordinates of a pixel in the destination image.\n\nThe kernel function needs to take the following arguments:\n• Destination image, . This image will become the output; the kernel should write all of it.\n• Frame index, . This is a counter starting from zero and increasing by one for each frame.\n• Source images, . These are the most recent images on each input. The kernel may read from them to generate the output, but they can’t be written to.\n• Copy the input to the output (output must be the same size as the input).\n• Apply a simple transformation, rotating the input by an amount increasing with the index counter. Pixel values are linearly interpolated by the sampler, and the output need not have the same dimensions as the input.\n• Blend two inputs together, with the amount of each input used varying with the index counter.\n\nDestination pixel at position (X, Y) will be picked from source (x, y) position where x = Xmap(X, Y) and y = Ymap(X, Y). If mapping values are out of range, zero value for pixel will be used for destination pixel.\n\nXmap and Ymap input video streams must be of same dimensions. Output video stream will have Xmap/Ymap video stream dimensions. Xmap and Ymap input video streams are 32bit float pixel format, single channel.\n\nThe filter accepts the following option:\n• Apply the Roberts cross operator with scale set to 2 and delta set to 10\n\nThe filter accepts the following option:\n• Apply sobel operator with scale set to 2 and delta set to 10\n\nIt accepts the following parameters:\n\nIt accepts the following parameters:\n\nAll parameters are optional and default to the equivalent of the string ’5:5:1.0:5:5:0.0’.\n• Apply a strong blur of both luma and chroma parameters:\n\nCross fade two videos with custom transition effect by using OpenCL.\n\nIt accepts the following options:\n\nThe program source file must contain a kernel function with the given name, which will be run once for each plane of the output. Each run on a plane gets enqueued as a separate 2D global NDRange with one work-item for each pixel to be generated. The global ID offset for each work-item is therefore the coordinates of a pixel in the destination image.\n\nThe kernel function needs to take the following arguments:\n• Destination image, . This image will become the output; the kernel should write all of it.\n• First Source image, . Second Source image, . These are the most recent images on each input. The kernel may read from them to generate the output, but they can’t be written to.\n• Transition progress, . This value is always between 0 and 1 inclusive.\n\nVAAPI Video filters are usually used with VAAPI decoder and VAAPI encoder. Below is a description of VAAPI video filters.\n\nTo enable compilation of these filters you need to configure FFmpeg with .\n\nTo use vaapi filters, you need to setup the vaapi device correctly. For more information, please read https://trac.ffmpeg.org/wiki/Hardware/VAAPI\n\nOverlay one video on the top of another.\n\nIt takes two inputs and has one output. The first input is the \"main\" video on which the second input is overlaid.\n\nThe filter accepts the following options:\n\nThis filter also supports the framesync options.\n• Overlay an image LOGO at the top-left corner of the INPUT video. Both inputs for this filter are yuv420p format.\n• Overlay an image LOGO at the offset (200, 100) from the top-left corner of the INPUT video. The inputs have same memory layout for color channels, the overlay has additional alpha plane, like INPUT is yuv420p, and the LOGO is yuva420p.\n\nPerform HDR-to-SDR or HDR-to-HDR tone-mapping. It currently only accepts HDR10 as input.\n\nIt accepts the following parameters:\n\nThis is the VA-API variant of the hstack filter, each input stream may have different height, this filter will scale down/up each input stream while keeping the original aspect.\n\nIt accepts the following options:\n\nThis is the VA-API variant of the vstack filter, each input stream may have different width, this filter will scale down/up each input stream while keeping the original aspect.\n\nIt accepts the following options:\n\nThis is the VA-API variant of the xstack filter, each input stream may have different size, this filter will scale down/up each input stream to the given output size, or the size of the first input stream.\n\nIt accepts the following options:\n\nAdd paddings to the input image, and place the original input at the provided , coordinates.\n\nIt accepts the following options:\n\nThe value for the , , , and options are expressions containing the following constants:\n\nIt accepts the following parameters:\n\nThe parameters for , , and and are expressions containing the following constants:\n• Draw a black box around the edge of the input image:\n• Draw a box with color red and an opacity of 50%: The previous example can be specified as:\n\nBelow is a description of the currently available Vulkan video filters.\n\nTo enable compilation of these filters you need to configure FFmpeg with and either or .\n\nRunning Vulkan filters requires you to initialize a hardware device and to pass that device to all filters in any filter graph.\n\nFor more detailed information see https://www.ffmpeg.org/ffmpeg.html#Advanced-Video-options\n• Example of choosing the first device and running nlmeans_vulkan filter with default parameters on it.\n\nAs Vulkan filters are not able to access frame data in normal memory, all frame data needs to be uploaded (hwupload) to hardware surfaces connected to the appropriate device before being used and then downloaded (hwdownload) back to normal memory. Note that hwupload will upload to a frame with the same layout as the software frame, so it may be necessary to add a format filter immediately before to get the input into the right format and hwdownload does not support all formats on the output - it is usually necessary to insert an additional format filter immediately following in the graph to get the output in a supported format.\n\nApply an average blur filter, implemented on the GPU using Vulkan.\n\nThe filter accepts the following options:\n\nBlend two Vulkan frames into each other.\n\nThe filter takes two input streams and outputs one stream, the first input is the \"top\" layer and second input is \"bottom\" layer. By default, the output terminates when the longest input terminates.\n\nA description of the accepted options follows.\n\nDeinterlacer using bwdif, the \"Bob Weaver Deinterlacing Filter\" algorithm, implemented on the GPU using Vulkan.\n\nIt accepts the following parameters:\n\nApply an effect that emulates chromatic aberration. Works best with RGB inputs, but provides a similar effect with YCbCr inputs too.\n\nVideo source that creates a Vulkan frame of a solid color. Useful for benchmarking, or overlaying.\n\nIt accepts the following parameters:\n\nFlips an image along both the vertical and horizontal axis.\n\nThe filter accepts the following options:\n\nDenoise frames using Non-Local Means algorithm, implemented on the GPU using Vulkan. Supports more pixel formats than nlmeans or nlmeans_opencl, including alpha channel support.\n\nThe filter accepts the following options.\n\nOverlay one video on top of another.\n\nIt takes two inputs and has one output. The first input is the \"main\" video on which the second input is overlaid. This filter requires all inputs to use the same pixel format. So, format conversion may be needed.\n\nThe filter accepts the following options:\n\nTranspose rows with columns in the input video and optionally flip it. For more in depth examples see the transpose video filter, which shares mostly the same options.\n\nIt accepts the following parameters:\n\nTranspose rows with columns in the input video and optionally flip it. For more in depth examples see the transpose video filter, which shares mostly the same options.\n\nIt accepts the following parameters:\n\nBelow is a description of the currently available QSV video filters.\n\nTo enable compilation of these filters you need to configure FFmpeg with or .\n\nTo use QSV filters, you need to setup the QSV device correctly. For more information, please read https://trac.ffmpeg.org/wiki/Hardware/QuickSync\n\nThis is the QSV variant of the hstack filter, each input stream may have different height, this filter will scale down/up each input stream while keeping the original aspect.\n\nIt accepts the following options:\n\nThis is the QSV variant of the vstack filter, each input stream may have different width, this filter will scale down/up each input stream while keeping the original aspect.\n\nIt accepts the following options:\n\nThis is the QSV variant of the xstack filter.\n\nIt accepts the following options:\n\nBelow is a description of the currently available video sources.\n\nBuffer video frames, and make them available to the filter chain.\n\nThis source is mainly intended for a programmatic use, in particular through the interface defined in .\n\nIt accepts the following parameters:\n\nwill instruct the source to accept video frames with size 320x240 and with format \"yuv410p\", assuming 1/24 as the timestamps timebase and square pixels (1:1 sample aspect ratio). Since the pixel format with name \"yuv410p\" corresponds to the number 6 (check the enum AVPixelFormat definition in ), this example corresponds to:\n\nAlternatively, the options can be specified as a flat string, but this syntax is deprecated:\n\nThe initial state of the cellular automaton can be defined through the and options. If such options are not specified an initial state is created randomly.\n\nAt each new frame a new row in the video is filled with the result of the cellular automaton next generation. The behavior when the whole frame is filled is defined by the option.\n\nThis source accepts the following options:\n• Read the initial state from , and specify an output of size 200x400.\n• Generate a random initial row with a width of 200 cells, with a fill ratio of 2/3:\n• Create a pattern generated by rule 18 starting by a single alive cell centered on an initial row with width 100:\n\nVideo source generated on GPU using Apple’s CoreImage API on OSX.\n\nThis video source is a specialized version of the coreimage video filter. Use a core image generator at the beginning of the applied filterchain to generate the content.\n\nThe coreimagesrc video source accepts the following options:\n\nAdditionally, all options of the coreimage video filter are accepted. A complete filterchain can be used for further processing of the generated input without CPU-HOST transfer. See coreimage documentation and examples for details.\n• Use CIQRCodeGenerator to create a QR code for the FFmpeg homepage, given as complete and escaped command-line for Apple’s standard bash shell: This example is equivalent to the QRCode example of coreimage without the need for a nullsrc video source.\n\nThe filter exclusively returns D3D11 Hardware Frames, for on-gpu encoding or processing. So an explicit hwdownload is needed for any kind of software processing.\n\nIt accepts the following options:\n\nYou can also skip the lavfi device and directly use the filter. Also demonstrates downloading the frame and encoding with libx264. Explicit output format specification is required in this case:\n\nIf you want to capture only a subsection of the desktop, this can be achieved by specifying a smaller size and its offsets into the screen:\n\nThis source supports the some above options as commands.\n\nGenerate a Mandelbrot set fractal, and progressively zoom towards the point specified with and .\n\nThis source accepts the following options:\n\nGenerate various test patterns, as generated by the MPlayer test filter.\n\nThe size of the generated video is fixed, and is 256x256. This source is useful in particular for testing encoding features.\n\nThis source accepts the following options:\n\nTo enable compilation of this filter you need to install the frei0r header and configure FFmpeg with .\n\nThis source accepts the following parameters:\n\nFor example, to generate a frei0r partik0l source with size 200x200 and frame rate 10 which is overlaid on the overlay filter main input:\n\nThis source is based on a generalization of John Conway’s life game.\n\nThe sourced input represents a life grid, each pixel represents a cell which can be in one of two possible states, alive or dead. Every cell interacts with its eight neighbours, which are the cells that are horizontally, vertically, or diagonally adjacent.\n\nAt each interaction the grid evolves according to the adopted rule, which specifies the number of neighbor alive cells which will make a cell stay alive or born. The option allows one to specify the rule to adopt.\n\nThis source accepts the following options:\n• Read a grid from , and center it on a grid of size 300x300 pixels:\n• Generate a random grid of size 200x200, with a fill ratio of 2/3:\n• Full example with slow death effect (mold) using :\n\nPerlin noise is a kind of noise with local continuity in space. This can be used to generate patterns with continuity in space and time, e.g. to simulate smoke, fluids, or terrain.\n\nIn case more than one octave is specified through the option, Perlin noise is generated as a sum of components, each one with doubled frequency. In this case the option specify the ratio of the amplitude with respect to the previous component. More octave components enable to specify more high frequency details in the generated noise (e.g. small size variations due to boulders in a generated terrain).\n• Use Perlin noise with 7 components, each one with a halved contribution to total amplitude:\n• Chain Perlin noise with the lutyuv to generate a black&white effect:\n• Stretch noise along the y axis, and convert gray level to red-only signal:\n\nGenerate a QR code using the libqrencode library (see https://fukuchi.org/works/qrencode/).\n\nTo enable the compilation of this source, you need to configure FFmpeg with .\n\nThe QR code is generated from the provided text or text pattern. The corresponding QR code is scaled and put in the video output according to the specified output size options.\n\nIn case no text is specified, the QR code is not generated, but an empty colored output is returned instead.\n\nThis source accepts the following options:\n• Generate a QR code encoding the specified text with the default size:\n• Same as below, but select blue on pink colors:\n• Generate a QR code with width of 200 pixels and padding, making the padded width 4/3 of the QR code width:\n• Generate a QR code with padded width of 200 pixels and padding, making the QR code width 3/4 of the padded width:\n\nThe source returns frames of size 4096x4096 of all rgb colors.\n\nThe source returns frames of size 4096x4096 of all yuv colors.\n\nThe source provides an uniformly colored input.\n\nThe source provides an identity Hald CLUT. See also haldclut filter.\n\nThe source returns unprocessed video frames. It is mainly useful to be employed in analysis / debugging tools, or as the source for filters which ignore the input data.\n\nThe source generates a color bars pattern, based on EBU PAL recommendations with 75% color levels.\n\nThe source generates a color bars pattern, based on EBU PAL recommendations with 100% color levels.\n\nThe source generates an RGB test pattern useful for detecting RGB vs BGR issues. You should see a red, green and blue stripe from top to bottom.\n\nThe source generates a color bars pattern, based on the SMPTE Engineering Guideline EG 1-1990.\n\nThe source generates a color bars pattern, based on the SMPTE RP 219-2002.\n\nThe source generates a test video pattern, showing a color pattern, a scrolling gradient and a timestamp. This is mainly intended for testing purposes.\n\nThe source is similar to testsrc, but supports more pixel formats instead of just . This allows using it as an input for other tests without requiring a format conversion.\n\nThe source generates an YUV test pattern. You should see a y, cb and cr stripe from top to bottom.\n\nThe sources accept the following parameters:\n• Generate a video with a duration of 5.3 seconds, with size 176x144 and a frame rate of 10 frames per second:\n• The following graph description will generate a red source with an opacity of 0.2, with size \"qcif\" and a frame rate of 10 frames per second:\n• If the input content is to be ignored, can be used. The following command generates noise in the luma plane by employing the filter:\n\nThe source supports the following commands:\n\nFor details of how the program loading works, see the program_opencl filter.\n• Generate a colour ramp by setting pixel values from the position of the pixel in the output image. (Note that this will work with all pixel formats, but the generated output will not be the same.)\n• Generate a Sierpinski carpet pattern, panning by a single pixel each frame. __kernel void sierpinski_carpet(__write_only image2d_t dst, unsigned int index) { int2 loc = (int2)(get_global_id(0), get_global_id(1)); float4 value = 0.0f; int x = loc.x + index; int y = loc.y + index; while (x > 0 || y > 0) { if (x % 3 == 1 && y % 3 == 1) { value = 1.0f; break; } x /= 3; y /= 3; } write_imagef(dst, loc, value); }\n\nThis source accepts the following options:\n\nThis source accepts the following options:\n\nThis source supports the some above options as commands.\n\nBelow is a description of the currently available video sinks.\n\nBuffer video frames, and make them available to the end of the filter graph.\n\nThis sink is mainly intended for programmatic use, in particular through the interface defined in or the options system.\n\nIt accepts a pointer to an AVBufferSinkContext structure, which defines the incoming buffers’ formats, to be passed as the opaque parameter to for initialization.\n\nNull video sink: do absolutely nothing with the input video. It is mainly useful as a template and for use in analysis / debugging tools.\n\nBelow is a description of the currently available multimedia filters.\n\nThe filter accepts the following options:\n\nFilter supports the some above options as commands.\n\nThe filter accepts the following options:\n\nThe filter accepts the following options:\n\nMeasures phase of input audio, which is exported as metadata , representing mean phase of current audio frame. A video output can also be produced and is enabled by default. The audio is passed through as first output.\n\nAudio will be rematrixed to stereo if it has a different channel layout. Phase value is in range where means left and right channels are completely out of phase and means channels are in phase.\n\nThe filter accepts the following options, all related to its video output:\n\nThe filter also detects out of phase and mono sequences in stereo streams. It logs the sequence start, end and duration when it lasts longer or as long as the minimum set.\n\nThe filter accepts the following options for this detection:\n• Complete example with to detect 1 second of mono with 0.001 phase tolerance:\n\nThe filter is used to measure the difference between channels of stereo audio stream. A monaural signal, consisting of identical left and right signal, results in straight vertical line. Any stereo separation is visible as a deviation from this line, creating a Lissajous figure. If the straight (or deviation from it) but horizontal line appears this indicates that the left and right channels are out of phase.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands except options and .\n\nThe filter accepts the following options:\n\nConcatenate audio and video streams, joining them together one after the other.\n\nThe filter works on segments of synchronized video and audio streams. All segments must have the same number of streams of each type, and that will also be the number of streams at output.\n\nThe filter accepts the following options:\n\nThe filter has + outputs: first video outputs, then audio outputs.\n\nThere are x( + ) inputs: first the inputs for the first segment, in the same order as the outputs, then the inputs for the second segment, etc.\n\nRelated streams do not always have exactly the same duration, for various reasons including codec frame size or sloppy authoring. For that reason, related synchronized streams (e.g. a video and its audio track) should be concatenated at once. The concat filter will use the duration of the longest stream in each segment (except the last one), and if necessary pad shorter audio streams with silence.\n\nFor this filter to work correctly, all segments must start at timestamp 0.\n\nAll corresponding streams must have the same parameters in all segments; the filtering system will automatically select a common pixel format for video streams, and a common sample format, sample rate and channel layout for audio streams, but other settings, such as resolution, must be converted explicitly by the user.\n\nDifferent frame rates are acceptable but will result in variable frame rate at output; be sure to configure the output file to handle it.\n• Concatenate an opening, an episode and an ending, all in bilingual version (video in stream 0, audio in streams 1 and 2):\n• Concatenate two parts, handling audio and video separately, using the (a)movie sources, and adjusting the resolution: Note that a desync will happen at the stitch if the audio and video streams do not have exactly the same duration in the first file.\n\nThis filter supports the following commands:\n\nEBU R128 scanner filter. This filter takes an audio stream and analyzes its loudness level. By default, it logs a message at a frequency of 10Hz with the Momentary loudness (identified by ), Short-term loudness ( ), Integrated loudness ( ) and Loudness Range ( ).\n\nThe filter can only analyze streams which have sample format is double-precision floating point. The input stream will be converted to this specification, if needed. Users may need to insert aformat and/or aresample filters after this filter to obtain the original parameters.\n\nThe filter also has a video output (see the option) with a real time graph to observe the loudness evolution. The graphic contains the logged message mentioned above, so it is not printed anymore when this option is set, unless the verbose logging is set. The main graphing area contains the short-term loudness (3 seconds of analysis), and the gauge on the right is for the momentary loudness (400 milliseconds), but can optionally be configured to instead display short-term loudness (see ).\n\nThe green area marks a +/- 1LU target range around the target loudness (-23LUFS by default, unless modified through ).\n\nMore information about the Loudness Recommendation EBU R128 on http://tech.ebu.ch/loudness.\n\nThe filter accepts the following options:\n\nThese filters read frames from several inputs and send the oldest queued frame to the output.\n\nInput streams must have well defined, monotonically increasing frame timestamp values.\n\nIn order to submit one frame to output, these filters need to enqueue at least one frame for each input, so they cannot work in case one input is not yet terminated and will not receive incoming frames.\n\nFor example consider the case when one input is a filter which always drops input frames. The filter will keep reading from that input, but it will never be able to send new frames to output until the input sends an end-of-stream signal.\n\nAlso, depending on inputs synchronization, the filters will drop frames in case one input receives more frames than the other ones, and the queue is already filled.\n\nThese filters accept the following options:\n• Interleave frames belonging to different streams using :\n\nReport previous filter filtering latency, delay in number of audio samples for audio filters or number of video frames for video filters.\n\nOn end of input stream, filter will report min and max measured latency for previous running filter in filtergraph.\n\nThis filter accepts the following options:\n• Print all metadata values for frames with key with values between 0 and 1.\n• Direct all metadata to a pipe with file descriptor 4.\n\nThese filters are mainly aimed at developers to test direct path in the following filter in the filtergraph.\n\nThe filters accept the following options:\n\nNote: in case of auto-inserted filter between the permission filter and the following one, the permission might not be received as expected in that following filter. Inserting a format or aformat filter before the perms/aperms filter can avoid this problem.\n\nThese filters will pause the filtering for a variable amount of time to match the output rate with the input timestamps. They are similar to the option to .\n\nThey accept the following options:\n\nBoth filters supports the all above options as commands.\n\nThis filter does opposite of concat filters.\n\nThis filter accepts the following options:\n\nIn all cases, prefixing an each segment with ’+’ will make it relative to the previous segment.\n• Split input audio stream into three output audio streams, starting at start of input audio stream and storing that in 1st output audio stream, then following at 60th second and storing than in 2nd output audio stream, and last after 150th second of input audio stream store in 3rd output audio stream:\n\nThis filter accepts the following options:\n\nThe expression can contain the following constants:\n\nThe default value of the select expression is \"1\".\n• Select all frames in input: The example above is the same as:\n• Select only frames contained in the 10-20 time interval:\n• Select only I-frames contained in the 10-20 time interval:\n• Use aselect to select only audio frames with samples number > 100:\n• Create a mosaic of the first scenes: Comparing against a value between 0.3 and 0.5 is generally a sane choice.\n• Send even and odd frames to separate outputs, and compose them:\n• Select useful frames from an ffconcat file which is using inpoints and outpoints but where the source files are not intra frame only.\n\nSend commands to filters in the filtergraph.\n\nThese filters read commands to be sent to other filters in the filtergraph.\n\nmust be inserted between two video filters, must be inserted between two audio filters, but apart from that they act the same way.\n\nThe specification of commands can be provided in the filter arguments with the option, or in a file specified by the option.\n\nThese filters accept the following options:\n\nA commands description consists of a sequence of interval specifications, comprising a list of commands to be executed when a particular event related to that interval occurs. The occurring event is typically the current frame time entering or leaving a given time interval.\n\nAn interval is specified by the following syntax:\n\nThe time interval is specified by the and times. is optional and defaults to the maximum time.\n\nThe current frame time is considered within the specified interval if it is included in the interval [ , ), that is when the time is greater or equal to and is lesser than .\n\nconsists of a sequence of one or more command specifications, separated by \",\", relating to that interval. The syntax of a command specification is given by:\n\nis optional and specifies the type of events relating to the time interval which enable sending the specified command, and must be a non-null sequence of identifier flags separated by \"+\" or \"|\" and enclosed between \"[\" and \"]\".\n\nThe following flags are recognized:\n\nIf is not specified, a default value of is assumed.\n\nspecifies the target of the command, usually the name of the filter class or a specific filter instance name.\n\nspecifies the name of the command for the target filter.\n\nis optional and specifies the optional list of argument for the given .\n\nBetween one interval specification and another, whitespaces, or sequences of characters starting with until the end of line, are ignored and can be used to annotate comments.\n\nA simplified BNF description of the commands specification syntax follows:\n• Specify audio tempo change at second 4:\n• Specify a list of drawtext and hue commands in a file. # show text in the interval 5-10 5.0-10.0 [enter] drawtext reinit 'fontfile=FreeSerif.ttf:text=hello world', [leave] drawtext reinit 'fontfile=FreeSerif.ttf:text='; # desaturate the image in the interval 15-20 15.0-20.0 [enter] hue s 0, [enter] drawtext reinit 'fontfile=FreeSerif.ttf:text=nocolor', [leave] hue s 1, [leave] drawtext reinit 'fontfile=FreeSerif.ttf:text=color'; # apply an exponential saturation fade-out effect, starting from time 25 25 [enter] hue s exp(25-t) A filtergraph allowing to read and process the above command list stored in a file , can be specified with:\n\nChange the PTS (presentation timestamp) of the input frames.\n\nThis filter accepts the following options:\n\nThe expression is evaluated through the eval API and can contain the following constants:\n• Set fixed rate of 25 frames per second:\n• Apply an offset of 10 seconds to the input PTS:\n• Generate timestamps from a \"live source\" and rebase onto the current timebase:\n\nBoth filters support all above options as commands.\n\nThe filter marks the color range property for the output frames. It does not change the input frame, but only sets the corresponding property, which affects how the frame is treated by following filters.\n\nThe filter accepts the following options:\n\nSet the timebase to use for the output frames timestamps. It is mainly useful for testing timebase configuration.\n\nIt accepts the following parameters:\n\nThe value for is an arithmetic expression representing a rational. The expression can contain the constants \"AVTB\" (the default timebase), \"intb\" (the input timebase) and \"sr\" (the sample rate, audio only). Default value is \"intb\".\n\nConvert input audio to a video output representing frequency spectrum logarithmically using Brown-Puckette constant Q transform algorithm with direct frequency domain coefficient calculation (but the transform itself is not really constant Q, instead the Q factor is actually variable/clamped), with musical tone scale, from E0 to D#10.\n\nThe filter accepts the following options:\n• Same as above, but with frame rate 30 fps:\n• Same as above, but with more accuracy in frequency domain:\n• Custom gamma, now spectrum is linear to the amplitude.\n• Custom fontcolor and fontfile, C-note is colored green, others are colored blue:\n\nConvert input audio to video output representing frequency spectrum using Continuous Wavelet Transform and Morlet wavelet.\n\nThe filter accepts the following options:\n\nConvert input audio to video output representing the audio power spectrum. Audio amplitude is on Y-axis while frequency is on X-axis.\n\nThe filter accepts the following options:\n\nConvert stereo input audio to a video output, representing the spatial relationship between two channels.\n\nThe filter accepts the following options:\n\nThe filter accepts the following options:\n\nThe usage is very similar to the showwaves filter; see the examples in that section.\n• Complete example for a colored and sliding spectrum per channel using :\n\nThe filter accepts the following options:\n• Extract an audio spectrogram of a whole audio track in a 1024x1024 picture using :\n\nThe filter accepts the following options:\n\nThe filter accepts the following options:\n• Output the input file audio and the corresponding video representation at the same time:\n• Create a synthetic signal and show it with showwaves, forcing a frame rate of 30 frames per second:\n\nThe filter accepts the following options:\n• Extract a channel split representation of the wave form of a whole audio track in a 1024x800 picture using :\n\nDelete frame side data, or select frames based on it.\n\nThis filter accepts the following options:\n\nSynthesize audio from 2 input video spectrums, first input stream represents magnitude across time and second represents phase across time. The filter will transform from frequency domain as displayed in videos back to time domain as presented in audio output.\n\nThis filter is primarily created for reversing processed showspectrum filter outputs, but can synthesize sound from other spectrograms too. But in such case results are going to be poor if the phase data is not available, because in such cases phase data need to be recreated, usually it’s just recreated from random noise. For best results use gray only output ( color mode in showspectrum filter) and scale for magnitude video and scale for phase video. To produce phase, for 2nd video, use option. Inputs videos should generally use slide mode as that saves resources needed for decoding video.\n\nThe filter accepts the following options:\n• First create magnitude and phase videos from audio, assuming audio is stereo with 44100 sample rate, then resynthesize videos back to audio with spectrumsynth:\n\nThe filter accepts a single parameter which specifies the number of outputs. If unspecified, it defaults to 2.\n• Create two separate outputs from the same input:\n• To create 3 or more outputs, you need to specify the number of outputs, like in:\n• Create two separate outputs from the same input, one cropped and one padded:\n• Create 5 copies of the input audio with :\n\nReceive commands sent through a libzmq client, and forward them to filters in the filtergraph.\n\nand work as a pass-through filters. must be inserted between two video filters, between two audio filters. Both are capable to send messages to any filter type.\n\nTo enable these filters you need to install the libzmq library and headers and configure FFmpeg with .\n\nFor more information about libzmq see: http://www.zeromq.org/\n\nThe and filters work as a libzmq server, which receives messages sent through a network interface defined by the (or the abbreviation \" \") option. Default value of this option is . You may want to alter this value to your needs, but do not forget to escape any ’:’ signs (see filtergraph escaping).\n\nThe received message must be in the form:\n\nspecifies the target of the command, usually the name of the filter class or a specific filter instance name. The default filter instance name uses the pattern ‘ ’, but you can override this by using the ‘ ’ syntax (see Filtergraph syntax).\n\nspecifies the name of the command for the target filter.\n\nis optional and specifies the optional argument list for the given .\n\nUpon reception, the message is processed and the corresponding command is injected into the filtergraph. Depending on the result, the filter will send a reply to the client, adopting the format:\n\nLook at for an example of a zmq client which can be used to send commands processed by these filters.\n\nConsider the following filtergraph generated by . In this example the last overlay filter has an instance name. All other filters will have default instance names.\n\nTo change the color of the left side of the video, the following command can be used:\n\nTo change the right side:\n\nTo change the position of the right side:\n\nBelow is a description of the currently available multimedia sources.\n\nThis is the same as movie source, except it selects an audio stream by default.\n\nGenerated stream periodically shows flash video frame and emits beep in audio. Useful to inspect A/V sync issues.\n\nIt accepts the following options:\n\nThis source supports the some above options as commands.\n\nIt accepts the following parameters:\n\nIt allows overlaying a second video on top of the main input of a filtergraph, as shown in this graph:\n• Skip 3.2 seconds from the start of the AVI file in.avi, and overlay it on top of the input labelled \"in\": movie=in.avi:seek_point=3.2, scale=180:-1, setpts=PTS-STARTPTS [over]; [in] setpts=PTS-STARTPTS [main]; [main][over] overlay=16:16 [out]\n• Read from a video4linux2 device, and overlay it on top of the input labelled \"in\": movie=/dev/video0:f=video4linux2, scale=180:-1, setpts=PTS-STARTPTS [over]; [in] setpts=PTS-STARTPTS [main]; [main][over] overlay=16:16 [out]\n• Read the first video stream and the audio stream with id 0x81 from dvd.vob; the video is connected to the pad named \"video\" and the audio is connected to the pad named \"audio\":\n\nBoth movie and amovie support the following commands:\n\nFFmpeg can be hooked up with a number of external libraries to add support for more formats. None of them are used by default, their use has to be explicitly requested by passing the appropriate flags to .\n\nFFmpeg can make use of the AOM library for AV1 decoding and encoding.\n\nGo to http://aomedia.org/ and follow the instructions for installing the library. Then pass to configure to enable it.\n\nFFmpeg can use the AMD Advanced Media Framework library for accelerated H.264 and HEVC(only windows) encoding on hardware with Video Coding Engine (VCE).\n\nTo enable support you must obtain the AMF framework header files(version 1.4.9+) from https://github.com/GPUOpen-LibrariesAndSDKs/AMF.git.\n\nCreate an directory in the system include path. Copy the contents of into that directory. Then configure FFmpeg with .\n\nInitialization of amf encoder occurs in this order: 1) trying to initialize through dx11(only windows) 2) trying to initialize through dx9(only windows) 3) trying to initialize through vulkan\n\nTo use h.264(AMD VCE) encoder on linux amdgru-pro version 19.20+ and amf-amdgpu-pro package(amdgru-pro contains, but does not install automatically) are required.\n\nThis driver can be installed using amdgpu-pro-install script in official amd driver archive.\n\nFFmpeg can read AviSynth scripts as input. To enable support, pass to configure after installing the headers provided by AviSynth+. AviSynth+ can be configured to install only the headers by either passing to the normal CMake-based build system, or by using the supplied .\n\nFor Windows, supported AviSynth variants are AviSynth 2.6 RC1 or higher for 32-bit builds and AviSynth+ r1718 or higher for 32-bit and 64-bit builds.\n\nFor Linux, macOS, and BSD, the only supported AviSynth variant is AviSynth+, starting with version 3.5.\n\nFFmpeg can make use of the Chromaprint library for generating audio fingerprints. Pass to configure to enable it. See https://acoustid.org/chromaprint.\n\nFFmpeg can make use of the codec2 library for codec2 decoding and encoding. There is currently no native decoder, so libcodec2 must be used for decoding.\n\nGo to http://freedv.org/, download \"Codec 2 source archive\". Build and install using CMake. Debian users can install the libcodec2-dev package instead. Once libcodec2 is installed you can pass to configure to enable it.\n\nThe easiest way to use codec2 is with .c2 files, since they contain the mode information required for decoding. To encode such a file, use a .c2 file extension and give the libcodec2 encoder the -mode option: . Playback is as simple as . For a list of supported modes, run . Raw codec2 files are also supported. To make sense of them the mode in use needs to be specified as a format option: .\n\nFFmpeg can make use of the dav1d library for AV1 video decoding.\n\nGo to https://code.videolan.org/videolan/dav1d and follow the instructions for installing the library. Then pass to configure to enable it.\n\nFFmpeg can make use of the davs2 library for AVS2-P2/IEEE1857.4 video decoding.\n\nGo to https://github.com/pkuvcl/davs2 and follow the instructions for installing the library. Then pass to configure to enable it.\n\nFFmpeg can make use of the uavs3d library for AVS3-P2/IEEE1857.10 video decoding.\n\nGo to https://github.com/uavs3/uavs3d and follow the instructions for installing the library. Then pass to configure to enable it.\n\nFFmpeg can make use of the Game Music Emu library to read audio from supported video game music file formats. Pass to configure to enable it. See https://bitbucket.org/mpyne/game-music-emu/overview.\n\nFFmpeg can use Intel QuickSync Video (QSV) for accelerated decoding and encoding of multiple codecs. To use QSV, FFmpeg must be linked against the dispatcher, which loads the actual decoding libraries.\n\nThe dispatcher is open source and can be downloaded from https://github.com/lu-zero/mfx_dispatch.git. FFmpeg needs to be configured with the option and needs to be able to locate the dispatcher’s files.\n\nFFmpeg can make use of the Kvazaar library for HEVC encoding.\n\nGo to https://github.com/ultravideo/kvazaar and follow the instructions for installing the library. Then pass to configure to enable it.\n\nFFmpeg can make use of the LAME library for MP3 encoding.\n\nGo to http://lame.sourceforge.net/ and follow the instructions for installing the library. Then pass to configure to enable it.\n\nFFmpeg can make use of the liblcevc_dec library for LCEVC enhacement layer decoding on supported bitstreams.\n\nGo to https://github.com/v-novaltd/LCEVCdec and follow the instructions for installing the library. Then pass to configure to enable it.\n\niLBC is a narrowband speech codec that has been made freely available by Google as part of the WebRTC project. libilbc is a packaging friendly copy of the iLBC codec. FFmpeg can make use of the libilbc library for iLBC decoding and encoding.\n\nGo to https://github.com/TimothyGu/libilbc and follow the instructions for installing the library. Then pass to configure to enable it.\n\nJPEG XL is an image format intended to fully replace legacy JPEG for an extended period of life. See https://jpegxl.info/ for more information, and see https://github.com/libjxl/libjxl for the library source. You can pass to configure in order enable the libjxl wrapper.\n\nFFmpeg can make use of the libvpx library for VP8/VP9 decoding and encoding.\n\nGo to http://www.webmproject.org/ and follow the instructions for installing the library. Then pass to configure to enable it.\n\nFFmpeg can make use of this library, originating in Modplug-XMMS, to read from MOD-like music files. See https://github.com/Konstanty/libmodplug. Pass to configure to enable it.\n\nSpun off Google Android sources, OpenCore, VisualOn and Fraunhofer libraries provide encoders for a number of audio codecs.\n\nFFmpeg can make use of the OpenCORE libraries for AMR-NB decoding/encoding and AMR-WB decoding.\n\nGo to http://sourceforge.net/projects/opencore-amr/ and follow the instructions for installing the libraries. Then pass and/or to configure to enable them.\n\nFFmpeg can make use of the VisualOn AMR-WBenc library for AMR-WB encoding.\n\nGo to http://sourceforge.net/projects/opencore-amr/ and follow the instructions for installing the library. Then pass to configure to enable it.\n\nFFmpeg can make use of the Fraunhofer AAC library for AAC decoding & encoding.\n\nGo to http://sourceforge.net/projects/opencore-amr/ and follow the instructions for installing the library. Then pass to configure to enable it.\n\nFFmpeg can make use of the Google LC3 library for LC3 decoding & encoding.\n\nGo to https://github.com/google/liblc3/ and follow the instructions for installing the library. Then pass to configure to enable it.\n\nFFmpeg can make use of the OpenH264 library for H.264 decoding and encoding.\n\nGo to http://www.openh264.org/ and follow the instructions for installing the library. Then pass to configure to enable it.\n\nFor decoding, this library is much more limited than the built-in decoder in libavcodec; currently, this library lacks support for decoding B-frames and some other main/high profile features. (It currently only supports constrained baseline profile and CABAC.) Using it is mostly useful for testing and for taking advantage of Cisco’s patent portfolio license (http://www.openh264.org/BINARY_LICENSE.txt).\n\nFFmpeg can use the OpenJPEG libraries for decoding/encoding J2K videos. Go to http://www.openjpeg.org/ to get the libraries and follow the installation instructions. To enable using OpenJPEG in FFmpeg, pass to .\n\nFFmpeg can make use of rav1e (Rust AV1 Encoder) via its C bindings to encode videos. Go to https://github.com/xiph/rav1e/ and follow the instructions to build the C library. To enable using rav1e in FFmpeg, pass to .\n\nFFmpeg can make use of the Scalable Video Technology for AV1 library for AV1 encoding.\n\nGo to https://gitlab.com/AOMediaCodec/SVT-AV1/ and follow the instructions for installing the library. Then pass to configure to enable it.\n\nFFmpeg can make use of the TwoLAME library for MP2 encoding.\n\nGo to http://www.twolame.org/ and follow the instructions for installing the library. Then pass to configure to enable it.\n\nFFmpeg can read VapourSynth scripts as input. To enable support, pass to configure. Vapoursynth is detected via . Versions 42 or greater supported. See http://www.vapoursynth.com/.\n\nDue to security concerns, Vapoursynth scripts will not be autodetected so the input format has to be forced. For ff* CLI tools, add before the input .\n\nFFmpeg can make use of the x264 library for H.264 encoding.\n\nGo to http://www.videolan.org/developers/x264.html and follow the instructions for installing the library. Then pass to configure to enable it.\n\nFFmpeg can make use of the x265 library for HEVC encoding.\n\nGo to http://x265.org/developers.html and follow the instructions for installing the library. Then pass to configure to enable it.\n\nFFmpeg can make use of the xavs library for AVS encoding.\n\nGo to http://xavs.sf.net/ and follow the instructions for installing the library. Then pass to configure to enable it.\n\nFFmpeg can make use of the xavs2 library for AVS2-P2/IEEE1857.4 video encoding.\n\nGo to https://github.com/pkuvcl/xavs2 and follow the instructions for installing the library. Then pass to configure to enable it.\n\nFFmpeg can make use of the XEVE library for EVC video encoding.\n\nGo to https://github.com/mpeg5/xeve and follow the instructions for installing the XEVE library. Then pass to configure to enable it.\n\nFFmpeg can make use of the XEVD library for EVC video decoding.\n\nGo to https://github.com/mpeg5/xevd and follow the instructions for installing the XEVD library. Then pass to configure to enable it.\n\nZVBI is a VBI decoding library which can be used by FFmpeg to decode DVB teletext pages and DVB teletext subtitles.\n\nGo to http://sourceforge.net/projects/zapping/ and follow the instructions for installing the library. Then pass to configure to enable it.\n\nYou can use the and options to have an exhaustive list.\n\nFFmpeg supports the following file formats through the library:\n\nmeans that the feature in that column (encoding / decoding) is supported.\n\nFFmpeg can read and write images for each frame of a video sequence. The following image formats are supported:\n\nmeans that the feature in that column (encoding / decoding) is supported.\n\nmeans that support is provided through an external library.\n\nmeans that the feature in that column (encoding / decoding) is supported.\n\nmeans that support is provided through an external library.\n\nmeans that the feature in that column (encoding / decoding) is supported.\n\nmeans that support is provided through an external library.\n\nmeans that an integer-only version is available, too (ensures high performance on systems without hardware floating point support).\n\nmeans that the feature is supported.\n\nmeans that support is provided through an external library.\n\nmeans that the protocol is supported.\n\nmeans that support is provided through an external library.\n\nFor details about the authorship, see the Git history of the project (https://git.ffmpeg.org/ffmpeg), e.g. by typing the command in the FFmpeg source directory, or browsing the online repository at https://git.ffmpeg.org/ffmpeg.\n\nMaintainers for the specific components are listed in the file in the source code tree.\n\nThis document was generated on March 23, 2025 using makeinfo."
    },
    {
        "link": "https://ottverse.com/ffmpeg-drawtext-filter-dynamic-overlays-timecode-scrolling-text-credits",
        "document": "Learn FFmpeg’s drawtext filter to dynamically overlay text on video and display information such as timecode, frame resolution, watermarks, etc. Also, let’s learn how to configure the font, font-size, position, background-color, alignment, multiple lines, etc. using FFmpeg’s drawtext filter.\n\nStep 0: Ensure your FFmpeg Is Compiled with libfreetype\n\nIn order to use , you needed to have configured FFmpeg with . As per the documentation, you need the following options as well if you want to,\n• enable default font fallback and the font option you need to configure FFmpeg with .\n• enable the text_shaping option, you need to configure FFmpeg with .\n\nThe complete list of options for filter can be accessed here. It is far too much for me to explain here, but, if you have any questions, that is the first place you should refer.\n\nIn this article, I’ll walk through several common use-cases that should make the concepts easy to understand.\n\nDisplay Text on the Video using drawtext filter\n\nHere is the commandline and an explanation of the options\n• is the input video on which you want to display the text; and the output (containing the text) is to be stored in\n• no audio re-encoding as indicated by\n• we use the drawtext filter as indicated by the commands\n• is the text that will be shown on the video (you could make it your name for watermarking the video, right?)\n• position of the text\n• indicates that the x and y coordinates as and . Also, as a side note, the video’s resolution is .\n\nLet’s see how the output looks, shall we?\n\nA better way to this is to offset the text by the length of the text that you are printing on the screen.\n\nIf you look at the image above, you’ll see that it starts at the center of the video and extends towards the right.\n\nIf you want to center the text itself, then you can subtract the height and width of the rendered text when telling where to render the text.\n\nHere’s how. You use the command and it will center the text. Here is our new commandline –\n\nNow, the text looks nice and pretty 🙂\n\nFantastic – you now know how to overlay text onto a video using FFmpeg’s filter. Do you think you can add your own watermark or copyright? Let’s try 🙂\n\nLet’s modify the command as follows to indicate my name and the copyright symbol.\n\nThis produces an output like this – looks good right? You can play around with the and coordinates to align the text the way you want to.\n\nTo add a background color, we need\n\nThe new commands here are –\n• : this is either (enabled) or (disabled)\n• is the width of the box’s border and the border color is taken from .\n\nAnd there you have it, text with a background. In this example, I switched the color of the text to black so that it contrasts well with a light background bounding box (which in-turn contrasts well with a dark background.)\n\nThis is a very useful application of the filter and is used in demonstrating low-latency applications or visual quality testing so that one knows precisely what the timestamps/timecodes are at each time.\n\nThis uses the and options to display time in hour:min:sec format using the format specifier. The notations and formatting are complex in my opion! So, a lot of trial and error might be needed before you format your display correctly.\n\nHere is how the video looks. Hope Vimeo shows you the video without a lot of delay 🙂\n\nAnd here is the same command, but using the option to provide microsecond time accuracy! Fancy 🙂\n\nFinally, let’s learn how to show a movie’s credits using the draw text filter. Here are two main concepts that you need to understand.\n• providing a lot of text: you can’t do this via the commandline, so, you need to read a text file that contains the text. And you can read that using the option.\n• and, specify the speed of scrolling using the text position. Here, you can provide an equation instead of a constant number. You start off by telling by FFmpeg that the y-position is so that everytime the value of time increases, the value of decreases, and the text is displayed higher. Makes sense?\n\nTip: change to or and see the effect on the scrolling speed.\n\nHere is the output:\n\nThat’s it for this tutorial on using FFmpeg’s filter to produce dynamic overlays on your videos. It is a very versatile and handy tool that you can use to overlay text, timecodes, credits, copyrights notices on your videos.\n\nIf you are interested in video compression, check out our comparison of LCEVC (MPEG5 Part 2) vs H.264/AVC. Stunning results, I tell you 🙂"
    },
    {
        "link": "https://forums.developer.nvidia.com/t/ffmpeg-and-filters-drawtext-drawbox-using-gpu-acceleration/115107",
        "document": "I am trying to find a way to use the drawbox and drawtext ffmpeg filters to overlay text onto video,and speed this process up using GPU acceleration. From the reference materials listed below, I have been unsuccessful in finding a way to do so, but I wanted to check with the community to see if there is are additional approaches.\n\nBelow is an example of drawtext and drawbox being used together.\n\nBelow is an example using GPU hardware acceleration from documentation.\n\nWhat I’d like to do is somehow combine these commands so I get the output of command 1 with the speed of command 2. I’ve tried placing the drawtext portion standalone portion within command 2 without success."
    },
    {
        "link": "https://video.stackexchange.com/questions/22664/ffmpeg-use-drawtext-and-complex-filters",
        "document": "Stack Exchange network consists of 183 Q&A communities including Stack Overflow, the largest, most trusted online community for developers to learn, share their knowledge, and build their careers."
    }
]