[
    {
        "link": "https://github.com/amckenna41/izhikevich-spiking-neuron",
        "document": "Implementation of an Izhikevich Spiking Neuron model using Simulink and Xilinx tools.\n\nNeuromorphic computer architectures attempt to model the behaviour of the human brain byconstructing networks of neurons. Unlike the neurons used in artifical neural network (such as thoseused in deep learning), these neurons arespiking, and their networks known as Spiking NeuralNetworks (SNNs) [1]. The general structure of a neuron and its is shown in Figure 1. The dendrites act as inputs to the main cell body orsoma. They transport streams of electrical current, in the form of potentials, from connected neurons. These accumulate an action potential inthe soma. When the voltage reaches a critical value, the soma discharges, its internal voltage and apotential spike is output of theaxonto neighbouring neurons [2]. An example model of this behaviour is the Izhikevich neuron, which has various types of neuron models, as shown in Figure 2.\n\nFigure 1: Structure of a neuron and its behaviour.\n\nIn the Izhikevich spiking neuron the internal potential v is updated to its next value v‚Ä≤ by a stream of incoming voltages on the dendrites, according to:\n\nThe neuron was realised using Simulink and Xilinx System Generator components. Firstly, Simulink was used to create an untimed simulation of the model where any timing and overheads were not considered, such as type fo arthmetic used. Basic arthmetic components were used to implement the above equations to generate the spiking behaviour including adds, multiply's, subtract, divides etc. Below is what the untimed version of the model looked like:\n\nNext, the same model was realised using Xilinx System Generator (https://www.xilinx.com/products/design-tools/vivado/integration/sysgen.html). It allowed for the neuron to be realised with timed components on an FPGA (field-programmable-gate-array), giving a more realistic implementation, taking into account the physical resource cost and performance of the individual components in the model [3]. The implementations were optimised using different variations of components as well as the best arthmetic type (floating or fixed-point). Each model was extracted into HDL so that its resource cost could be calculated using the Vivado Design Suite. An example of one of these models can be seen below:\n‚Ä¢ - MATLAB script storing parameters for generating the various behaviours for the Izhikevich spiking neuron models [4].\n\n[1] E. M. Izhikevich, \"Simple model of spiking neurons,\" in IEEE Transactions on Neural Networks, vol. 14, no. 6, pp. 1569-1572, Nov. 2003, doi: 10.1109/TNN.2003.820440. \n\n [2] Sidiropoulou, K., Pissadaki, E. K., & Poirazi, P. (2006). Inside the brain of a neuron. EMBO reports, 7(9), 886‚Äì892. https://doi.org/10.1038/sj.embor.7400789. \n\n [3] System generator model-based FPGA design optimization and hardware co-simulation for Lorenz chaotic generator. Lei Zhang. Published June 2017. https://ieeexplore.ieee.org/document/7986087?denied. \n\n [4] https://github.com/iandol/spikes/blob/master/Various/izhikevich.m"
    },
    {
        "link": "https://researchgate.net/publication/281292894_Implementation_of_the_Izhikevich_Neuron_Spiking_Model_-_Term_Project_Report",
        "document": ""
    },
    {
        "link": "https://scholarshare.temple.edu/bitstream/handle/20.500.12613/4150/Tufts_temple_0225M_11472.pdf?sequence=1&isAllowed=y",
        "document": ""
    },
    {
        "link": "https://izhikevich.org/publications/whichmod.pdf",
        "document": ""
    },
    {
        "link": "https://docs.neuroml.org/Userdocs/SingleNeuronExample.html",
        "document": "In this section, we wish to simulate a single regular spiking Izhikevich neuron ([Izh07]) and record/visualise its membrane potential (as shown in the figure below):\n\nThis plot, saved as , is generated using the following Python NeuroML script:\n\n# component_factory: form one: provide name of NeuroML class as string # advantage of this form: do not need to import all the ComponentType classes # Define the Izhikevich cell and add it to the model in the document # the `add` will create and validate the new component, and add it to the # Inspect the component, also show all members: # Create a network and add it to the model # Because a Population is necessary in a Network, but we have not provided one. # - create population first, and pass that to component_factory here # Create a population of defined cells and add it to the model # Define an external stimulus and add it to the model # Validate the NeuroML model against the NeuroML schema # The NeuroML file has now been created and validated. The rest of the code # involves writing a LEMS simulation file to run an instance of the model # Run the simulation using the jNeuroML simulator # Load the data from the file and plot the graph for the membrane potential\n\nPython is the suggested programming language to use for working with NeuroML. The Python NeuroML tools and libraries provide a convenient, easy to use interface to use NeuroML. Let us step through the different sections of the Python script. To start writing a model in NeuroML, we first create a . This ‚Äúdocument‚Äù represents the complete model and is the top level container for everything that the model should contain. Let us define an Izhikevich cell that we will use to simulate a neuron. The Izhikevich neuron model can take sets of parameters to exhibit different types of spiking behaviour. Here, we define a component (object) of the general Izhikevich cell using parameters to show regular spiking. Now that the neuron has been defined and added to the document, we declare a network with a population of these neurons to create a network in a similar way. Here, our model includes one network which includes only one population, which in turn only consists of a single neuron. Once the network, its populations, and their neurons have been declared, we again add them to our model: # Create a population of defined cells and add it to the model Question: why did we disable validation when we created the new network component? Let us try creating a network without disabling validation: This is because a network must have at least one population for it to be valid. To fix this, we can either create the population before the network, or we can disable validation. Here we chose to disable validation because we knew we were immediately creating our population and adding it to our network. Moving on, since we are providing a single input to the single cell in our network, we can add an to our network. See the supplementary section on the function below to learn how you can find out that could be used here. The list of inputs included in the NeuroML specification can be found on the inputs page. We use a pulse generator here, creating a new component and adding it to our NeuroML document. To connect it to our neuron, we specify the neuron as the using an explicit input. # Define an external stimulus and add it to the model This completes our model. It includes a single network, with one population of one neuron that is driven by one pulse generator. At this point, we can save our model to a file and validate it again to check if it conforms to the NeuroML schema (more on this later). # Validate the NeuroML model against the NeuroML schema Note that the validation here will re-run the tests our component factory and other methods use, but it also runs a series of additional tests that can only be run on the complete model. So, it is necessary to validate the model after it has been fully constructed.\n\nUntil now, we have just declared the model in NeuroML. We have not, however, included any information related to the simulation of this model, e.g. how long to run it for, what to save from the simulation etc. With NeuroML v2, the information required to simulate the model is provided using a LEMS Simulation file. We will not go into the details of LEMS just yet. We will limit ourselves to the bits necessary to simulate our Izhikevich neuron only. The following lines of code instantiate a new simulation with certain simulation parameters: , , . Additionally, they also define what information is being recorded from the simulation. In this case, we create an output file, and then add a new column to record the membrane potential from our one neuron in the one population in it. You can read more about recording from NeuroML simulations here. Finally, like we had saved our NeuroML model to a file, we also save our LEMS document to a file. Finally, pyNeuroML also includes functions that allow you to run the simulation from the Python script itself: Here, we are running our simulation using the jNeuroML simulator, which is bundled with pyNeuroML. Since NeuroML is a well defined standard, models defined in NeuroML can also be run using other supported simulators.\n\nThe sections here explain concepts that have been used above. These will help give you a deeper understanding of NeuroML, so we do suggest you go through them also. Let us investigate the generated NeuroML XML file: NeuroML files are written in XML. So, they consist of tags and attributes and can be processed by general purpose XML tools. Each entity between chevrons is a tag: , and each tag may have multiple attributes that are defined using the format. For example is a tag, that contains the attribute with value . For details on XML, have a look through this tutorial. A NeuroML file needs to be both 1) well-formed, as in complies with the general rules of the XML language syntax, and 2) valid, i.e. contains the expected NeuroML specific tags/attributes. Is the XML shown above well-formed? See for yourself. Copy the NeuroML file listed above and check it using an online XML syntax checker. Let us step through this file to understand the different constructs used in it. The first segment introduces the tag that includes information on the specification that this NeuroML file adheres to. The first attribute, defines the XML namespace. All the tags that are defined for use in NeuroML are defined for use in the NeuroML namespace. This prevents conflicts with other XML schemas that may use the same tags. Read more on XML namespaces here. The remaining lines in this snippet refer to the XML Schema that is defined for NeuroML. XML itself does not define any tags, so any tags can be used in a general XML document. Here is an example of a valid XML document, a simple HTML snippet: NeuroML, however, does not use these tags. It defines its own set of standard tags using an XML Schema. In other words, the NeuroML XML schema defines the structure and contents of a valid NeuroML document. Various tools can then compare NeuroML documents to the NeuroML Schema to validate them. The NeuroML Schema defines the structure and contents of a valid NeuroML document. The attribute documents that NeuroML has a defined XML Schema. The next attribute, tells us the locations of the NeuroML Schema. Here, two locations are provided:\n‚Ä¢ None and the location of the Schema Definition file (an file) relative to this example file in the GitHub repository. We will look at the NeuroML schema in detail in later sections. All NeuroML files must include the tag, and the attributes related to the NeuroML Schema. The last attribute, is the identification (or the name) of this particular NeuroML document. The remaining part of the file is the declaration of the model and its dynamics: The cell, is defined in the tag, which has a number of attributes as we saw before (see here for the schema definition):\n‚Ä¢ None : the name that we want to give to this cell. To refer to it later, for example,\n‚Ä¢ None : the initial membrane potential for the cell,\n‚Ä¢ None , , , and : are parameters of the Izhikevich neuron model. Similarly, the is also defined, and the tag includes the and . We observe that even though we have declared the entities, and the values for parameters that govern them, we do not state what and how these parameters are used. This is because NeuroML is a declarative language that defines the structure of models. We do not need to define how the dynamics of the different parts of the model are implemented. As we will see further below, these are already defined in NeuroML. Users describe the various components of the model but do not need to worry about how they are implemented. We have seen how an Izhikevich cell can be declared in NeuroML, with all its parameters. As is evident, XML files are excellent for storing structured data, but may not be easy to write by hand. However, NeuroML users are not expected to write in XML. They should use the Python tools as demonstrated here. Given that NeuroML develops a standard and defines what tags and attributes can be used, let us see how these are defined for the Izhikevich cell. The Izhikevich cell is defined in version 2 of the NeuroML schema here: The prefix indicates that these are all part of an XML Schema. The Izhikevich cell and all its parameters are defined in the schema. As we saw before, parameters of the model are defined as attributes in NeuroML files. So, here in the schema, they are also defined as of the that the schema describes. The schema also specifies which of the parameters are necessary, and what their dimensions (units) are using the and properties. This schema gives us all the information we need to describe an Izhikevich cell in NeuroML. Using the specification in the Schema, any number of Izhikevich cells can be defined in a NeuroML file with the necessary parameter sets to create networks of Izhikevich cells. The generated LEMS simulation file is shown below: This LEMS file has been automatically generated using PyNeuroML v1.1.13 (libNeuroML v0.5.8) <!-- Specify which component to run --> <!-- Note seed: ensures same random numbers used every run --> Similar to NeuroML, a LEMS Simulation file also has a well defined structure, i.e., a set of valid tags which define the contents of the LEMS file. We observe that whereas the NeuroML tags were related to the modelling parameters, the LEMS tags are related to simulation. We also note that our NeuroML model has been ‚Äúincluded‚Äù in the LEMS file, so that all entities defined there are now known to the LEMS simulation also. Like NeuroML, users are not expected to write the LEMS XML component by hand. They should continue to use the NeuroML Python tools. In the code above, we‚Äôve used the utility function that is included in the module. This is, as the name notes, a ‚Äúfactory function‚Äù. When we provide the name of a NeuroML component type (the Python class) to it as the first argument along with any parameters, it will create a new component (Python object) and return it to us to use, after running a few checks under the hood:\n‚Ä¢ None are all the necessary parameters set?\n‚Ä¢ None are any extra parameters given? We will see some of these checks in action later as we create more components for our model. The can accept two forms. We can either pass the component type (class) to the function, or we can pass its name as a string. The difference is that we do not need to the class in our script before using it if we specify its name as a string. The component factory function will import the class for us for us internally. Either form works, so you can choose which you prefer. It is important to only remain consistent and use one form to aid readability. We‚Äôve used another utility method in the code above: . The method calls the for us internally to create a new object of the required component. We could also use the , followed by , which would result in the same thing: In fact, we could do it all without using either method: This last form is not suggested because here, the extra checks that the and methods run are not carried out. You also need to know the name of the variable in the object to be able to append to it. The output of the method will list all the member names, but the method inspects the parent component and places the child in the right place for us. An exercise here would be to try providing invalid arguments to the or methods. For example:\n‚Ä¢ None try giving the wrong units for a parameter For example, I have used the wrong units for the parameter here, instead of : and it will throw a telling us that this does not match the expected string for : The specific error here includes the ‚Äúpattern restrictions‚Äù (regular expression) for valid values of the parameter. There are a number of tutorials on regular expressions on the internet that you can use to learn more about the meaning of the provided pattern restriction. The one restriction that we are interested in here is that the value of must end in one of , , , or . Anything else will result in an invalid value, and the factory will throw a . The NeuroML specification declares valid units for all its components. This allows us to validate models and components while building the model‚Äîeven before we have a complete model that we want to simulate. In fact, NeuroML also defines a list of units and dimensions that can be used. NeuroML defines a standard set of units that can be used in models. Learn more about units and dimensions in NeuroML and LEMS here. Now that we have a document, what if we want to inspect it to see what components it can hold, and what its current contents are? Each NeuroML component type includes the function that gives us a quick summary of information about the component: The output will be of this form: This shows all the valid NeuroML components that the top level component can directly contain. It also tells us the component type (class) corresponding to the component (object). It also tells us whether this component is optional or required. In the second form, where we also pass , it will also show the contents of each member if any. We can use this to inspect our created Izhikevich cell component: We can see that all the required parameters are correctly set for this component. We can also inspect the full document: Try running this at the beginning of the script right after creating the document, and at the end when the model has been completed. You should notice a major change, that our cell has been correctly added to the document. The function is very useful to see what components can belong to another. For example, to see what components can be added to our network, we can run this: This tells us what can contain. For setting the input, for example, it would seem that we should use one of either or here. The function can be used to get more information about these (next). There are multiple ways of getting information on a component type. The first, of course, is to look at the schema documentation online. The documentation for ExplicitInput is here, and for InputList is here. The schema documentation will also include examples of usage for most component types under the ‚ÄúUsage:Python‚Äù tab. includes the utility function, that like the method, provides information about component types ( in stands for ). Note that component types are classes and the method cannot be used on them. It can only be used once objects have been created from the component type classes. So, we could do (create a new dummy object of the class and call on it): but will do this for us: Finally, for completeness, we can also get information from the API documentation for libNeuroML here. Since this is documentation that is ‚Äúembedded‚Äù in the Python classes, we can also use the Python in-built help function to see it: The information provided by the different sources will be similar, but is perhaps the most NeuroML specific (whereas the Python function provides Python language related information also.) IDEs make programming easier. For example, a good IDE will show you the documentation that the Python function shows. Another useful function is the function. Like it provides some information about the component/object: This tells us that components of type can be added to components of the type, in the member. Of course, we will use the function in our network object , and that will add the component to the correct member."
    },
    {
        "link": "https://stackoverflow.com/questions/2082739/optimizing-for-space-instead-of-speed-in-c",
        "document": "P.P.S. References to articles and books are welcome too!\n\nP.S. One could argue that \"prematurely\" optimizing for space in embedded systems isn't all that evil, because you leave yourself more room for data storage and feature creep. It also allows you to cut hardware production costs because your code can run on smaller ROM/RAM.\n\nWhen you say \"optimization\", people tend to think \"speed\". But what about embedded systems where speed isn't all that critical, but memory is a major constraint? What are some guidelines, techniques, and tricks that can be used for shaving off those extra kilobytes in ROM and RAM? How does one \"profile\" code to see where the memory bloat is?\n‚Ä¢ If speed isn't critical, execute the code directly from flash.\n‚Ä¢ Declare constant data tables using . This will avoid the data being copied from flash to RAM\n‚Ä¢ Pack large data tables tightly using the smallest data types, and in the correct order to avoid padding.\n‚Ä¢ Use compression for large sets of data (as long as the compression code doesn't outweigh the data)\n‚Ä¢ Did anybody mention using -Os? ;-) One of the rules of Unix philosophy can help make code more compact: Rule of Representation: Fold knowledge into data so program logic can be stupid and robust. I can't count how many times I've seen elaborate branching logic, spanning many pages, that could've been folded into a nice compact table of rules, constants, and function pointers. State machines can often be represented this way (State Pattern). The Command Pattern also applies. It's all about the declarative vs imperative styles of programming. Instead of logging plain text, log event codes and binary data. Then use a \"phrasebook\" to reconstitute the event messages. The messages in the phrasebook can even contain printf-style format specifiers, so that the event data values are displayed neatly within the text. Each thread needs it own memory block for a stack and TSS. Where you don't need preemption, consider making your tasks execute co-operatively within the same thread (cooperative multi-tasking). Use memory pools instead of hoarding To avoid heap fragmentation, I've often seen separate modules hoard large static memory buffers for their own use, even when the memory is only occasionally required. A memory pool could be used instead so the the memory is only used \"on demand\". However, this approach may require careful analysis and instrumentation to make sure pools are not depleted at runtime. In embedded systems where only one application runs indefinitely, you can use dynamic allocation in a sensible way that doesn't lead to fragmentation: Just dynamically allocate once in your various initialization routines, and never free the memory. your containers to the correct capacity and don't let them auto-grow. If you need to frequently allocate/free buffers of data (say, for communication packets), then use memory pools. I once even extended the C/C++ runtimes so that it would abort my program if anything tried to dynamically allocate memory after the initialization sequence.\n\nCompile in VS with /Os. Often times this is even faster than optimizing for speed anyway, because smaller code size == less paging. Comdat folding should be enabled in the linker (it is by default in release builds) Be careful about data structure packing; often time this results in the compiler generated more code (== more memory) to generate the assembly to access unaligned memory. Using 1 bit for a boolean flag is a classic example. Also, be careful when choosing a memory efficient algorithm over an algorithm with a better runtime. This is where premature optimizations come in.\n\non top what others suggest: Limit use of c++ features, write like in ANSI C with minor extensions. Standard (std::) templates use a large system of dynamic allocation. If you can, avoid templates altogether. While not inherently harmful, they make it way too easy to generate lots and lots of machine code from just a couple simple, clean, elegant high-level instructions. This encourages writing in a way that - despite all the \"clean code\" advantages - is very memory hungry. If you must use templates, write your own or use ones designed for embedded use, pass fixed sizes as template parameters, and write a test program so you can test your template AND check your -S output to ensure the compiler is not generating horrible assembly code to instantiate it. Align your structures by hand, or use #pragma pack {char a; long b; char c; long d; char e; char f; } //is 18 bytes, {char a; char c; char d; char f; long b; long d; } //is 12 bytes. For the same reason, use a centralized global data storage structure instead of scattered local static variables. If you need a subset of functionality of given library, consider writing your own. Don't do that for longer ones. Pack multiple files together to let the compiler inline short functions and perform various optimizations Linker can't.\n\nDon't be afraid to write 'little languages' inside your program. Sometimes a table of strings and an interpreter can get a LOT done. For instance, in a system I've worked on, we have a lot of internal tables, which have to be accessed in various ways (loop through, whatever). We've got an internal system of commands for referencing the tables that forms a sort of half-way language that's quite compact for what it gets donw. But, BE CAREFUL! Know that you are writing such things (I wrote one accidentally, myself), and DOCUMENT what you are doing. The original developers do NOT seem to have been conscious of what they were doing, so it's much harder to manage than it should be.\n\nOptimizing is a popular term but often technically incorrect. It literally means to make optimal. Such a condition is never actually achieved for either speed or size. We can simply take measures to move toward optimization. Many (but not all) of the techniques used to move toward minimum time to a computing result sacrifices memory requirement, and many (but not all) of the techniques used to move toward minimum memory requirement lengthens the time to result. Reduction of memory requirements amounts to a fixed number of general techniques. It is difficult to find a specific technique that does not neatly fit into one or more of these. If you did all of them, you'd have something very close to the minimal space requirement for the program if not the absolute minimum possible. For a real application, it could take a team of experienced programmers a thousand years to do it.\n‚Ä¢ Remove all need for storing data that could be streamed instead.\n‚Ä¢ Allocate only the number of bytes needed, never a single more.\n‚Ä¢ Free data as soon as it is no longer possibly needed.\n‚Ä¢ Remove all unused algorithms and branches within algorithms.\n‚Ä¢ Find the algorithm that is represented in the minimally sized execution unit. This is a computer science view of the topic, not a developer's one. For instance, packing a data structure is an effort that combines (3) and (9) above. Compressing data is a way to at least partly achieve (1) above. Reducing overhead of higher level programming constructs is a way to achieve some progress in (7) and (8). Dynamic allocation is an attempt to exploit a multitasking environment to employ (3). Compilation warnings, if turned on, can help with (5). Destructors attempt to assist with (6). Sockets, streams, and pipes can be used to accomplish (2). Simplifying a polynomial is a technique to gain ground in (8). Understanding of the meaning of nine and the various ways to achieve them is the result of years of learning and checking memory maps resulting from compilation. Embedded programmers often learn them more quickly because of limited memory available. Using the -Os option on a gnu compiler makes a request to the compiler to attempt to find patterns that can be transformed to accomplish these, but the -Os is an aggregate flag that turns on a number of optimization features, each of which attempts to perform transformations to accomplish one of the 9 tasks above. Compiler directives can produce results without programmer effort, but automated processes in the compiler rarely correct problems created by lack of awareness in the writers of the code."
    },
    {
        "link": "https://callmerohit.medium.com/8-c-performance-tips-i-discovered-after-years-of-coding-in-c-57599d6dc192",
        "document": "Certainly! Here are eight performance tips for C++ that can help you write more efficient code based on common practices and experiences from seasoned developers:\n‚Ä¢ Tip: Leverage move semantics to avoid unnecessary copies of objects. Use to transfer ownership of resources instead of copying them.\n‚Ä¢ Example: When returning large objects from functions, return them by value and let the compiler optimize with move semantics.\n‚Ä¢ Tip: Use or other STL containers instead of raw arrays. They manage memory automatically, provide bounds checking, and offer a variety of useful functions.\n‚Ä¢ Example: can dynamically resize and manage its own memory, reducing the chance of memory leaks.\n‚Ä¢ Tip: Minimize the use of virtual functions in performance-critical paths. If polymorphism is needed, consider alternatives like templates or .\n‚Ä¢ Example: Use static polymorphism with templates to achieve similar functionality without the overhead of virtual calls.\n‚Ä¢ Tip: Use memory pools or custom allocators for frequently allocated and deallocated objects to reduce fragmentation and allocation overhead.\n‚Ä¢ Example: For small objects, consider using or a custom allocator that minimizes the number of calls to and .\n‚Ä¢ Tip: Mark variables and functions as when applicable. Use for compile-time constants and functions to enable better optimizations.\n‚Ä¢ Example: can be evaluated at compile time, reducing runtime overhead.\n‚Ä¢ Tip: Always profile your code to identify bottlenecks before making optimizations. Focus on the areas that have the most significant impact on performance.\n‚Ä¢ Example: Use tools like Valgrind, gprof, or Visual Studio‚Äôs built-in profiler to gather data on where your program spends most of its time.\n‚Ä¢ Tip: Use references and pointers to avoid unnecessary copying. When passing large objects to functions, use to pass by reference.\n‚Ä¢ Example: Instead of , use to avoid copying the entire object.\n‚Ä¢ Tip: Use compiler optimization flags when building your application (e.g., , for GCC/Clang). Also, consider using link-time optimization (LTO).\n‚Ä¢ Example: Compile with to enable aggressive optimizations that can result in significant performance improvements.\n\nBy implementing these tips, you can improve the performance of your C++ applications significantly. Remember that optimization should be guided by profiling and understanding the specific needs of your application, as premature optimization can lead to complex and less maintainable code.\n\nCheck out more details on BLACKBOX.AI üëá\n\nhttps://www.blackbox.ai/share/014e4308-0772-4339-8b61-98ad9968e167\n\nLike, Comment and Follow me for more daily tips."
    },
    {
        "link": "https://stackoverflow.com/questions/2030189/general-c-performance-improvement-tips",
        "document": "It's not about making a particular software faster, also it's not about how to create a clean software design, but rather programming habits that - if you always apply them, you will make your code rather a little bit faster than a little bit slower.\n\nCould someone point me to an article, or write some tips right here about some C++ programming habits that are generally valid (no real drawbacks) and improves performance? I do not mean programming patterns and algorithm complexity - I need small things like how you define your functions, things to do/to avoid in loops, what to allocate on the stack, what on the heap, and so on.\n\nWant to improve this question? Update the question so it focuses on one problem only by editing this post .\n\n. This question needs to be more focused . It is not currently accepting answers.\n\nIf I understand you correctly, you're asking about avoiding Premature Pessimization, a good complement to avoiding Premature Optimization. The #1 thing to avoid, based on my experience, is to not copy large objects whenever possible. This includes:\n‚Ä¢ make sure you declare a reference variable when you need it This last bullet requires some explanation. I can't tell you how many times I've seen this: class Foo { const BigObject & bar(); }; // ... somewhere in code ... BigObject obj = foo.bar(); // OOPS! This creates a copy! These guidelines apply to anything larger than a smart pointer or a built-in type. Also, I highly recommend investing time learning to profile your code. A good profiling tool will help catch wasteful operations.\n\nThe \"Optimizing Software in C++\" by Agner Fog is generally one of the best references for optimization techniques both simple, but definitely also more advanced. One other great advantage is that it is free to read on his website. (See link in his name for his website, and link on paper title for pdf). Edit: Also remember that 90% (or more) of the time is spent in 10% (or less) of the code. So in general optimizing code is really about pinpointing your bottlenecks. Further more it is important and useful to know that modern compilers will do optimzation much better than most coders, especially micro optimizations such as delaying initialization of variables, etc. The compilers are often extremely good at optimizing, so spend your time writing stable, reliable and simple code. I'd argue that it pays to focus more on the choice of algorithm than on micro optimizations, at least for the most part.\n\nIt seems from your question that you already know about the \"premature optimization is evil\" philosophy, so I won't preach about that. :) Modern compilers are already pretty smart at micro-optimizing things for you. If you try too hard, you can often make things slower than the original straight-forward code. For small \"optimizations\" you can do safely without thinking, and which doesn't affect much the readability/maintability of the code, check out the \"Premature Pessimization\" section of the book C++ Coding Standards by Sutter & Alexandrescu. For more optimization techniques, check out Efficient C++ by Bulka & Mayhew. Only use when justified by profiling! For good general C++ programming practices, check out:\n‚Ä¢ C++ Coding Standards by Sutter & Alexandrescu (must have, IMHO) Off the top of my head, one good general performance practice is to pass heavyweight objects by reference, instead of by copy. For example: // Not a good idea, a whole other temporary copy of the (potentially big) vector will be created. int sum(std::vector<int> v) { // sum all values of v return sum; } // Better, vector is passed by constant reference int sum(const std::vector<int>& v) { // v is immutable (\"read-only\") in this context // sum all values of v. return sum; } For a small object like a complex number or 2-dimensional (x, y) point, the function will likely run faster with the object passed by copy. When it comes to fixed-size, medium-weight objects, it's not so clear if the function will run faster with a copy or a reference to the object. Only profiling will tell. I usually just pass by const reference (if the function doesn't need a local copy) and only worry about it if profiling tells me to. Some will say that you can inline small class methods without thinking. This may give you a runtime performance boost, but it may also lengthen your compile time if there is a heavy amount of inlining. If a class method is part of a library API, it might be better not to inline it, no matter how small it is. This is because the implementation of inline functions has to be visible to other modules/classes. If you change something in that inline function/method, then other modules that reference it need to be re-compiled. When I first started to program, I would try to micro-optimize everything (that was the electrical engineer in me). What a waste of time! If you're into embedded systems, then things change and you can't take memory for granted. But that's another whole can of worms.\n\nI like this question because it is asking for some \"good habits\". I have found that certain things that are desirable in programming are initially a chore, but become acceptable and even easy once they become habits. One example is always using smart pointers instead of raw pointers to control heap memory lifetime. Another, related of course, is developing the habit of always using RAII for resource acquisition and release. Another is always using exceptions for error handling. These three tend to simplify code, thereby making it smaller and so faster, as well as easier to understand. \n\n You could also make getters and setters implicitly inline; always make full use of initializer lists in constructors; and always use the find and other related functions that are provided in the std library, instead of crafting your own loops. Not specifically C++, but it is often worthwhile to avoid data copying. In long-running programs with a lot of memory allocation it can be worthwhile to consider memory allocation as a major part of the design, so that the memory you use comes from pools that are reused, although this is not necessarily a common enough thing to be considered worthy of forming a habit. One more thing - do not copy code from one place to another if you need the functionality - use a function. This keeps code size small and makes it easier to optimize all the places that use this functionality.\n\nThis page sum up all you have to know about optimization in C++ (be it while or after writing software). It's really good advice and is very lear -- and can be used as a useful reminder in optimization phase on a project. It's a bit old so you also have to know wich optimizations are already done by your compiler (like NRVO). Other than that, reading the Effective C++, More Effective C++, Effective STL and C++ Coding Standards that have already been cited is important too, because it explains a lot of things about what occurs in the language and in the STL, allowing you to better optimize your specific case by using a better understanding of what's happening exactly.\n\nWhy nobody mentioned it so far? Why is everyone into poor little ? One of the best little things you can easily do, to not pessimize your code: // this is a better option void some_function(const std::string &str); // than this: void some_function(std::string str); In case of short you might not win much, but passing big objects like that, can save you quite a lot of computing power as you avoid redundant copying. And can also save you from a bug or two if you forgot to implement your copy constructor."
    },
    {
        "link": "https://reddit.com/r/cpp_questions/comments/122zh0v/memory_best_practices",
        "document": "I'm currently improving my C++ skills and have been reading about how the stack and heap memory work in C++.\n\nI was wondering which are in your opinion the best practices with this regards. Malloc, pointers, references, arrays,... when to use them and when to avoid them.\n\nHow to use the power of stack and heap memory in the best way possible."
    },
    {
        "link": "https://hackernoon.com/c-performance-optimization-best-practices",
        "document": "Performance optimization is a critical aspect of C++ programming, as it can significantly impact the speed and efficiency of your applications. In this article, we'll explore various techniques and best practices for optimizing C++ code. Whether you're a beginner or an experienced developer, these tips will help you write faster and more efficient C++ programs.\n\n1. Use the Right Data Structures\n\nChoosing the appropriate data structures can have a massive impact on performance. Use for dynamic arrays, or for key-value pairs and or for unique values. Avoid linked lists when you need random access, as they can lead to poor cache performance.\n\nExample: Using for Dynamic Arrays\n\nCopying objects can be expensive. Use references or move semantics ( ) when passing and returning objects to minimize unnecessary copying. If you use then try to change it in some cases, it will have a better performance.\n\nAllocate objects on the stack whenever possible, as stack allocation is faster than heap allocation. Use dynamic allocation (e.g., and ) only when the object's lifetime extends beyond the current scope.\n\nHowever, it's important to note that stack allocation has limitations:\n‚Ä¢ Fixed Size: Stack memory is of fixed size and is limited. This means you can't allocate very large objects or a dynamic number of objects on the stack.\n‚Ä¢ Risk of Stack Overflow: Excessive stack memory usage can lead to a stack overflow if the available stack space is exhausted. Heap memory doesn't have this limitation.\n\nProfiling tools can help identify performance bottlenecks. Use tools like (GNU Profiler) or platform-specific profilers to analyze your code's execution time and memory usage.\n‚Ä¢ Identify what areas of code are taking how much time\n‚Ä¢ See if you can use better data structures/ algorithms to make things faster\n\nExcessive memory allocation and deallocation can lead to performance issues. Reuse objects when possible and consider using object pools for frequently created and destroyed objects.\n\nLoops are often the core of algorithms. Optimize loops by minimizing loop overhead, reducing unnecessary calculations, and using the right loop constructs (e.g., range-based loops).\n\nModern C++ compilers provide optimization flags (e.g., , ) that can significantly improve code performance. Use these flags during compilation to enable various optimization techniques.\n‚Ä¢ : Enables basic optimization. This includes optimizations such as common subexpression elimination and instruction scheduling. It's a good balance between optimization and compilation time.\n‚Ä¢ : Enables more aggressive optimization, including inlining functions, loop optimizations, and better code scheduling. It provides a significant performance boost.\n‚Ä¢ : Enables even more aggressive optimizations. It can lead to faster code but may increase compilation time and the size of the executable.\n\nMinimize function calls within tight loops. Inlining functions (e.g., using or compiler optimizations) can eliminate function call overhead.\n\nOptimize for cache efficiency by minimizing cache misses. Access data sequentially, avoid non-contiguous memory accesses, and use data structures that promote cache locality.\n\nBenchmark your code after each optimization step to measure the impact of changes accurately. Iteratively apply optimizations, focusing on the most significant bottlenecks.\n\nOptimizing C++ code is a crucial skill for achieving high-performance applications. You can significantly enhance your code's speed and efficiency by using the right data structures, avoiding unnecessary copying, and following best practices. Profiling, benchmarking, and iterative optimization are essential tools for achieving optimal performance. Remember that premature optimization is not always beneficial; focus on optimizing critical sections of your code when necessary."
    }
]