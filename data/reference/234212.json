[
    {
        "link": "https://stable-diffusion-art.com/prompt-guide",
        "document": "Developing a process to build good prompts is the first step every Stable Diffusion user tackles. This article summarizes the process and techniques developed through experimentations and other users‚Äô inputs. The goal is to write down all I know about prompts so you can know them all in one place.\n\nA good prompt needs to be detailed and specific. A good process is to look through a list of keyword categories and decide whether you want to use any of them.\n\nAn extensive list of keywords from each category is available in the prompt generator. You can also find a short list here.\n\nYou don‚Äôt have to include keywords from all categories. Treat them as a checklist to remind you what could be used.\n\nLet‚Äôs review each category and generate some images by adding keywords. I will use the Dreamshaper model, an excellent model for beginners.\n\nTo see the effect of the prompt alone, I won‚Äôt be using negative prompts for now. Don‚Äôt worry. We will study negative prompts in the later part of this article.\n\nAll images are generated with 25 steps of DPM++ 2M Karas sampler and an image size of 512√ó768.\n\nThe subject is what you want to see in the image. A common mistake is not writing enough about the subjects.\n\nLet‚Äôs say we want to generate a sorceress casting magic. A newbie may write\n\nYou get some decent images, but this prompt leaves too much room for imagination. (It is common to see the face garbled in Stable Diffusion. There are ways to fix it.)\n\nHow do you want the sorceress to look? Do you have any keywords to describe her more specifically? What does she wear? What kind of magic is she casting? Is she standing, running, or floating in the air? What‚Äôs the background scene?\n\nStable Diffusion cannot read our minds. We have to say exactly what we want.\n\nAs a demo, let‚Äôs say she is powerful and mysterious and uses lightning magic. She wears a leather outfit with gemstones. She sits down on a rock. She wears a hat. The background is a castle.\n\nNow, we generate more specific images. The outfit, the pose and the background are consistent across images.\n\nMedium is the material used to make artwork. Some examples are illustration, oil painting, 3D rendering, and photography. Medium has a strong effect because one keyword alone can dramatically change the style.\n\nThe images switched from a realistic painting style to being more like computer graphics. I think we can stop here. Just kidding.\n\nThe style refers to the artistic style of the image. Examples include impressionist, surrealist, pop art, etc.\n\nNow, the scene has become darker and more gloomy.\n\nNiche graphic websites such as Artstation and Deviant Art aggregate many images of distinct genres. Using them in a prompt is a sure way to steer the image toward these styles.\n\nIt‚Äôs not a huge change, but the images do look like what you would find on Artstation.\n\nResolution represents how sharp and detailed the image is. Let‚Äôs add keywords highly detailed and sharp focus.\n\nWell, it‚Äôs not a huge effect, perhaps because the previous images are already pretty sharp and detailed. But it doesn‚Äôt hurt to add.\n\nAdditional details are sweeteners added to modify an image. We will add sci-fi and dystopian to add some vibe to the image.\n\nYou can control the overall color of the image by adding color keywords. The colors you specified may appear as a tone or in objects.\n\nLet‚Äôs add some golden color to the image with the keyword iridescent gold.\n\nThe gold comes out great in a few places!\n\nAny photographer would tell you lighting is key to creating successful images. Lighting keywords can have a huge effect on how the image looks. Let‚Äôs add studio lighting to make it studio photo-like.\n\nThis completes our example prompt.\n\nYou may have noticed the images are already pretty good with only a few keywords added. More is not always better when building a prompt. You often don‚Äôt need many keywords to get good images.\n\nUsing negative prompts is another great way to steer the image, but instead of putting in what you want, you put in what you don‚Äôt want. They don‚Äôt need to be objects. They can also be styles and unwanted attributes. (e.g., ugly, deformed)\n\nUsing negative prompts is a must for v2 models. Without it, the images would look far inferior to v1‚Äôs. They are optional for v1 and SDXL models, but I routinely use a boilerplate negative prompt because they either help or don‚Äôt hurt.\n\nI will use a simple universal negative prompt that doesn‚Äôt modify the style. You can read more about it to understand how it works.\n\nYou should approach prompt building as an iterative process. As the previous section shows, the images could be pretty good with just a few keywords added to the subject.\n\nI always start with a simple prompt with subject, medium, and style only. Generate at least 4 images at a time to see what you get. Most prompts do not work 100% of the time. You want to get some idea of what they can do statistically.\n\nAdd at most two keywords at a time. Likewise, generate at least 4 images to assess its effect.\n\nYou can use a universal negative prompt if you are starting out.\n\nAdding keywords to the negative prompt can be part of the iterative process. The keywords can be objects or body parts you want to avoid (Since v1 models are not very good at rendering hands, it‚Äôs not a bad idea to use ‚Äúhand‚Äù in the negative prompt to hide them.)\n\nYou can modify a keyword‚Äôs importance by switching to a different one at a certain sampling step.\n\nThe following syntaxes apply to AUTOMATIC1111 GUI. You can run this GUI with one click using the Colab notebook in the Quick Start Guide. You can also install it on Windows and Mac.\n\nYou can adjust the weight of a keyword by the syntax . is a value such that less than 1 means less important and larger than 1 means more important.\n\nFor example, we can adjust the weight of the keyword in the following prompt\n\nIncreasing the weight of tends to generate more dogs. Decreasing it tends to generate fewer. It is not always true for every single image. But it is true in a statistical sense.\n\nThis technique can be applied to subject keywords and all categories, such as style and lighting.\n\nAn equivalent way to adjust keyword strength is to use and . increases the strength of the keyword by a factor of 1.1 and is the same as . decrease the strength by a factor of 0.9 and is the same as .\n\nYou can use multiple of them, just like in Algebra‚Ä¶ The effect is multiplicative.\n\nSimilarly, the effects of using multiple are:\n\nAUTOMATIC1111 TIP: You can use Ctrl + Up/Down Arrow (Windows) or Cmd + Up/Down Arrow to increase/decrease the weight of a keyword.\n\nYou can mix two keywords. The proper term is prompt scheduling. The syntax is\n\ncontrols at which step keyword1 is switched to keyword2. It is a number between 0 and 1.\n\nFor example, if I use the prompt\n\nThat means the prompt in steps 1 to 15 is\n\nAnd the prompt in steps 16 to 30 becomes\n\nThe factor determines when the keyword is changed. it is after 30 steps x 0.5 = 15 steps.\n\nThe effect of changing the factor is blending the two presidents to different degrees.\n\nYou may have noticed Trump is in a white suit which is more of a Joe outfit. This is a perfect example of a very important rule for keyword blending: The first keyword dictates the global composition. The early diffusion steps set the overall composition. The later steps refine details.\n\nQuiz: What would you get if you swapped Donald Trump and Joe Biden?\n\nA common use case is to create a new face with a particular look, borrowing from actors and actresses. For example, [Emma Watson: Amber heard: 0.85], 40 steps is a look between the two:\n\nWhen carefully choosing the two names and adjusting the factor, we can get the look we want precisely.\n\nAlternatively, you can use multiple celebrity names with keyword weights to adjust facial features. For example:\n\nSee this tutorial if you want to generate a consistent face across multiple images.\n\nUsing keyword blending, you can achieve effects similar to prompt-to-prompt, generating pairs of highly similar images with edits. The following two images are generated with the same prompt except for a prompt schedule to substitute with . The seed and number of steps were kept the same.\n\nThe factor needs to be carefully adjusted. How does it work? The theory behind this is the overall composition of the image was set by the early diffusion process. Once the diffusion is trapped in a small space, swapping any keywords won‚Äôt have a large effect on the overall image. It would only change a small part.\n\nUsing multiple celebrity names is an easy way to blend two or more faces. The blending will be consistent across images. You don‚Äôt even need to use prompt scheduling. When you use multiple names, Stable Diffusion understands it as generating one person but with those facial features.\n\nThe following phrase uses multiple names to blend three faces with different weights.\n\nPutting this technique into action, the prompt is:\n\nHere are images with the same prompt:\n\nSee this face repeating across the images!\n\nUse multiple celebrity names and keyword weights to carefully tune your desired facial feature. You can also use celebrity names in the negative prompt to avoid facial features you DON‚ÄôT want.\n\nSee more techniques to generate consistent faces.\n\nHow long can a prompt be?\n\nDepending on what Stable Diffusion service you are using, there could be a maximum number of keywords you can use in the prompt. In the basic Stable Diffusion v1 model, that limit is 75 tokens.\n\nNote that tokens are not the same as words. The CLIP model Stable Diffusion automatically converts the prompt into tokens, a numerical representation of words it knows. If you put in a word it has not seen before, it will be broken up into 2 or more sub-words until it knows what it is. The words it knows are called tokens, which are represented as numbers.\n\nFor example, dream is one token and beach is another token. But dreambeach is two tokens because the model doesn‚Äôt know this word, and so the model breaks the word up to and which it knows.\n\nAUTOMATIC1111 has no token limits. If a prompt contains more than 75 tokens, the limit of the CLIP tokenizer, it will start a new chunk of another 75 tokens, so the new ‚Äúlimit‚Äù becomes 150. The process can continue forever or until your computer runs out of memory‚Ä¶\n\nEach chunk of 75 tokens is processed independently, and the resulting representations are concatenated before feeding into Stable Diffusion‚Äôs U-Net.\n\nIn AUTOMATIC1111, You can check the number of tokens by looking at the small box at the top right corner of the prompt input box.\n\nWhat if you want to start a new prompt chunk before reaching 75 tokens? Sometimes you want to do that because the token in the beginning of a chunk can be more effective, and you may want to group related keywords in a chunk.\n\nYou can use the keyword BREAK to start a chunk. The following prompt uses two chunks to specify the hat is white and the dress is blue.\n\nWithout the BREAK, Stable Diffusion is more likely to mix up the color of the hat and the dress.\n\nThe fact that you see people using a keyword doesn‚Äôt mean that it is effective. Like homework, we all copy each other‚Äôs prompts, sometimes without much thought.\n\nYou can check the effectiveness of a keyword by just using it as a prompt. For example, does the v1.5 model know the American portrait painter John Singer Sargent? Let‚Äôs check with the prompt\n\nHow about the Artstation sensation wlop?\n\nWell, doesn‚Äôt look like it. That‚Äôs why you shouldn‚Äôt use ‚Äúby wlop‚Äù. That‚Äôs just adding noise.\n\nYou can use this technique to examine the effect of mixing two or more artists.\n\nTo be good at building prompts, you need to think like Stable Diffusion. At its core, it is an image sampler, generating pixel values that we humans likely say it‚Äôs legit and good. You can even use it without prompts, and it would generate many unrelated images. In technical terms, this is called unconditioned or unguided diffusion.\n\nThe prompt is a way to guide the diffusion process to the sampling space where it matches. I said earlier that a prompt needs to be detailed and specific. It‚Äôs because a detailed prompt narrows down the sampling space. Let‚Äôs look at an example.\n\nBy adding more describing keywords in the prompt, we narrow down the sampling of castles. In We asked for any image of a castle in the first example. Then we asked to get only those with a blue sky background. Finally, we demanded it is taken as a wide-angle photo.\n\nThe more you specify in the prompt, the less variation in the images.\n\nSome attributes are strongly correlated. When you specify one, you will get the other. Stable Diffusion generates the most likely images that could have an unintended association effect.\n\nLet‚Äôs say we want to generate photos of women with blue eyes.\n\nWhat if we change to brown eyes?\n\nNowhere in the prompts, I specified ethnicity. But because people with blue eyes are predominantly Europeans, Caucasians were generated. Brown eyes are more common across different ethnicities, so you will see a more diverse sample of races.\n\nStereotyping and bias is a big topic in AI models. I will confine to the technical aspect in this article.\n\nEvery keyword has some unintended associations. That‚Äôs especially true for celebrity names. Some actors and actresses like to be in certain poses or wear certain outfits when taking pictures, and hence in the training data. If you think about it, model training is nothing but learning by association. If Taylor Swift (in the training data) always crosses her legs, the model would think leg crossing is Taylor Swift too.\n\nWhen you use Taylor Swift in the prompt, you may mean to use her face. But there‚Äôs an effect of the subject‚Äôs pose and outfit too. The effect can be studied by using her name alone as the prompt.\n\nPoses and outfits are global compositions. If you want her face but not her poses, you can use keyword blending to swap her in at a later sampling step.\n\nPerhaps the most prominent example of association is seen when using artist names.\n\nThe 19th-century Czech painter Alphonse Mucha is a popular occurrence in portrait prompts because the name helps generate interesting embellishments, and his style blends very well with digital illustrations. But it also often leaves a signature circular or dome-shaped pattern in the background. They could look unnatural in outdoor settings.\n\nEmbeddings, the result of textual inversion, are nothing but combinations of keywords. You can expect them to do a bit more than what they claim.\n\nLet‚Äôs see the following base images of Ironman making a meal without using embeddings.\n\nStyle-Empire is an embedding I like to use because it adds a dark tone to portrait images and creates an interesting lighting effect. Since it was trained on an image with a street scene at night, you can expect it adds some blacks AND perhaps buildings and streets. See the images below with the embedding added.\n‚Ä¢ The background of the first image changed to city buildings at night.\n‚Ä¢ Iron man tends to show his face. Perhaps the training image is a portrait?\n\nSo even if an embedding is intended to modify the style, it is just a bunch of keywords and can have unintended effects.\n\nUsing a custom model is the easiest way to achieve a style, guaranteed. This is also a unique charm of Stable Diffusion. Because of the large open-source community, thousands of custom models are freely available.\n\nWhen using a model, we need to be aware that the meaning of a keyword can change. This is especially true for styles.\n\nLet‚Äôs use John Singer Sargent as the prompt with the Stable Diffusion v1.5 model.\n\nUsing the DreamShaper with the same prompt, a model fine-tuned for realistic portrait illustrations, we get the following images instead.\n\nThe style becomes more realistic. The DreamShaper model has a strong basis for generating clear and pretty woman faces.\n\nCheck before you use a style in a custom model. van Gogh may not be van Gogh anymore!\n\nDo you know you can specify different prompts for different regions of the image?\n\nFor example, you can put the moon at the top left:\n\nOr at the top right:\n\nYou can do that by using the Regional Prompter extension. It‚Äôs a great way to control image composition!"
    },
    {
        "link": "https://stability.ai/learning-hub/stable-diffusion-3-5-prompt-guide",
        "document": "Prompting is a valuable technique for effectively using generative AI image models. The structure of a prompt directly affects the generated images' quality, creativity, and accuracy. Stable Diffusion 3.5 excels in customizability, efficient performance, diverse outputs, and versatile styles, making it ideal for beginners and experts alike. This guide offers practical prompting tips for SD3.5, allowing you to refine image concepts quickly and precisely.\n\nTo get started, you can use all of the SD3.5 models on Hugging Face, Stability AI API, and Stable Assistant.\n\nTreat the SD3.5 models as a creative partner. By expressing your ideas clearly in natural language, you give the model the best opportunity to generate an image that aligns with your vision.\n\nTo structure a prompt effectively, start by identifying the key elements:\n‚Ä¢ None Style:\n\nDefine the aesthetic direction, such as illustration style, painting medium, digital art style, or photography. Experiment and blend styles such as line art, watercolor, oil painting, surrealism, expressionism, and product photography."
    },
    {
        "link": "https://reddit.com/r/StableDiffusion/comments/18lym36/prompt_documentation_for_beginners",
        "document": "I have been exploring this reddit and it's amazing. I am skilled with GPT, but would like to improve my stable diffusion game now, notably to know the basics of prompting.\n\nI have two use cases in mind - learn how to properly:\n\n- Generate image from prompt text\n\n- Generate a modified image from prompt text\n\nAny documentation (free) that you would recommend to speed up my learning process, and not just focusing on the Stability documentation?"
    },
    {
        "link": "https://strikingloo.github.io/stable-diffusion-vs-dalle-2",
        "document": "I wrote this post when Stability AI had just released the AI art model Stable Diffusion. In this guide, you will learn how to write prompts by example. If you want, you can skip the discussion and examples and just go straight to the prompt template and explanation\n\nStable Diffusion is similarly powerful to DALL-E 2, but open source, and open to the public through Dream Studio, where anyone gets 50 free uses just by signing up with an email address.\n\nSince it is open source and anyone who has 5GB of GPU VRAM can download it (and Emad Mostaque, Stability.ai‚Äôs founder has come out and said more efficient models are coming) to get unlimited uses, expect to keep seeing headlines about AI art for a while.\n\nI am tired of repeating the same old speech, but thinking back to how primitive models were just a year and a half ago with DALL-E and other VQVAE, this is completely insane. I can only imagine what applications artists and other users will come up with in the near future by leveraging StableDiffusion‚Äôs embeddings and its text-to-image capabilities, let alone whatever the next generation of models will be able to do.\n\nExtrapolating from how much this field has grown in the last 18 months, I wouldn‚Äôt be surprised if in 2 more years you can write a script for a comic book, feed it to some large language encoder and a text-to-image model like this, and get a fully illustrated, style-coherent graphic novel. This would also apply for frames for an animated movie or a storyboard.\n\nAre we really that close to something so big? I feel like the technology is there if enough compute and budget were allocated, but I am not sure whether someone will do it. I don‚Äôt see any obvious blockers or barriers to the next generation of models being even bigger or understanding style better.\n\nGiven this context, many people are concerned some artists may lose their jobs. After lots of discussion in Reddit and at parties, I will try to summarize my current opinion on that topic.\n\nFor use-cases where having a human artist brings the least value, I think text-to-image models will dominate the market. However, for those cases I think we already had stock images. For instance if I am adorning a random blog post, I‚Äôd rather get a free stock image in the header than pay an artist for a new professional photo, as I don‚Äôt think my readers care that much (see ‚ÄúI replaced all our blog thumbnails using DALL¬∑E 2‚Äù for an example).\n\nEspecially if this site was monetized and the big picture was just there to make you scroll further down and get more engagement and ad views.\n\nFor cases where the artist‚Äôs vision matters, like original paintings for decorating my home, or the panels for a graphic novel, I think StableDiffusion or DALL-E 2 for that matter are far away from beating humans. So far.\n\nHowever, I guess many freelance artists who work for commissions may find less demand for their work as bloggers or random people get their art itch scratched by AI art. I would love to hear the opinion of artists from that sort of market on this, as I am quite ignorant of how the whole process works (for instance, what kind of people commission art in the first place).\n\nI think models like this can also enhance artists‚Äô work. Say you are asked to draw 5 illustrations of a character doing different things. You could use dall-e to get 5 relevant background scenes in 5 minutes, then use your time to add in the characters and some details on top of them. AI art models are significantly better at drawing background scenes than action and characters, so this is a combination of the best capabilities of both human and machine.\n\nCompare against drawing the whole thing from scratch.\n\nObviously style matching would not be easy, but when we reach a point where you can supply your own mini dataset of backgrounds to condition on and style transfer, this may make artists able to work much faster.\n\nNow is this a good thing? I guess it is a double edged sword: will artists who leverage AI art models to become more productive drive down the general price of art? Or will the new supply generate new demand? I am not an economist, nor an artist, so speculating further would be futile and I will leave the rest to my dear readers.\n\nExtra reflection on the topic: As I said on Reddit, I would be worried that eventually you may remove the artist from some parts of the design loop if user feedback or artist feedback could be used as a reward for a reinforcement learning agent.\n\nSomething like asking ‚Äúdid you like this city‚Äôs design / this building? 1 to 10‚Äù to users, and using this for a policy gradient or similar algorithm.\n\nI am reminded of Evolution through Large Models where this approach is used for autogenerated instances made through genetic mutation algorithms. Imagine the same but for text-to-image.\n\nAs in my DALL-E 2 article, I tried the same prompts I had already tried in Craiyon or Guided diffusion (I know, that‚Äôs so early 2022!) to show just how much these systems have improved.\n\nMy focus was on fantasy, science fiction, and steampunk illustrations, because that is what I like, but I also experimented with more complex scenes and descriptions, to see how well StableDiffusion understands things like scene composition, prepositions and element interactions.\n\nAs with DALL-E 2, I found the model yields much better results for prompts that do not ask for a certain action or verb to be performed, but rather static scenes, especially if there are no humanoids or moving characters in them.\n\nFor everything else though, the results were astounding. I feel like StableDiffusion beats DALL-E 2 in character design and realism, but loses in the beauty of its landscapes and backgrounds, but take this with a 10% certainty as it is only my intuition and is likely be biased by my prompt choices.\n\nOne thing I‚Äôve found helps a lot in getting more beautiful results is thinking of which exact visual effect would add to the picture, and specifying it. For instance, for fantasy illustrations usually adding fireflies or sparkles helps. For landscapes, I like naming specific flowers like azaleas, and for buildings naming features like a column, a fountain, or anything else to ground the picture in a certain time and place, even if the detail is only tangential to the whole picture.\n\nIn my opinion these details can steer the model even better than many vague cues like ‚Äú4k‚Äù, in this generation of models -unlike in CLIP or GLIDE-. See Appendix A: StableDiffusion Prompt Guide to see how I choose most of my prompts, and some advice.\n\nI will begin with some scenes that I already tried with other models. These are some of the stable diffusion prompts that I liked best.\n\nI like the last one especially.\n\nFor scene composition, it is still struggling. Here are a ferret and a badger (which the model turned into another ferret) fencing with swords.\n\nAs expected, some of the best prompts that had worked for DALL-E 2 and Craiyon worked great with stable diffusion too.\n\nIn my opinion, both prompts yielded better images here than they did for DALLE-2, but you can be the judge of that.\n\nLest you think these models can only do 2d art, here is a very good photograph prompt for Stable Diffusion.\n\nI‚Äôve found this prompt works wonderfully well with DALLE, but it does not produce images with the same quality for Stable Diffusion. Here‚Äôs the same prompt‚Äôs output in DALLE.\n\nAgain back to comparing with DALL-E 2 and Craiyon, I tried asking for landscape paintings of mansions, castles and general garden scenes. I expected a more classical feel as opposed to the more digital illustration like air of the images from before.\n\nI tried adding the name of the painter Beksinski as a style cue, and the results were mixed: many of them blocked by StableDiffusion‚Äôs content policy, which I guess means there was something awful in them, but the survivors looked amazing.\n\nOne thing I struggled to get right with other models was anthropomorphic animals, especially if I also asked for medieval, steampunk or fantasy clothes. My dream of drawing a Mouseguard party with DALL-E would never come to fruition. With StableDiffusion one trick that worked for me was, instead of, say, prompting ‚Äúferret with pirate clothes/dressed as a pirate‚Äù, using the prompt ‚Äúferret wearing a pirate costume‚Äù.\n\nThen I also got a prompt from Twitter and iterated it, which was ‚Äúcute and adorable [animal], wearing [clothes], steampunk/clockpunk/fantasy‚Ä¶‚Äù plus style prompts.\n\nThis one worked like a charm. Rather than telling you the prompt I used for each individual picture, I will just show you the ones I liked best, so you can see what possibilities exist by tweaking a prompt like that one (I guess you can deduce the animal, etc. from the images themselves).\n\nIn general, removing Jean Paptiste Monge didn‚Äôt change results that much, switching coat and suit by tailcoat gave me the results I liked best, and adding portrait made all the paintings into humans. Also, the lantern keyword was copied from Twitter but it didn‚Äôt really bring much to the table (And most of these animals aren‚Äôt carrying a lantern because of that).\n\nI really went crazy with these, and these are my selection, so you can imagine how many I tried.\n\nAnother thing that I love about StableDiffusion (Which DALL-E 2 also gets right) is how well it renders textures. I can imagine how a 3d artist may use one of these models to enhance their own 3d objects by creating many different textures fast and combining them with domain knowledge.\n\nBecause of that, and my love for moss, I made many forest scenes with abundant growth and moss. Many of these look like a Magic: the Gathering illustration. I will not post the prompts as each one was different, but they mostly followed my digital fantasy illustration template.\n\nStableDiffusion was the first AI art model where I have successfully got a centaur. Not a deformed monstrosity, not a horse, not a weird human. A real centaur! So that made me happy and I had to share it.\n\nI honestly made a lot more illustrations I loved (like 400 total, I think?) but I guess most readers will get bored long before they finish scrolling this post, so I will not keep you any longer.\n\nThese are the last ones, I swear.\n\nBefore we reach the end, I want to raise a concern and propose a challenge. No matter what I tried, I could not make either DALL-E 2, or StableDiffusion make characters in the style of Jojo‚Äôs Bizarre Adventure (or Araki, in general). I tried the obvious style cues and others, and none worked. So if any of you manages to make one of these models draw Spongebob Squarepants in the style of Jojo‚Äôs, or any other recognizable character, you will get a thousand internet points from me.\n\nIn general, the best stable diffusion prompts will have this form:\n\nSome types of picture include digital illustration, oil painting (usually good results), matte painting, 3d render, medieval map.\n\nThe main subject can be anything you‚Äôre thinking of, but StableDiffusion still struggles with compositionality, so it shouldn‚Äôt be more than one or two main things (say, a beaver wearing a suit, or a cat samurai with a pet pug). The main subject should be mostly composed of adjectives and nouns. Avoid verbs, as Stable Diffusion has a hard time interpreting them correctly.\n\nStyle cues can be anything you want to condition the image on. I wouldn‚Äôt add too many, maybe only 1 to 3. These can really vary a lot but some good ones are: concept art, steampunk, trending in artstation, good composition, hyper realistic, oil on canvas, vivid colors.\n\nAdditionally, adding the name of an artist as a cue will make the picture look like something that artist made, though it may condition the image‚Äôs contents, especially if that artist had narrow themes (Beatrix Potter gets you spurious rabbits, for instance).\n\nFor a detailed guide to crafting the best stable diffusion prompts, see A Guide to Writing Prompts for Text-to-image AI if you feel like you want to read more.\n\nYou can also find many great prompts if you use the Lexica.art prompt search engine to find images you like and make tweaks to their prompts.\n\nGiven how much has happened lately (even John Oliver is talking about DALL-E!), here are some other articles you may want to read.\n‚Ä¢ Stable Diffusion: The Most Important AI Art Model Ever covering the more social/economic side of this.\n‚Ä¢ A traveler‚Äôs guide to the latent space: a guide on prompt engineering that goes really in depth. I haven‚Äôt actually read the whole thing.\n‚Ä¢ A guide to Writing Prompts for Text-to-Image AI: The best quick primer I‚Äôve found on prompt engineering and writing prompts for DALL-E 2/StableDiffusion or any other text-to-image AI.\n‚Ä¢ Art Prompts: My Experiments with Mini DALL-E: My first post on text-to-image AI, where I included my own AI art prompt guide. Here you can see how far we‚Äôve come and how fast.\n‚Ä¢ DALL-E 2 Experiments: The post I wrote two weeks ago when DALL-E 2 beta release was news and StableDiffusion hadn‚Äôt come out yet. See if you can spot the same prompts‚Äô different results.\n‚Ä¢ How to Draw: Where a user uses StableDiffusion‚Äôs img2img version to convert an MSPaint drawing into a realistic sci-fi image.\n‚Ä¢ Image2Image StableDiffusion, available on Replicate for free. You can draw a rough sketch of what you want in jspaint (the browser copy of MSPaint), then upload it to Stable Diffusion img2img and use that as a starting point for your AI art. See also Stable Diffusion is a really big deal, Simon Willison: This came out a little after I wrote this post, when Stability.ai released the img2img StableDiffusion model. It is amazing! You can make a sketch in MSPaint (Or JsPaint) and make the AI turn it into a painting or illustration in the style you want.\n‚Ä¢ High-performance image generation using Stable Diffusion in KerasCV: Stable Diffusion was finally ported to keras. It runs smoothly both on GPU or CPU if you have keras installed, and this is the version I‚Äôve been using to make AI art in my local computer.\n‚Ä¢ Lexica.art a great search engine for prompts and artworks. It is an amazing resource to find good images and see which prompts generated them, which you can then copy and tune to your needs. An easy way to build on the best stable diffusion prompts other people has already found.\n‚Ä¢ If you like anime, Waifu Diffusion is a text-to-image diffusion model that was conditioned on high-quality anime images through fine-tuning, using Stable Diffusion as a starting point. It generates anime illustrations and it‚Äôs awesome.\n‚Ä¢ The Illustrated Stable Diffusion explains how Stable Diffusion works, step by step and through different levels of abstraction, and has great illustrations.\n\nIf you liked this article, please share it with someone you think will like reading it too. I wrote this for you guys."
    },
    {
        "link": "https://huggingface.co/docs/diffusers/v0.12.0/en/stable_diffusion",
        "document": "and get access to the augmented documentation experience\n\nStable Diffusion is a Latent Diffusion model developed by researchers from the Machine Vision and Learning group at LMU Munich, a.k.a CompVis.\n\n Model checkpoints were publicly released at the end of August 2022 by a collaboration of Stability AI, CompVis, and Runway with support from EleutherAI and LAION. For more information, you can check out the official blog post.\n\nSince its public release the community has done an incredible job at working together to make the stable diffusion checkpoints faster, more memory efficient, and more performant.\n\nüß® Diffusers offers a simple API to run stable diffusion with all memory, computing, and quality improvements.\n\nThis notebook walks you through the improvements one-by-one so you can best leverage StableDiffusionPipeline for inference.\n\nWhen running *Stable Diffusion* in inference, we usually want to generate a certain type, or style of image and then improve upon it. Improving upon a previously generated image means running inference over and over again with a different prompt and potentially a different seed until we are happy with our generation.\n\nSo to begin with, it is most important to speed up stable diffusion as much as possible to generate as many pictures as possible in a given amount of time.\n\nThis can be done by both improving the computational efficiency (speed) and the memory efficiency (GPU RAM).\n\nLet‚Äôs start by looking into computational efficiency first.\n\nThroughout the notebook, we will focus on runwayml/stable-diffusion-v1-5:\n\nWe aim at generating a beautiful photograph of an old warrior chief and will later try to find the best prompt to generate such a photograph. For now, let‚Äôs keep the prompt simple:\n\nTo begin with, we should make sure we run inference on GPU, so let‚Äôs move the pipeline to GPU, just like you would with any PyTorch module.\n\nTo generate an image, you should use the [~ ] method.\n\nTo make sure we can reproduce more or less the same image in every call, let‚Äôs make use of the generator. See the documentation on reproducibility here for more information.\n\nNow, let‚Äôs take a spin on it.\n\nCool, this now took roughly 30 seconds on a T4 GPU (you might see faster inference if your allocated GPU is better than a T4).\n\nThe default run we did above used full float32 precision and ran the default number of inference steps (50). The easiest speed-ups come from switching to float16 (or half) precision and simply running fewer inference steps. Let‚Äôs load the model now in float16 instead.\n\nAnd we can again call the pipeline to generate an image.\n\nCool, this is almost three times as fast for arguably the same image quality.\n\nWe strongly suggest always running your pipelines in float16 as so far we have very rarely seen degradations in quality because of it.\n\nNext, let‚Äôs see if we need to use 50 inference steps or whether we could use significantly fewer. The number of inference steps is associated with the denoising scheduler we use. Choosing a more efficient scheduler could help us decrease the number of steps.\n\nLet‚Äôs have a look at all the schedulers the stable diffusion pipeline is compatible with.\n\nüß® Diffusers is constantly adding a bunch of novel schedulers/samplers that can be used with Stable Diffusion. For more information, we recommend taking a look at the official documentation here.\n\nAlright, right now Stable Diffusion is using the which usually requires around 50 inference steps. However, other schedulers such as or seem to get away with just 20 to 25 inference steps. Let‚Äôs try them out.\n\nYou can set a new scheduler by making use of the from_config function.\n\nNow, let‚Äôs try to reduce the number of inference steps to just 20.\n\nThe image now does look a little different, but it‚Äôs arguably still of equally high quality. We now cut inference time to just 4 seconds though üòç.\n\nLess memory used in generation indirectly implies more speed, since we‚Äôre often trying to maximize how many images we can generate per second. Usually, the more images per inference run, the more images per second too.\n\nThe easiest way to see how many images we can generate at once is to simply try it out, and see when we get a ‚ÄúOut-of-memory (OOM)‚Äù error.\n\nWe can run batched inference by simply passing a list of prompts and generators. Let‚Äôs define a quick function that generates a batch for us.\n\nThis function returns a list of prompts and a list of generators, so we can reuse the generator that produced a result we like.\n\nWe also need a method that allows us to easily display a batch of images.\n\nCool, let‚Äôs see how much memory we can use starting with .\n\nGoing over a batch_size of 4 will error out in this notebook (assuming we are running it on a T4 GPU). Also, we can see we only generate slightly more images per second (3.75s/image) compared to 4s/image previously.\n\nHowever, the community has found some nice tricks to improve the memory constraints further. After stable diffusion was released, the community found improvements within days and shared them freely over GitHub - open-source at its finest! I believe the original idea came from this GitHub thread.\n\nBy far most of the memory is taken up by the cross-attention layers. Instead of running this operation in batch, one can run it sequentially to save a significant amount of memory.\n\nIt can easily be enabled by calling as is documented here.\n\nGreat, now that attention slicing is enabled, let‚Äôs try to double the batch size again, going for .\n\nNice, it works. However, the speed gain is again not very big (it might however be much more significant on other GPUs).\n\nWe‚Äôre at roughly 3.5 seconds per image üî• which is probably the fastest we can be with a simple T4 without sacrificing quality.\n\nNext, let‚Äôs look into how to improve the quality!\n\nNow that our image generation pipeline is blazing fast, let‚Äôs try to get maximum image quality.\n\nFirst of all, image quality is extremely subjective, so it‚Äôs difficult to make general claims here.\n\nThe most obvious step to take to improve quality is to use better checkpoints. Since the release of Stable Diffusion, many improved versions have been released, which are summarized here:\n\nNewer versions don‚Äôt necessarily mean better image quality with the same parameters. People mentioned that 2.0 is slightly worse than 1.5 for certain prompts, but given the right prompt engineering 2.0 and 2.1 seem to be better.\n\nOverall, we strongly recommend just trying the models out and reading up on advice online (e.g. it has been shown that using negative prompts is very important for 2.0 and 2.1 to get the highest possible quality. See for example this nice blog post.\n\nAdditionally, the community has started fine-tuning many of the above versions on certain styles with some of them having an extremely high quality and gaining a lot of traction.\n\nWe recommend having a look at all diffusers checkpoints sorted by downloads and trying out the different checkpoints.\n\nFor the following, we will stick to v1.5 for simplicity.\n\nNext, we can also try to optimize single components of the pipeline, e.g. switching out the latent decoder. For more details on how the whole Stable Diffusion pipeline works, please have a look at this blog post.\n\nNow we can set it to the vae of the pipeline to use it.\n\nLet‚Äôs run the same prompt as before to compare quality.\n\nSeems like the difference is only very minor, but the new generations are arguably a bit sharper.\n\nOur goal was to generate a photo of an old warrior chief. Let‚Äôs now try to bring a bit more color into the photos and make the look more impressive.\n\nOriginally our prompt was ‚Äùportrait photo of an old warrior chief‚Äú.\n\nTo improve the prompt, it often helps to add cues that could have been used online to save high-quality photos, as well as add more details.\n\n Essentially, when doing prompt engineering, one has to think:\n‚Ä¢ How was the photo or similar photos of the one I want probably stored on the internet?\n‚Ä¢ What additional detail can I give that steers the models into the style that I want?\n\nand let‚Äôs also add some cues that usually help to generate higher quality images.\n\nCool, let‚Äôs now try this prompt.\n\nPretty impressive! We got some very high-quality image generations there. The 2nd image is my personal favorite, so I‚Äôll re-use this seed and see whether I can tweak the prompts slightly by using ‚Äúoldest warrior‚Äù, ‚Äúold‚Äù, \"\", and ‚Äúyoung‚Äù instead of ‚Äúold‚Äù.\n\nThe first picture looks nice! The eye movement slightly changed and looks nice. This finished up our 101-guide on how to use Stable Diffusion ü§ó.\n\nFor more information on optimization or other guides, I recommend taking a look at the following:\n‚Ä¢ FlashAttention: XFormers flash attention can optimize your model even further with more speed and memory improvements.\n‚Ä¢ Dreambooth - Quickly customize the model by fine-tuning it.\n‚Ä¢ General info on Stable Diffusion - Info on other tasks that are powered by Stable Diffusion."
    },
    {
        "link": "https://medium.com/@andrewwongai/how-to-come-up-with-good-prompts-for-ai-image-generation-f28355e46d21",
        "document": "In this post, I‚Äôll teach you how to create good prompts for generating AI art work images. I will base my method on Stable Diffusion, an open-source text-to-image AI model.\n\nStable Diffusion is a text-to-image AI model. It is trained on 2.3 billion image and text description pairs. Because it has seen so much, the model encodes relationship between image pixel values and it‚Äôs text descriptions.\n\nAs a result, if you put in description like ‚ÄúA Photo of a cat sitting on top of a building‚Äù, it would give you images like these:\n\nYou may be thinking what‚Äôs so special about these images? Couldn‚Äôt we get millions of them in a Google search? What‚Äôs intriguing about this technology is that you can prompt the model to generate high quality images that do not exist before. For example, you can ask for a portrait painting of Emma Watson by the 19th century American painter John Singer Sargent:\n\nIt is incredible that such images can be produced from keyword-pixel correlations! What‚Äôs mind-boggling is that it gets the artistic style, faces (which our brains are very unforgiving of tiny mistakes) and shadows correctly, and blends them all together in an aesthetically pleasing manner. I believe that the wonder of large numbers is beyond the comprehension of human minds.\n\nWhat so special about Stable Diffusion?\n\nThis year we have seen a few image generation AI such as DALLE 2 and MidJourney. They too are capable of generating stunning images from text prompts. What‚Äôs so special about Stable Diffusion are\n\nThe implication of these two together is big. You can download the model and run it on your local computer. Now there are PC and Apple version available. There‚Äôs an explosion of free-to-use image generation AI powered by Stable Diffusion. The low cost of run allows entrepreneurs to explore Freemium or Ad-supported business models. The end result is going to be making the AI technology more accessible.\n\nWhere can I try my prompts?\n\nThe easiest way to use Stable Diffusion is via DreamStudio.AI. It is from the creator of Stable Diffusion. You will get some free credits after signing up.\n\nThere are proven techniques to generate high quality, specific images. Your prompt should cover most if not all of these areas\n\nFirst you will need a description of the subject with as much detail as possible. E.g.\n\nWe got the following image, which matches the prompt pretty well.\n\nWe can be more specific. Let‚Äôs add a medium. Some examples are: digital painting, photograph, oil painting. Let‚Äôs use\n\nThe new prompt is\n\nYou can see the image changes from photograph to digital art.\n\nYou get the idea. Let‚Äôs define the rest of them\n\nPutting them all together, the prompt is\n\nBy adding keywords to the prompt, we can engineer the image to get the style we want.\n‚Ä¢ Use multiple exclamation marks !! to stress a word and brackets () to reduce its strength.\n‚Ä¢ Use appropriate medium type consistent with the artist.\n‚Ä¢ Artist name is a very strong style modifier. Use wisely.\n‚Ä¢ Use websites like Lexica to study other people‚Äôs prompts. If you like a particular image, use the prompt as starting point.\n\nSome good keywords for you\n\nBelow are some of my favorite keywords and their effects. (Tested with Stable Diffusion v1.4)\n\nThese keywords further refine the art style.\n\nMentioning the artist in the prompt is a strong effect. Study their work and choose wisely.\n\nMentioning an art or photo site is a strong effect, probably because each site has its niche genre.\n\nIn this post, we have gone through the basic structure of a good prompt. This should be used as a guide rather than rules. The Stable Diffusion model is very flexible. Let it surprise you with some creative combination of keywords!\n\nIf you have problem generating stunning artworks, this Stable Diffusion prompt generator would be able to help you. In the next post, I will show you how to make this prompt generator using Notion."
    },
    {
        "link": "https://reddit.com/r/StableDiffusion/comments/14tol5n/best_text_prompt_for_creating_stable_diffusion",
        "document": "I've played around with this and have a decent one that worked well with GPT4...it is VERY long and the local LLMs I tried back then all choked on it. What do you use? Curious about what other people have come up with as a text prompt template to get a textual AI to respond with a solid Stable Diffusion prompt when requested with no or some parts of the prompt provided. Thanks.\n\nHere is my current one. Always looking to make it better and smaller:\n\nOne example I just tied with GPT4, you can see it's not perfect, but it's something...\n\nHere is share-link: https://chat.openai.com/share/db7022b7-a418-4f24-817d-8e2f490d6966 (may help for formatting it for you if you are on mobile to view the link)"
    },
    {
        "link": "https://zapier.com/blog/ai-art-prompts",
        "document": "See how Zapier can take your RevOps and GTM engine to the next level"
    },
    {
        "link": "https://hunch.tools/blog/top-10-ai-prompting-tips-for-image-gen",
        "document": "If a picture is worth a thousand words, then the inverse is also true. Though that might be an exaggeration for a prompt for AI image generation, typically the more descriptive details, the better the resulting image will be.\n\nTop tips for maximizing image accuracy with models like FLUX, DALLE-3, Stable Diffusion, and Ideogram.\n‚Ä¢ Be specific and descriptive: Provide clear details about what you want to see in the image using straightforward language, without any contradictory or confusing descriptors. Provide enough information to guide the creation process, but avoid overly long or complicated descriptions.\n‚Ä¢ Order matters: Prioritize important elements by mentioning the most crucial aspects of the image first.\n‚Ä¢ Use adjectives: Colors, textures, lighting conditions, and mood (e.g., dramatic, serene, nostalgic) all help towards better image accuracy. Include dynamic poses or movements, if needed.\n‚Ä¢ Provide stylistic categories: Mention artistic styles, techniques, mediums, or inspirations you want to emulate, such as \"oil painting,\" \"watercolor,\" and \"photorealistic.\"\n‚Ä¢ Reference photography techniques: Mention camera types, film stocks, or digital effects when applicable. Include terms like \"high contrast,\" \"soft focus,\" or \"vibrant colors.\"\n‚Ä¢ Incorporate symbolism and concepts: Suggest conceptual elements (e.g., lifecycle, coexistence of natural and artificial). Use metaphors or symbolic representations when appropriate.\n‚Ä¢ Include contextual information and composition: Setting, time of day, atmosphere or weather conditions, layout, perspective (e.g., top-down view, low angle), arrangement of elements (e.g., vertical alignment, centered), and framing all contribute to higher-quality images.\n‚Ä¢ Use creative combinations: Combine and mix unexpected elements to create a rich, multifaceted image. Balance main subjects with background elements and details.\n‚Ä¢ Reference specific examples: Refer to artists, art movements, or specific works for inspiration. This can help guide the style and composition.\n‚Ä¢ Organize information logically: Effective prompt structure can significantly impact the quality of generated images. Avoid ambiguity and complex sentence structures.\n‚Ä¢ Consider technical aspects: Mention the desired format (e.g., logo, 3D scene, analog photo). Include any specific design elements or techniques (e.g., vector illustration, line art).\n‚Ä¢ Use proper grammar and punctuation: This can affect how the AI interprets your prompt. For example, make sure to use commas to help separate different elements of the description.\n‚Ä¢ Leverage multiple models: Jeremiah Harvey, a frequent Hunch user, suggests that you can get great results with FLUX.1 by first feeding a reference image into GPT-4o, having it output a detailed description of the image, then using that description to prompt FLUX.1.\n‚Ä¢ Iterate and refine: Use the initial results to adjust your prompt and experiment with different phrasings by trying various ways to describe your desired outcome.\n‚Ä¢ Poor prompt: \"A cat\"\n\nGood prompt: \"A fluffy orange tabby cat sitting on a velvet cushion, bathed in warm sunlight streaming through a Victorian window\"\n‚Ä¢ Poor prompt: \"A sunset and a lake\n\nGood prompt: \"A vibrant sunset over a serene lake, with silhouettes of pine trees reflected in the water, painted in watercolor style\"\n‚Ä¢ Poor prompt: \"A picture of a elderly man\"\n\nGood prompt: \"A close-up portrait of an elderly man with deep wrinkles, twinkling blue eyes, and a warm smile, photographed in black and white with dramatic lighting\"\n\nThe effectiveness of prompts can vary between models and even between different versions of the same model. As with all prompting best practices, it's always a good idea to experiment and adapt your approach based on the results you get."
    },
    {
        "link": "https://quora.com/How-do-I-make-a-unique-and-good-prompt-for-an-AI-images-generator",
        "document": "Something went wrong. Wait a moment and try again."
    }
]