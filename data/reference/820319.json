[
    {
        "link": "https://discuss.huggingface.co/t/inference-api-stopped-working-for-my-model/37610",
        "document": "I have a pinned model ( ) on the inference API that has been working well for over a year, but it recently stopped workingâ€¦ When I make a request like this: I get a 503 error telling me that the model is currently loadingâ€¦ But the model never seems to fully load, and the â€œestimated_timeâ€ never changes.\n\nhi @benjismith , could you make the model public? Another way to test it, is to run the steps on our public api-inference docs GitHub - huggingface/api-inference-community\n\n This is how Iâ€™d test your model if the main backend is not responding.\n\nYes, that repository is part of what drives the hosted API inference, so itâ€™s useful for testing if thereâ€™s something wrong with the model. However, in your case, it seems that the model loading is timing out due to its size of . Iâ€™m not sure what the constraints are, perhaps @Narsil could provide more information. One last thing to try would be to convert your model to safetensors, as this can improve loading time, and then attempt use the hosted API. Convert to Safetensors - a Hugging Face Space by safetensors\n\nBTW, Iâ€™m also interested in potentially setting up Inference Endpoints to host this model when it goes into productionâ€¦ But I donâ€™t know what size GPU instance would be necessary. This is a fine-tuned GPT-J-6B model, whose weights have been converted to FP16. Iâ€™m also working on another model which will be based on GPT-NeoX-20B, and I have the same question. What size instances will I need when I setup production Inference Endpoints?\n\nIâ€™ve clicked that â€œConvert to Safetensorsâ€ button about 50 times since yesterdayâ€¦ Most of the time I get the same â€œtoo busyâ€ error. Occasionally, my job goes into the queue but then it times out before running. Is this the only way to restore the functionality of my model? It was running just fine for over a year and only recently disappearedâ€¦"
    },
    {
        "link": "https://discuss.huggingface.co/t/inference-api-in-javascript/11537",
        "document": "Yes, the Inference API is just using usual HTTP requests, so you can use Python, JavaScript or even direct curl requests to use it. You can find more about the structure of the requests at the API documentation.\n\nHere you can find a Gist showing how to use a model. Itâ€™s a good self contained example. Here would be a minimal version for bert. There are multiple mechanisms to make HTTP requests in JavaScript and Iâ€™m not an expert in the area, but you get the idea."
    },
    {
        "link": "https://discuss.huggingface.co/t/what-is-model-is-currently-loading/13917",
        "document": "Hi.\n\n Iâ€™m a beginner at NLP.\n\n Iâ€™m going to summarize sentences using T5 modelâ€™s information API.\n\n The model is currently loading keeps popping up and does not proceed.\n\n Can you tell me what this error is?\n\n also\n\n I want to summarize more than 5,000 characters into 1,000 to 2,000 characters.\n\n How should I write the parameters?\n\n@Doogie\n\n Hello \n\n Inference API loads models on demand, so if itâ€™s your first time using it in a while it will load the model first, then you can try to send the request again in couple of seconds.\n\nAs per Detailed parameters â€” Api inference documentation, you can use the option to wait for the response instead of having to do multiple requests.\n\nI hope this model is always ready.\n\n What should I do to do that?\n\n Model should always show me response immediately.\n\nis documented in the link shared above. If false, you will get a 503 when itâ€™s loading. If true, your process will hang waiting for the response, which might take a bit while the model is loading. You can pin models for instant loading (see Hugging Face â€“ Pricing)\n\nI get a message that wait_for_model is no longer valid {â€˜inputsâ€™: {â€˜past_user_inputsâ€™: , â€˜generated_responsesâ€™: , â€˜textâ€™: â€˜yoâ€™}, â€˜parametersâ€™: {â€˜min_lengthâ€™: 1, â€˜max_lengthâ€™: 500, â€˜repetition_penaltyâ€™: 50.0, â€˜temperatureâ€™: 50.0, â€˜use_cacheâ€™: True, â€˜wait_for_modelâ€™: True}}\n\n {â€œerrorâ€: â€œThe following are not used by the model: [â€˜wait_for_modelâ€™] (note: typos in the generate arguments will also show up in this list)â€}\n\nHi. With â€œtogethercomputer/GPT-NeoXT-Chat-Base-20Bâ€ Iâ€™m using the â€œwait_for_modelâ€ parameter set to true, but I still have the â€œModel is currently loadingâ€. Is it because the model is too big ?\n\nIâ€™ve set the parameter to True in the payload in the same way as @deseipel and it doesnâ€™t work for me either. I donâ€™t get a specific error about the request, I just get the usual 503 error in response: â€œModel is currently loadingâ€.\n\nI have finally managed to use the flag - the â€œparametersâ€ dictionary actually needs to be called the â€œoptionsâ€ dictionary according to the documentation https://huggingface.co/docs/api-inference/detailed_parameters However, after overcoming that error and duly waiting for a response, I waited, and got a 504 error instead: Gateway Timeout. Is HF down today?"
    },
    {
        "link": "https://github.com/huggingface/huggingface.js/issues/5",
        "document": "Important: A pro account is required to use and test streaming. I began a partial implementation to add streaming support several months ago. Leaving this patch below for future reference."
    },
    {
        "link": "https://discuss.huggingface.co/t/error-model-is-loading-inference-api/90424",
        "document": "Here is the model I am using for a product - speechbrain/lang-id-voxlingua107-ecapa Â· Hugging Face.\n\nI see a 503 error for the first few times (the model is loading). I see a keyword argument â€œwait_for_modelâ€ in the text modality models, but I donâ€™t see such a thing here.\n\nWhat is the workaround for this? How to solve this gracefully so that the user experience is not hampered much"
    },
    {
        "link": "https://stackoverflow.com/questions/17494732/how-to-make-a-loading-indicator-for-every-asynchronous-action-using-q-in-an-a",
        "document": "I know there are several approaches to loading-indicators in angular js (this one, for example: https://gist.github.com/maikeldaloo/5140733).\n\nBut they either have to be configured for every single call, or - if they act globally, as I want - just apply to http-requests, but not to $q-promises being used in services.\n\nThe global loading indicators, I've seen so far, work with\n\nIs there something similiar for , like a ? And if not, what would be the most convenient way to implement such a functionality? Is it possible to use a decorator-pattern of some kind for example?"
    },
    {
        "link": "https://stackoverflow.com/questions/8019078/how-to-display-loading-when-making-a-synchronous-ajax-call-in-pure-javascrip",
        "document": "I want to make sure the result is shown to user, so I make synchronous AJAX call. It's quite simple to display a 'Loading' indicator with asynchronous AJAX (the examples are everywhere), but when I use synchronous AJAX call with XMLHttpRequest, the loading indicator GIF image doesn't show up at all.\n\nSome said that it's impossible to show indicator when doing a synchronous call (block until having response from server). But I just want to ask to see whether there's a way to do it."
    },
    {
        "link": "https://dev.to/hariseldon27/custom-loading-animation-on-fetch-call-in-vanilla-javascript-css-html-5-1a9n",
        "document": "Do you have an API call that is sorta slow, and you want to keep your user occupied while it loads and you're not quite sure how to do it? Well, it's easier than you'd think!\n\nWhy a custom loading animation for vanilla javascript? Meh...why not?\n\nThis blog post comes out of the project that I co-created for the first phase of the Flatiron School Software Engineer Program. We built a game that called the Met Museum API, which was really neat! Unfortunately, it had to make 4-7 API calls per game-round to make sure it got enough clean data. Eventually it all worked, but it took as much as 5-seconds for the API calls to return and the Dom to update.\n\nSeeing those gross loading times puts us in a good place to explore where to go next.\n\nThe biggest UX issue we have is that when you finish a round the game appears to be frozen while it's waiting for the new data to load.\n\nEven though the final version of the game implements a few modal windows to occupy the user, we wanted to really cover the loading time with some branded loading animation. As it turned out it was pretty straight forward to hook a loading animation into the asynchronous fetch call.\n\nNow, let's be straight: there's lots of ways to implement a loading spinner - this way is fine for small special applications. Is it what I'd use in any kind of larger deployment, even a vanilla javascript one? No, no I probably wouldn't But, this is a cool little trick to get things going in a single page application.\n\nWhat are we doing for a loading animation?\n\nOur loading animation is pretty simple, we're going to embed an animated SVG in a hidden div, and when we call it with Javascript it goes from hidden to visible. Pretty easy - let's see how it's done!\n\n^Sounds like the title of a Russian fairy tale.\n\n\n\n So, how about that animated SVG I keep talking about, what's that all about? Let's see:\n\n SVG stands for Scalable Vector Graphics, and it's basically an inbuilt vector rendering engine that is XML based. So, what it takes to get it going is the same as everything else around here, a bit of code.\n\nTry this one below, it comes from Mozilla MDN and is a lovely example.\n\nWhat the heck, let's use that as part of our example. Put that svg into a at the bottom of your body section, set it to and then set a few more options to get the background to cover the page. And the easy way to do it is to simply change the to a (or whatever you need) when the right moment occurs in your script.\n\nSee it in action on repl.it here:\n\n\n\nHooking it to the Fetch Request\n\nYeah yeah yeah - okay we get it - it's easy as pie to make a hidden div appear. And yeah, we get it - maybe it isn't the cleanest way to do it, but if when push comes to shove and you can only have one file it's a worthwhile tool to have around.\n\nSo how do we go about hooking the appearance of our cool new loading animation that we borrowed from MDN? Well, it's pretty easy actually. But, it's worth talking a little about about how to ensure you are making an asynchronous fetch request first.\n\nIn order to improve the user experience of sites that rely on calls to servers or APIs for information, Javascript gives us a neat way to manage what code is running when.\n\nUnder normal circumstances our code runs in more or less a linear fashion in Javascript - aside from neat features like function hoisting and scope. However, we can ask Javascript to treat a function asynchronously from the other code that it's being asked to execute. This also means we can ask other functions to await that asynchronous code to finish before it tries to complete its task. We will make use of this in our fetch call, especially with our loader in mind.\n\nTo move your fetch function into the asynchronous pile put before the function declaration like so:\n\nThen in our callback function we can simply remind it that it needs to that fetch to complete before it tries to run.\n\nSo where do we hook in the loading animation? Well, we've already got our fancy-schmancy show/hide functions in JS if you remember.\n\nThose and functions can just go at the points where it makes sense: show it right after the fetch, and hide it right after we finish populating the dom with data.\n\nIn our Met Guess game we fire the function right after our very first fetch request, and we don't run the function until after the API returns data. What could we have done better? Well, look at the console logs and see if you can guess! If you said, have the spinner appear immediately and then stay up until the DOM render completed, you're right...always room for improvement around here!\n\nIn this post we've talked about why we might want to use a loading animation, we looked at one sorta janky way to implement it, and more importantly we looked at a way to hook your loading animation into the fetch call of your Javascript.\n\nHave questions? Me too! Let's talk.\n\nI respond to any and all responses!"
    },
    {
        "link": "https://digitalocean.com/community/tutorials/how-to-handle-async-data-loading-lazy-loading-and-code-splitting-with-react",
        "document": "The author selected Creative Commons to receive a donation as part of the Write for DOnations program.\n\nAs a JavaScript web developer, asynchronous code gives you the ability to run some parts of your code while other parts are still waiting for data or resolving. This means that important parts of your app will not have to wait for less important parts before they render. With asynchronous code you can also update your application by requesting and displaying new information, giving users a smooth experience even when long functions and requests are processing in the background.\n\nIn React development, asynchronous programming presents unique problems. When you use React functional components for example, asynchronous functions can create infinite loops. When a component loads, it can start an asynchronous function, and when the asynchronous function resolves it can trigger a re-render that will cause the component to recall the asynchronous function. This tutorial will explain how to avoid this with a special Hook called , which will run functions only when specific data changes. This will let you run your asynchronous code deliberately instead of on each render cycle.\n\nAsynchronous code is not just limited to requests for new data. React has a built-in system for lazy loading components, or loading them only when the user needs them. When combined with the default webpack configuration in Create React App, you can split up your code, reducing a large application into smaller pieces that can be loaded as needed. React has a special component called that will display placeholders while the browser is loading your new component. In future versions of React, youâ€™ll be able to use to load data in nested components without render blocking.\n\nIn this tutorial, youâ€™ll handle asynchronous data in React by creating an app that displays information on rivers and simulates requests to Web APIs with . By the end of this tutorial, youâ€™ll be able to load asynchronous data using the Hook. Youâ€™ll also be able to safely update the page without creating errors if the component unmounts before data resolution. Finally, youâ€™ll split a large application into smaller parts using code splitting.\nâ€¢ You will need a development environment running Node.js; this tutorial was tested on Node.js version 10.20.1 and npm version 6.14.4. To install this on macOS or Ubuntu 18.04, follow the steps in How to Install Node.js and Create a Local Development Environment on macOS or the Installing Using a PPA section of How To Install Node.js on Ubuntu 18.04.\nâ€¢ A React development environment set up with Create React App, with the non-essential boilerplate removed. To set this up, follow Step 1 â€” Creating an Empty Project of the How To Manage State on React Class Components tutorial. This tutorial will use as the project name.\nâ€¢ You will be using React events and Hooks, including the and the Hooks. You can learn about events in our How To Handle DOM and Window Events with React tutorial, and Hooks at How to Manage State with Hooks on React Components.\nâ€¢ You will also need a basic knowledge of JavaScript and HTML, which you can find in our How To Build a Website with HTML series and in How To Code in JavaScript. Basic knowledge of CSS would also be useful, which you can find at the Mozilla Developer Network.\n\nIn this step, youâ€™ll use the Hook to load asynchronous data into a sample application. Youâ€™ll use the Hook to prevent unnecessary data fetching, add placeholders while the data is loading, and update the component when the data resolves. By the end of this step, youâ€™ll be able to load data with and set data using the Hook when it resolves.\n\nTo explore the topic, you are going to create an application to display information about the longest rivers in the world. Youâ€™ll load data using an asynchronous function that simulates a request to an external data source.\n\nSave and close the file. Now you need to import and render the new component to your root component. Open :\n\nImport and render the component by adding in the highlighted code:\n\nFinally, in order to make the app easier to read, add some styling. Open :\n\nAdd some padding to the class by replacing the CSS with the following:\n\nSave and close the file. When you do, the browser will refresh and render the basic components.\n\nIn this tutorial, youâ€™ll make generic services for returning data. A service refers to any code that can be reused to accomplish a specific task. Your component doesnâ€™t need to know how the service gets its information. All it needs to know is that the service will return a Promise. In this case, the data request will be simulated with , which will wait for a specified amount of time before providing data.\n\nCreate a new directory called under the directory:\n\nThis directory will hold your asynchronous functions. Open a file called :\n\nInside the file, export a function called that returns a promise. Inside the promise, add a function that will resolve the promise after milliseconds. This will give you some time to see how the component will render while waiting for data to resolve:\n\nIn this snippet, you are hard-coding the river information, but this function will be similar to any asynchronous functions you may use, such as an API call. The important part is that the code returns a promise.\n\nNow that you have a service that returns the data, you need to add it to your component. This can sometimes lead to a problem. Suppose you called the asynchronous function inside of your component and then set the data to a variable using the Hook. The code will be like this:\n\nWhen you set the data, the Hook change will trigger a components re-render. When the component re-renders, the function will run again, and when it resolves it will set the state, which will trigger another re-render. The loop will continue forever.\n\nTo solve this problem, React has a special Hook called that will only run when specific data changes.\n\nThe Hook accepts a function as the first argument and an array of triggers as the second argument. The function will run on the first render after the layout and paint. After that, it will only run if one of the triggers changes. If you supply an empty array, it will only run one time. If you do not include an array of triggers, it will run after every render.\n\nUse the Hook to create a variable called and a function called . Youâ€™ll update the component by setting the when the asynchronous function resolves. Then wrap the function with . Be sure to pass an empty array as a second argument. When the promise resolves, update the with the function:\n\nAfter the asynchronous function resolves, update an unordered list with the new information.\n\nSave and close the file. When you do the browser will refresh and youâ€™ll find the data after the function resolves:\n\nNotice that the component renders before the data is loaded. The advantage with asynchronous code is that it wonâ€™t block the initial render. In this case, you have a component that shows the list without any data, but you could also render a spinner or a scalable vector graphic (SVG) placeholder.\n\nThere are times when youâ€™ll only need to load data once, such as if you are getting user information or a list of resources that never change. But many times your asynchronous function will require some arguments. In those cases, youâ€™ll need to trigger your use Hook whenever the data changes.\n\nTo simulate this, add some more data to your service. Open :\n\nThen add an object that contains data for a few more rivers. Select the data based on a argument:\n\nSave and close the file. Next, open so you can add more options:\n\nInside , create a stateful variable and function to hold the selected river with the Hook. Then add a button for each river with an handler to update the selected river. Pass the to using a prop called :\n\nSave and close the file. Next, open :\n\nPull in the as a prop and pass it to the function. Be sure to add to the array for , otherwise it will not rerun:\n\nIn this code, you also added a weak typing system with , which will make sure that the prop is a string.\n\nSave the file. When you do, the browser will refresh and you can select different rivers. Notice the delay between when you click and when the data renders:\n\nIf you had left out the prop from the array, you would receive a build error in the browser console. It would be something like this:\n\nThis error tells you that the function in your effect has dependencies that you are not explicitly setting. In this situation, itâ€™s clear that the effect wouldnâ€™t work, but there are times when you may be comparing prop data to stateful data inside the component, which makes it possible to lose track of items in the array.\n\nThe last thing to do is to add some defensive programming to your component. This is a design principle that emphasizes high availability for your application. You want to ensure that your component will render even if the data is not in the correct shape or if you do not get any data at all from an API request.\n\nAs your app is now, the effect will update the with any type of data it receives. This will usually be an object, but in cases where itâ€™s not, you can use optional chaining to ensure that you will not throw an error.\n\nInside , replace the instance of an object dot chaining with optional chaining. To test if it works, remove the default object from the function:\n\nSave and close the file. When you do, the file will still load even though the code is referencing properties on instead of an object:\n\nDefensive programming is usually considered a best practice, but itâ€™s especially important on asynchronous functions such as API calls when you canâ€™t guarantee a response.\n\nIn this step, you called asynchronous functions in React. You used the Hook to fetch information without triggering re-renders and triggered a new update by adding conditions to the array.\n\nIn the next step, youâ€™ll make some changes to your app so that it updates components only when they are mounted. This will help your app avoid memory leaks.\n\nIn this step, youâ€™ll prevent data updates on unmounted components. Since you can never be sure when data will resolve with asynchronous programming, thereâ€™s always a risk that the data will resolve after the component has been removed. Updating data on an unmounted component is inefficient and can introduce memory leaks in which your app is using more memory than it needs to.\n\nBy the end of this step, youâ€™ll know how to prevent memory leaks by adding guards in your Hook to update data only when the component is mounted.\n\nThe current component will always be mounted, so thereâ€™s no chance that the code will try and update the component after it is removed from the DOM, but most components arenâ€™t so reliable. They will be added and removed from the page as the user interacts with the application. If a component is removed from a page before the asynchronous function resolves, you can have a memory leak.\n\nTo test out the problem, update to be able to add and remove the river details.\n\nAdd a button to toggle the river details. Use the Hook to create a function to toggle the details and a variable to store the toggled state:\n\nSave the file. When you do the browse will reload and youâ€™ll be able to toggle the details.\n\nClick on a river, then immediately click on the Toggle Details button to hide details. React will generate an error warning that there is a potential memory leak.\n\nTo fix the problem you need to either cancel or ignore the asynchronous function inside . If you are using a library such as RxJS, you can cancel an asynchronous action when the component unmounts by returning a function in your Hook. In other cases, youâ€™ll need a variable to store the mounted state.\n\nInside the function, create a variable called and set it to . Inside the callback, use a conditional to set the data if is true:\n\nNow that you have the variable, you need to be able to flip it when the component unmounts. With the Hook, you can return a function that will run when the component unmounts. Return a function that sets to :\n\nSave the file. When you do, youâ€™ll be able to toggle the details without an error.\n\nWhen you unmount, the component updates the variable. The asynchronous function will still resolve, but it wonâ€™t make any changes to unmounted components. This will prevent memory leaks.\n\nIn this step, you made your app update state only when a component is mounted. You updated the Hook to track if the component is mounted and returned a function to update the value when the component unmounts.\n\nIn the next step, youâ€™ll asynchronously load components to split code into smaller bundles that a user will load as needed.\n\nIn this step, youâ€™ll split your code with React and . As applications grow, the size of the final build grows with it. Rather than forcing users to download the whole application, you can split the code into smaller chunks. React and work with webpack and other build systems to split your code into smaller pieces that a user will be able to load on demand. In the future, you will be able to use to load a variety of data, including API requests.\n\nBy the end of this step, youâ€™ll be able to load components asynchronously, breaking large applications into smaller, more focused chunks.\n\nSo far youâ€™ve only worked with asynchronously loading data, but you can also asynchronously load components. This process, often called code splitting, helps reduce the size of your code bundles so your users donâ€™t have to download the full application if they are only using a portion of it.\n\nMost of the time, you import code statically, but you can import code dynamically by calling as a function instead of a statement. The code would be something like this:\n\nReact gives you an additional set of tools called and . React will eventually expand to handle data loading, but for now you can use it to load components.\n\nThen import and from :\n\nand have two distinct jobs. You use the function to dynamically import the component and set it to a variable. is a built-in component you use to display a fallback message while the code is loading.\n\nReplace with a call to . Assign the result to a variable called . Then wrap with the component and a with a message of to the prop:\n\nSave the file. When you do, reload the page and youâ€™ll find that the component is dynamically loaded. If you want to see the loading message, you can throttle the response in the Chrome web browser.\n\nIf you navigate to the Network tab in Chrome or Firefox, youâ€™ll find that the code is broken into different chunks.\n\nEach chunk gets a number by default, but with Create React App combined with webpack, you can set the chunk name by adding a comment by the dynamic import.\n\nIn , add a comment of inside the function:\n\nSave and close the file. When you do, the browser will refresh and the chunk will have a unique name.\n\nIn this step, you asynchronously loaded components. You used and to dynamically import components and to show a loading message while the component loads. You also gave custom names to webpack chunks to improve readability and debugging.\n\nAsynchronous functions create efficient user-friendly applications. However, their advantages come with some subtle costs that can evolve into bugs in your program. You now have tools that will let you split large applications into smaller pieces and load asynchronous data while still giving the user a visible application. You can use the knowledge to incorporate API requests and asynchronous data manipulations into your applications creating fast and reliable user experiences.\n\nIf you would like to read more React tutorials, check out our React Topic page, or return to the How To Code in React.js series page."
    },
    {
        "link": "https://basefactor.com/react-how-to-display-a-loading-indicator-on-fetch-calls",
        "document": ""
    },
    {
        "link": "https://huggingface.co/docs/transformers/en/main_classes/model",
        "document": ""
    },
    {
        "link": "https://huggingface.co/docs/transformers.js/en/pipelines",
        "document": "Just like the transformers Python library, Transformers.js provides users with a simple way to leverage the power of transformers. The function is the easiest and fastest way to use a pretrained model for inference.\n\nStart by creating an instance of and specifying a task you want to use it for. For example, to create a sentiment analysis pipeline, you can do:\n\nWhen running for the first time, the will download and cache the default pretrained model associated with the task. This can take a while, but subsequent calls will be much faster.\n\nYou can now use the classifier on your target text by calling it as a function:\n\nIf you have multiple inputs, you can pass them as an array:\n\nYou can also specify a different model to use for the pipeline by passing it as the second argument to the function. For example, to use a different model for sentiment analysis (like one trained to predict sentiment of a review as a number of stars between 1 and 5), you can do:\n\nTransformers.js supports loading any model hosted on the Hugging Face Hub, provided it has ONNX weights (located in a subfolder called ). For more information on how to convert your PyTorch, TensorFlow, or JAX model to ONNX, see the conversion section.\n\nThe function is a great way to quickly use a pretrained model for inference, as it takes care of all the preprocessing and postprocessing for you. For example, if you want to perform Automatic Speech Recognition (ASR) using OpenAIâ€™s Whisper model, you can do:\n\nWe offer a variety of options to control how models are loaded from the Hugging Face Hub (or locally). By default, when running in-browser, a quantized version of the model is used, which is smaller and faster, but usually less accurate. To override this behaviour (i.e., use the unquantized model), you can use a custom object as the third parameter to the function:\n\nCheck out the section on quantization to learn more.\n\nYou can also specify which revision of the model to use, by passing a parameter. Since the Hugging Face Hub uses a git-based versioning system, you can use any valid git revision specifier (e.g., branch name or commit hash).\n\nFor the full list of options, check out the PretrainedOptions documentation.\n\nMany pipelines have additional options that you can specify. For example, when using a model that does multilingual translation, you can specify the source and target languages like this:\n\nWhen using models that support auto-regressive generation, you can specify generation parameters like the number of new tokens, sampling methods, temperature, repetition penalty, and much more. For a full list of available parameters, see to the GenerationConfig class.\n\nFor example, to generate a poem using , you can do:\n\nLogging to the console gives:\n\nSome pipelines such as or support streaming output. This is achieved using the class. For example, when using a chat model like , you can specify a callback function that will be called with each generated token text (if unset, new tokens will be printed to the console).\n\nLogging to the console gives:\n\nThis streaming feature allows you to process the output as it is generated, rather than waiting for the entire output to be generated before processing it.\n\nFor more information on the available options for each pipeline, refer to the API Reference. If you would like more control over the inference process, you can use the , , or classes instead."
    },
    {
        "link": "https://huggingface.co/docs/transformers/main/en/main_classes/model",
        "document": "and get access to the augmented documentation experience\n\nThe base classes PreTrainedModel, TFPreTrainedModel, and FlaxPreTrainedModel implement the common methods for loading/saving a model either from a local file or directory, or from a pretrained model configuration provided by the library (downloaded from HuggingFaceâ€™s AWS S3 repository).\n\nPreTrainedModel and TFPreTrainedModel also implement a few methods which are common among all the models to:\nâ€¢ resize the input token embeddings when new tokens are added to the vocabulary\nâ€¢ prune the attention heads of the model.\n\nThe other methods that are common to each model are defined in ModuleUtilsMixin (for the PyTorch models) and (for the TensorFlow models) or for text generation, GenerationMixin (for the PyTorch models), TFGenerationMixin (for the TensorFlow models) and FlaxGenerationMixin (for the Flax/JAX models).\n\nCustom models should also include a , which determines if superfast init can apply on the particular model. Signs that your model needs this are if fails. If so, set this to .\n\nFlaxPreTrainedModel takes care of storing the configuration of the models and handles methods for loading, downloading and saving models.\nâ€¢ config_class (PretrainedConfig) â€” A subclass of PretrainedConfig to use as configuration class for this model architecture.\nâ€¢ base_model_prefix ( ) â€” A string indicating the attribute associated to the base model in derived classes of the same architecture adding modules on top of the base model.\nâ€¢ main_input_name ( ) â€” The name of the principal input to the model (often for NLP models, for vision models and for speech models).\nâ€¢ None repo_id ( ) â€” The name of the repository you want to push your model to. It should contain your organization name when pushing to a given organization.\nâ€¢ None use_temp_dir ( , optional) â€” Whether or not to use a temporary directory to store the files saved before they are pushed to the Hub. Will default to if there is no directory named like , otherwise.\nâ€¢ None commit_message ( , optional) â€” Message to commit while pushing. Will default to .\nâ€¢ None private ( , optional) â€” Whether to make the repo private. If (default), the repo will be public unless the organizationâ€™s default is private. This value is ignored if the repo already exists.\nâ€¢ None token ( or , optional) â€” The token to use as HTTP bearer authorization for remote files. If , will use the token generated when running (stored in ). Will default to if is not specified.\nâ€¢ None max_shard_size ( or , optional, defaults to ) â€” Only applicable for models. The maximum size for a checkpoint before being sharded. Checkpoints shard will then be each of size lower than this size. If expressed as a string, needs to be digits followed by a unit (like ). We default it to so that users can easily load models on free-tier Google Colab instances without any CPU OOM issues.\nâ€¢ None create_pr ( , optional, defaults to ) â€” Whether or not to create a PR with the uploaded files or directly commit.\nâ€¢ None safe_serialization ( , optional, defaults to ) â€” Whether or not to convert the model weights in safetensors format for safer serialization.\nâ€¢ None revision ( , optional) â€” Branch to push the uploaded files to.\nâ€¢ None commit_description ( , optional) â€” The description of the commit that will be created\nâ€¢ None tags ( , optional) â€” List of tags to push on the Hub. Upload the model checkpoint to the ğŸ¤— Model Hub. Returns whether this model can generate sequences with . Returns: : Whether this model can generate sequences with .\nâ€¢ None pretrained_model_name_or_path ( or ) â€” Can be either:\nâ€¢ A string, the model id of a pretrained model hosted inside a model repo on huggingface.co.\nâ€¢ A path to a directory containing model weights saved using save_pretrained(), e.g., .\nâ€¢ A path or url to a pt index checkpoint file (e.g, ). In this case, should be set to .\nâ€¢ None dtype ( , optional, defaults to ) â€” The data type of the computation. Can be one of , (on GPUs) and (on TPUs). , optional, defaults to) â€” The data type of the computation. Can be one of(on GPUs) and(on TPUs). This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If specified all the computation will be performed with the given . Note that this only specifies the dtype of the computation and does not influence the dtype of model parameters. If you wish to change the dtype of the model parameters, see to_fp16() and to_bf16().\nâ€¢ None model_args (sequence of positional arguments, optional) â€” All remaining positional arguments will be passed to the underlying modelâ€™s method.\nâ€¢ None config ( , optional) â€” Can be either:\nâ€¢ an instance of a class derived from PretrainedConfig,\nâ€¢ a string or path valid as input to from_pretrained(). , optional) â€” Can be either: Configuration for the model to use instead of an automatically loaded configuration. Configuration can be automatically loaded when:\nâ€¢ The model is a model provided by the library (loaded with the model id string of a pretrained model).\nâ€¢ The model was saved using save_pretrained() and is reloaded by supplying the save directory.\nâ€¢ The model is loaded by supplying a local directory as and a configuration JSON file named config.json is found in the directory.\nâ€¢ None cache_dir ( , optional) â€” Path to a directory in which a downloaded pretrained model configuration should be cached if the standard cache should not be used.\nâ€¢ None from_pt ( , optional, defaults to ) â€” Load the model weights from a PyTorch checkpoint save file (see docstring of argument).\nâ€¢ None ignore_mismatched_sizes ( , optional, defaults to ) â€” Whether or not to raise an error if some of the weights from the checkpoint do not have the same size as the weights of the model (if for instance, you are instantiating a model with 10 labels from a checkpoint with 3 labels).\nâ€¢ None force_download ( , optional, defaults to ) â€” Whether or not to force the (re-)download of the model weights and configuration files, overriding the cached versions if they exist.\nâ€¢ None resume_download â€” Deprecated and ignored. All downloads are now resumed by default when possible. Will be removed in v5 of Transformers.\nâ€¢ None proxies ( , optional) â€” A dictionary of proxy servers to use by protocol or endpoint, e.g., . The proxies are used on each request.\nâ€¢ None local_files_only( , optional, defaults to ) â€” Whether or not to only look at local files (i.e., do not try to download the model).\nâ€¢ None token ( or , optional) â€” The token to use as HTTP bearer authorization for remote files. If , or not specified, will use the token generated when running (stored in ).\nâ€¢ None revision ( , optional, defaults to ) â€” The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a git-based system for storing models and other artifacts on huggingface.co, so can be any identifier allowed by git. The warning Weights from XXX not initialized from pretrained model means that the weights of XXX do not come pretrained with the rest of the model. It is up to you to train those weights with a downstream fine-tuning task. The warning Weights from XXX not used in YYY means that the layer XXX is not used by YYY, therefore those weights are discarded.\nâ€¢ None shard_files ( â€” The list of shard files to load. A nested dictionary of the model parameters, in the expected format for flax models : . This is the same as (https:lax.readthedocs.io/en/latest/_modules/flax/serialization.html#from_bytes) but for a sharded checkpoint. This load is performed efficiently: each checkpoint shard is loaded one by one in RAM and deleted after being loaded in the model.\nâ€¢ None auto_class ( or , optional, defaults to ) â€” The auto class to register this new model with. Register this class with a given auto class. This should only be used for custom models as the ones in the library are already mapped with an auto class. This API is experimental and may have some slight breaking changes in the next releases.\nâ€¢ None save_directory ( or ) â€” Directory to which to save. Will be created if it doesnâ€™t exist.\nâ€¢ None push_to_hub ( , optional, defaults to ) â€” Whether or not to push your model to the Hugging Face model hub after saving it. You can specify the repository you want to push to with (will default to the name of in your namespace).\nâ€¢ None max_shard_size ( or , optional, defaults to ) â€” The maximum size for a checkpoint before being sharded. Checkpoints shard will then be each of size lower than this size. If expressed as a string, needs to be digits followed by a unit (like ). If a single weight of the model is bigger than , it will be in its own checkpoint shard which will be bigger than . or, optional, defaults to) â€” The maximum size for a checkpoint before being sharded. Checkpoints shard will then be each of size lower than this size. If expressed as a string, needs to be digits followed by a unit (like).\nâ€¢ None token ( or , optional) â€” The token to use as HTTP bearer authorization for remote files. If , or not specified, will use the token generated when running (stored in ).\nâ€¢ None kwargs ( , optional) â€” Additional key word arguments passed along to the , optional) â€” Additional key word arguments passed along to the push_to_hub() method.\nâ€¢ None safe_serialization ( , optional, defaults to ) â€” Whether to save the model using or through msgpack. Save a model and its configuration file to a directory, so that it can be re-loaded using the class method\nâ€¢ None mask ( ) â€” A with same structure as the tree. The leaves should be booleans, for params you want to cast, and should be for those you want to skip. Cast the floating-point to . This returns a new tree and does not cast the in place. This method can be used on TPU to explicitly convert the model parameters to bfloat16 precision to do full half-precision training or to save weights in bfloat16 for inference in order to save memory and improve speed.\nâ€¢ None mask ( ) â€” A with same structure as the tree. The leaves should be booleans, for params you want to cast, and should be for those you want to skip Cast the floating-point to . This returns a new tree and does not cast the in place. This method can be used on GPU to explicitly convert the model parameters to float16 precision to do full half-precision training or to save weights in float16 for inference in order to save memory and improve speed.\nâ€¢ None mask ( ) â€” A with same structure as the tree. The leaves should be booleans, for params you want to cast, and should be for those you want to skip Cast the floating-point to . This method can be used to explicitly convert the model parameters to fp32 precision. This returns a new tree and does not cast the in place."
    },
    {
        "link": "https://huggingface.co/docs/datasets/en/package_reference/loading_methods",
        "document": "and get access to the augmented documentation experience\n\n( path: str name: typing.Optional[str] = None data_dir: typing.Optional[str] = None data_files: typing.Union[str, collections.abc.Sequence[str], collections.abc.Mapping[str, typing.Union[str, collections.abc.Sequence[str]]], NoneType] = None split: typing.Union[str, datasets.splits.Split, NoneType] = None cache_dir: typing.Optional[str] = None features: typing.Optional[datasets.features.features.Features] = None download_config: typing.Optional[datasets.download.download_config.DownloadConfig] = None download_mode: typing.Union[datasets.download.download_manager.DownloadMode, str, NoneType] = None verification_mode: typing.Union[datasets.utils.info_utils.VerificationMode, str, NoneType] = None keep_in_memory: typing.Optional[bool] = None save_infos: bool = False revision: typing.Union[str, datasets.utils.version.Version, NoneType] = None token: typing.Union[bool, str, NoneType] = None streaming: bool = False num_proc: typing.Optional[int] = None storage_options: typing.Optional[dict] = None trust_remote_code: typing.Optional[bool] = None **config_kwargs ) â†’ Dataset or DatasetDict\nâ€¢ None path ( ) â€” Path or name of the dataset.\nâ€¢ if is a dataset repository on the HF hub (list all available datasets with ) -> load the dataset from supported files in the repository (csv, json, parquet, etc.) e.g. , a dataset repository on the HF hub containing the data files.\nâ€¢ if is a local directory -> load the dataset from supported files in the directory (csv, json, parquet, etc.) e.g. .\nâ€¢ if is the name of a dataset builder and or is specified (available builders are â€œjsonâ€, â€œcsvâ€, â€œparquetâ€, â€œarrowâ€, â€œtextâ€, â€œxmlâ€, â€œwebdatasetâ€, â€œimagefolderâ€, â€œaudiofolderâ€, â€œvideofolderâ€) -> load the dataset from the files in or e.g. . ) â€” Path or name of the dataset. It can also point to a local dataset script but this is not recommended.\nâ€¢ None name ( , optional) â€” Defining the name of the dataset configuration.\nâ€¢ None data_dir ( , optional) â€” Defining the of the dataset configuration. If specified for the generic builders (csv, text etc.) or the Hub datasets and is , the behavior is equal to passing as to reference all the files in a directory.\nâ€¢ None data_files ( or or , optional) â€” Path(s) to source data file(s).\nâ€¢ None split ( or ) â€” Which split of the data to load. If , will return a with all splits (typically and ). If given, will return a single Dataset. Splits can be combined and specified like in tensorflow-datasets.\nâ€¢ None features ( , optional) â€” Set the features type to use for this dataset.\nâ€¢ None verification_mode ( , defaults to ) â€” Verification mode determining the checks to run on the downloaded/processed dataset information (checksums/size/splits/â€¦). VerificationMode or, defaults to) â€” Verification mode determining the checks to run on the downloaded/processed dataset information (checksums/size/splits/â€¦).\nâ€¢ None keep_in_memory ( , defaults to ) â€” Whether to copy the dataset in-memory. If , the dataset will not be copied in-memory unless explicitly enabled by setting to nonzero. See more details in the , defaults to) â€” Whether to copy the dataset in-memory. If, the dataset will not be copied in-memory unless explicitly enabled by settingto nonzero. See more details in the improve performance section.\nâ€¢ None revision ( , optional) â€” Version of the dataset script to load. As datasets have their own git repository on the Datasets Hub, the default version â€œmainâ€ corresponds to their â€œmainâ€ branch. You can specify a different version than the default â€œmainâ€ by using a commit SHA or a git tag of the dataset repository. Version or, optional) â€” Version of the dataset script to load. As datasets have their own git repository on the Datasets Hub, the default version â€œmainâ€ corresponds to their â€œmainâ€ branch. You can specify a different version than the default â€œmainâ€ by using a commit SHA or a git tag of the dataset repository.\nâ€¢ None token ( or , optional) â€” Optional string or boolean to use as Bearer token for remote files on the Datasets Hub. If , or not specified, will get token from .\nâ€¢ None streaming ( , defaults to ) â€” If set to , donâ€™t download the data files. Instead, it streams the data progressively while iterating on the dataset. An , defaults to) â€” If set to, donâ€™t download the data files. Instead, it streams the data progressively while iterating on the dataset. An IterableDataset or IterableDatasetDict is returned instead in this case. Note that streaming works for datasets that use data formats that support being iterated over like txt, csv, jsonl for example. Json files may be downloaded completely. Also streaming from remote zip or gzip files is supported but other compressed formats like rar and xz are not yet supported. The tgz format doesnâ€™t allow streaming.\nâ€¢ None num_proc ( , optional, defaults to ) â€” Number of processes when downloading and generating the dataset locally. Multiprocessing is disabled by default. , optional, defaults to) â€” Number of processes when downloading and generating the dataset locally. Multiprocessing is disabled by default.\nâ€¢ None storage_options ( , optional, defaults to ) â€” Experimental. Key/value pairs to be passed on to the dataset file-system backend, if any. , optional, defaults to) â€”. Key/value pairs to be passed on to the dataset file-system backend, if any.\nâ€¢ None trust_remote_code ( , optional, defaults to ) â€” Whether or not to allow for datasets defined on the Hub using a dataset script. This option should only be set to for repositories you trust and in which you have read the code, as it will execute code present on the Hub on your local machine. defaults to if not specified. , optional, defaults to) â€” Whether or not to allow for datasets defined on the Hub using a dataset script. This option should only be set tofor repositories you trust and in which you have read the code, as it will execute code present on the Hub on your local machine.\nâ€¢ None *config_kwargs (additional keyword arguments) â€” Keyword arguments to be passed to the and used in the (additional keyword arguments) â€” Keyword arguments to be passed to theand used in the DatasetBuilder\nâ€¢ if is not : the dataset requested,\nâ€¢ if is , a DatasetDict with each split. or IterableDataset or IterableDatasetDict: if\nâ€¢ if is not , the dataset is requested\nâ€¢ if is , a with each split. Load a dataset from the Hugging Face Hub, or a local dataset. You can find the list of datasets on the Hub or with . A dataset is a directory that contains some data files in generic formats (JSON, CSV, Parquet, etc.) and possibly in a generic structure (Webdataset, ImageFolder, AudioFolder, VideoFolder, etc.) This function does the following under the hood:\nâ€¢ \nâ€¢ Find the most common data format in the dataset and pick its associated builder (JSON, CSV, Parquet, Webdataset, ImageFolder, AudioFolder, etc.)\nâ€¢ Find which file goes into which split (e.g. train/test) based on file and directory names or on the YAML configuration\nâ€¢ It is also possible to specify manually, and which dataset builder to use (e.g. â€œparquetâ€).\nâ€¢ \nâ€¢ None Download the data files from the dataset if they are not already available locally or cached.\nâ€¢ None Process and cache the dataset in typed Arrow tables for caching. Arrow table are arbitrarily long, typed tables which can store nested objects and be mapped to numpy/pandas/python generic types. They can be directly accessed from disk, loaded in RAM or even streamed over the web.\nâ€¢ Donâ€™t download or cache anything. Instead, the dataset is lazily loaded and will be streamed on-the-fly when iterating on it.\nâ€¢ None Return a dataset built from the requested splits in (default: all). It can also use a custom dataset builder if the dataset contains a dataset script, but this feature is mostly for backward compatibility. In this case the dataset script file must be named after the dataset repository or directory and end with â€œ.pyâ€. Load an image dataset with the dataset builder:\nâ€¢ None path ( ) â€” Path or name of the dataset.\nâ€¢ if is a dataset repository on the HF hub (list all available datasets with ) -> load the dataset builder from supported files in the repository (csv, json, parquet, etc.) e.g. , a dataset repository on the HF hub containing the data files.\nâ€¢ if is a local directory -> load the dataset builder from supported files in the directory (csv, json, parquet, etc.) e.g. .\nâ€¢ if is the name of a dataset builder and or is specified (available builders are â€œjsonâ€, â€œcsvâ€, â€œparquetâ€, â€œarrowâ€, â€œtextâ€, â€œxmlâ€, â€œwebdatasetâ€, â€œimagefolderâ€, â€œaudiofolderâ€, â€œvideofolderâ€) -> load the dataset builder from the files in or e.g. . ) â€” Path or name of the dataset. It can also point to a local dataset script but this is not recommended.\nâ€¢ None name ( , optional) â€” Defining the name of the dataset configuration.\nâ€¢ None data_dir ( , optional) â€” Defining the of the dataset configuration. If specified for the generic builders (csv, text etc.) or the Hub datasets and is , the behavior is equal to passing as to reference all the files in a directory.\nâ€¢ None data_files ( or or , optional) â€” Path(s) to source data file(s).\nâ€¢ None features ( Features , optional) â€” Set the features type to use for this dataset.\nâ€¢ None revision ( , optional) â€” Version of the dataset script to load. As datasets have their own git repository on the Datasets Hub, the default version â€œmainâ€ corresponds to their â€œmainâ€ branch. You can specify a different version than the default â€œmainâ€ by using a commit SHA or a git tag of the dataset repository. Version or, optional) â€” Version of the dataset script to load. As datasets have their own git repository on the Datasets Hub, the default version â€œmainâ€ corresponds to their â€œmainâ€ branch. You can specify a different version than the default â€œmainâ€ by using a commit SHA or a git tag of the dataset repository.\nâ€¢ None token ( or , optional) â€” Optional string or boolean to use as Bearer token for remote files on the Datasets Hub. If , or not specified, will get token from .\nâ€¢ None storage_options ( , optional, defaults to ) â€” Experimental. Key/value pairs to be passed on to the dataset file-system backend, if any. , optional, defaults to) â€”. Key/value pairs to be passed on to the dataset file-system backend, if any.\nâ€¢ None trust_remote_code ( , optional, defaults to ) â€” Whether or not to allow for datasets defined on the Hub using a dataset script. This option should only be set to for repositories you trust and in which you have read the code, as it will execute code present on the Hub on your local machine. defaults to if not specified. , optional, defaults to) â€” Whether or not to allow for datasets defined on the Hub using a dataset script. This option should only be set tofor repositories you trust and in which you have read the code, as it will execute code present on the Hub on your local machine.\nâ€¢ None *config_kwargs (additional keyword arguments) â€” Keyword arguments to be passed to the (additional keyword arguments) â€” Keyword arguments to be passed to the BuilderConfig and used in the DatasetBuilder Load a dataset builder which can be used to:\nâ€¢ Inspect general information that is required to build a dataset (cache directory, config, dataset info, features, data files, etc.)\nâ€¢ Download and prepare the dataset as Arrow files in the cache\nâ€¢ Get a streaming dataset without downloading or caching anything You can find the list of datasets on the Hub or with . A dataset is a directory that contains some data files in generic formats (JSON, CSV, Parquet, etc.) and possibly in a generic structure (Webdataset, ImageFolder, AudioFolder, VideoFolder, etc.)\n\nConfigurations used to load data files. They are used when loading local files or a dataset repository:\n\nYou can pass arguments to to configure data loading. For example you can specify the parameter to define the CsvConfig that is used to load the data:"
    },
    {
        "link": "https://discuss.huggingface.co/t/what-is-model-is-currently-loading/13917",
        "document": "Hi.\n\n Iâ€™m a beginner at NLP.\n\n Iâ€™m going to summarize sentences using T5 modelâ€™s information API.\n\n The model is currently loading keeps popping up and does not proceed.\n\n Can you tell me what this error is?\n\n also\n\n I want to summarize more than 5,000 characters into 1,000 to 2,000 characters.\n\n How should I write the parameters?\n\n@Doogie\n\n Hello \n\n Inference API loads models on demand, so if itâ€™s your first time using it in a while it will load the model first, then you can try to send the request again in couple of seconds.\n\nAs per Detailed parameters â€” Api inference documentation, you can use the option to wait for the response instead of having to do multiple requests.\n\nI hope this model is always ready.\n\n What should I do to do that?\n\n Model should always show me response immediately.\n\nis documented in the link shared above. If false, you will get a 503 when itâ€™s loading. If true, your process will hang waiting for the response, which might take a bit while the model is loading. You can pin models for instant loading (see Hugging Face â€“ Pricing)\n\nI get a message that wait_for_model is no longer valid {â€˜inputsâ€™: {â€˜past_user_inputsâ€™: , â€˜generated_responsesâ€™: , â€˜textâ€™: â€˜yoâ€™}, â€˜parametersâ€™: {â€˜min_lengthâ€™: 1, â€˜max_lengthâ€™: 500, â€˜repetition_penaltyâ€™: 50.0, â€˜temperatureâ€™: 50.0, â€˜use_cacheâ€™: True, â€˜wait_for_modelâ€™: True}}\n\n {â€œerrorâ€: â€œThe following are not used by the model: [â€˜wait_for_modelâ€™] (note: typos in the generate arguments will also show up in this list)â€}\n\nHi. With â€œtogethercomputer/GPT-NeoXT-Chat-Base-20Bâ€ Iâ€™m using the â€œwait_for_modelâ€ parameter set to true, but I still have the â€œModel is currently loadingâ€. Is it because the model is too big ?\n\nIâ€™ve set the parameter to True in the payload in the same way as @deseipel and it doesnâ€™t work for me either. I donâ€™t get a specific error about the request, I just get the usual 503 error in response: â€œModel is currently loadingâ€.\n\nI have finally managed to use the flag - the â€œparametersâ€ dictionary actually needs to be called the â€œoptionsâ€ dictionary according to the documentation https://huggingface.co/docs/api-inference/detailed_parameters However, after overcoming that error and duly waiting for a response, I waited, and got a 504 error instead: Gateway Timeout. Is HF down today?"
    }
]