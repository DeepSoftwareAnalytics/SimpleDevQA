[
    {
        "link": "https://github.com/christianversloot/machine-learning-articles/blob/main/build-an-lstm-model-with-tensorflow-and-keras.md",
        "document": "Long Short-Term Memory (LSTM) based neural networks have played an important role in the field of Natural Language Processing. In addition, they have been used widely for sequence modeling. The reason why LSTMs have been used widely for this is because the model connects back to itself during a forward pass of your samples, and thus benefits from context generated by previous predictions when prediction for any new sample.\n\nIn this article, we're going to take a look at how we can build an LSTM model with TensorFlow and Keras. For doing so, we're first going to take a brief look at what LSTMs are and how they work. Don't worry, we won't cover this in much detail, because we already did so in another article. It is necessary though to understand what is happening before we actually get to work. That's how you build intuition for the models you'll use for Machine Learning tasks.\n\nOnce we know about LSTMs, we're going to take a look at how we can build one with TensorFlow. More specifically, we're going to use , or TensorFlow's tightly coupled (or frankly, embedded) version of Keras for the job. First of all, we're going to see how LSTMs are represented as . We'll then move on and actually build the model. With step-by-step explanations, you will understand what is going on at each line and build an understanding of LSTM models in code.\n\nThe code example below gives you a working LSTM based model with TensorFlow 2.x and Keras. If you want to understand it in more detail, make sure to read the rest of the article below.\n\nBefore we will actually write any code, it's important to understand what is happening inside an LSTM. First of all, we must say that an LSTM is an improvement upon what is known as a vanilla or traditional Recurrent Neural Network, or RNN. Such networks look as follows:\n\nA fully recurrent network. Created by fdeloche at Wikipedia, licensed as CC BY-SA 4.0. No changes were made.\n\nIn a vanilla RNN, an input value ( ) is passed through the model, which has a hidden or learned state at that point in time. The model produces the output which is in the target representation. Using this way of working, we can convert inputs in English into outputs in German, to give just an example. Vanilla RNNs are therefore widely used as sequence-to-sequence models.\n\nHowever, we can do the same with classic neural networks. Their benefit compared to classic MLPs is that they pass the output back to themselves, so that it can be used during the next pass. This provides the neural network with context with respect to previous inputs (which in semantically confusing tasks like translation can sometimes be really important). Classic RNNs are therefore nothing more than a fully-connected network that passes neural outputs back to the neurons.\n\nSo far, so good. RNNs really boosted the state-of-the-art back in the days. But well, there's a problem. It emerges when you want to train classic Recurrent Neural Networks. If you apply backpropagation to training a regular neural network, errors are computed backwards, so that the gradient update becomes known that can be applied by the optimizer. Recurrent backpropagation is something that is however not so easy or available, so another approach had to be taken. Effectively, this involved unfolding the network, effectively making copies of the network (with exactly the same initialization) and improving upon them. This way, we can compute gradients more easily, and chain them together. It allowed for the training of RNNs.\n\nBut chaining gradients together effectively means that you have to apply multiplications. And here's the catch: classic RNNs were combined with activation functions like Sigmoid and Tanh, but primarily Sigmoid. As the output of the derivative of these functions is almost always < 1.0, you get a severe case of vanishing gradients. Classic RNNs could therefore not be used when sequences got long; they simply got stuck or trained very slowly.\n\nEnter LSTMs. These Long Short-Term Memory networks effectively split up the output and memory. In so-called memory cells, they allow all functionality to happen, the prediction to be generated, and memory to be updated. Visually, this looks as follows:\n\nLet's take a brief look at all the components in a bit more detail:\nâ€¢ All functionality is embedded into a memory cell, visualized above with the rounded border.\nâ€¢ The and variables represent the outputs of the memory cell at respectively and . In plain English: the output of the previous cell into the current cell, and the output of the current cell to the next one.\nâ€¢ The and variables represent the memory itself, at the known time steps. As you can see, memory has been cut away from the output variable, being an entity on its own.\nâ€¢ We have three so-called gates, represented by the three blocks of elements within the cell:\nâ€¢ On the left, we see a forget gate. It takes the previous output and current input and by means of Sigmoid activation computes what can be forgotten and hence removed from memory related to current and previous input. By multiplying this with the memory, the removal is performed.\nâ€¢ In the middle, we see an input gate. It takes the previous output and current input and applies both a Sigmoid and Tanh activation. The Sigmoid activation effectively learns what must be kept from the inputs, whereas the Tanh normalizes the values into the range , stabilizing the training process. As you can see, the results are first multiplied (to ensure that normalization occurs) after which it is added into memory.\nâ€¢ On the right, we see an output gate. It takes a normalized value for memory through Tanh and a Sigmoid activated value for the previous output and current input, effectively learning what must be predicted for the current input value. This value is then output, and the memory and output values are also passed to the next cell.\n\nThe benefit of LSTMs with respect to simple RNNs lies in the fact that memory has been separated from the actual output mechanisms. As you can see, all vanishing gradient-causing mechanisms lie within the cell. In inter-cell communication, the only elements that are encountered during gradient computation are multiplication (x) and addition (+). These are linear operations, and by consequence the LSTM can ensure that gradients between cells are always 1.0. Hence, with LSTMs, the vanishing gradients problem is resolved.\n\nThis makes them a lot faster than vanilla RNNs.\n\nNow that we understand how LSTMs work in theory, let's take a look at constructing them in TensorFlow and Keras. Of course, we must take a look at how they are represented first. In TensorFlow and Keras, this happens through the class, and it is described as:\n\nIndeed, that's the LSTM we want, although it might not have all the gates yet - gates were changed in another paper that was a follow-up to the Hochreiter paper. Nevertheless, understanding the LSTM with all the gates is a good idea, because that's what most of them look like today.\n\nIn code, it looks as follows:\n\nThese are the attributes that can be configured:\nâ€¢ With units, we can define the dimensionality of the output space, as we are used to e.g. with Dense layers.\nâ€¢ The activation attribute defines the activation function that will be used. By default, it is the Tanh function.\nâ€¢ With recurrent_activation, you can define the activation function for the recurrent functionality.\nâ€¢ The use_bias attribute can be used to configure whether bias must be used to steer the model as well.\nâ€¢ The initializers can be used to initialize the weights of the kernels and recurrent segment, as well as the biases.\nâ€¢ The unit_forget_bias represents the bias value (+1) at the forget gate. This is recommended in a follow-up study to the original LSTM paper.\nâ€¢ The regularizers and constraints allow you to constrain the training process, possibly blocking vanishing and exploding gradients, and keeping the model at adequate complexity.\nâ€¢ Dropout can be added to avoid overfitting, to both the cell itself as well as the recurrent segment.\nâ€¢ With return_sequences, you can indicate whether you want only the prediction for the current input as the output, or that with all the previous predictions appended.\nâ€¢ With return_state, you can indicate whether you also want to have state returned besides the outputs.\nâ€¢ With go_backwards, you can indicate whether you want to have the sequence returned in reverse order.\nâ€¢ If you set stateful to True, the recurrent segment will work on a batch level rather than model level.\nâ€¢ Structure of your input (timesteps, batch, features or batch, timesteps, features) can be switched with time_major.\nâ€¢ With unroll, you can still unroll the network at training. If set to False, a symbolic loop will be used.\nâ€¢ Additional arguments can be passed with **kwargs.\n\nNow that we understand how LSTMs work and how they are represented within TensorFlow, it's time to actually build one with Python, TensorFlow and its Keras APIs. We'll walk you through the process with step-by-step examples. The process is composed of the following steps:\nâ€¢ Importing the Keras functionality that we need into the Python script.\nâ€¢ Listing the configuration for our LSTM model and preparing for training.\nâ€¢ Loading and preparing a dataset; we'll use the IMDB dataset today.\n\nOpen up a code editor and create a file, e.g. called , and let's go!\n\nLet's specify the model imports first:\nâ€¢ We'll need TensorFlow so we import it as .\nâ€¢ From the TensorFlow Keras Datasets, we import the one.\nâ€¢ We'll need word embeddings ( ), MLP layers ( ) and LSTM layers ( ), so we import them as well.\nâ€¢ Our loss function will be binary cross entropy.\nâ€¢ As we'll stack all layers on top of each other with , we need (the Keras Sequential API) for constructing our variable in the first place.\nâ€¢ For optimization we use an extension of classic gradient descent called Adam.\nâ€¢ Finally, we need to import . We're going to use the IMDB dataset which has sequences of reviews. While we'll specify a maximum length, this can mean that shorter sequences are present as well; these are not cutoff and therefore have different sizes than our desired one (i.e. the maximum length). We'll have to pad them with zeroes in order to make them of equal length.\n\nThe next step is specifying the model configuration. While strictly not necessary (we can also specify them hardcoded), I always think it's a good idea to group them together. This way, you can easily see how your model is configured, without having to take a look through all the aspects.\n\nBelow, we can see that our model will be trained with a batch size of 128, using binary crossentropy loss and Adam optimization, and only for five epochs (we only have to show you that it works). 20% of our training data will be used for validation purposes, and the output will be verbose, with verbosity mode set to 1 out of 0, 1 and 2. Our learned word embedding will have 15 hidden dimensions and each sequence passed through the model is 300 characters at max. Our vocabulary will contain 5000 words at max.\n\nYou might now also want to disable Eager Execution in TensorFlow. While it doesn't work for all, some people report that the training process speeds up after using it. However, it's not necessary to do so - simply test how it behaves on your machine:\n\nOnce this is complete, we can load and prepare the data. To make things easier, Keras comes with a standard set of datasets, of which the IMDB dataset can be used for sentiment analysis (essentially text classification with two classes). Using , we can load the data.\n\nOnce the data has been loaded, we apply . This ensures that sentences shorter than the maximum sentence length are brought to equal length by applying padding with, in this case, zeroes, because that often corresponds with the padding character.\n\nWe can then define the Keras model. As we are using the Sequential API, we can initialize the variable with . The first layer is an layer, which learns a word embedding that in our case has a dimensionality of 15. This is followed by an layer providing the recurrent segment (with default activation enabled), and a layer that has one output - through Sigmoid a number between 0 and 1, representing an orientation towards a class.\n\nThe model can then be compiled. This initializes the model that has so far been a skeleton, a foundation, but no actual model yet. We do so by specifying the optimizer, the loss function, and the additional metrics that we had specified before.\n\nThis is also a good place to generate a summary of what the model looks like.\n\nThen, we can instruct TensorFlow to start the training process.\n\nThe pairs passed to the model are the padded inputs and their corresponding class labels. Training happens with the batch size, number of epochs, verbosity mode and validation split that were also defined in the configuration section above.\n\nWe cannot evaluate the model on the same dataset that was used for training it. We fortunately have testing data available through the train/test split performed in the section, and can use built-in evaluation facilities to evaluate the model. We then print the test results on screen.\n\nIf you want to get the full model code just at once, e.g. for copy-and-run, here you go:\n\nTime to run the model! Open up a terminal where at least TensorFlow and Python have been installed, and run the model - .\n\nYou should see that the model starts training after e.g. a few seconds. If you have the IMDB dataset not downloaded to your machine, it will be downloaded first.\n\nEventually, you'll approximately see an 87.1% accuracy on the evaluation set:\n\nIf you face speed issues with training the TensorFlow LSTM on your GPU, you might decide to temporarily disable its access to your GPUs by adding the following before :\n\nLong Short-Term Memory Networks (LSTMs) are a type of recurrent neural network that can be used in Natural Language Processing, time series and other sequence modeling tasks. In this article, we covered their usage within TensorFlow and Keras in a step-by-step fashion.\n\nWe first briefly looked at LSTMs in general. What are they? What can they be used for? How do they improve compared to previous RNN based approaches? This analysis gives you the necessary context in order to understand what is going on within your code.\n\nWe then looked at how LSTMs are represented in TensorFlow and Keras. We saw that there is a separate layer that can be configured with a wide variety of attributes. In the article, we looked at the meaning for each attribute and saw how everything interrelates. Once understanding this, we moved on to actually implementing the model with TensorFlow. In a step-by-step phased approach, we explained in detail why we made certain choices, allowing you to see exactly how the model was constructed.\n\nAfter training on the IMDB dataset, we saw that the model achieves an accuracy of approximately 87.1% on the evaluation set.\n\nI hope that you have learned something from this article. If you did, please feel free to drop a message, as I'd love to hear from you ðŸ’¬ Please do the same if you have any questions, or click the Ask Questions button to the right. Thank you for reading MachineCurve today and happy engineering! ðŸ˜Ž\n\nKeras Team. (n.d.). Keras documentation: The sequential class. Keras: the Python deep learning API. https://keras.io/api/models/sequential/"
    },
    {
        "link": "https://pieriantraining.com/tensorflow-lstm-example-a-beginners-guide",
        "document": "LSTM (Long Short-Term Memory) is a type of Recurrent Neural Network (RNN) that is widely used in deep learning. It is particularly useful in processing and making predictions based on sequential data, such as time series, speech recognition, and natural language processing.\n\nTensorFlow is an open-source platform for machine learning developed by Google Brain Team. It provides a comprehensive set of tools and libraries for building and deploying machine learning models.\n\nIn this tutorial, we will walk through a step-by-step example of how to use TensorFlow to build an LSTM model for time series prediction. We will start by importing the necessary libraries and loading the dataset. Then we will preprocess the data and split it into training and testing sets.\n\nNext, we will define the LSTM model architecture using TensorFlowâ€™s Sequential API. We will also specify the hyperparameters for the model, such as the number of epochs and batch size.\n\nAfter defining the model, we will train it on the training set and evaluate its performance on the testing set. We will visualize the results using matplotlib to see how well our model is able to predict future values in the time series.\n\nOverall, this tutorial aims to provide a beginner-friendly introduction to using TensorFlow and LSTM for time series prediction. By following along with this example, you should gain a better understanding of how to build and train your own deep learning models using TensorFlow.\n\nTensorFlow is an open-source machine learning library developed by Google Brain team. It is used to build and train machine learning models, including deep neural networks. TensorFlow is highly flexible and can be used for a wide range of applications, including image and speech recognition, natural language processing, and recommendation systems.\n\nOne of the key features of TensorFlow is its ability to handle large datasets efficiently. It uses data flow graphs to represent computations, which allows it to distribute computations across multiple CPUs or GPUs. This makes it possible to train complex models on large datasets in a reasonable amount of time.\n\nTensorFlow also provides a high-level API called Keras, which makes it easy to build and train deep learning models. Keras provides a simple interface for defining layers, specifying activation functions, and configuring optimization algorithms.\n\nIn this blog post, we will use TensorFlow to build an LSTM model for predicting stock prices. We will walk through each step of the process, from loading the data to evaluating the modelâ€™s performance. By the end of this tutorial, you should have a good understanding of how LSTM models work and how to implement them using TensorFlow.\n\nLSTM stands for Long Short-Term Memory, which is a type of Recurrent Neural Network (RNN) architecture. RNNs are designed to handle sequential data by processing each input based on the previous inputs. In other words, they have memory of the past inputs.\n\nLSTM takes this concept further by introducing a cell state that can keep information over long periods of time. This cell state is controlled by three gates: the input gate, the forget gate, and the output gate. These gates determine what information to keep or discard from the cell state.\n\nThe input gate decides what information to add to the cell state, while the forget gate decides what information to remove from the cell state. The output gate controls what information to output from the cell state.\n\nLSTM has become a popular choice in natural language processing tasks, such as language translation and sentiment analysis. This is because it can effectively handle long-term dependencies in sequential data, which is common in natural language.\n\nIn TensorFlow, you can implement LSTM using the `tf.keras.layers.LSTM` layer. This layer takes in a sequence of inputs and outputs a sequence of hidden states and a final cell state. You can then use these outputs for further processing or prediction tasks.\n\nLetâ€™s take a look at an example implementation of LSTM in TensorFlow.\n\nIn this example, we define an LSTM model with an input shape of `(10, 1)`, meaning it takes in a sequence of 10 inputs with 1 feature each. We then compile the model with a mean squared error loss function and the Adam optimizer. Finally, we train the model on some input and target data for 10 epochs.\n\nOverall, LSTM is a powerful tool for handling sequential data in machine learning tasks, and TensorFlow provides easy-to-use tools for implementing it in your models.\n\nTo get started with the TensorFlow LSTM example, we first need to set up our environment. Here are the steps you need to follow:\n\n1. Install Python: You can download Python from the official website and install it on your machine. Make sure you install the latest version of Python.\n\n2. Install TensorFlow: Once you have installed Python, you can use pip (Pythonâ€™s package manager) to install TensorFlow. Open your command prompt and run the following command:\n\nThis will install the latest version of TensorFlow on your machine.\n\n3. Install NumPy: NumPy is a popular Python library for numerical computing. You can install it using pip by running the following command:\n\n4. Install Pandas: Pandas is another popular Python library for data manipulation and analysis. You can install it using pip by running the following command:\n\n5. Install Matplotlib: Matplotlib is a plotting library for Python. You can install it using pip by running the following command:\n\nOnce you have installed all these libraries, you are ready to start working with the TensorFlow LSTM example. In the next section, we will dive into the code and see how we can implement an LSTM network using TensorFlow.\n\nIn order to train a TensorFlow LSTM model, we need to first load the data. In this example, we will be using the famous â€œAlice in Wonderlandâ€ book as our dataset. We will use the Natural Language Toolkit (NLTK) library to preprocess the text data.\n\nNext, we can import NLTK and download the necessary resources:\n\nNow we can load the text file and convert it into a list of sentences using NLTKâ€™s `sent_tokenize()` function:\n\nWe can also preprocess the sentences by removing stop words and converting all words to lowercase:\n\nNow we have our preprocessed data ready to be used for training our TensorFlow LSTM model.\n\nBefore we can use the data for our LSTM model, we need to preprocess it. First, we will load the dataset using pandas and split it into training and testing sets. We will use 80% of the data for training and the remaining 20% for testing.\n\nNext, we need to normalize the data so that it falls within a certain range, typically between 0 and 1. This helps the model converge faster during training. We can use scikit-learnâ€™s MinMaxScaler to do this.\n\nFinally, we need to reshape the data into the format expected by our LSTM model. The input to an LSTM model is a 3D array of shape (samples, timesteps, features). In our case, samples refer to the number of rows in our dataset, timesteps refer to the number of time steps in each sample sequence, and features refer to the number of variables in each time step.\n\nWith the data preprocessed and in the correct format, we can now move on to building our LSTM model.\n\nTo build an LSTM model using TensorFlow, we need to first import the necessary libraries. We will be using the Keras API of TensorFlow to build our model.\n\nNext, we define the architecture of our LSTM model. The first layer is the LSTM layer with 128 units and input shape of (X_train.shape[1], X_train.shape[2]). The return sequences parameter is set to True as we want to stack multiple LSTM layers.\n\nWe then add two more LSTM layers with 64 units each and return sequences set to True.\n\nFinally, we add a dense layer with a single output unit and compile the model with mean squared error loss and Adam optimizer.\n\nThatâ€™s it! We have successfully built our LSTM model using TensorFlow.\n\nTo train the LSTM model, we need to define the loss function and optimizer. In this example, we will use the mean squared error as the loss function and the Adam optimizer.\n\nNext, we can train the model using the `fit()` method. We will train for 100 epochs with a batch size of 1.\n\nDuring training, we can monitor the loss and visualize it using a graph. This can help us determine if our model is overfitting or underfitting.\n\nAfter training, we can evaluate the model on the test data and calculate its accuracy.\n\nItâ€™s important to note that LSTM models can be computationally expensive to train. Depending on the size of your data and complexity of your model, training may take a significant amount of time.\n\nNow that we have trained our LSTM model, itâ€™s time to evaluate its performance. In TensorFlow, we can do this by using the `evaluate()` method of the model object.\n\nFirst, we need to load the test data and preprocess it in the same way as we did for the training data. Once we have preprocessed the test data, we can evaluate the model using the `evaluate()` method. This method takes two arguments: the test data and its corresponding labels.\n\nThe `evaluate()` method returns two values: the loss and accuracy of the model on the test data. The loss is a measure of how well the model is able to predict the correct output, while the accuracy is a measure of how often the model is correct.\n\nItâ€™s important to note that we should only use the test data for evaluation purposes and not for training. Using the same data for both training and evaluation can lead to overfitting, where the model performs well on the training data but poorly on new, unseen data.\n\nIn addition to evaluating the overall performance of our model, we can also look at individual predictions using the `predict()` method. This method takes a single input example and returns its predicted output.\n\nBy examining individual predictions, we can gain insights into how our model is making decisions and identify areas where it may be making errors. This can help us improve our model and make it more accurate for future predictions.\n\nTo predict future values using TensorFlow LSTM, we can use the trained model to generate new sequences of data. These new sequences can then be used to predict future values.\n\nFirst, we need to create a seed sequence of data that the model will use to generate the new sequence. This seed sequence should be similar to the data used to train the model.\n\nOnce we have the seed sequence, we can use the trained model to generate a new sequence of data. To do this, we need to call the `model.predict()` method and pass in the seed sequence.\n\nThe `model.predict()` method will return a new sequence of data that we can use to predict future values. We can repeat this process multiple times to generate longer sequences of data.\n\nFinally, we can use the generated sequence of data to predict future values. The predicted values will be based on the patterns learned by the LSTM model during training.\n\nHereâ€™s an example code snippet that demonstrates how to predict future values using TensorFlow LSTM:\n\nIn this example, we create a seed sequence with four values and then generate a new sequence of ten values using the trained LSTM model. The generated sequence is then printed out for inspection.\n\nItâ€™s important to note that predicting future values using LSTM models is not an exact science. The predictions are based on patterns learned during training and may not always be accurate. Itâ€™s always a good idea to validate the predictions using real-world data.\n\nIn conclusion, this TensorFlow LSTM example has provided a beginnerâ€™s guide to understanding the basics of LSTM neural networks and their implementation using TensorFlow. We have seen how LSTMs can be used for time series prediction tasks and how they can effectively model sequential data.\n\nWe started by discussing the architecture of an LSTM cell and its components, such as the forget gate, input gate, output gate, and cell state. We then moved on to implement an LSTM model using TensorFlow, which involved defining the model architecture, compiling it with an optimizer and loss function, and training it on our dataset.\n\nFinally, we evaluated our modelâ€™s performance using metrics such as mean squared error and visualized our predictions against the actual values. Overall, this example provides a solid foundation for anyone looking to dive deeper into the world of deep learning with LSTMs and TensorFlow.\n\nInterested in learning more? Check out our Introduction to Python course!"
    },
    {
        "link": "https://tensorflow.org/api_docs/python/tf/keras/layers/LSTM",
        "document": "Used in the notebooks\n\nBased on available runtime hardware and constraints, this layer will choose different implementations (cuDNN-based or backend-native) to maximize the performance. If a GPU is available and all the arguments to the layer meet the requirement of the cuDNN kernel (see below for details), the layer will use a fast cuDNN implementation when using the TensorFlow backend. The requirements to use the cuDNN implementation are:\nâ€¢ Inputs, if use masking, are strictly right-padded.\nâ€¢ Eager execution is enabled in the outermost context.\n\nThis method is the reverse of , capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by )."
    },
    {
        "link": "https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras",
        "document": "Unlike regression predictive modeling, time series also adds the complexity of a sequence dependence among the input variables.\n\nA powerful type of neural network designed to handle sequence dependence is called a recurrent neural network. The Long Short-Term Memory network or LSTM network is a type of recurrent neural network used in deep learning because very large architectures can be successfully trained.\n\nIn this post, you will discover how to develop LSTM networks in Python using the Keras deep learning library to address a demonstration time-series prediction problem.\n\nAfter completing this tutorial, you will know how to implement and develop LSTM networks for your own time series prediction problems and other more general sequence problems. You will know:\nâ€¢ How to develop LSTM networks for regression, window, and time-step-based framing of time series prediction problems\nâ€¢ How to develop and make predictions using LSTM networks that maintain state (memory) across very long sequences\n\nIn this tutorial, we will develop a number of LSTMs for a standard time series prediction problem. The problem and the chosen configuration for the LSTM networks are for demonstration purposes only; they are not optimized.\n\nThese examples will show exactly how you can develop your own differently structured LSTM networks for time series predictive modeling problems.\n\nKick-start your project with my new book Deep Learning for Time Series Forecasting, including step-by-step tutorials and the Python source code files for all examples.\nâ€¢ Update Oct/2016: There was an error in how RMSE was calculated in each example. Reported RMSEs were just plain wrong. Now, RMSE is calculated directly from predictions, and both RMSE and graphs of predictions are in the units of the original dataset. Models were evaluated using Keras 1.1.0, TensorFlow 0.10.0, and scikit-learn v0.18. Thanks to all those that pointed out the issue and to Philip Oâ€™Brien for helping to point out the fix.\nâ€¢ Update Mar/2017: Updated example for Keras 2.0.2, TensorFlow 1.0.1 and Theano 0.9.0\nâ€¢ Update Apr/2017: For a more complete and better-explained tutorial of LSTMs for time series forecasting, see the post Time Series Forecasting with the Long Short-Term Memory Network in Python\n\nThe example in this post is quite dated. You can view some better examples using LSTMs on time series with:\n\nThe problem you will look at in this post is the International Airline Passengers prediction problem.\n\nThis is a problem where, given a year and a month, the task is to predict the number of international airline passengers in units of 1,000. The data ranges from January 1949 to December 1960, or 12 years, with 144 observations.\n\nBelow is a sample of the first few lines of the file.\n\nYou can load this dataset easily using the Pandas library. You are not interested in the date, given that each observation is separated by the same interval of one month. Therefore, when you load the dataset, you can exclude the first column.\n\nOnce loaded, you can easily plot the whole dataset. The code to load and plot the dataset is listed below.\n\nYou can see an upward trend in the dataset over time.\n\nYou can also see some periodicity in the dataset that probably corresponds to the Northern Hemisphere vacation period.\n\nLetâ€™s keep things simple and work with the data as-is.\n\nNormally, it is a good idea to investigate various data preparation techniques to rescale the data and make it stationary.\n\nThe Long Short-Term Memory network, or LSTM network, is a recurrent neural network trained using Backpropagation Through Time that overcomes the vanishing gradient problem.\n\nAs such, it can be used to create large recurrent networks that, in turn, can be used to address difficult sequence problems in machine learning and achieve state-of-the-art results.\n\nInstead of neurons, LSTM networks have memory blocks connected through layers.\n\nA block has components that make it smarter than a classical neuron and a memory for recent sequences. A block contains gates that manage the blockâ€™s state and output. A block operates upon an input sequence, and each gate within a block uses the sigmoid activation units to control whether it is triggered or not, making the change of state and addition of information flowing through the block conditional.\n\nThere are three types of gates within a unit:\nâ€¢ Forget Gate: conditionally decides what information to throw away from the block\nâ€¢ Input Gate: conditionally decides which values from the input to update the memory state\nâ€¢ Output Gate: conditionally decides what to output based on input and the memory of the block\n\nEach unit is like a mini-state machine where the gates of the units have weights that are learned during the training procedure.\n\nYou can see how you may achieve sophisticated learning and memory from a layer of LSTMs, and it is not hard to imagine how higher-order abstractions may be layered with multiple such layers.\n\nYou can phrase the problem as a regression problem.\n\nThat is, given the number of passengers (in units of thousands) this month, what is the number of passengers next month?\n\nYou can write a simple function to convert the single column of data into a two-column dataset: the first column containing this monthâ€™s (t) passenger count and the second column containing next monthâ€™s (t+1) passenger count to be predicted.\n\nBefore you start, letâ€™s first import all the functions and classes you will use. This assumes a working SciPy environment with the Keras deep learning library installed.\n\nBefore you do anything, it is a good idea to fix the random number seed to ensure your results are reproducible.\n\nYou can also use the code from the previous section to load the dataset as a Pandas dataframe. You can then extract the NumPy array from the dataframe and convert the integer values to floating point values, which are more suitable for modeling with a neural network.\n\nLSTMs are sensitive to the scale of the input data, specifically when the sigmoid (default) or tanh activation functions are used. It can be a good practice to rescale the data to the range of 0-to-1, also called normalizing. You can easily normalize the dataset using the MinMaxScaler preprocessing class from the scikit-learn library.\n\nAfter you model the data and estimate the skill of your model on the training dataset, you need to get an idea of the skill of the model on new unseen data. For a normal classification or regression problem, you would do this using cross validation.\n\nWith time series data, the sequence of values is important. A simple method that you can use is to split the ordered dataset into train and test datasets. The code below calculates the index of the split point and separates the data into the training datasets, with 67% of the observations used to train the model, leaving the remaining 33% for testing the model.\n\nNow, you can define a function to create a new dataset, as described above.\n\nThe function takes two arguments: the dataset, which is a NumPy array you want to convert into a dataset, and the look_back, which is the number of previous time steps to use as input variables to predict the next time periodâ€”in this case, defaulted to 1.\n\nThis default will create a dataset where X is the number of passengers at a given time (t), and Y is the number of passengers at the next time (t + 1).\n\nIt can be configured by constructing a differently shaped dataset in the next section.\n\nLetâ€™s take a look at the effect of this function on the first rows of the dataset (shown in the unnormalized form for clarity).\n\nIf you compare these first five rows to the original dataset sample listed in the previous section, you can see the X=t and Y=t+1 pattern in the numbers.\n\nLetâ€™s use this function to prepare the train and test datasets for modeling.\n\nThe LSTM network expects the input data (X) to be provided with a specific array structure in the form of [samples, time steps, features].\n\nCurrently, the data is in the form of [samples, features], and you are framing the problem as one time step for each sample. You can transform the prepared train and test input data into the expected structure using numpy.reshape() as follows:\n\nYou are now ready to design and fit your LSTM network for this problem.\n\nThe network has a visible layer with 1 input, a hidden layer with 4 LSTM blocks or neurons, and an output layer that makes a single value prediction. The default sigmoid activation function is used for the LSTM blocks. The network is trained for 100 epochs, and a batch size of 1 is used.\n\nOnce the model is fit, you can estimate the performance of the model on the train and test datasets. This will give you a point of comparison for new models.\n\nNote that you will invert the predictions before calculating error scores to ensure that performance is reported in the same units as the original data (thousands of passengers per month).\n\nFinally, you can generate predictions using the model for both the train and test dataset to get a visual indication of the skill of the model.\n\nBecause of how the dataset was prepared, you must shift the predictions so that they align on the x-axis with the original dataset. Once prepared, the data is plotted, showing the original dataset in blue, the predictions for the training dataset in green, and the predictions on the unseen test dataset in red.\n\nYou can see that the model did an excellent job of fitting both the training and the test datasets.\n\nFor completeness, below is the entire code example.\n\nNote: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n\nRunning the example produces the following output.\n\nYou can see that the model has an average error of about 23 passengers (in thousands) on the training dataset and about 49 passengers (in thousands) on the test dataset. Not that bad.\n\nLSTM for Regression Using the Window Method\n\nYou can also phrase the problem so that multiple, recent time steps can be used to make the prediction for the next time step.\n\nThis is called a window, and the size of the window is a parameter that can be tuned for each problem.\n\nFor example, given the current time (t) to predict the value at the next time in the sequence (t+1), you can use the current time (t), as well as the two prior times (t-1 and t-2) as input variables.\n\nWhen phrased as a regression problem, the input variables are t-2, t-1, and t, and the output variable is t+1.\n\nThe create_dataset() function created in the previous section allows you to create this formulation of the time series problem by increasing the look_back argument from 1 to 3.\n\nA sample of the dataset with this formulation is as follows:\n\nYou can re-run the example in the previous section with the larger window size. The whole code listing with just the window size change is listed below for completeness.\n\nNote: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n\nRunning the example provides the following output:\n\nYou can see that the error was increased slightly compared to that of the previous section. The window size and the network architecture were not tuned: This is just a demonstration of how to frame a prediction problem.\n\nYou may have noticed that the data preparation for the LSTM network includes time steps.\n\nSome sequence problems may have a varied number of time steps per sample. For example, you may have measurements of a physical machine leading up to the point of failure or a point of surge. Each incident would be a sample of observations that lead up to the event, which would be the time steps, and the variables observed would be the features.\n\nTime steps provide another way to phrase your time series problem. Like above in the window example, you can take prior time steps in your time series as inputs to predict the output at the next time step.\n\nInstead of phrasing the past observations as separate input features, you can use them as time steps of the one input feature, which is indeed a more accurate framing of the problem.\n\nYou can do this using the same data representation as in the previous window-based example, except when you reshape the data, you set the columns to be the time steps dimension and change the features dimension back to 1. For example:\n\nThe entire code listing is provided below for completeness.\n\nNote: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n\nRunning the example provides the following output:\n\nYou can see that the results are slightly better than the previous example, although the structure of the input data makes a lot more sense.\n\nThe LSTM network has memory capable of remembering across long sequences.\n\nNormally, the state within the network is reset after each training batch when fitting the model, as well as each call to model.predict() or model.evaluate().\n\nYou can gain finer control over when the internal state of the LSTM network is cleared in Keras by making the LSTM layer â€œstateful.â€ This means it can build a state over the entire training sequence and even maintain that state if needed to make predictions.\n\nIt requires that the training data not be shuffled when fitting the network. It also requires explicit resetting of the network state after each exposure to the training data (epoch) by calls to model.reset_states(). This means that you must create your own outer loop of epochs and within each epoch call model.fit() and model.reset_states(). For example:\n\nFinally, when the LSTM layer is constructed, the stateful parameter must be set to True. Instead of specifying the input dimensions, you must hard code the number of samples in a batch, the number of time steps in a sample, and the number of features in a time step by setting the batch_input_shape parameter. For example:\n\nThis same batch size must then be used later when evaluating the model and making predictions. For example:\n\nYou can adapt the previous time step example to use a stateful LSTM. The full code listing is provided below.\n\nNote: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n\nRunning the example provides the following output:\n\nYou do see that results are better than some, worse than others. The model may need more modules and may need to be trained for more epochs to internalize the structure of the problem.\n\nFinally, letâ€™s take a look at one of the big benefits of LSTMs: the fact that they can be successfully trained when stacked into deep network architectures.\n\nLSTM networks can be stacked in Keras in the same way that other layer types can be stacked. One addition to the configuration that is required is that an LSTM layer prior to each subsequent LSTM layer must return the sequence. This can be done by setting the return_sequences parameter on the layer to True.\n\nYou can extend the stateful LSTM in the previous section to have two layers, as follows:\n\nThe entire code listing is provided below for completeness.\n\nNote: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n\nRunning the example produces the following output.\n\nThe predictions on the test dataset are again worse. This is more evidence to suggest the need for additional training epochs.\n\nIn this post, you discovered how to develop LSTM recurrent neural networks for time series prediction in Python with the Keras deep learning network.\nâ€¢ How to create an LSTM for a regression and a window formulation of the time series problem\nâ€¢ How to create an LSTM with a time step formulation of the time series problem\nâ€¢ How to create an LSTM with state and stacked LSTMs with state to learn long sequences\n\nDo you have any questions about LSTMs for time series prediction or about this post?\n\n Ask your questions in the comments below, and I will do my best to answer.\n\nThe example in this post is quite dated. See these better examples available for using LSTMs on time series:"
    },
    {
        "link": "https://github.com/christianversloot/machine-learning-articles/blob/main/bidirectional-lstms-with-tensorflow-and-keras.md",
        "document": "Long Short-Term Memory networks or LSTMs are Neural Networks that are used in a variety of tasks. Used in Natural Language Processing, time series and other sequence related tasks, they have attained significant attention in the past few years. Thanks to their recurrent segment, which means that LSTM output is fed back into itself, LSTMs can use context when predicting a next sample.\n\nTraditionally, LSTMs have been one-way models, also called unidirectional ones. In other words, sequences such as tokens (i.e. words) are read in a left-to-right or right-to-left fashion. This does not necessarily reflect good practice, as more recent Transformer based approaches like BERT suggest. In fact, bidirectionality - or processing the input in a left-to-right and a right-to-left fashion, can improve the performance of your Machine Learning model.\n\nIn this tutorial, we will take a closer look at Bidirectionality in LSTMs. We will take a look LSTMs in general, providing sufficient context to understand what we're going to do. We also focus on how Bidirectional LSTMs implement bidirectionality. We then continue and actually implement a Bidirectional LSTM with TensorFlow and Keras. We're going to use the layer for this purpose.\n\nAfter reading this tutorial, you will...\nâ€¢ Understand what Bidirectional LSTMs are and how they compare to regular LSTMs.\nâ€¢ Know how Bidirectional LSTMs are implemented.\nâ€¢ Be able to create a TensorFlow 2.x based Bidirectional LSTM.\n\nHere's a quick code example that illustrates how TensorFlow/Keras based models can be wrapped with . This converts them from unidirectional recurrent models into bidirectional ones. Click here to understand the attribute. If you want to understand bidirectional LSTMs in more detail, or construct the rest of the model and actually run it, make sure to read the rest of this tutorial too! :)\n\nBefore we take a look at the code of a Bidirectional LSTM, let's take a look at them in general, how unidirectionality can limit LSTMs and how bidirectionality can be implemented conceptually.\n\nA Long Short-Term Memory network or LSTM is a type of recurrent neural network (RNN) that was developed to resolve the vanishing gradients problem. This problem, which is caused by the chaining of gradients during error backpropagation, means that the most upstream layers in a neural network learn very slowly.\n\nIt is especially problematic when your neural network is recurrent, because the type of backpropagation involved there involves unrolling the network for each input token, effectively chaining copies of the same model. The longer the sequence, the worse the vanishing gradients problem is. We therefore don't use classic or vanilla RNNs so often anymore.\n\nLSTMs fix this problem by separating memory from the hidden outputs. An LSTM consists of memory cells, one of which is visualized in the image below. As you can see, the output from the previous layer [latex]h[t-1][/latex] and to the next layer [latex]h[t][/latex] is separated from the memory, which is noted as [latex]c[/latex]. Interactions between the previous output and current input with the memory take place in three segments or gates:\nâ€¢ The forget gate, which is the first segment. It feeds both the previous output and the current input through a Sigmoid ([latex]\\sigma[/latex]) function, then multiplying the result with memory. It thus removes certain short-term elements from memory.\nâ€¢ The input or update gate, which is the second segment. It also utilizes a Sigmoid function and learns what must be added memory, updating it based on the current input and the output from the previous layer. In addition, this Sigmoid activated data is multiplied with a Tanh generated output from memory and input, normalizing the memory update and keeping memory values low.\nâ€¢ The output gate, which is the third segment. It utilizes a Sigmoid activated combination from current input and previous output and multiplies it with a Tanh-normalized representation from memory. The output is then presented and is used in the next cell, which is a copy of the current one with the same parameters.\n\nWhile many nonlinear operations are present within the memory cell, the memory flow from [latex]c[t-1][/latex] to [latex]c[t][/latex] is linear - the multiplication and addition operations are linear operations. By consequence, through a smart implementation, the gradient in this segment is always kept at and hence vanishing gradients no longer occur. This aspect of the LSTM is therefore called a Constant Error Carrousel, or CEC.\n\nSuppose that you are processing the sequence [latex]\\text{I go eat now}[/latex] through an LSTM for the purpose of translating it into French. Recall that processing such data happens on a per-token basis; each token is fed through the LSTM cell which processes the input token and passes the hidden state on to itself. When unrolled (as if you utilize many copies of the same LSTM model), this process looks as follows:\n\nThis immediately shows that LSTMs are unidirectional. In other words, the sequence is processed into one direction; here, from left to right. This makes common sense, as - except for a few languages - we read and write in a left-to-right fashion. For translation tasks, this is therefore not a problem, because you don't know what will be said in the future and hence have no business about knowing what will happen after your current input word.\n\nBut unidirectionality can also limit the performance of your Machine Learning model. This is especially true in the cases where the task is language understanding rather than sequence-to-sequence modeling. For example, if you're reading a book and have to construct a summary, or understand the context with respect to the sentiment of a text and possible hints about the semantics provided later, you'll read in a back-and-forth fashion.\n\nYes: you will read the sentence from the left to the right, and then also approach the same sentence from the right. In other words, in some language tasks, you will perform bidirectional reading. And for these tasks, unidirectional LSTMs might not suffice.\n\nIn those cases, you might wish to use a Bidirectional LSTM instead. With such a network, sequences are processed in both a left-to-right and a right-to-left fashion. In other words, the phrase [latex]\\text{I go eat now}[/latex] is processed as [latex]\\text{I} \\rightarrow \\text{go} \\rightarrow \\text{eat} \\rightarrow \\text{now}[/latex] and as [latex]\\text{I} \\leftarrow \\text{go} \\leftarrow \\text{eat} \\leftarrow \\text{now}[/latex].\n\nThis provides more context for the tasks that require both directions for better understanding.\n\nWhile conceptually bidirectional LSTMs work in a bidirectional fashion, they are not bidirectional in practice. Rather, they are just two unidirectional LSTMs for which the output is combined. Outputs can be combined in multiple ways (TensorFlow, n.d.):\nâ€¢ Vector concatenation. Here, the output vector is twice the dimensionality of the input vectors, because they are concatenated rather than combined.\n\nNow that we understand how bidirectional LSTMs work, we can take a look at implementing one. In this tutorial, we will use TensorFlow 2.x and its Keras implementation for doing so.\n\nBidirectionality of a recurrent Keras Layer can be added by implementing (TensorFlow, n.d.). It is a wrapper layer that can be added to any of the recurrent layers available within Keras, such as , and . It looks as follows:\n\nThe layer attributes are as follows:\nâ€¢ The first argument represents the (one of the recurrent ) that must be turned into a bidirectional one.\nâ€¢ The represents the way that outputs are constructed. Recall that results can be summated, averaged, multiplied and concatenated. By default, it's from the options . When set to , nothing happens to the outputs, and they are returned as a list (TensorFlow, n.d.).\nâ€¢ With , a different layer can be passed for backwards processing, should left-to-right and right-to-left directionality be processed differently.\n\nThe first step in creating a Bidirectional LSTM is defining a regular one. This can be done with the layer, which we have explained in another tutorial. For the sake of brevity, we won't copy the entire model here multiple times - so we'll just show the segment that represents the model. As you can see, creating a regular LSTM in TensorFlow involves initializing the model (here, using ), adding a word embedding, followed by the LSTM layer. Using a final Dense layer, we perform a binary classification problem.\n\nConverting the regular or unidirectional LSTM into a bidirectional one is really simple. The only thing you have to do is to wrap it with a layer and specify the as explained above. In this case, we set the merge mode to summation, which deviates from the default value of concatenation.\n\nOf course, we will also show you the full model code for the examples above. This teaches you how to implement a full bidirectional LSTM. Let's explain how it works. Constructing a bidirectional LSTM involves the following steps...\nâ€¢ Specifying the model imports. As you can see, we import a lot of TensorFlow modules. We're using the provided IMDB dataset for educational purposes, for learned embeddings, the layer type for classification, and / for constructing the bidirectional LSTM. Binary crossentropy loss is used together with the Adam optimizer for optimization. With , we can ensure that our inputs are of equal length. Finally, we'll use - the Sequential API - for creating the initial model.\nâ€¢ Listing the configuration options. I always think it's useful to specify all the configuration options before using them throughout the code. It simply provides the overview that we need. They are explained in more detail in the tutorial about LSTMs.\nâ€¢ Loading and preparing the dataset. We use for loading the dataset given our configuration options, and use to ensure that sentences that are shorter than our maximum limit are padded with zeroes so that they are of equal length. The IMDB dataset can be used for sentiment analysis: we'll find out whether a review is positive or negative.\nâ€¢ Defining the Keras model. In other words, constructing the skeleton of our model. Using , we initialize a model, and stack the , , and layers on top of each other.\nâ€¢ Compiling the model. This actually converts the model skeleton into a model that can be trained and used for predictions. Here, we specify the optimizer, loss function and additional metrics.\nâ€¢ Generating a summary. This allows us to inspect the model in more detail.\nâ€¢ Training and evaluating the model. With , we start the training process using our training data, with subsequent evaluation on our testing data using .\n\nWe can now run our Bidirectional LSTM by running the code in a terminal that has TensorFlow 2.x installed. This is what you should see:\n\nAn 86.5% accuracy for such a simple model, trained for only 5 epochs - not too bad! :)\n\nIn this tutorial, we saw how we can use TensorFlow and Keras to create a bidirectional LSTM. Using step-by-step explanations and many Python examples, you have learned how to create such a model, which should be better when bidirectionality is naturally present within the language task that you are performing.\n\nWe saw that LSTMs can be used for sequence-to-sequence tasks and that they improve upon classic RNNs by resolving the vanishing gradients problem. However, they are unidirectional, in the sense that they process text (or other sequences) in a left-to-right or a right-to-left fashion. This can be problematic when your task requires context 'from the future', e.g. when you are using the full context of the text to generate, say, a summary.\n\nBidirectionality can easily be added to LSTMs with TensorFlow thanks to the layer. Being a layer wrapper to all Keras recurrent layers, it can be added to your existing LSTM easily, as you have seen in the tutorial. Configuration is also easy.\n\nI hope that you have learned something from this article! If you did, please feel free to leave a comment in the comments section ðŸ’¬ Please do the same if you have any remarks or suggestions for improvement. If you have questions, click the Ask Questions button on the right. I will try to respond as soon as I can :)\n\nThank you for reading MachineCurve today and happy engineering! ðŸ˜Ž"
    },
    {
        "link": "https://machinelearningmastery.com/prepare-text-data-deep-learning-keras",
        "document": "You cannot feed raw text directly into deep learning models.\n\nText data must be encoded as numbers to be used as input or output for machine learning and deep learning models.\n\nThe Keras deep learning library provides some basic tools to help you prepare your text data.\n\nIn this tutorial, you will discover how you can use Keras to prepare your text data.\n\nAfter completing this tutorial, you will know:\nâ€¢ About the convenience methods that you can use to quickly prepare text data.\nâ€¢ The Tokenizer API that can be fit on training data and used to encode training, validation, and test documents.\nâ€¢ The range of 4 different document encoding schemes offered by the Tokenizer API.\n\nKick-start your project with my new book Deep Learning for Natural Language Processing, including step-by-step tutorials and the Python source code files for all examples.\n\nThis tutorial is divided into 4 parts; they are:\n\nA good first step when working with text is to split it into words.\n\nWords are called tokens and the process of splitting text into tokens is called tokenization.\n\nKeras provides the text_to_word_sequence() function that you can use to split text into a list of words.\n\nBy default, this function automatically does 3 things:\n\nYou can change any of these defaults by passing arguments to the function.\n\nBelow is an example of using the text_to_word_sequence() function to split a document (in this case a simple string) into a list of words.\n\nRunning the example creates an array containing all of the words in the document. The list of words is printed for review.\n\nThis is a good first step, but further pre-processing is required before you can work with the text.\n\nIt is popular to represent a document as a sequence of integer values, where each word in the document is represented as a unique integer.\n\nKeras provides the one_hot() function that you can use to tokenize and integer encode a text document in one step. The name suggests that it will create a one-hot encoding of the document, which is not the case.\n\nInstead, the function is a wrapper for the hashing_trick() function described in the next section. The function returns an integer encoded version of the document. The use of a hash function means that there may be collisions and not all words will be assigned unique integer values.\n\nAs with the text_to_word_sequence() function in the previous section, the one_hot() function will make the text lower case, filter out punctuation, and split words based on white space.\n\nIn addition to the text, the vocabulary size (total words) must be specified. This could be the total number of words in the document or more if you intend to encode additional documents that contains additional words. The size of the vocabulary defines the hashing space from which words are hashed. Ideally, this should be larger than the vocabulary by some percentage (perhaps 25%) to minimize the number of collisions. By default, the â€˜hashâ€™ function is used, although as we will see in the next section, alternate hash functions can be specified when calling the hashing_trick() function directly.\n\nWe can use the text_to_word_sequence() function from the previous section to split the document into words and then use a set to represent only the unique words in the document. The size of this set can be used to estimate the size of the vocabulary for one document.\n\nWe can put this together with the one_hot() function and one hot encode the words in the document. The complete example is listed below.\n\nThe vocabulary size is increased by one-third to minimize collisions when hashing words.\n\nRunning the example first prints the size of the vocabulary as 8. The encoded document is then printed as an array of integer encoded words.\n\nA limitation of integer and count base encodings is that they must maintain a vocabulary of words and their mapping to integers.\n\nAn alternative to this approach is to use a one-way hash function to convert words to integers. This avoids the need to keep track of a vocabulary, which is faster and requires less memory.\n\nKeras provides the hashing_trick() function that tokenizes and then integer encodes the document, just like the one_hot() function. It provides more flexibility, allowing you to specify the hash function as either â€˜hashâ€™ (the default) or other hash functions such as the built in md5 function or your own function.\n\nBelow is an example of integer encoding a document using the md5 hash function.\n\nRunning the example prints the size of the vocabulary and the integer encoded document.\n\nWe can see that the use of a different hash function results in consistent, but different integers for words as the one_hot() function in the previous section.\n\nSo far we have looked at one-off convenience methods for preparing text with Keras.\n\nKeras provides a more sophisticated API for preparing text that can be fit and reused to prepare multiple text documents. This may be the preferred approach for large projects.\n\nKeras provides the Tokenizer class for preparing text documents for deep learning. The Tokenizer must be constructed and then fit on either raw text documents or integer encoded text documents.\n\nOnce fit, the Tokenizer provides 4 attributes that you can use to query what has been learned about your documents:\nâ€¢ word_counts: A dictionary of words and their counts.\nâ€¢ word_docs: A dictionary of words and how many documents each appeared in.\nâ€¢ word_index: A dictionary of words and their uniquely assigned integers.\nâ€¢ document_count:An integer count of the total number of documents that were used to fit the Tokenizer.\n\nOnce the Tokenizer has been fit on training data, it can be used to encode documents in the train or test datasets.\n\nThe texts_to_matrix() function on the Tokenizer can be used to create one vector per document provided per input. The length of the vectors is the total size of the vocabulary.\n\nThis function provides a suite of standard bag-of-words model text encoding schemes that can be provided via a mode argument to the function.\nâ€¢ â€˜binaryâ€˜: Whether or not each word is present in the document. This is the default.\nâ€¢ â€˜countâ€˜: The count of each word in the document.\nâ€¢ â€˜tfidfâ€˜: The Text Frequency-Inverse DocumentFrequency (TF-IDF) scoring for each word in the document.\nâ€¢ â€˜freqâ€˜: The frequency of each word as a ratio of words within each document.\n\nWe can put all of this together with a worked example.\n\nRunning the example fits the Tokenizer with 5 small documents. The details of the fit Tokenizer are printed. Then the 5 documents are encoded using a word count.\n\nEach document is encoded as a 9-element vector with one position for each word and the chosen encoding scheme value for each word position. In this case, a simple word count mode is used.\n\nThis section provides more resources on the topic if you are looking go deeper.\n\nIn this tutorial, you discovered how you can use the Keras API to prepare your text data for deep learning.\nâ€¢ About the convenience methods that you can use to quickly prepare text data.\nâ€¢ The Tokenizer API that can be fit on training data and used to encode training, validation, and test documents.\nâ€¢ The range of 4 different document encoding schemes offered by the Tokenizer API.\n\nDo you have any questions?\n\n Ask your questions in the comments below and I will do my best to answer."
    },
    {
        "link": "https://machinelearningmastery.com/evaluate-performance-deep-learning-models-keras",
        "document": "Keras is an easy-to-use and powerful Python library for deep learning.\n\nThere are a lot of decisions to make when designing and configuring your deep learning models. Most of these decisions must be resolved empirically through trial and error and by evaluating them on real data.\n\nAs such, it is critically important to have a robust way to evaluate the performance of your neural networks and deep learning models.\n\nIn this post, you will discover a few ways to evaluate model performance using Keras.\n\nKick-start your project with my new book Deep Learning With Python, including step-by-step tutorials and the Python source code files for all examples.\nâ€¢ Update Mar/2017: Updated example for Keras 2.0.2, TensorFlow 1.0.1 and Theano 0.9.0\nâ€¢ Update Mar/2018: Added alternate link to download the dataset as the original appears to have been taken down\n\nYou must make a myriad of decisions when designing and configuring your deep learning models.\n\nMany of these decisions can be resolved by copying the structure of other peopleâ€™s networks and using heuristics. Ultimately, the best technique is to actually design small experiments and empirically evaluate problems using real data.\n\nThis includes high-level decisions like the number, size, and type of layers in your network. It also includes the lower-level decisions like the choice of the loss function, activation functions, optimization procedure, and the number of epochs.\n\nDeep learning is often used on problems that have very large datasets. That is tens of thousands or hundreds of thousands of instances.\n\nAs such, you need to have a robust test harness that allows you to estimate the performance of a given configuration on unseen data and reliably compare the performance to other configurations.\n\nThe large amount of data and the complexity of the models require very long training times.\n\nAs such, it is typical to separate data into training and test datasets or training and validation datasets.\n\nKeras provides two convenient ways of evaluating your deep learning algorithms this way:\n\nKeras can separate a portion of your training data into a validation dataset and evaluate the performance of your model on that validation dataset in each epoch.\n\nYou can do this by setting the validation_split argument on the fit() function to a percentage of the size of your training dataset.\n\nFor example, a reasonable value might be 0.2 or 0.33 for 20% or 33% of your training data held back for validation.\n\nThe example below demonstrates the use of an automatic validation dataset on a small binary classification problem. All examples in this post use the Pima Indians onset of diabetes dataset. You can download it from the UCI Machine Learning Repository and save the data file in your current working directory with the filename pima-indians-diabetes.csv (update: download from here).\n\nNote: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n\nRunning the example, you can see that the verbose output on each epoch shows the loss and accuracy on both the training dataset and the validation dataset.\n\nKeras also allows you to manually specify the dataset to use for validation during training.\n\nIn this example, you can use the handy train_test_split() function from the Python scikit-learn machine learning library to separate your data into a training and test dataset. Use 67% for training and the remaining 33% of the data for validation.\n\nThe validation dataset can be specified to the function in Keras by the argument. It takes a tuple of the input and output datasets.\n\nNote: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n\nLike before, running the example provides a verbose output of training that includes the loss and accuracy of the model on both the training and validation datasets for each epoch.\n\nThe gold standard for machine learning model evaluation is k-fold cross validation.\n\nIt provides a robust estimate of the performance of a model on unseen data. It does this by splitting the training dataset into k subsets, taking turns training models on all subsets except one, which is held out, and evaluating model performance on the held-out validation dataset. The process is repeated until all subsets are given an opportunity to be the held-out validation set. The performance measure is then averaged across all models that are created.\n\nIt is important to understand that cross validation means estimating a model design (e.g., 3-layer vs. 4-layer neural network) rather than a specific fitted model. You do not want to use a specific dataset to fit the models and compare the result since this may be due to that particular dataset fitting better on one model design. Instead, you want to use multiple datasets to fit, resulting in multiple fitted models of the same design, taking the average performance measure for comparison.\n\nCross validation is often not used for evaluating deep learning models because of the greater computational expense. For example, k-fold cross validation is often used with 5 or 10 folds. As such, 5 or 10 models must be constructed and evaluated, significantly adding to the evaluation time of a model.\n\nNevertheless, when the problem is small enough or if you have sufficient computing resources, k-fold cross validation can give you a less-biased estimate of the performance of your model.\n\nIn the example below, you will use the handy StratifiedKFold class from the scikit-learn Python machine learning library to split the training dataset into 10 folds. The folds are stratified, meaning that the algorithm attempts to balance the number of instances of each class in each fold.\n\nThe example creates and evaluates 10 models using the 10 splits of the data and collects all the scores. The verbose output for each epoch is turned off by passing to the and functions on the model.\n\nThe performance is printed for each model, and it is stored. The average and standard deviation of the model performance are then printed at the end of the run to provide a robust estimate of model accuracy.\n\nNote: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n\nRunning the example will take less than a minute and will produce the following output:\n\nIn this post, you discovered the importance of having a robust way to estimate the performance of your deep learning models on unseen data.\n\nYou discovered three ways that you can estimate the performance of your deep learning models in Python using the Keras library:\n\nDo you have any questions about deep learning with Keras or this post? Ask your question in the comments, and I will do my best to answer it."
    },
    {
        "link": "https://medium.com/@iitkarthik/master-the-keras-evaluate-method-a-comprehensive-guide-for-deep-learning-enthusiasts-874cfde363b2",
        "document": "Are you leveraging the full potential of your deep learning models? Understanding how to evaluate your modelâ€™s performance is crucial, and Keras makes it seamless with its powerful method. In this article, weâ€™ll explore how this method works, why it matters, and how you can use it to refine your machine learning projects.\n\nWhat is the Keras Method?\n\nThe Keras method is a straightforward yet powerful function designed to assess the performance of your trained machine learning models. It calculates the loss and metrics of your model on a specified dataset, providing valuable insights into how well your model performs in real-world scenarios.\nâ€¢ Customizable Metrics: Measure accuracy, precision, recall, or any other metric you define during model compilation.\n\nWhy is Evaluation Critical in Deep Learning?\n\nEvaluating a model isnâ€™t just a checkbox task; itâ€™s a cornerstone of the machine learning workflow. Hereâ€™s why:\nâ€¢ Prevents Overfitting: Check if your model generalizes well to new data.\n\nHow to Use the Method in Keras\n\nEnsure your model is already trained. If youâ€™re starting from scratch, train your model using the method:\n\nSet aside a test dataset to evaluate your modelâ€™s performance:\n\nPass the test dataset to the function:\n\nAnalyze the loss and metrics to gauge your modelâ€™s effectiveness. A lower loss and higher accuracy indicate better performance.\n\nAvoid using your test set during training. A dedicated validation set ensures unbiased performance metrics.\n\nEvaluate your model on datasets that represent various scenarios to ensure robustness.\n\nUse callbacks to streamline the evaluation process during training, such as and .\nâ€¢ Over-reliance on Accuracy: Accuracy isnâ€™t always the best metric, especially for imbalanced datasets.\nâ€¢ Neglecting Data Preprocessing: Ensure your test data is preprocessed the same way as your training data.\nâ€¢ Ignoring Evaluation Frequency: Regularly evaluate during and after training to monitor progress.\n\nLetâ€™s say youâ€™re building a model to classify retinal images. After training, you evaluate it on a test set:\n\nBy analyzing these metrics, you identify areas for improvement, such as refining the preprocessing pipeline or augmenting the dataset.\n\nMastering the method in Keras empowers you to unlock deeper insights into your machine learning projects. By following best practices and avoiding common pitfalls, you can ensure your models are not only accurate but also reliable.\n\nReady to take your AI skills to the next level? Check out my Udemy courses for in-depth tutorials and expert guidance on building cutting-edge AI solutions."
    },
    {
        "link": "https://stackoverflow.com/questions/53456413/deep-learning-data-preparation",
        "document": "I have a text dataset, that contains 6 classes. for each sample, I have the percent value and sum of the 6 percent values is 100% (features are related to each other). For example :\n\nhow can I feed a deep learning algorithm with this dataset? I actually want the prediction to be exactly in the shape of training data."
    },
    {
        "link": "https://domino.ai/blog/deep-learning-illustrated-building-natural-language-processing-models",
        "document": "Many thanks to Addison-Wesley Professional for providing the permissions to excerpt \"Natural Language Processing\" from the book, Deep Learning Illustrated by Krohn, Beyleveld, and Bassens. The excerpt covers how to create word vectors and utilize them as an input into a deep learning model.\n\nWhile the field of computational linguistics, or Natural Language Processing (NLP), has been around for decades, the increased interest in and use of deep learning models has also propelled applications of NLP forward within industry. Data scientists and researchers require an extensive array of techniques, packages, and tools to accelerate core work flow tasks including prepping, processing, and analyzing data. Utilizing NLP helps researchers and data scientists complete core tasks faster. As Domino is committed to accelerating data science work flows, we reached out to Addison-Wesley Professional (AWP) for permissions to excerpt the extensive â€œNatural Language Processingâ€ chapter from the book, Deep Learning Illustrated.\n\nIn Chapter 2 [in the book], we introduced computational representations of language, particularly highlighting word vectors as a potent approach for quantitatively capturing word meaning. In the present chapter [excerpt], we cover code that will enable you to create your own word vectors as well as to provide them as an input into a deep learning model.\n\nThe natural language processing models you build in this chapter will incorporate neural network layers weâ€™ve applied already: dense layers from Chapters 5 through 9 [in the book], and convolutional layers from Chapter 10 [in the book]. Our NLP models will also incorporate new layer typesâ€”ones from the family of recurrent neural networks. RNNs natively handle information that occurs in sequences such as natural language, but they can, in fact, handle any sequential dataâ€”such as financial time series or temperatures at a given geographic locationâ€”so theyâ€™re quite versatile. The chapter concludes with a section on deep learning networks that process data via multiple parallel streamsâ€”a concept that dramatically widens the scope for creativity when you design your model architectures and, as youâ€™ll see, can also improve model accuracy.\n\nThere are steps you can take to preprocess natural language data such that the modeling you carry out downstream may be more accurate. Common natural language preprocessing options include:\nâ€¢ None Tokenization: This is the splitting of a document (e.g., a book) into a list of discrete elements of language (e.g., words), which we call tokens.\nâ€¢ None Converting all characters to lowercase. A capitalized word at the beginning of a sentence (e.g., She) has the same meaning as when itâ€™s used later in a sentence (She). By converting all characters in a corpus to lowercase, we disregard any use of capitalization.\nâ€¢ None Removing stop words: These are frequently occurring words that tend to contain relatively little distinctive meaning, such as the, at, which, and of. There is no universal consensus on the precise list of stop words, but depending on your application it may be sensible to ensure that certain words are (or arenâ€™t!) considered to be stop words. For example, in this chapter, weâ€™ll build a model to classify movie reviews as positive or negative. Some lists of stop words include negations like didnâ€™t, isnâ€™t, and wouldnâ€™t that might be critical for our model to identify the sentiment of a movie review, so these words probably shouldnâ€™t be removed.\nâ€¢ None Removing punctuation: Punctuation marks generally donâ€™t add much value to a natural language model and so are often removed.\nâ€¢ None Stemming: Stemming is the truncation of words down to their stem. For example, the words house and housing both have the stem hous. With smaller datasets in particular, stemming can be productive because it pools words with similar meanings into a single token. There will be more examples of this stemmed tokenâ€™s con- text, enabling techniques like word2vec or GloVe to more accurately identify an appropriate location for the token in word-vector space (see Figures 2.5 and 2.6) [ in the book] . [Note: Lemmatization, a more sophisticated alternative to stemming, requires the use of a reference vocabulary. For our purposes in this book, stemming is a sufficient approach for considering multiple related words as a single token.]\nâ€¢ None Handling n-grams: Some words commonly co-occur in such a way that the combination of words is better suited to being considered a single concept than several separate concepts. As examples, New York is a bigram (an n-gram of length two), and New York City is a trigram (an n-gram of length three). When chained together, the words new, york, and city have a specific meaning that might be better captured by a single token (and therefore a single location in word-vector space) than three separate ones.\n\nDepending on the particular task that weâ€™ve designed our model for, as well as the dataset that weâ€™re feeding into it, we may use all, some, or none of these data preprocessing steps. As you consider applying any preprocessing step to your particular problem, you can use your intuition to weigh whether it might ultimately be valuable to your downstream task. Weâ€™ve already mentioned some examples of this:\nâ€¢ None Stemming may be helpful for a small corpus but unhelpful for a large one.\nâ€¢ None Likewise, converting all characters to lowercase is likely to be helpful when youâ€™re working with a small corpus, but, in a larger corpus that has many more examples of individual uses of words, the distinction of, say, general (an adjective meaning â€œwidespreadâ€) versus General (a noun meaning the commander of an army) may be valuable.\nâ€¢ None Removing punctuation would not be an advantage in all cases. Consider, for example, if you were building a question-answering algorithm, which could use question marks to help it identify questions.\nâ€¢ None Negations may be helpful as stop words for some classifiers but probably not for a sentiment classifier, for example. Which words you include in your list of stop words could be crucial to your particular application, so be careful with this one. In many instances, it will be best to remove only a limited number of stop words.\n\nIf youâ€™re unsure whether a given preprocessing step may be helpful or not, you can investigate the situation empirically by incorporating the step and observing whether it impacts the accuracy of your deep learning model downstream. As a general rule, the larger a corpus becomes, the fewer preprocessing steps that will be helpful. With a small corpus, youâ€™re likely to be concerned about encountering words that are rare or that are outside the vocabulary of your training dataset. By pooling several rare words into a single common token, youâ€™ll be more likely to train a model effectively on the meaning of the group of related words. As the corpus becomes larger, however, rare words and out-of-vocabulary words become less and less of an issue. With a very large corpus, then, it is likely to be helpful to avoid pooling several words into a single common token. Thatâ€™s because there will be enough instances of even the less-frequently-occurring words to effectively model their unique meaning as well as to model the relatively subtle nuances between related words (that might otherwise have been pooled together).\n\nTo provide practical examples of these preprocessing steps in action, we invite you to check out our Natural Language Preprocessing Jupyter notebook.\n\nIt begins by loading a number of dependencies:\n\nMost of these dependencies are from (the Natural Language Toolkit) and gensim (another natural language library for Python). We explain our use of each individual dependency when we apply it in the example code that follows.\n\nThe dataset we used in this notebook is a small corpus of out-of-copyright books from Project Gutenberg. [Note: Named after the printing-press inventor Johannes Gutenberg, Project Gutenberg is a source of tens of thousands of electronic books. These books are classic works of literature from across the globe whose copyright has now expired, making them freely available. See gutenberg.org.]\n\nThis corpus is available within nltk so it can be easily loaded using this code:\n\nThis wee corpus consists of a mere 18 literary works, including Jane Austenâ€™s Emma, Lewis Carrollâ€™s Alice in Wonderland, and three plays by a little-known fellow named William Shakespeare. (Execute to print the names of all 18 documents.) By running , you can see that the corpus comes out to 2.6 million wordsâ€”a manageable quantity that means youâ€™ll be able to run all of the code examples in this section on a laptop.\n\nTo tokenize the corpus into a list of sentences, one option is to use nltkâ€™s method:\n\nAccessing the first element of the resulting list by running , you can see that the first book in the Project Gutenberg corpus is Emma, because this first element contains the bookâ€™s title page, chapter markers, and first sentence, all (erroneously) blended together with newline characters (\n\n):\n\nA stand-alone sentence is found in the second element, which you can view by executing :\n\nYou can further tokenize this sentence down to the word level using nltkâ€™s method\n\nThis prints a list of words with all whitespace, including newline characters, stripped out (see Figure 11.1). The word father, for example, is the 15th word in the second sentence, as you can see by running this line of code:\n\nAlthough the and methods may come in handy for working with your own natural language data, with this Project Gutenberg corpus, you can instead conveniently employ its built-in method to achieve the same aims in a single step:\n\nThis command produces , a tokenized list of lists. The higher-level list consists of individual sentences, and each sentence contains a lower-level list of words within it. Appropriately, the method also separates the title page and chapter markers into their own individual elements, as you can observe with a call to\n\nBecause of this, the first actual sentence of Emma is now on its own as the fourth element of , and so to access the 15th word (father) in the second actual sentence, we now use .\n\nFor the remaining natural language preprocessing steps, we begin by applying them iteratively to a single sentence. As we wrap up the section later on, weâ€™ll apply the steps across the entire 18-document corpus.\n\nLooking back at Figure 11.1, we see that this sentence begins with the capitalized word She.\n\nIf weâ€™d like to disregard capitalization so that this word is considered to be identical to she, then we can use the Python method from the library, as shown in Example 11.1.\n\nThis line returns the same list as in Figure 11.1 with the exception that the first element in the list is now she instead of .\n\nAnother potential inconvenience with the sentence in Figure 11.1 is that itâ€™s littered with both stop words and punctuation. To handle these, letâ€™s use the operator to concatenate together nltkâ€™s list of English stop words with the libraryâ€™s list of punctuation marks:\n\nIf you examine the list that youâ€™ve created, youâ€™ll see that it contains many common words that often donâ€™t contain much particular meaning, such as a, an, and the. [Note These three particular words are called articles, or determiners. However, it also contains words like not and other negative words that could be critical if we were building a sentiment classifier, such as in the sentence, â€œThis film was not good.â€]\n\nIn any event, to remove all of the elements of stpwrds from a sentence we could use a list comprehension as we do in Example 11.2, which incorporates the lowercasing we used in Example 11.1.\n\nExample 11.2 Removing stop words and punctuation with a list comprehension\n\nRelative to Figure 11.1, running this line of code returns a much shorter list that now contains only words that each tend to convey a fair bit of meaning:\n\nTo stem words, you can use the Porter algorithm [Note: Porter, M. F. (1980). An algorithm for suffix stripping. Program, 14, 130â€“7.] provided by nltk. To do this, you create an instance of a object and then add its to the list comprehension you began in Example 11.2, as shown in Example 11.3.\n\nExample 11.3 Adding word stemming to our list comprehension\n\nThis outputs the following:\n\nThis is similar to our previous output of the sentence except that many of the words have been stemmed:\nâ€¢ None daughters to daughter (allowing the plural and singular terms to be treated identically)\nâ€¢ None house to hous (allowing related words like house and housing to be treated as the same)\nâ€¢ None early to earli (allowing differing tenses such as early, earlier, and earliest to be treated as the same)\n\nThese stemming examples may be advantageous with a corpus as small as ours, because there are relatively few examples of any given word. By pooling similar words together, we obtain more occurrences of the pooled version, and so it may be assigned to a more accurate location in vector space (Figure 2.6). With a very large corpus, however, where you have many more examples of rarer words, there might be an advantage to treating plural and singular variations on a word differently, treating related words as unique, and retaining multiple tenses; the nuances could prove to convey valuable meaning.\n\nTo treat a bigram like New York as a single token instead of two, we can use the and methods from the gensim library. As demonstrated in Example 11.4, we use them in this way:\nâ€¢ None to train a â€œdetectorâ€ to identify how often any given pair of words occurs together in our corpus (the technical term for this is bigram collocation) relative to how often each word in the pair occurs by itself\nâ€¢ None to take the bigram collocations detected by the object and then use this information to create an object that can efficiently be passed over our corpus, converting all bigram collocations from two consecutive tokens into a single token\n\nBy running , we output a dictionary of the count and score of each bigram. The topmost lines of this dictionary are provided in Figure 11.2.\n\nEach bigram in Figure 11.2 has a count and a score associated with it. The bigram two daughters, for example, occurs a mere 19 times across our Gutenberg corpus. This bigram has a fairly low score (12.0), meaning the terms two and daughters do not occur together very frequently relative to how often they occur apart. In contrast, the bigram Miss Taylor occurs more often (48 times), and the terms Miss and Taylor occur much more frequently together relative to how often they occur on their own (score of 453.8).\n\nScanning over the bigrams in Figure 11.2, notice that they are marred by capitalized words and punctuation marks. Weâ€™ll resolve those issues in the next section, but in the meantime letâ€™s explore how the object weâ€™ve created can be used to convert bigrams from two consecutive tokens into one. Letâ€™s tokenize a short sentence by using the method on a string of characters wherever thereâ€™s a space, as follows:\n\nIf we print , we output a list of unigrams only: . If, however, we pass the list through our gensim bigram object by using , the list then contains the bigram New York: .\n\nAfter youâ€™ve identified bigrams across your corpus by running it through the bigram object, you can detect trigrams (such as New York City) by passing this new, bigram-filled corpus through the and methods. This could be repeated again to identify 4-grams (and then again to identify 5-grams, and so on); however, there are diminishing returns from this. Bigrams (or at most trigrams) should suffice for the majority of applications. By the way, if you go ahead and detect trigrams with the Project Gutenberg corpus, New York City is unlikely to be detected. Our corpus of classic literature doesnâ€™t mention it often enough.\n\nHaving run through some examples of preprocessing steps on individual sentences, we now compose some code to preprocess the entire Project Gutenberg corpus. This will also enable us to collocate bigrams on a cleaned-up corpus that no longer contains capital letters or punctuation.\n\nLater on in this chapter, weâ€™ll use a corpus of film reviews that was curated by Andrew Maas and his colleagues at Stanford University to predict the sentiment of the reviews with NLP models. [Note: Maas, A., et al. (2011). Learning word vectors for sentiment analysis. Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, 142â€“50.] During their data preprocessing steps, Maas and his coworkers decided to leave in stop words because they are â€œindicative of sentiment.â€ [Note: This is in line with our thinking, as we mentioned earlier in the chapter.] They also decided not to stem words because they felt their corpus was sufficiently large that their word-vector-based NLP model â€œlearns similar representations of words of the same stem when the data suggest it.â€ Said another way, words that have a similar meaning should find their way to a similar location in word-vector space (Figure 2.6) [in the book] during model training.\n\nFollowing their lead, weâ€™ll also forgo stop-word removal and stemming when preprocessing the Project Gutenberg corpus, as in Example 11.5.\n\nExample 11.5 Removing capitalization and punctuation from Project Gutenberg corpus\n\nIn this example, we begin with an empty list we call , and then we append preprocessed sentences to it using a for loop. [Note: If youâ€™re preprocessing a large corpus, weâ€™d recommend using optimizable and parallelizable functional program- ming techniques in place of our simple (and therefore simple-to-follow) for loop.] For preprocessing each sentence within the loop, we used a variation on the list comprehension from Example 11.2, in this case removing only punctuation marks while converting all characters to lowercase.\n\nWith punctuation and capitals removed, we can set about detecting collocated bigrams across the corpus afresh:\n\nRelative to Example 11.4, this time we created our gensim lower_bigram object in a single line by chaining the and methods together. The top of the output of a call to is provided in Figure 11.3: Comparing these bigrams with those from Figure 11.2, we do indeed observe that they are all in lowercase (e.g., miss taylor) and bigrams that included punctuation marks are nowhere to be seen.\n\nExamining the results in Figure 11.3 further, however, it appears that the default minimum thresholds for both count and score are far too liberal. That is, word pairs like two daughters and her sister should not be considered bigrams. To attain bigrams that we thought were more sensible, we experimented with more conservative count and score thresholds by increasing them by powers of 2. Following this approach, we were generally satisfied by setting the optional arguments to a min(imum) count of 32 and to a score threshold of 64, as shown in Example 11.6.\n\nExample 11.6 Detecting collocated bigrams with more conservative thresholds\n\nAlthough itâ€™s not perfect, [Note: These are statistical approximations, of course!] because there are still a few questionable bigrams like great deal and few minutes, the output from a call to is now largely defensible, as shown in Figure 11.4.\n\nArmed with our well-appointed object from Example 11.6, we can at last use a for loop to iteratively append for ourselves a corpus of cleaned-up sentences, as in Example 11.7.\n\nWith the cleaned corpus of natural language clean_sents now available to us, we are well positioned to embed words from the corpus into word-vector space (Figure 2.6). As youâ€™ll see in this section, such word embeddings can be produced with a single line of code. This single line of code, however, should not be executed blindly, and it has quite a few optional arguments to consider carefully. Given this, weâ€™ll cover the essential theory behind word vectors before delving into example code.\n\nIn Chapter 2, we provided an intuitive understanding of what word vectors are. We also discussed the underlying idea that because you can â€œknow a word by the company it keepsâ€ then a given wordâ€™s meaning can be well represented as the average of the words that tend to occur around it. word2vec is an unsupervised learning techniqueâ€”that is, it is applied to a corpus of natural language without making use of any labels that may or may not happen to exist for the corpus. This means that any dataset of natural language could be appropriate as an input to word2vec. [Note: Mikolov, T., et al. (2013). Efficient estimation of word representations in vector space. arXiv:1301.3781]\n\nWhen running word2vec, you can choose between two underlying model architecturesâ€”skip-gram (SG) or continuous bag of words (CBOW; pronounced see-bo)â€” either of which will typically produce roughly comparable results despite maximizing probabilities from â€œoppositeâ€ perspectives. To make sense of this, reconsider our toy-sized corpus from Figure 2.5:\n\nIn it, we are considering word to be the target word, and the three words to the right of it as well as the three words to the left of it are considered to be context words. (This corresponds to a window size of three wordsâ€”one of the primary hyperparameters we must take into account when applying word2vec.) With the SG architecture, context words are predicted given the target word. [Note: In more technical machine learning terms, the cost function of the skip-gram architecture is to maximize the log probability of any possible context word from a corpus given the current target word.] With CBOW, it is the inverse: The target word is predicted based on the context words. [Note: Again, in technical ML jargon, the cost function for CBOW is maximizing the log probability of any possible target word from a corpus given the current context words. ]\n\nTo understand word2vec more concretely, letâ€™s focus on the CBOW architecture in greater detail (although we equally could have focused on SG instead). With CBOW, the target word is predicted to be the average of all the context words considered jointly. â€œJointlyâ€ means â€œall at onceâ€: The particular position of context words isnâ€™t taken into consideration, nor whether the context word occurs before or after the target word. That the CBOW architecture has this attribute is right there in the â€œbag of wordsâ€ part of its name:\nâ€¢ None We take all the context words within the windows to the right and the left of the target word.\nâ€¢ None We (figuratively!) throw all of these context words into a bag. If it helps you remember that the sequence of words is irrelevant, you can even imagine shaking up the bag.\nâ€¢ None We calculate the average of all the context words contained in the bag, using this average to estimate what the target word could be.\n\nHaving considered the intuitiveness of the â€œBOWâ€ component of the CBOW moniker, letâ€™s also consider the â€œcontinuousâ€ part of it: The target word and context word windows slide continuously one word at a time from the first word of the corpus all the way through to the final word. At each position along the way, the target word is estimated given the context words. Via stochastic gradient descent, the location of words within vector space can be shifted, and thereby these target-word estimates can gradually be improved.\n\nIn practice, and as summarized in Table 11.1, the SG architecture is a better choice when youâ€™re working with a small corpus. It represents rare words in word-vector space well. In contrast, CBOW is much more computationally efficient, so it is the better option when youâ€™re working with a very large corpus. Relative to SG, CBOW also represents frequently occurring words slightly better. [Note: Regardless of whether you use the SG or CBOW architecture, an additional option you have while running word2vec is the training method. For this, you have two different options: hierarchical softmax and negative sampling. The former involves normalization and is better suited to rare words. The latter, on the other hand, forgoes normalization, making it better suited to common words and low-dimensional word-vector spaces. For our purposes in this book, the differences between these two training methods are insignificant and we donâ€™t cover them further.]\n\nAlthough word2vec is comfortably the most widely used approach for embedding words from a corpus of natural language into vector space, it is by no means the only approach. A major alternative to word2vec is GloVeâ€”global vectors for word representationâ€”which was introduced by the prominent natural language researchers Jeffrey Pennington, Richard Socher, and Christopher Manning. [Note: 15. Pennington, J., et al. (2014). GloVe: Global vectors for word representations. Proceedings of the Conference on Empirical Methods in Natural Language Processing.] At the timeâ€”in 2014â€”the three were colleagues working together at Stanford University.\n\nGloVe and word2vec differ in their underlying methodology: word2vec uses predictive models, while GloVe is count based. Ultimately, both approaches tend to pro- duce vector-space embeddings that perform similarly in downstream NLP applications, with some research suggesting that word2vec may provide modestly better results in select cases. One potential advantage of GloVe is that it was designed to be parallelized over multiple processors or even multiple machines, so it might be a good option if youâ€™re looking to create a word-vector space with many unique words and a very large corpus.\n\nThe contemporary leading alternative to both word2vec and GloVe is fastText. [Note: The open-source fastText library is available at fasttext.cc. Joulin, A., et al. (2016). Bag of tricks for efficient text classification. arXiv: 1607.01759. Bojanowski, P., et al. (2016). Enriching word vectors with subword information. arXiv: 1607.04606. Note that the lead author of the landmark word2vec paper, Tomas Mikolov, is the final author of both of these landmark fastText papers.] This approach was developed by researchers at Facebook. A major benefit of fastText is that it operates on a subword levelâ€”its â€œwordâ€ vectors are actually subcomponents of words. This enables fastText to work around some of the issues related to rare words and out-of-vocabulary words addressed in the preprocessing section at the outset of this chapter.\n\nHowever you create your word vectorsâ€”be it with word2vec or an alternative approachâ€”there are two broad perspectives you can consider when evaluating the quality of word vectors: intrinsic and extrinsic evaluations.\n\nExtrinsic evaluations involve assessing the performance of your word vectors within whatever your downstream NLP application of interest isâ€”your sentiment-analysis classifier, say, or perhaps your named-entity recognition tool. Although extrinsic evaluations can take longer to carry out because they require you to carry out all of your downstream processing stepsâ€”including perhaps training a computationally intensive deep learning modelâ€”you can be confident that itâ€™s worthwhile to retain a change to your word vectors if they relate to an appreciable improvement in the accuracy of your NLP application.\n\nIn contrast, intrinsic evaluations involve assessing the performance of your word vectors not on your final NLP application, but rather on some specific intermediate sub- task. One common such task is assessing whether your word vectors correspond well to arithmetical analogies like those shown in Figure 2.7. For example, if you start at the word-vector location for king, subtract , and , do you end up near the word-vector location for queen? [Note: A test set of 19,500 such analogies was developed by Tomas Mikolov and his colleagues in their 2013 word2vec paper. This test set is available at download.tensorflow.org/data/questions-words.txt.]\n\nRelative to extrinsic evaluations, intrinsic tests are quick. They may also help you better understand (and therefore troubleshoot) intermediate steps within your broader NLP process. The limitation of intrinsic evaluations, however, is that they may not ultimately lead to improvements in the accuracy of your NLP application downstream unless youâ€™ve identified a reliable, quantifiable relationship between performance on the intermediate test and your NLP application.\n\nAs mentioned earlier, and as shown in Example 11.8, word2vec can be run in a single line of codeâ€”albeit with quite a few arguments.\n\nHereâ€™s a breakdown of each of the arguments we passed into the Word2Vec() method from the gensim library:\nâ€¢ None : Pass in a list of lists like clean_sents as a corpus. Elements in the higher-level list are sentences, whereas elements in the lower-level list can be word- level tokens.\nâ€¢ None : The number of dimensions in the word-vector space that will result from running word2vec. This is a hyperparameter that can be varied and evaluated extrinsically or intrinsically. Like other hyperparameters in this book, there is a Goldilocks sweet spot. You can home in on an optimal value by specifying, say, 32 dimensions and varying this value by powers of 2. Doubling the number of dimensions will double the computational complexity of your downstream deep learning model, but if doing this results in markedly higher : The number of dimensions in the word-vector space that will result from running word2vec. This is a hyperparameter that can be varied and evaluated extrinsically or intrinsically. Like other hyperparameters in this book, there is a Goldilocks sweet spot. You can home in on an optimal value by specifying, say, 32 dimensions and varying this value by powers of 2. Doubling the number of dimensions will double the computational complexity of your downstream deep learning model, but if doing this results in markedly higher model accuracy then this extrinsic evaluation suggests that the extra complexity could be worthwhile. On the other hand, halving the number of dimensions halves computational complexity downstream: If this can be done without appreciably decreasing your NLP modelâ€™s accuracy, then it should be. By performing a handful of intrinsic inspections (which weâ€™ll go over shortly), we found 64 dimensions to provide more sensible word vectors than 32 dimensions for this particular case. Doubling this figure to 128, however, provided no noticeable improvement.\nâ€¢ None : Set to 1 to choose the skip-gram architecture, or leave at the 0 default to choose CBOW. As summarized in Table 11.1, SG is generally better suited to small datasets like our Gutenberg corpus.\nâ€¢ None : For SG, a window size of 10 (for a total of 20 context words) is a good bet, so we set this hyperparameter to 10. If we were using CBOW, then a window size of 5 (for a total of 10 context words) could be near the optimal value. In either case, this hyperparameter can be experimented with and evaluated extrinsically or intrinsically. Small adjustments to this hyperparameter may not be perceptibly impactful, however.\nâ€¢ None : By default, the gensim method iterates over the corpus fed into it (i.e., slides over all of the words) five times. Multiple iterations of word2vec is analogous to multiple epochs of training a deep learning model. With a small corpus like ours, the word vectors improve over several iterations. With a very large corpus, on the other hand, it might be cripplingly computationally expensive to run even two iterationsâ€”and, because there are so many examples of words in a very large corpus anyway, the word vectors might not be any better.\nâ€¢ None : This is the minimum number of times a word must occur across the corpus in order to fit it into word-vector space. If a given target word occurs only once or a few times, there are a limited number of examples of its contextual words to consider, and so its location in word-vector space may not be reliable. Because of this, a minimum count of about 10 is often reasonable. The higher the count, the smaller the vocabulary of words that will be available to your downstream NLP task. This is yet another hyperparameter that can be tuned, with extrinsic evaluations likely being more illuminating than intrinsic ones because the size of the vocabulary you have to work with could make a considerable impact on your downstream NLP application.\nâ€¢ None : This is the number of processing cores youâ€™d like to dedicate to training. If the CPU on your machine has, say, eight cores, then eight is the largest number of parallel worker threads you can have. In this case, if you choose to use fewer than eight cores, youâ€™re leaving compute resources available for other tasks.\n\nIn our GitHub repository, we saved our model using the save() method of word2vec objects:\n\nInstead of running word2vec yourself, then, youâ€™re welcome to load up our word vectors using this code:\n\nIf you do choose the word vectors we created, then the following examples will produce the same outputs. [Note: Every time word2vec is run, the initial locations of every word of the vocabulary within word-vector space are assigned randomly. Because of this, the same data and arguments provided to Word2Vec() will nevertheless produce unique word vectors every time, but the semantic relationships should be similar.] We can see the size of our vocabulary by calling . This tells us that there are 10,329 words (well, more specifically, tokens) that occur at least 10 times within our clean_sents corpus. [Note: Vocabulary size is equal to the number of tokens from our corpus that had occurred at least 10 times, because we set when calling in Example 11.8.] One of the words in our vocabulary is dog. As shown in Figure 11.6, we can output its location in 64-dimensional word-vector space by running .\n\nAs a rudimentary intrinsic evaluation of the quality of our word vectors, we can use the most_similar() method to confirm that words with similar meanings are found in similar locations within our word-vector space. [Note: Technically speaking, the similarity between two given words is computed here by calculating the cosine similarity.] For example, to output the three words that are most similar to father in our word-vector space, we can run this code:\n\nThis outputs the following:\n\nThis output indicates that mother, brother, and sister are the most similar words to father in our word-vector space. In other words, within our 64-dimensional space, the word that is closest. [Note: That is, has the shortest Euclidean distance in that 64-dimensional vector space.] to father is the word mother. Table 11.2 provides some additional examples of the words most similar to (i.e., closest to) particular words that weâ€™ve picked from our word- vector vocabulary, all five of which appear pretty reasonable given our small Gutenberg corpus. [Note that the final test word in Table 11.2â€”maâ€™amâ€”is only available because of the bigram collocation (see Examples 11.6 and 11.7).]\n\nSuppose we run the following line of code:\n\nWe get the output dog, indicating that dog is the least similar relative to all the other possible word pairs. We can also use the following line to observe that the similarity score between father and dog is a mere 0.44:\n\nThis similarity score of 0.44 is much lower than the similarity between father and any of mother, brother, or sister, and so itâ€™s unsurprising that dog is relatively distant from the other four words within our word-vector space.\n\nAs a final little intrinsic test, we can compute word-vector analogies as in Figure 2.7. For example, we can execute this code:\n\nThe top-scoring word comes out as mother, which is the correct answer to the analogy.\n\nIn this case, the top-scoring word comes out as wife, again the correct answer, thereby\n\nsuggesting that our word-vector space may generally be on the right track.\n\nThis contrasts with some other approaches that involve n-dimensional vector spaces, where the axes are intended to represent some specific explanatory variable. One such approach that many people are familiar with is principal component anal- ysis (PCA), a technique for identifying linearly uncorrelated (i.e., orthogonal) vectors that contribute to variance in a given dataset. A corollary of this difference between information stored as points in PCA versus in word-vector space is that in PCA, the first principal components contribute most of the variance, and so you can focus on them and ignore later principal components; but in a word-vector space, all of the dimensions may be important and need to be taken into consideration. In this way, approaches like PCA are useful for dimensionality reduction because we do not need to consider all of the dimensions.\n\nHuman brains are not well suited to visualizing anything in greater than three dimensions. Thus, plotting word vectorsâ€”which could have dozens or even hundreds of dimensionsâ€”in their native format is out of the question. Thankfully, we can use techniques for dimensionality reduction to approximately map the locations of words from high- dimensional word-vector space down to two or three dimensions. Our recommended approach for such dimensionality reduction is t-distributed stochastic neighbor embedding (t-SNE; pronounced tee-snee), which was developed by Laurens van der Maaten in col- laboration with Geoff Hinton (Figure 1.16). [Note: van der Maaten, L., & Hinton, G. (2008). Visualizing data using t-SNE. Journal of Machine Learning Research, 9, 2579â€“605.]\n\nExample 11.9 provides the code from our Natural Language Preprocessing notebook for reducing our 64-dimensional Project Gutenberg-derived word-vector space down to two dimensions, and then storing the resulting x and y coordinates within a Pandas DataFrame. There are two arguments for the method (from the scikit-learn library) that we need to focus on:\nâ€¢ None is the number of dimensions that should be returned, so setting this to 2 results in a two-dimensional output, whereas 3 would result in a three- dimensional output.\nâ€¢ None is the number of iterations over the input data. As with word2vec (Example 11.8), iterations are analogous to the epochs associated with training a neural network. More iterations corresponds to a longer training time but may improve the results (although only up to a point).\n\nRunning t-SNE as in Example 11.9 may take some time on your machine, so youâ€™re welcome to use our results if youâ€™re feeling impatient by running the following code:\n\n[Note: We created this CSV after running t-SNE on our word-vectors using this command: , ). Note that because t-SNE is stochastic, you will obtain a unique result every time you run it.]\n\nWhether you ran t-SNE to produce on your own or you loaded in ours, you can check out the first few lines of the DataFrame by using the :\n\nOur output from executing head() is shown in Figure 11.7. Example 11.10 provides code for creating a static scatterplot (Figure 11.8) of the two-dimensional data we created with t-SNE (in Example 11.9).\n\nOn its own, the scatterplot displayed in Figure 11.8 may look interesting, but thereâ€™s little actionable information we can take away from it. Instead, we recommend using the bokeh library to create a highly interactiveâ€”and actionableâ€”plot, as with the code provided in Example 11.11. [Note: In Example 11.11, we used the Pandas sample() method to reduce the dataset down to 5,000 tokens, because we found that using more data than this corresponded to a clunky user experience when using the bokeh plot interactively.]\n\nThe code in Example 11.11 produces the interactive scatterplot in Figure 11.9 using the and coordinates generated using t-SNE.\n\nBy toggling the Wheel Zoom button in the top-right corner of the plot, you can use your mouse to zoom into locations within the cloud so that the words become legible. For example, as shown in Figure 11.10, we identified a region composed largely of items of clothing, with related clusters nearby, including parts of the human anatomy, colors, and fabric types. Exploring in this way provides a largely subjective intrinsic evaluation of whether related termsâ€”and particularly synonymsâ€”cluster together as youâ€™d expect them to. Doing similar, you may also notice particular shortcomings of your natural-language preprocessing steps, such as the inclusion of punctuation marks, bigrams, or other tokens that you may prefer werenâ€™t included within your word-vector vocabulary.\n\nThe Area under the ROC Curve\n\nOur apologies for interrupting the fun, interactive plotting of word vectors. We need to take a brief break from natural language-specific content here to introduce a metric that will come in handy in the next section of the chapter, when we will evaluate the performance of deep learning NLP models.\n\nUp to this point in the book, most of our models have involved multiclass outputs: When working with the MNIST digits, for example, we used 10 output neurons to rep- resent each of the 10 possible digits that an input image could represent. In the remaining sections of this chapter, however, our deep learning models will be binary classifiers: They will distinguish between only two classes. More specifically, we will build binary classifiers to predict whether the natural language of film reviews corresponds to a favorable review or negative one.\n\nUnlike artificial neural networks tasked with multiclass problems, which require as many output neurons as classes, ANNs that are acting as binary classifiers require only a single output neuron. This is because there is no extra information associated with having two output neurons. If a binary classifier is provided some input x and it calculates some output [latex]\\hat{y}[/latex] for one of the classes, then the output for the other class is simply 1 - [latex]\\hat{y}[/latex]. As an example, if we feed a movie review into a binary classifier and it outputs that the probability that this review is a positive one is 0.85, then it must be the case that the probability of the review being negative is 1 âˆ’ 0.85 = 0.15.\n\nBecause binary classifiers have a single output, we can take advantage of metrics for evaluating our modelâ€™s performance that are sophisticated relative to the excessively black-and-white accuracy metric that dominates multiclass problems. A typical accuracy calculation, for example, would contend that if [latex]\\hat{y}[/latex] > 0.5 then the model is predicting that the input x belongs to one class, whereas if it outputs anything less than 0.5, it belongs to the other class. To illustrate why having a specific binary threshold like this is overly simplistic, consider a situation where inputting a movie review results in a binary classifier outputting [latex]\\hat{y}[/latex] = 0.48: A typical accuracy calculation threshold would hold thatâ€”because this [latex]\\hat{y}[/latex] is lower than 0.5--it is being classed as a negative review. If a second film review corresponds to an output of [latex]\\hat{y}[/latex] = 0.51, the model has barely any more confidence that this review is positive relative to the first review. Yet, because 0.51 is greater than the 0.5 accuracy threshold, the second review is classed as a positive review.\n\nThe starkness of the accuracy metric threshold can hide a fair bit of nuance in the quality of our modelâ€™s output, and so when evaluating the performance of binary classifiers, we prefer a metric called the area under the curve of the receiver operating characteristic. The ROC AUC, as the metric is known for short, has its roots in the Second World War, when it was developed to assess the performance of radar engineersâ€™ judgment as they attempted to identify the presence of enemy objects.\n\nWe like the ROC AUC for two reasons:\nâ€¢ None It blends together two useful metricsâ€”true positive rate and false positive rateâ€”into a single summary value.\nâ€¢ None It enables us to evaluate the performance of our binary classifierâ€™s output across the full range of [latex]\\hat{y}[/latex], from 0.0 to 1.0. This contrasts with the accuracy metric, which evaluates the performance of a binary classifier at a single threshold value onlyâ€” usually [latex]\\hat{y}[/latex] = 0.50.\n\nThe first step toward understanding how to calculate the ROC AUC metric is to under- stand the so-called confusion matrix, whichâ€”as youâ€™ll seeâ€”isnâ€™t actually all that confusing. Rather, the matrix is a straightforward 2 Ã— 2 table of how confused a model (or, as back in WWII, a person) is while attempting to act as a binary classifier. You can see an example of a confusion matrix in Table 11.3.\n\nTo bring the confusion matrix to life with an example, letâ€™s return to the hot dog / not hot dog binary classifier that weâ€™ve used to construct silly examples over many of the preceding chapters:\n\nWhen we provide some input x to a model and it predicts that the input represents a hot dog, then weâ€™re dealing with the first row of the table, because the predicted y = 1. In that case,\nâ€¢ None True positive: If the input is actually a hot dog (i.e., actual y = 1), then the model correctly classified the input.\nâ€¢ None False positive: If the input is actually not a hot dog (i.e., actual y = 0), then the model is confused.\nâ€¢ None False negative: If the input is actually a hot dog (i.e., actual y = 1), then the model is also confused in this circumstance.\nâ€¢ None True negative: If the input is actually not a hot dog (i.e., actual y = 0), then the model correctly classified the input.When we provide some input x to a model and it predicts that the input does not represent a hot dog, then weâ€™re dealing with the second row of the table, because predicted y = 0. In that case,\n\nBriefed on the confusion matrix, we can now move forward and calculate the ROC AUC metric itself, using a toy-sized example. Letâ€™s say, as shown in Table 11.4, we provide four inputs to a binary-classification model.\n\nTwo of these inputs are actually hot dogs (y = 1), and two of them are not hot dogs (y = 0). For each of these inputs, the model outputs some predicted [latex]\\hat{y}[/latex], all four of which are provided in Table 11.4.\n\nTo calculate the ROC AUC metric, we consider each of the [latex]\\hat{y}[/latex] values output by the model as the binary-classification threshold in turn. Letâ€™s start with the lowest [latex]\\hat{y}[/latex], which is 0.3 (see the â€œ0.3 thresholdâ€ column in Table 11.5). At this threshold, only the first input is classed as not a hot dog, whereas the second through fourth inputs (all with [latex]\\hat{y}[/latex] > 0.3) are all classed as hot dogs. We can compare each of these four predicted classifications with the confusion matrix in Table 11.3:\nâ€¢ None True negative (TN): This is actually not a hot dog (y = 0) and was correctly predicted as such.\nâ€¢ None True positive (TP): This is actually a hot dog (y = 1) and was correctly predicted as such.\nâ€¢ None False positive (FP): This is actually not a hot dog (y = 0) but it was erroneously predicted to be one.\nâ€¢ None True positive (TP): Like input 2, this is actually a hot dog (y = 1) and was correctly predicted as such.\n\nThe same process is repeated with the classification threshold set to 0.5 and yet again with the threshold set to 0.6, allowing us to populate the remaining columns of Table 11.5. As an exercise, it might be wise to work through these two columns, comparing the classifications at each threshold with the actual y values and the confusion matrix (Table 11.3) to ensure that you have a good handle on these concepts.Finally, note that the highest [latex]\\hat{y}[/latex] value (in this case, .09) can be skipped as a potential threshold, because at such a high threshold weâ€™d be considering all four instances to not be hot dogs, making it a ceiling instead of a classification boundary.\n\nThe next step toward computing the ROC AUC metric is to calculate both the true positive rate (TPR) and the false positive rate (FPR) at each of the three thresholds. Equations 11.1 and 11.2 use the â€œ0.3 thresholdâ€ column to provide examples of how to calculate the true positive rate and false positive rate, respectively.\n\nShorthand versions of the arithmetic for calculating TPR and FPR for the thresholds 0.5 and 0.6 are also provided for your convenience at the bottom of Table 11.5. Again, perhaps you should test if you can compute these values yourself on your own time.\n\nThe final stage in calculating ROC AUC is to create a plot like the one we provide in Figure 11.11. The points that make up the shape of the receiver operating characteristic (ROC) curve are the false positive rate (horizontal, x-axis coordinate) and true positive rate (vertical, y-axis coordinate) at each of the available thresholds (which in this case is three) in Table 11.5, plus two extra points in the bottom-left and top-right corners of the plot. Specifically, these five points (shown as orange dots in Figure 11.11) are:\n\nIn this toy-sized example, we only used four distinct [latex]\\hat{y}[/latex] values so there are only five points that determine the shape of the ROC curve, making the curve rather step shaped. When there are many available predictions providing many distinct [latex]\\hat{y}[/latex] valuesâ€”as is typically the case in real-world examplesâ€”the ROC curve has many more points, and so itâ€™s much less step shaped and much more, well, curve shaped. The area under the curve (AUC) of the ROC curve is exactly what it sounds like: In Figure 11.11, weâ€™ve shaded this area in orange and, in this example, the AUC constitutes 75 percent of all the possible area and so the ROC AUC metric comes out to 0.75.\n\nA binary classifier that works as well as chance will generate a straight diagonal running from the bottom-left corner of the plot to its top-right corner, so an ROC AUC of 0.5 indicates that the classifier works as well as flipping a coin. A perfect ROC AUC is 1.0, which is attained by having FPR = 0 and TPR = 1 across all of the available [latex]\\hat{y}[/latex] thresholds. When youâ€™re designing a binary classifier to perform well on the ROC AUC metric, the goal is thus to minimize FPR and maximize TPR across the range of [latex]\\hat{y}[/latex] thresholds. That said, for most problems you encounter, attaining a perfect ROC AUC of 1.0 is not possible: There is usually some noiseâ€”perhaps a lot of noiseâ€”in the data that makes perfection unattainable. Thus, when youâ€™re working with any given dataset, there is some (typically unknown!) maximum ROC AUC score, such that no matter how ideally suited your model is to act as a binary classifier for the problem, thereâ€™s an ROC AUC ceiling that no model can crack through.\n\nOver the remainder of this chapter we use the illuminating ROC AUC metric, alongside the simpler accuracy and cost metrics you are already acquainted with, to evaluate the performance of the binary-classifying deep learning models that we design and train.\n\nIn this section, we tie together concepts that were introduced in this chapterâ€”natural language preprocessing best practices, the creation of word vectors, and the ROC AUC metricâ€”with the deep learning theory from previous chapters. As we already alluded to earlier, the natural language processing model youâ€™ll experiment with over the remainder of the chapter will be a binary classifier that predicts whether a given film review is a positive one or a negative one. We begin by classifying natural language documents using types of neural networks that youâ€™re already familiar withâ€”dense and convolutionalâ€” before moving along to networks that are specialized to handle data that occur in a sequence.\n\nAs a performance baseline, weâ€™ll initially train and test a relatively simple dense network. All of the code for doing this is provided within our Dense Sentiment Classifier Jupyter notebook.\n\nExample 11.12 provides the dependencies we need for our dense sentiment classifier. Many of these dependencies will be recognizable from previous chapters, but others (e.g., for loading a dataset of film reviews, saving model parameters as we train, calculating ROC AUC) are new. As usual, we cover the details of these dependencies as we apply them later on.\n\nItâ€™s a good programming practice to put as many hyperparameters as you can at the top of your file. This makes it easier to experiment with these hyperparameters. It also makes it easier for you (or, indeed, your colleagues) to understand what you were doing in the file when you return to it (perhaps much) later. With this in mind, we place all of our hyperparameters together in a single cell within our Jupyter notebook. The code is provided in Example 11.13.\n\nLetâ€™s break down the purpose of each of these variables:\nâ€¢ None\nâ€¢ None : A directory name (ideally, a unique one) in which to store our modelâ€™s parameters after each epoch, allowing us to return to the parameters from any epoch of our choice at a later time.\nâ€¢ None : The number of epochs that weâ€™d like to train for, noting that NLP models often overfit to the training data in fewer epochs than machine vision models.\nâ€¢ None : As before, the number of training examples used during each round of model training (see Figure 8.5).\nâ€¢ None : The number of dimensions weâ€™d like our word-vector space to have.\nâ€¢ None : With word2vec earlier in this chapter, we included tokens in our word-vector vocabulary only if they occurred at least a certain number of times within our corpus. An alternative approachâ€”the one we take hereâ€”is to sort all of the tokens in our corpus by the number of times they occur, and then only use a certain number of the most popular words. Andrew Maas and his coworkers [Note: We mentioned Maas et al. (2011) earlier in this chapter. They put together the movie-review corpus weâ€™re using in this notebook.] opted to use the 5,000 most popular words across their film-review corpus and so weâ€™ll do the same.[Note: This 5,000-word threshold may not be optimal, but we didnâ€™t take the time to test lower or higher values. You are most welcome to do so yourself!]\nâ€¢ None : Instead of removing a manually curated list of stop words from their word-vector vocabulary, Maas et al. made the assumption that the 50 most frequently occurring words across their film-review corpus would serve as a decent list of stop words. We followed their lead and did the same.[Note: Note again that following Maas et al.â€™s lead may not be the optimal choice. Further, note that this means weâ€™ll actually be including the 51st most popular word through to the 5050th most popular word in our word-vector vocabulary.]\nâ€¢ None : Each movie review must have the same length so that TensorFlow knows the shape of the input data that will be flowing through our deep learning model. For this model, we selected a review length of 100 words.[Note: You are free to experiment with lengthier or shorter reviews.] Any reviews longer than 100 are truncated. Any reviews shorter than 100 are padded with a special padding character (analogous to the zero padding that can be used in machine vision, as in Figure 10.3).\nâ€¢ None : By selecting , we add padding characters to the start of every review. The alternative is , which adds them to the end. With a dense network like the one in this notebook, it shouldnâ€™t make much difference which of these options we pick. Later in this chapter, when weâ€™re working with specialized, sequential-data layer types, [Note: For example, RNN, LSTM.] itâ€™s generally best to use 'pre' because the content at the end of the document is more influential in the model and so we want the largely uninformative padding characters to be at the beginning of the document.\nâ€¢ None : As with , our truncation options are or . The former will remove words from the beginning of the review, whereas the latter will remove them from the end. By selecting , weâ€™re making (a bold!) assumption that the end of film reviews tend to include more information on review sentiment than the beginning.\nâ€¢ None : The number of neurons to include in the dense layer of our neural network architecture. We waved our finger in the air to select 64, so some experimentation and optimization are warranted at your end if you feel like it. For simplicityâ€™s sake, we also are using a single layer of dense neurons, but you could opt to have several.\nâ€¢ None : How much dropout to apply to the neurons in the dense layer. Again, we did not take the time to optimize this hyperparameter (set at 0.5) ourselves.\n\nLoading in the film review data is a one-liner, provided in Example 11.14.\n\nThis dataset from Maas et al. (2011) is made up of the natural language of reviews from the publicly available Internet Movie Database (IMDb; imdb.com). It consists of 50,000 reviews, half of which are in the training dataset , and half of which are for model validation . When submitting their review of a given film, users also provide a star rating, with a maximum of 10 stars. The labels are binary, based on these star ratings:\nâ€¢ None Reviews with a score of four stars or fewer are considered to be a negative review (y = 0).\nâ€¢ None Reviews with a score of seven stars or more, meanwhile, are classed as a positive review (y = 1).\nâ€¢ None Moderate reviewsâ€”those with five or six starsâ€”are not included in the dataset, making the binary classification task easier for any model.\n\nBy specifying values for the and arguments when calling , we are limiting the size of our word-vector vocabulary and removing the most common (stop) words, respectively.\n\nExecuting , we can examine the first six reviews from the training dataset, the first two of which are shown in Figure 11.12. These reviews are natively in an integer-index format, where each unique token from the dataset is represented by an integer. The first few integers are special cases, following a general convention that is widely used in NLP:\nâ€¢ None 0: Reserved as the padding token (which weâ€™ll soon add to the reviews that are shorter than ).\nâ€¢ None 1: Would be the starting token, which would indicate the beginning of a review. As per the next bullet point, however, the starting token is among the top 50 most common tokens and so is shown as â€œunknown.â€\nâ€¢ None 2: Any tokens that occur very frequently across the corpus (i.e., theyâ€™re in the top 50 most common words) or rarely (i.e., theyâ€™re below the top 5,050 most common words) will be outside of our word-vector vocabulary and so are replaced with this unknown token.\nâ€¢ None 3: The most frequently occurring word in the corpus.\nâ€¢ None 5: The third-most frequently occurring, and so on.\n\nUsing the following code from Example 11.15, we can see the length of the first six reviews in the training dataset.\n\nExample 11.15 Printing the number of tokens in six reviews\n\nThey are rather variable, ranging from 43 tokens up to 550 tokens. Shortly, weâ€™ll handle these discrepancies, standardizing all reviews to the same length.\n\nThe film reviews are fed into our neural network model in the integer-index format of Figure 11.12 because this is a memory-efficient way to store the token information. It would require appreciably more memory to feed the tokens in as character strings, for example. For us humans, however, it is uninformative (and, frankly, uninteresting) to examine reviews in the integer-index format. To view the reviews as natural language, we create an index of words as follows, where , , and are customary for representing padding, starting, and unknown tokens, respectively:\n\nThen we can use the code in Example 11.16 to view the film review of our choiceâ€”in this case, the first review from the training data.\n\nThe resulting string should look identical to the output shown in Figure 11.13.\n\nRemembering that the review in Figure 11.13 contains the tokens that are fed into our neural network, we might nevertheless find it enjoyable to read the full review without all of the tokens. In some cases of debugging model results, it might indeed even be practical to be able to view the full review. For example, if weâ€™re being too aggressive or conservative with either our or thresholds, it might become apparent by comparing a review like the one in Figure 11.13 with a full one. With our index of words already available to us, we simply need to download the full reviews:\n\nThen we modify Example 11.16 to execute on the full-review list of our choice (i.e., or ), as provided in Example 11.17.\n\nExecuting this outputs the full text of the review of our choiceâ€”again, in this case, the first training reviewâ€”as shown in Figure 11.14.\n\nStandardizing the Length of the Reviews\n\nBy executing Example 11.15 earlier, we discovered that there is variability in the length of the film reviews. In order for the Keras-created TensorFlow model to run, we need to specify the size of the inputs that will be flowing into the model during training. This enables TensorFlow to optimize the allocation of memory and compute resources. Keras provides a convenient method that enables us to both pad and truncate documents of text in a single line. Here we standardize our training and validation data in this way, as shown in Example 11.18.\n\nExample 11.18 Standardizing input length by padding and truncating\n\nNow, when printing reviews (e.g., with ) or their lengths (e.g., with the code from Example 11.15), we see that all of the reviews have the same length of 100 (because we set ). Examining x_train[5]â€”which previously had a length of only 43 tokensâ€”with code similar to Example 11.16, we can observe that the beginning of the review has been padded with 57 tokens (see Figure 11.15).\n\nWith sufficient NLP theory behind us, as well as our data loaded and preprocessed, weâ€™re at long last prepared to make use of a neural network architecture to classify film reviews by their sentiment. A baseline dense network model for this task is shown in Example 11.19.\n\nLetâ€™s break the architecture down line by line:\nâ€¢ None Weâ€™re using a Keras method to invoke a sequential model, as we have for all of the models so far in this book.\nâ€¢ None As with word2vec, the layer enables us to create word vectors from a corpus of documentsâ€”in this case, the 25,000 movie reviews of the IMDb training dataset. Relative to independently creating word vectors with word2vec (or GloVe, etc.) as we did earlier in this chapter, training your word vectors via backpropagation as a component of your broader NLP model has a potential advantage: The locations that words are assigned to within the vector space reflect not only word similarity but also the relevance of the words to the ultimate, specific purpose of the model (e.g., binary classification of IMDb reviews by sentiment). The size of the word-vector vocabulary and the number of dimensions of the vector space are specified by n_unique_words and n_dim, respectively. Because the embedding layer is the first hidden layer in our network, we must also pass into it the shape of our input layer: We do this with the input_length argument.\nâ€¢ None As in Chapter 10, the layer enables us to pass a many-dimensional output (here, a two-dimensional output from the embedding layer) into a one- dimensional dense layer.\nâ€¢ None Speaking of layers, we used a single one consisting of relu activations in this architecture, with applied to it.\nâ€¢ None We opted for a fairly shallow neural network architecture for our baseline model, but you can trivially deepen it by adding further layers (see the lines that are commented out)./li>\nâ€¢ None Finally, because there are only two classes to classify, we require only a single output neuron (because, as discussed earlier in this chapter, if one class has the probability p then the other class has the probability 1 âˆ’ p). This neuron is sigmoid because weâ€™d like it to output probabilities between 0 and 1 (refer to Figure 6.9).\n\nAs with using a ConvNet trained on the millions of images in ImageNet (Chapter 10), this natural language transfer learning is powerful, because these word vectors may have been trained on extremely large corpuses (e.g., all of Wikipedia, or the English-language Internet) that provide large, nuanced vocabularies that would be expensive to train yourself. Examples of pretrained word vectors are available in this repo and here. The fast- Text library also offers subword embeddings in 157 languages; these can be downloaded from fasttext.cc.\n\nIn this book, we donâ€™t cover substituting pretrained word vectors (be they down- loaded or trained separately from your deep learning model, as we did with earlier in this chapter) in place of the embedding layer, because there are many different permutations on how you might like to do this. See this neat tutorial from FrancÌ§ois Chollet, the creator of Keras.\n\nExecuting , we discover that our fairly simple NLP model has quite a few parameters, as shown in Figure 11.16:\nâ€¢ None In the embedding layer, the 320,000 parameters come from having 5,000 words, each one with a location specified in a 64-dimensional word-vector space (64 Ã— 5,000 = 320,000).\nâ€¢ None Flowing out of the embedding layer through the flatten layer and into the dense layer are 6,400 values: Each of our film-review inputs consists of 100 tokens, with each token specified by 64 word-vector-space coordinates (64 Ã— 100 = 6,400).\nâ€¢ None Each of the 64 neurons in the dense hidden layer receives input from each of the 6,400 values flowing out of the flatten layer, for a total of 64 Ã— 6,400 = 409,600 weights. And, of course, each of the 64 neurons has a bias, for a total of 409,664 parameters in the layer.\nâ€¢ None Finally, the single neuron of the output layer has 64 weightsâ€”one for the activation output by each of the neurons in the preceding layerâ€”plus its bias, for a total of 65 parameters.\nâ€¢ None Summing up the parameters from each of the layers, we have a grand total of 730,000 of them.\n\nAs shown in Example 11.20, we compile our dense sentiment classifier with a line of code that should already be familiar from recent chapters, except thatâ€”because we have a single output neuron within a binary classifierâ€”we use cost in place of the cost we used for our multiclass MNIST classifiers.\n\nWith the code provided in Example 11.21, we create a object that will allow us to save our model parameters after each epoch during training. By doing this, we can return to the parameters from our epoch of choice later on during model evaluation or to make inferences in a production system. If the directory doesnâ€™t already exist, we use the method to make it.\n\nExample 11.21 Creating an object and directory for checkpointing model parameters after each epoch\n\nLike the compile step, the model-fitting step (Example 11.22) for our sentiment classifier should be familiar except, perhaps, for our use of the callbacks argument to pass in the object. [Note: This isnâ€™t our first use of the callbacks argument. We previously used this argument, which can take in a list of multiple different , to provide data on model training progress to TensorBoard (see Chapter 9)].\n\nAs shown in Figure 11.17, we achieve our lowest validation loss (0.349) and highest validation accuracy (84.5 percent) in the second epoch. In the third and fourth epochs, the model is heavily overfit, with accuracy on the training set considerably higher than on the validation set. By the fourth epoch, training accuracy stands at 99.6 percent while validation accuracy is much lower, at 83.4 percent.\n\nTo evaluate the results of the best epoch more thoroughly, we use the Keras method to load the parameters from the second epoch ( ) back into our model, as in Example 11.23. [Note: Although the method is called , it loads in all model parameters, including biases. Because weights typically constitute the vast majority of parameters in a model, deep learning practitioners often call parameter files â€œweightsâ€ files. Earlier versions of Keras used zero indexing for epochs, but more recent versions index starting at 1.]\n\nWe can then calculate validation set y_hat values for the best epoch by passing the method on the dataset, as shown in Example 11.24.\n\nExample 11.24 Predicting y_hat for all validation\n\nWith y_hat[0], for example, we can now see the modelâ€™s prediction of the sentiment of the first movie review in the validation set. For this review, [latex]\\hat{y}[/latex] = 0.09, indicating the model estimates that thereâ€™s a 9 percent chance the review is positive and, therefore, a 91 percent chance itâ€™s negative. Executing y_valid[0] informs us that [latex]\\hat{y}[/latex] = 0 for this reviewâ€”that is, it is in fact a negative reviewâ€”so the modelâ€™s [latex]\\hat{y}[/latex] is pretty good! If youâ€™re curious about what the content of the negative review was, you can run a slight modification on Example 11.17 to access the full text of the list item, as shown in Example 11.25.\n\nExamining individual scores can be interesting, but we get a much better sense of our modelâ€™s performance by looking at all of the validation results together. We can plot a histogram of all the validation [latex]\\hat{y}[/latex] values by running the code in Example 11.26.\n\nThe histogram output is provided in Figure 11.18.\n\nThe plot shows that the model often has a strong opinion on the sentiment of a given review: Some 8,000 of the 25,000 re- views (~32 percent of them) are assigned a [latex]\\hat{y}[/latex] of less than 0.1, and ~6,500 (~26 percent) are given a [latex]\\hat{y}[/latex] greater than 0.9.\n\nThe vertical orange line in Figure 11.18 marks the 0.5 threshold above which reviews are considered by a simple accuracy calculation to be positive. As discussed earlier in the chapter, such a simple threshold can be misleading, because a review with a yË† just be- low 0.5 is not predicted by the model to have much difference in sentiment relative to\n\na review with a [latex]\\hat{y}[/latex] just above 0.5. To obtain a more nuanced assessment of our modelâ€™s performance as a binary classifier, we can use the method from the scikit-learn metrics library to straightforwardly calculate the ROC AUC score across the validation data, as shown in Example 11.27.\n\nPrinting the output in an easy-to-read format with the method, we see that the percentage of the area under the receiver operating characteristic curve is (a fairly high) 92.9 percent.\n\nTo get a sense of where the model breaks down, we can create a DataFrame of y and [latex]\\hat{y}[/latex] validation set values, using the code in Example 11.28.\n\nExample 11.28 Creating a ydf DataFrame of y and Ë†y values\n\nPrinting the first 10 rows of the resulting DataFrame with , we see the output shown in Figure 11.19.\n\nQuerying the DataFrame as we do in Examples 11.29 and 11.30 and then examining the individual reviews these queries surface by varying the list index in Example 11.25, you can get a sense of the kinds of reviews that cause the model to make its largest errors.\n\nExample 11.29 Ten cases of negative validation reviews with high [latex]\\hat{y}[/latex] scores\n\nExample 11.30 Ten cases of positive validation reviews with low [latex]\\hat{y}[/latex] scores\n\nAn example of a false positiveâ€”a negative review (y = 0) with a very high model score ([latex]\\hat{y}[/latex] = 0.97)â€”that was identified by running the code in Example 11.29 is provided in Figure 11.20. [Note: We output this particular reviewâ€”the 387th in the validation datasetâ€”by running the following code: for .] And an example of a false negativeâ€”a positive review (y = 1) with a very low model score ([latex]\\hat{y}[/latex] = 0.06)â€”that was identified by running the code in Example 11.30 is provided in Figure 11.21. [Note: R ) to print out this same review yourself.]\n\nCarrying out this kind of post hoc analysis of our model, one potential shortcoming that surfaces is that our dense classifier is not specialized to detect patterns of multiple tokens occurring in a sequence that might predict film-review sentiment. For example, it might be handy for patterns like the token-pair not-good to be easily detected by the model as predictive of negative sentiment.\n\nAs covered in Chapter 10, convolutional layers are particularly adept at detecting spatial patterns. In this section, we use them to detect spatial patterns among wordsâ€”like the not-good sequenceâ€”and see whether they can improve upon the performance of our dense network at classifying film reviews by their sentiment. All of the code for this ConvNet can be found in our Convolutional Sentiment Classifier notebook.\n\nThe dependencies for this model are identical to those of our dense sentiment classifier (see Example 11.12), except that it has three new Keras layer types, as provided in Example 11.31.\n\nThe hyperparameters for our convolutional sentiment classifier are provided in Example 11.32.\n\nRelative to the hyperparameters from our dense sentiment classifier (see Example 11.13):\nâ€¢ None We have a new, unique directory name (' ') for storing model parameters after each epoch of training.\nâ€¢ None Our number of epochs and batch size remain the same.\nâ€¢ None\nâ€¢ None We quadrupled to 400. We did this because, despite the fairly dramatic increase in input volume as well as an increase in our number of hidden layers, our convolutional classifier will still have far fewer parameters relative to our dense sentiment classifier.\nâ€¢ None With , weâ€™ll be adding dropout to our embedding layer. Our vector-space embedding hyperparameters remain the same, except that\nâ€¢ None\nâ€¢ None A convolutional layer with 256 filters ( ), each with a single dimension (a length) of 3 ( ). When working with two-dimensional images in Chapter 10, our convolutional layers had filters with two dimensions. Natural languageâ€”be it written or spokenâ€”has only one dimension associated with it (the dimension of time) and so the convolutional layers used in this chapter will have one-dimensional filters.\nâ€¢ None A dense layer with 256 neurons ( ) and dropout of 20 percent. Our convolutional sentiment classifier will have two hidden layers after the embedding layer:\n\nThe steps for loading the IMDb data and standardizing the length of the reviews are identical to those in our Dense Sentiment Classifier notebook (see Examples 11.14 and 11.18). The model architecture is of course rather different, and is provided in Example 11.33.\nâ€¢ None Our embedding layer is the same as before, except that it now has dropout applied to it.\nâ€¢ None We no longer require , because the layer takes in both dimensions of the embedding layer output.\nâ€¢ None We use activation within our one-dimensional convolutional layer. The layer has 256 unique filters, each of which is free to specialize in activating when it passes over a particular three-token sequence. The activation map for each of the 256 filters has a length of 398, for a 256Ã—398 output shape. [Note: As described in Chapter 10, when a two-dimensional filter convolves over an image, we lose pixels around the perimeter if we donâ€™t pad the image first. In this natural language model, our one-dimensional convolutional filter has a length of three, so, on the far left of the movie review, it begins centered on the second token and, on the far right, it ends centered on the second-to-last token. Because we didnâ€™t pad the movie reviews at both ends before feeding them into the convolutional layer, we thus lose a tokenâ€™s worth of information from each end: 400 âˆ’ 1 âˆ’ 1 = 398. Weâ€™re not upset about this loss.\nâ€¢ None If you fancy it, youâ€™re welcome to add additional convolutional layers, by, for example, uncommenting the second\nâ€¢ None Global max-pooling is common for dimensionality reduction within deep learning NLP models. We use it here to squash the activation map from 256 Ã— 398 to 256 Ã— 1. By applying it, only the magnitude of largest activation for a given convolutional filter is retained by the maximum-calculating operation, and we lose any temporal-position-specific information the filter may have output to its 398-element-long activation map.\nâ€¢ None Because the activations output from the global max-pooling layer are one- dimensional, they can be fed directly into the dense layer, which consists (again) of relu neurons and dropout is applied.\nâ€¢ None The output layer remains the same.\nâ€¢ None The model has a grand total of 435,000 parameters (see Figure 11.22), several hundred thousand fewer than our dense sentiment classifier. Per epoch, this model will nevertheless take longer to train because the convolutional operation is relatively computationally expensive.\n\nA critical item to note about this model architecture is that the convolutional filters are not detecting simply triplets of words. Rather, they are detecting triplets of word vectors. Following from our discussion in Chapter 2, contrasting discrete, one-hot word representations with the word-vector representations that gently smear meaning across a high-dimensional space (see Table 2.1), all of the models in this chapter become specialized in associating word meaning with review sentimentâ€”as opposed to merely associating individual words with review sentiment. As an example, if the network learns that the token pair not-good is associated with a negative review, then it should also associate the pair not-great with negative reviews, because good and great have similar meanings (and thus should occupy a similar location in word-vector space).\n\nThe compile, checkpoint, and model-fitting steps are the same as for our dense sentiment classifier (see Examples 11.20, 11.21, and 11.22, respectively). Model-fitting progress is shown in Figure 11.23. The epoch with the lowest validation loss (0.258) and highest validation accuracy (89.6 percent) was the third epoch. Loading the model parameters from that epoch back in (with the code from Example 11.23 but specifying ), we then predict [latex]\\hat{y}[/latex] for all validation data (exactly as in Example 11.24). Creating a histogram (Figure 11.24) of these [latex]\\hat{y}[/latex] values (with the same code as in Example 11.26), we can see visually that our CNN has a stronger opinion of review sentiment than our dense network did (refer to Figure 11.18): There are about a thousand more reviews with [latex]\\hat{y}[/latex] < 0.1 and several thousand more with [latex]\\hat{y}[/latex] > 0.9. Calculating ROC AUC (with the code from Example 11.27), we output a very high score of 96.12 percent, indicating that the CNNâ€™s confidence was not misplaced: It is a marked improvement over the already high ~93 percent score of the dense net.\n\nOur ConvNet classifier outperformed our dense netâ€”perhaps in large part because its convolutional layer is adept at learning patterns of words that predict some outcome, such as whether a film review is favorable or negative. The filters within convolutional layers tend to excel at learning short sequences like triplets of words (recall that we set in Example 11.32), but a document of natural language like a movie review might contain much longer sequences of words that, when considered all together, would enable the model to accurately predict some outcome. To handle long sequences of data like this, there exists a family of deep learning models called recurrent neural networks (RNNs), which include specialized layer types like long short-term memory units (LSTMs) and gated recurrent units (GRUs). In this section, we cover the essential theory of RNNs and apply several variants of them to our movie-review classification problem. We also introduce attentionâ€”an especially sophisticated approach to modeling natural language data that is setting new benchmarks across NLP applications.\n\nAs mentioned at the start of the chapter, the RNN family, including LSTMs and GRUs, is well suited to handling not only natural language data but also any input data that occur in a one-dimensional sequence. This includes price data (e.g., financial time series, stock prices), sales figures, temperatures, and disease rates (epidemiology). While RNN applications other than NLP are beyond the scope of this textbook, we collate resources for modeling quantitative data over time at jonkrohn.com/resources under the heading Time Series Prediction.\n\nConsider the following sentences:\n\nJon and Grant are writing a book together. They have really enjoyed writing it.\n\nThe human mind can track the concepts in the second sentence quite easily. You already know that â€œtheyâ€ in the second sentence refers to your authors, and â€œitâ€ refers to the book weâ€™re writing. Although this task is easy for you, however, it is not so trivial for a neural network.\n\nThe convolutional sentiment classifier we built in the previous section was able to consider a word only in the context of the two words on either side of it ( , as in Example 11.32). With such a small window of text, that neural network had no capacity to assess what â€œtheyâ€ or â€œitâ€ might be referring to. Our human brains can do it because our thoughts loop around each other, and we revisit earlier ideas in order to in- form our understanding of the current context. In this section we introduce the concept of recurrent neural networks, which set out to do just that: They have loops built into their structure that allow information to persist over time.\n\nThe high-level structure of a recurrent neural network (RNN) is shown in Figure 11.25. On the left, the purple line indicates the loop that passes information between steps in the network. As in a dense network, where there is a neuron for each input, so too is there a neuron for each input here. We can observe this more easily on the right, where the schematic of the RNN is unpacked. There is a recurrent module for each word in the sentence (only the first four words are shown here for brevity).[Note: This is also why we have to pad shorter sentences during preprocessing: The RNN expects a sequence of a particular length, and so if the sequence is not long enough we add PAD tokens to make up the difference.] However, each module receives an additional input from the previous module, and in doing so the network is able to pass along information from earlier timesteps in the sequence. In the case of Figure 11.25, each word is represented by a distinct timestep in the RNN sequence, so the network might be able to learn that â€œJonâ€ and â€œGrantâ€ were writing the book, thereby associating these terms with the word â€œtheyâ€ that occurs later in the sequence.\n\nRecurrent neural networks are, computationally, more complex to train than exclusively â€œfeedforwardâ€ neural networks like the dense nets and CNNs weâ€™ve used so far in the book. As depicted in Figure 8.6, feedforward networks involve backpropagating cost from the output layer back toward the input layer. If a network includes a recurrent layer (such as , , or ), then the cost must be backpropagated not only back toward the input layer, but back over the timesteps of the recurrent layer (from later timesteps back toward earlier timesteps), as well. Note that, in the same way that the gradient of learning vanishes as we backpropagate over later hidden layers toward earlier ones (see Figure 8.8), so, too, does the gradient vanish as we backpropagate over later timesteps within a recurrent layer toward earlier ones. Because of this, later timesteps in a sequence have more influence within the model than earlier ones do. [Note: If you suspect that the beginning of your sequences (e.g., the words at the beginning of a movie review) is generally more relevant to the problem youâ€™re solving with your model (sentiment classification) than the end (the words at the end of the review), you can reverse the sequence before passing it as an input into your network. In that way, within your networkâ€™s recurrent layers, the beginning of the sequence will be backpropagated over before the end is.]\n\nAdding a recurrent layer to a neural network architecture to create an RNN is straightforward in Keras, as we illustrate in our RNN Sentiment Classifier Jupyter notebook. For the sake of brevity and readability, please note that the following code cells are identical across all the Jupyter notebooks in this chapter, including the Dense and Convolutional Sentiment Classifier notebooks that weâ€™ve already covered:\nâ€¢ None Loading dependencies (Example 11.12), except that there are often one or two additional dependencies in a given notebook.\nâ€¢ None Weâ€™ll note these additions separatelyâ€”typically when we present the notebookâ€™s neural network architecture.\nâ€¢ None Creating the object and directory (Example 11.21).\nâ€¢ None Loading the model parameters from the best epoch (Example 11.23), with the critical exception that the particular epoch we select to load varies depending on which epoch has the lowest validation loss.\nâ€¢ None Predicting [latex]\\hat{y}[/latex] for all validation data (Example 11.24).\n\nThe code cells that vary are those in which we:\n\nThe hyperparameters for our RNN are as shown in Example 11.34.\n\nChanges relative to our previous sentiment classifier notebooks are:\nâ€¢ None We quadrupled epochs of training to 16 because overfitting didnâ€™t occur in the early epochs.\nâ€¢ None We lowered back down to 100, although even this is excessive for a simple RNN. We can backpropagate over about 100 timesteps (i.e., 100 tokens or words in a natural language model) with an LSTM (covered in the next section) before the gradient of learning vanishes completely, but the gradient in a plain old RNN vanishes completely after about 10 timesteps. Thus, max_review_length could probably be lowered to less than 10 before we would notice a reduction in this modelâ€™s performance.\nâ€¢ None For all of the RNN-family architectures in this chapter, we experimented with doubling the word-vector vocabulary to 10000 tokens. This seemed to provide improved results for these architectures, although we didnâ€™t test it rigorously.\nâ€¢ None We set , so we could say that this recurrent layer has 256 units, or, alternatively, we could say it has 256 cells. In the same way that having 256 convolutional filters enabled our CNN model to specialize in detecting 256 unique triplets of word meaning,43 this setting enables our RNN to detect 256 unique sequences of word meaning that may be relevant to review sentiment. [Note: â€œWord meaningâ€ here refers to a location in word-vector space]\n\nOur RNN model architecture is provided in Example 11.35.\n\nIn place of a convolutional layer or a dense layer (or both) within the hidden layers of this model, we have a Keras layer, which has a dropout argument; as a result, we didnâ€™t need to add dropout in a separate line of code. Unlike putting a dense layer after a convolutional layer, it is relatively uncommon to add a dense layer after a recurrent layer, because it provides little performance advantage. Youâ€™re welcome to try it by adding in a hidden layer anyway.\n\nThe results of running this model (which are shown in full in our RNN Sentiment Classifier notebook) were not encouraging. We found that the training loss, after going down steadily over the first half-dozen epochs, began to jump around after that. This indicates that the model is struggling to learn patterns even within the training data, whichâ€”relative to the validation dataâ€”it should be readily able to do. Indeed, all of the models fit so far in this book have had training losses that reliably attenuated epoch over epoch.\n\nAs the training loss bounced around, so too did the validation loss. We observed the lowest validation loss in the seventh epoch (0.504), which corresponded to a validation accuracy of 77.6 percent and an ROC AUC of 84.9 percent. All three of these metrics are our worst yet for a sentiment classifier model. This is because, as we mentioned earlier in this section, RNNs are only able to backpropagate through ~10 time steps before the gradient diminishes so much that parameter updates become negligibly small. Because of this, simple RNNs are rarely used in practice: More-sophisticated recurrent layer types like LSTMs, which can backpropagate through ~100 time steps, are far more common.[Note: The only situation we could think of where a simple RNN would be practical is one where your sequences only had 10 or fewer consecutive timesteps of information that are relevant to the problem youâ€™re solving with your model. This might be the case with some time series forecasting models or if you only had very short strings of natural language in your dataset.]\n\nAs stated at the end of the preceding section, simple RNNs are adequate if the space between the relevant information and the context where itâ€™s needed is small (fewer than 10 timesteps); however, if the task requires a broader context (which is often the case in NLP tasks), there is another recurrent layer type that is well suited to it: long short-term memory units, or LSTMs.\n\nLSTMs were introduced by Sepp Hochreiter and JuÌˆrgen Schmidhuber in 1997,[Note: Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. Neural Computation, 9, 1735â€“80.] but they are more widely used in NLP deep learning applications today than ever before. The basic structure of an LSTM layer is the same as the simple recurrent layers captured in Figure 11.25.\n\nLSTMs receive input from the sequence of data (e.g., a particular token from a natural language document), and they also receive input from the previous time point in the sequence. The difference is that inside each cell in a simple recurrent layer (e.g., in Keras), youâ€™ll find a single neural network activation function such as a tanh function, which transforms the RNN cellâ€™s inputs to generate its output. In contrast, the cells of an LSTM layer contain a far more complex structure, as depicted in Figure 11.26.\n\nThis schematic can appear daunting, and, admittedly, we agree that a full step-by-step breakdown of each component inside of an LSTM cell is unnecessarily detailed for this book. [Note: For a thorough exposition of LSTM cells, we recommend Christopher Olahâ€™s highly visual explainer, which is available at bit.ly/colahLSTM.] That said, there are a few key points that we should nevertheless touch on here. The first is the cell state running across the top of the LSTM cell. Notice that the cell state does not pass through any nonlinear activation functions. In fact, the cell state only undergoes some minor linear transformations, but otherwise it simply passes through from cell to cell. Those two linear transformations (a multiplication and an addition operation) are points where a cell in an LSTM layer can add information to the cell state, information that will be passed onto the next cell in the layer. In either case, there is a sigmoid activation (represented by Ïƒ in the figure) before the information is added to the cell state. Because a sigmoid activation produces values between 0 and 1, these sigmoids act as â€œgatesâ€ that decide whether new information (from the current timestep) is added to the cell state or not.\n\nThe new information at the current timestep is a simple concatenation of the current timestepâ€™s input and the hidden state from the preceding timestep. This concatenation has two chances to be incorporated into the cell stateâ€”either linearly or following a nonlin- ear tanh activationâ€”and in either case itâ€™s those sigmoid gates that decide whether the information is combined.\n\nAfter the LSTM has determined what information to add to the cell state, another sigmoid gate decides whether the information from the current input is added to the final cell state, and this results in the output for the current timestep. Notice that, under a different name (â€œhidden stateâ€), the output is also sent into the next LSTM module (which represents the next timestep in the sequence), where it is combined with the next timestepâ€™s input to begin the whole process again, and that (alongside the hidden state) the final cell state is also sent to the module representing the next timestep.\n\nWe know this might be a lot to come to grips with. Another way to distill this LSTM content is:\nâ€¢ None The cell state enables information to persist along the length of the sequence, through each timestep in a given LSTM cell. It is the long-term memory of the LSTM.\nâ€¢ None The hidden state is analogous to the recurrent connections in a simple RNN and represents the short-term memory of the LSTM.\nâ€¢ None Each module represents a particular point in the sequence of data (e.g., a particular token from a natural language document).\nâ€¢ None At each timestep, several decisions are made (using those sigmoid gates) about whether the information at that particular timestep in the sequence is relevant to the local (hidden state) and global (cell state) contexts.\nâ€¢ None The first two sigmoid gates determine whether the information from the current timestep is relevant to the global context (the cell state) and how it will be com- bined into that stream.\nâ€¢ None The final sigmoid gate determines whether the information from the current timestep is relevant to the local context (i.e., whether it is added to the hidden state, which doubles as the output for the current timestep).\n\nWe recommend taking a moment to reconsider Figure 11.26 and see if you can follow how information moves through an LSTM cell. This task should be easier if you keep in mind that the sigmoid gates decide whether information is let through or not. Regardless, the primary takeaways from this section are:\nâ€¢ None Simple RNN cells pass only one type of information (the hidden state) between timesteps and contain only one activation function.\nâ€¢ None LSTM cells are markedly more complex: They pass two types of information between timesteps (hidden state and cell state) and contain five activation functions.\n\nDespite all of their additional computational complexity, as demonstrated within our LSTM Sentiment Classifier notebook, implementing LSTMs with Keras is a breeze. As shown in Example 11.36, we selected the same hyperparameters for our LSTM as we did for our simple RNN, except:\nâ€¢ None We changed the output directory name.\nâ€¢ None We updated variable names to and .\nâ€¢ None We reduced the number of epochs of training to 4 because the LSTM begins to overfit to the training data much earlier than the simple RNN.\n\nOur LSTM model architecture is also the same as our RNN architecture, except that we replaced the layer with ; see Example 11.37.\n\nThe results of training the LSTM are provided in full in our LSTM Sentiment Classifier notebook. To summarize, training loss decreased steadily epoch over epoch, suggesting that model-fitting proceeded more conventionally than with our simple RNN. The results are not a slam dunk, however. Despite its relative sophistication, our LSTM per- formed only as well as our baseline dense model. The LSTMâ€™s epoch with the lowest validation loss is the second one (0.349); it had a validation accuracy of 84.8 percent and an ROC AUC of 92.8 percent.\n\nBidirectional LSTMs (or Bi-LSTMs, for short) are a clever variation on standard LSTMs. Whereas the latter involve backpropagation in only one direction (typically backward over timesteps, such as from the end of a movie review toward the beginning), bidirectional LSTMs involve backpropagation in both directions (backward and forward over timesteps) across some one-dimensional input. This extra backpropagation doubles computational complexity, but if accuracy is paramount to your application, it is often worth it: Bi-LSTMs are a popular choice in modern NLP applications because their ability to learn patterns both before and after a given token within an input document facilitates high-performing models.\n\nConverting our LSTM architecture (Example 11.37) into a Bi-LSTM architecture is painless. We need only wrap our layer within the wrapper, as shown in Example 11.38.\n\nThe straightforward conversion from LSTM to Bi-LSTM yielded substantial performance gains, as the results of model-fitting show (provided in full in our Bi LSTM Sentiment Classifier notebook). The epoch with the lowest validation loss (0.331) was the fourth, which had validation accuracy of 86.0 percent and an ROC AUC of 93.5 per- cent, making it our second-best model so far as it trails behind only our convolutional architecture.\n\nStacking multiple RNN-family layers (be they , , or another type) is not quite as straightforward as stacking dense or convolutional layers in Kerasâ€”although it certainly isnâ€™t difficult: It requires only specifying an extra argument when the layer is defined.\n\nAs weâ€™ve discussed, recurrent layers take in an ordered sequence of inputs. The recurrent nature of these layers comes from their processing each timestep in the sequence and passing along a hidden state as an input to the next timestep in the sequence. Upon reaching the final timestep in the sequence, the output of a recurrent layer is the final hidden state.\n\nSo in order to stack recurrent layers, we use the argument . This asks the recurrent layer to return the hidden states for each step in the layerâ€™s sequence. The resulting output now has three dimensions, matching the dimensions of the input sequence that was fed into it. The default behavior of a recurrent layer is to pass only the final hidden state to the next layer. This works perfectly well if weâ€™re passing this information to, say, a dense layer. If, however, weâ€™d like the subsequent layer in our network to be another recurrent layer, that subsequent recurrent layer must receive a sequence as its input. Thus, to pass the array of hidden states from across all individual timesteps in the sequence (as opposed to only the single final hidden state value) to this subsequent recurrent layer, we set the optional argument to . [Note: There is also a return_state argument (which, like return_sequences, defaults to False) that asks the network to return the final cell state in addition to the final hidden state. This optional argument is not used as often, but it is useful when weâ€™d like to initialize a recurrent layerâ€™s cell state with that of another layer, as we do in â€œencoder-decoderâ€ models (introduced in the next section)]\n\nTo observe this in action, check out the two-layer Bi-LSTM model shown in Example 11.39. (Notice that in this example we still leave the final recurrent layer with its default so that only the final hidden state of this final recurrent layer is returned for use further downstream in the network.)\n\nAs youâ€™ve discovered a number of times since Chapter 1 of this book, additional layers within a neural network model can enable it to learn increasingly complex and abstract representations. In this case, the abstraction facilitated by the supplementary Bi-LSTM layer translated to performance gains. The stacked Bi-LSTM outperformed its unstacked cousin by a noteworthy margin, with an ROC AUC of 94.9 percent and validation accuracy of 87.8 percent in its best epoch (the second, with its validation loss of 0.296). The full results are provided in our Stacked Bi LSTM Sentiment Classifier notebook.\n\nThe performance of our stacked Bi-LSTM architecture, despite being considerably more sophisticated than our convolutional architecture and despite being designed specifically to handle sequential data like natural language, nevertheless lags behind the accuracy of our ConvNet model. Perhaps some hyperparameter experimentation and fine-tuning would yield better results, but ultimately our hypothesis is that because the IMDb film review dataset is so small, our LSTM models donâ€™t have an opportunity to demonstrate their potential. We opine that a much larger natural language dataset would facilitate effective backpropagation over the many timesteps associated with LSTM layers. [Note: If youâ€™d like to test our hypothesis yourself, we provide appropriate sentiment analysis dataset suggestions in Chapter 14.]\n\nA relative of the LSTM within the family of RNNs is the gated recurrent unit (GRU). GRUs are slightly less computationally intensive than LSTMs because they involve only three activation functions, and yet their performance often approaches the performance of LSTMs. If a bit more compute isnâ€™t a deal breaker for you, we see little advantage in choosing a GRU over an LSTM. If youâ€™re interested in trying a GRU in Keras anyway, itâ€™s as easy as importing the GRU() layer type and dropping it into a model architecture where you might otherwise place an LSTM() layer. Check out our GRU Sentiment Classifier notebook for a hands-on example. [Note: Cho, K., et al. (2014). Learning phrase representations using RNN encoder-decoder for statistical machine translation. arXiv:1406.1078.]\n\nNatural language techniques that involve so-called sequence-to-sequence (seq2seq; pronounced â€œseek-to-seekâ€) models take in an input sequence and generate an output sequence as their product. Neural machine translation (NMT) is a quintessential class of seq2seq models, with Google Translateâ€™s machine-translation algorithm serving as an example of NMT being used in a production system. [Note: Google Translate has incorporated NMT since 2016. You can read more about it at bit.ly/translateNMT.]\n\nNMTs consist of an encoder-decoder structure, wherein the encoder processes the input sequence and the decoder generates the output sequence. The encoder and decoder are both RNNs, and so during the encoding step there exists a hidden state that is passed between units of the RNN. At the end of the encoding phase, the final hidden state is passed to the decoder; this final state can be referred to as the â€œcontext.â€ In this way, the decoder starts with a context for what is happening in the input sequence. Although this idea is sound in theory, the context is often a bottleneck: Itâ€™s difficult for models to handle really long sequences, and so the context loses its punch.\n\nAttention was developed to overcome the computational bottleneck associated with context. [Note: Bahdanau, D., et al. (2014). Neural machine translation by jointly learning to align and translate. arXiv:1409.0473]. In a nutshell, instead of passing a single hidden state vector (the final one) from the encoder to the decoder, with attention we pass the full sequence of hidden states to the decoder. Each of these hidden states is associated with a single step in the input sequence, although the decoder might need the context from multiple steps in the input to inform its behavior at any given step during decoding. To achieve this, for each step in the sequence the decoder calculates a score for each of the hidden states from the encoder. Each encoder hidden state is multiplied by the softmax of its score. [Note: Recall from Chapter 6 that the softmax function takes a vector of real numbers and generates a probability distribution with the same number of classes as the input vector.]\n\nThis serves to amplify the most relevant contexts (they would have high scores, and thus higher softmax probabilities) while muting the ones that arenâ€™t relevant; in essence, attention weights the available contexts for a given timestep. The weighted hidden states are summed, and this new context vector is used to predict the output for each timestep in the decoder sequence.\n\nFollowing this approach, the model selectively reviews what it knows about the input sequence and uses only the relevant information where necessary to inform the output. Itâ€™s paying attention to the most relevant elements of the whole sentence! If this book were dedicated solely to NLP, weâ€™d have at least a chapter covering seq2seq and attention. As it stands, weâ€™ll have to leave it to you to further explore these techniques, which are raising the bar of the performance of many NLP applications.\n\nMachine vision practitioners have for a number of years been helped along by the ready availability of nuanced models that have been pretrained on large, rich datasets. As covered in the â€œTransfer Learningâ€ section near the end of Chapter 10, casual users can download model architectures with pretrained weights and rapidly scale up their particular vision application to a state-of-the-art model. Well, more recently, such transfer learning has become readily available for NLP, too. [Note:When we introduced Keras Embedding() layers earlier in this chapter, we touched on transfer learning with word vectors. The transfer learning approaches covered in this sectionâ€”ULMFiT, ELMo, and BERTâ€”are closer in spirit to the transfer learning of machine vision, because (analogous to the hierarchical visual features that are represented by a deep CNN; see Figure 1.17) they allow for the hierarchical representation of the elements of natural language (e.g., subwords, words, and context, as in Figure 2.9). Word vectors, in contrast, have no hierarchy; they capture only the word level of language.]\n\nFirst came ULMFiT (universal language model fine-tuning), wherein tools were described and open-sourced that enabled others to use a lot of what the model learns during pretraining. [Note: Howard, J., and Ruder, S. (2018). Universal language model fine-tuning for text classification. arXiv:1801.06146] In this way, models can be fine-tuned on task-specific data, thus requiring less training time and fewer data to attain high-accuracy results.\n\nShortly thereafter, ELMo (embeddings from language models) was revealed to the world. [Note: Peters, M.E., et al. (2018). Deep contextualized word representations. arXiv:1802.05365.] In this update to the standard word vectors we introduced in this chapter, the word embeddings are dependent not only on the word itself but also on the context in which the word occurs. In place of a fixed word embedding for each word in the dictionary, ELMo looks at each word in the sentence before assigning each word a specific embedding. The ELMo model is pretrained on a very large corpus; if you had to train it yourself, it would likely strain your compute resources, but you can now nevertheless use it as a component in your own NLP models.\n\nThe final transfer learning development weâ€™ll mention is the release of BERT (bi-directional encoder representations from transformers) from Google. [Note: Devlin, J., et al. (2018). BERT: Pre-training of deep bidirectional transformers for language understanding. arXiv: 0810.04805.] Perhaps even more so than ULMFiT and ELMo, pretrained BERT models tuned to particular NLP tasks have been associated with the achievement of new state-of-the-art benchmarks across a broad range of applications, while requiring much less training time and fewer data to get there.\n\nTo solve a given problem, there are countless ways that the layer types weâ€™ve already covered in this book can be recombined to form deep learning model architectures. For example, see our Conv LSTM Stack Sentiment Classifier notebook, wherein we were extra creative in designing a model that involves a convolutional layer passing its activations into a Bi-LSTM layer.57 Thus far, however, our creativity has been constrained by our use of the Keras model, which requires each layer to flow directly into a following one.\n\nAlthough sequential models constitute the vast majority of deep learning models, there are times when non-sequential architecturesâ€”which permit infinite model-design possibilities and are often more complexâ€”could be warranted.58 In such situations, we can take advantage of the Keras functional API, which makes use of the Model class instead of the Sequential models weâ€™ve worked with so far in this book.\n\nAs an example of a non-sequential architecture, we decided to riff on our highest- performing sentiment classifier, the convolutional model, to see if we could squeeze more juice out of the proverbial lemon. As diagrammed in Figure 11.27, our idea was to have three parallel streams of convolutional layersâ€”each of which takes in word vectors from an layer.\n\nAs in our Convolutional Sentiment Classifier notebook, one of these streams would have a filter length of three tokens. One of the others will have a filter length of twoâ€”so it will specialize in learning word-vector pairs that appear to be relevant to classifying a film review as having positive or negative sentiment. The third convolutional stream will have a filter length of four tokens, so it will specialize in detecting relevant quadruplets of word meaning.\n\nThe hyperparameters for our three-convolutional-stream model are provided in Example 11.40 as well as in our Multi ConvNet Sentiment Classifier Jupyter notebook.\n\nThe novel hyperparameters are associated with the three convolutional layers. All three convolutional layers have 256 filters, but mirroring the diagram in Figure 11.27, the layers form parallel streamsâ€”each with a unique filter length that ranges from 2 up to 4.\n\nThe Keras code for our multi-ConvNet model architecture is provided in Example 11.41.\n\nThis architecture may look a little alarming if you havenâ€™t seen the Keras Model class used before, but as we break it down line-by-line here, it should lose any intimidating aspects it might have:\nâ€¢ None With the class, we specify the layer independently, as opposed to specifying it as the shape argument of the first hidden layer. We specified the data type ( ) explicitly: 16-bit integers ( ) can range up to 32,767, which will accommodate the maximum index of the words we input.59 As with all of the layers in this model, we specify a recognizable name argument so that when we print the model later (using ) it will be easy to make sense of everything.\nâ€¢ None Every layer is assigned to a unique variable name, such as , , and . We will use these variable names to specify the flow of data within our model.\nâ€¢ None The most noteworthy aspect of using the Model class, which will be familiar to developers who have worked with functional programming languages, is the variable within the second set of parentheses following any layer call.\nâ€¢ None This specifies which layerâ€™s outputs are flowing into a given layer. For example, ( ) in the second set of parentheses of the indicates that the output of the input layer flows into the embedding layer.\nâ€¢ None The and layers take the same arguments as before in this chapter.\nâ€¢ None The output of the layer (with a variable named ) is the input to three separate, parallel convolutional layers: .\nâ€¢ None As per Figure 11.27, each of the three convolutional streams includes a Conv1D layer (with a unique filter length) and a layer.\nâ€¢ None The activations output by the layer of each of the three convolutional streams are concatenated into a single array of activation values by the layer, which takes in a list of inputs ([ ]) as its only argument.\nâ€¢ None The concatenated convolutional-stream activations are provided as input to two hidden layers, each of which has a layer associated with it. (The second dense layer has one-quarter as many neurons as the first, as specified by .)\nâ€¢ None The activations output by the sigmoid output neuron ([latex]\\hat{y}[/latex] ) are assigned to the variable name predictions.\nâ€¢ None Finally, the Model class ties all of the modelâ€™s layers together by taking two arguments: the variable name of the input layer (i.e., ) and the output layer (i.e., ).\n\nOur elaborate parallel network architecture ultimately provided us with a modest bump in capability to give us the best-performing sentiment classifier in this chapter (see Table 11.6). As detailed in our Multi ConvNet Sentiment Classifier notebook, the lowest validation loss was attained in the second epoch (0.262), and this epoch was associated with a validation accuracy of 89.4 percent and an ROC AUC of 96.2 percentâ€”a tenth of a percent better than our convolutional model.\n\nIn this chapter, we discussed methods for preprocessing natural language data, ways to create word vectors from a corpus of natural language, and the procedure for calculating the area under the receiver operating characteristic curve. In the second half of the chapter, we applied this knowledge to experiment with a wide range of deep learning NLP models for classifying film reviews as favorable or negative. Some of these models involved layer types you were familiar with from earlier chapters (i.e., dense and convolutional layers), while later ones involved new layer types from the RNN family (LSTMs and GRUs) and, for the first time in this book, a non-sequential model architecture.\n\nA summary of the results of our sentiment-classifier experiments are provided in Table 11.6. We hypothesize that, had our natural language dataset been much larger, the Bi-LSTM architectures might have outperformed the convolutional ones.\n\nDomino editorial note: we've moved the \"footnotes\" to be embedded in the narrative to increase online readability."
    },
    {
        "link": "https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html",
        "document": "Hyperparameter tuning can make the difference between an average model and a highly accurate one. Often simple things like choosing a different learning rate or changing a network layer size can have a dramatic impact on your model performance.\n\nFortunately, there are tools that help with finding the best combination of parameters. Ray Tune is an industry standard tool for distributed hyperparameter tuning. Ray Tune includes the latest hyperparameter search algorithms, integrates with various analysis libraries, and natively supports distributed training through Rayâ€™s distributed machine learning engine.\n\nIn this tutorial, we will show you how to integrate Ray Tune into your PyTorch training workflow. We will extend this tutorial from the PyTorch documentation for training a CIFAR10 image classifier.\n\nAs you will see, we only need to add some slight modifications. In particular, we need to\nâ€¢ None and define the search space for the model tuning\n\nTo run this tutorial, please make sure the following packages are installed:\n\nNow it gets interesting, because we introduce some changes to the example from the PyTorch documentation. We wrap the training script in a function . The parameter will receive the hyperparameters we would like to train with. The specifies the directory where we load and store the data, so that multiple runs can share the same data source. We also load the model and optimizer state at the start of the run, if a checkpoint is provided. Further down in this tutorial you will find information on how to save the checkpoint and what it is used for. The learning rate of the optimizer is made configurable, too: We also split the training data into a training and validation subset. We thus train on 80% of the data and calculate the validation loss on the remaining 20%. The batch sizes with which we iterate through the training and test sets are configurable as well. Image classification benefits largely from GPUs. Luckily, we can continue to use PyTorchâ€™s abstractions in Ray Tune. Thus, we can wrap our model in to support data parallel training on multiple GPUs: By using a variable we make sure that training also works when we have no GPUs available. PyTorch requires us to send our data to the GPU memory explicitly, like this: The code now supports training on CPUs, on a single GPU, and on multiple GPUs. Notably, Ray also supports fractional GPUs so we can share GPUs among trials, as long as the model still fits on the GPU memory. Weâ€™ll come back to that later. The most interesting part is the communication with Ray Tune: Here we first save a checkpoint and then report some metrics back to Ray Tune. Specifically, we send the validation loss and accuracy back to Ray Tune. Ray Tune can then use these metrics to decide which hyperparameter configuration lead to the best results. These metrics can also be used to stop bad performing trials early in order to avoid wasting resources on those trials. The checkpoint saving is optional, however, it is necessary if we wanted to use advanced schedulers like Population Based Training. Also, by saving the checkpoint we can later load the trained models and validate them on a test set. Lastly, saving checkpoints is useful for fault tolerance, and it allows us to interrupt training and continue training later. The full code example looks like this: # get the inputs; data is a list of [inputs, labels] As you can see, most of the code is adapted directly from the original example.\n\nLastly, we need to define Ray Tuneâ€™s search space. Here is an example: The accepts a list of values that are uniformly sampled from. In this example, the and parameters should be powers of 2 between 4 and 256, so either 4, 8, 16, 32, 64, 128, or 256. The (learning rate) should be uniformly sampled between 0.0001 and 0.1. Lastly, the batch size is a choice between 2, 4, 8, and 16. At each trial, Ray Tune will now randomly sample a combination of parameters from these search spaces. It will then train a number of models in parallel and find the best performing one among these. We also use the which will terminate bad performing trials early. We wrap the function with to set the constant parameter. We can also tell Ray Tune what resources should be available for each trial: You can specify the number of CPUs, which are then available e.g. to increase the of the PyTorch instances. The selected number of GPUs are made visible to PyTorch in each trial. Trials do not have access to GPUs that havenâ€™t been requested for them - so you donâ€™t have to care about two trials using the same set of resources. Here we can also specify fractional GPUs, so something like is completely valid. The trials will then share GPUs among each other. You just have to make sure that the models still fit in the GPU memory. After training the models, we will find the best performing one and load the trained network from the checkpoint file. We then obtain the test set accuracy and report everything by printing. The full main function looks like this: # You can change the number of GPUs per trial here: 0% 0.00/170M [00:00<?, ?B/s] 0% 492k/170M [00:00<00:34, 4.91MB/s] 4% 7.57M/170M [00:00<00:03, 43.6MB/s] 9% 14.9M/170M [00:00<00:02, 56.9MB/s] 14% 24.4M/170M [00:00<00:02, 71.9MB/s] 19% 32.8M/170M [00:00<00:01, 76.1MB/s] 24% 40.4M/170M [00:00<00:01, 74.8MB/s] 30% 50.7M/170M [00:00<00:01, 83.8MB/s] 35% 59.1M/170M [00:00<00:01, 78.8MB/s] 40% 68.2M/170M [00:00<00:01, 82.2MB/s] 45% 76.4M/170M [00:01<00:01, 79.5MB/s] 50% 85.3M/170M [00:01<00:01, 82.1MB/s] 55% 93.6M/170M [00:01<00:00, 77.7MB/s] 61% 104M/170M [00:01<00:00, 84.4MB/s] 66% 112M/170M [00:01<00:00, 78.7MB/s] 72% 122M/170M [00:01<00:00, 83.9MB/s] 77% 131M/170M [00:01<00:00, 80.7MB/s] 82% 140M/170M [00:01<00:00, 84.3MB/s] 87% 149M/170M [00:01<00:00, 80.3MB/s] 93% 158M/170M [00:02<00:00, 85.5MB/s] 98% 167M/170M [00:02<00:00, 77.9MB/s] 100% 170M/170M [00:02<00:00, 77.6MB/s] 2025-03-21 17:07:17,236 WARNING services.py:1889 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 2147479552 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=10.24gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM. 2025-03-21 17:07:17,377 INFO worker.py:1642 -- Started a local Ray instance. 2025-03-21 17:07:18,751 INFO tune.py:228 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `tune.run(...)`. 2025-03-21 17:07:18,753 INFO tune.py:654 -- [output] This will use the new output engine with verbosity 2. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949 +--------------------------------------------------------------------+ | Configuration for experiment train_cifar_2025-03-21_17-07-18 | +--------------------------------------------------------------------+ | Search algorithm BasicVariantGenerator | | Scheduler AsyncHyperBandScheduler | | Number of trials 10 | +--------------------------------------------------------------------+ View detailed results here: /var/lib/ci-user/ray_results/train_cifar_2025-03-21_17-07-18 To visualize your results with TensorBoard, run: `tensorboard --logdir /var/lib/ci-user/ray_results/train_cifar_2025-03-21_17-07-18` Trial status: 10 PENDING Current time: 2025-03-21 17:07:19. Total running time: 0s Logical resource usage: 0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:M60) +-------------------------------------------------------------------------------+ | Trial name status l1 l2 lr batch_size | +-------------------------------------------------------------------------------+ | train_cifar_f231b_00000 PENDING 16 1 0.00213327 2 | | train_cifar_f231b_00001 PENDING 1 2 0.013416 4 | | train_cifar_f231b_00002 PENDING 256 64 0.0113784 2 | | train_cifar_f231b_00003 PENDING 64 256 0.0274071 8 | | train_cifar_f231b_00004 PENDING 16 2 0.056666 4 | | train_cifar_f231b_00005 PENDING 8 64 0.000353097 4 | | train_cifar_f231b_00006 PENDING 16 4 0.000147684 8 | | train_cifar_f231b_00007 PENDING 256 256 0.00477469 8 | | train_cifar_f231b_00008 PENDING 128 256 0.0306227 8 | | train_cifar_f231b_00009 PENDING 2 16 0.0286986 2 | +-------------------------------------------------------------------------------+ Trial train_cifar_f231b_00002 started with configuration: +--------------------------------------------------+ | Trial train_cifar_f231b_00002 config | +--------------------------------------------------+ | batch_size 2 | | l1 256 | | l2 64 | | lr 0.01138 | +--------------------------------------------------+ Trial train_cifar_f231b_00004 started with configuration: +--------------------------------------------------+ | Trial train_cifar_f231b_00004 config | +--------------------------------------------------+ | batch_size 4 | | l1 16 | | l2 2 | | lr 0.05667 | +--------------------------------------------------+ Trial train_cifar_f231b_00003 started with configuration: +--------------------------------------------------+ | Trial train_cifar_f231b_00003 config | +--------------------------------------------------+ | batch_size 8 | | l1 64 | | l2 256 | | lr 0.02741 | +--------------------------------------------------+ Trial train_cifar_f231b_00006 started with configuration: +--------------------------------------------------+ | Trial train_cifar_f231b_00006 config | +--------------------------------------------------+ | batch_size 8 | | l1 16 | | l2 4 | | lr 0.00015 | +--------------------------------------------------+ Trial train_cifar_f231b_00001 started with configuration: +--------------------------------------------------+ | Trial train_cifar_f231b_00001 config | +--------------------------------------------------+ | batch_size 4 | | l1 1 | | l2 2 | | lr 0.01342 | +--------------------------------------------------+ Trial train_cifar_f231b_00007 started with configuration: +--------------------------------------------------+ | Trial train_cifar_f231b_00007 config | +--------------------------------------------------+ | batch_size 8 | | l1 256 | | l2 256 | | lr 0.00477 | +--------------------------------------------------+ Trial train_cifar_f231b_00000 started with configuration: +--------------------------------------------------+ | Trial train_cifar_f231b_00000 config | +--------------------------------------------------+ | batch_size 2 | | l1 16 | | l2 1 | | lr 0.00213 | +--------------------------------------------------+ Trial train_cifar_f231b_00005 started with configuration: +--------------------------------------------------+ | Trial train_cifar_f231b_00005 config | +--------------------------------------------------+ | batch_size 4 | | l1 8 | | l2 64 | | lr 0.00035 | +--------------------------------------------------+ (func pid=4473) [1, 2000] loss: 2.324 Trial status: 8 RUNNING | 2 PENDING Current time: 2025-03-21 17:07:49. Total running time: 30s Logical resource usage: 16.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:M60) +-------------------------------------------------------------------------------+ | Trial name status l1 l2 lr batch_size | +-------------------------------------------------------------------------------+ | train_cifar_f231b_00000 RUNNING 16 1 0.00213327 2 | | train_cifar_f231b_00001 RUNNING 1 2 0.013416 4 | | train_cifar_f231b_00002 RUNNING 256 64 0.0113784 2 | | train_cifar_f231b_00003 RUNNING 64 256 0.0274071 8 | | train_cifar_f231b_00004 RUNNING 16 2 0.056666 4 | | train_cifar_f231b_00005 RUNNING 8 64 0.000353097 4 | | train_cifar_f231b_00006 RUNNING 16 4 0.000147684 8 | | train_cifar_f231b_00007 RUNNING 256 256 0.00477469 8 | | train_cifar_f231b_00008 PENDING 128 256 0.0306227 8 | | train_cifar_f231b_00009 PENDING 2 16 0.0286986 2 | +-------------------------------------------------------------------------------+ (func pid=4473) [1, 4000] loss: 1.152 [repeated 8x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.) (func pid=4476) [1, 4000] loss: 1.037 [repeated 6x across cluster] (func pid=4473) [1, 6000] loss: 0.769 [repeated 2x across cluster] Trial status: 8 RUNNING | 2 PENDING Current time: 2025-03-21 17:08:19. Total running time: 1min 0s Logical resource usage: 16.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:M60) +-------------------------------------------------------------------------------+ | Trial name status l1 l2 lr batch_size | +-------------------------------------------------------------------------------+ | train_cifar_f231b_00000 RUNNING 16 1 0.00213327 2 | | train_cifar_f231b_00001 RUNNING 1 2 0.013416 4 | | train_cifar_f231b_00002 RUNNING 256 64 0.0113784 2 | | train_cifar_f231b_00003 RUNNING 64 256 0.0274071 8 | | train_cifar_f231b_00004 RUNNING 16 2 0.056666 4 | | train_cifar_f231b_00005 RUNNING 8 64 0.000353097 4 | | train_cifar_f231b_00006 RUNNING 16 4 0.000147684 8 | | train_cifar_f231b_00007 RUNNING 256 256 0.00477469 8 | | train_cifar_f231b_00008 PENDING 128 256 0.0306227 8 | | train_cifar_f231b_00009 PENDING 2 16 0.0286986 2 | +-------------------------------------------------------------------------------+ Trial train_cifar_f231b_00006 finished iteration 1 at 2025-03-21 17:08:19. Total running time: 1min 0s +------------------------------------------------------------+ | Trial train_cifar_f231b_00006 result | +------------------------------------------------------------+ | checkpoint_dir_name checkpoint_000000 | | time_this_iter_s 54.59943 | | time_total_s 54.59943 | | training_iteration 1 | | accuracy 0.1021 | | loss 2.303 | +------------------------------------------------------------+ Trial train_cifar_f231b_00006 saved a checkpoint for iteration 1 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-03-21_17-07-18/train_cifar_f231b_00006_6_batch_size=8,l1=16,l2=4,lr=0.0001_2025-03-21_17-07-18/checkpoint_000000 (func pid=4479) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2025-03-21_17-07-18/train_cifar_f231b_00006_6_batch_size=8,l1=16,l2=4,lr=0.0001_2025-03-21_17-07-18/checkpoint_000000) Trial train_cifar_f231b_00003 finished iteration 1 at 2025-03-21 17:08:22. Total running time: 1min 3s +------------------------------------------------------------+ | Trial train_cifar_f231b_00003 result | +------------------------------------------------------------+ | checkpoint_dir_name checkpoint_000000 | | time_this_iter_s 57.08528 | | time_total_s 57.08528 | | training_iteration 1 | | accuracy 0.148 | | loss 2.42043 | +------------------------------------------------------------+ Trial train_cifar_f231b_00003 saved a checkpoint for iteration 1 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-03-21_17-07-18/train_cifar_f231b_00003_3_batch_size=8,l1=64,l2=256,lr=0.0274_2025-03-21_17-07-18/checkpoint_000000 Trial train_cifar_f231b_00003 completed after 1 iterations at 2025-03-21 17:08:22. Total running time: 1min 3s Trial train_cifar_f231b_00008 started with configuration: +--------------------------------------------------+ | Trial train_cifar_f231b_00008 config | +--------------------------------------------------+ | batch_size 8 | | l1 128 | | l2 256 | | lr 0.03062 | +--------------------------------------------------+ Trial train_cifar_f231b_00007 finished iteration 1 at 2025-03-21 17:08:22. Total running time: 1min 3s +------------------------------------------------------------+ | Trial train_cifar_f231b_00007 result | +------------------------------------------------------------+ | checkpoint_dir_name checkpoint_000000 | | time_this_iter_s 56.81921 | | time_total_s 56.81921 | | training_iteration 1 | | accuracy 0.4478 | | loss 1.51747 | +------------------------------------------------------------+ Trial train_cifar_f231b_00007 saved a checkpoint for iteration 1 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-03-21_17-07-18/train_cifar_f231b_00007_7_batch_size=8,l1=256,l2=256,lr=0.0048_2025-03-21_17-07-18/checkpoint_000000 (func pid=4473) [1, 8000] loss: 0.576 [repeated 5x across cluster] (func pid=4478) [1, 8000] loss: 0.470 [repeated 3x across cluster] (func pid=4484) [2, 2000] loss: 1.400 [repeated 3x across cluster] Trial status: 8 RUNNING | 1 TERMINATED | 1 PENDING Current time: 2025-03-21 17:08:49. Total running time: 1min 30s Logical resource usage: 16.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:M60) +------------------------------------------------------------------------------------------------------------------------------------+ | Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy | +------------------------------------------------------------------------------------------------------------------------------------+ | train_cifar_f231b_00000 RUNNING 16 1 0.00213327 2 | | train_cifar_f231b_00001 RUNNING 1 2 0.013416 4 | | train_cifar_f231b_00002 RUNNING 256 64 0.0113784 2 | | train_cifar_f231b_00004 RUNNING 16 2 0.056666 4 | | train_cifar_f231b_00005 RUNNING 8 64 0.000353097 4 | | train_cifar_f231b_00006 RUNNING 16 4 0.000147684 8 1 54.5994 2.303 0.1021 | | train_cifar_f231b_00007 RUNNING 256 256 0.00477469 8 1 56.8192 1.51747 0.4478 | | train_cifar_f231b_00008 RUNNING 128 256 0.0306227 8 | | train_cifar_f231b_00003 TERMINATED 64 256 0.0274071 8 1 57.0853 2.42043 0.148 | | train_cifar_f231b_00009 PENDING 2 16 0.0286986 2 | +------------------------------------------------------------------------------------------------------------------------------------+ (func pid=4474) [1, 10000] loss: 0.462 [repeated 3x across cluster] (func pid=4479) [2, 4000] loss: 1.146 [repeated 3x across cluster] (func pid=4473) [1, 12000] loss: 0.384 [repeated 3x across cluster] Trial train_cifar_f231b_00001 finished iteration 1 at 2025-03-21 17:09:03. Total running time: 1min 45s +------------------------------------------------------------+ | Trial train_cifar_f231b_00001 result | +------------------------------------------------------------+ | checkpoint_dir_name checkpoint_000000 | | time_this_iter_s 98.71102 | | time_total_s 98.71102 | | training_iteration 1 | | accuracy 0.1028 | | loss 2.30777 | +------------------------------------------------------------+(func pid=4474) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2025-03-21_17-07-18/train_cifar_f231b_00001_1_batch_size=4,l1=1,l2=2,lr=0.0134_2025-03-21_17-07-18/checkpoint_000000) [repeated 3x across cluster] Trial train_cifar_f231b_00001 saved a checkpoint for iteration 1 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-03-21_17-07-18/train_cifar_f231b_00001_1_batch_size=4,l1=1,l2=2,lr=0.0134_2025-03-21_17-07-18/checkpoint_000000 Trial train_cifar_f231b_00001 completed after 1 iterations at 2025-03-21 17:09:03. Total running time: 1min 45s Trial train_cifar_f231b_00009 started with configuration: +-------------------------------------------------+ | Trial train_cifar_f231b_00009 config | +-------------------------------------------------+ | batch_size 2 | | l1 2 | | l2 16 | | lr 0.0287 | +-------------------------------------------------+ Trial train_cifar_f231b_00004 finished iteration 1 at 2025-03-21 17:09:05. Total running time: 1min 46s +------------------------------------------------------------+ | Trial train_cifar_f231b_00004 result | +------------------------------------------------------------+ | checkpoint_dir_name checkpoint_000000 | | time_this_iter_s 100.22192 | | time_total_s 100.22192 | | training_iteration 1 | | accuracy 0.0973 | | loss 2.31545 | +------------------------------------------------------------+ Trial train_cifar_f231b_00004 saved a checkpoint for iteration 1 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-03-21_17-07-18/train_cifar_f231b_00004_4_batch_size=4,l1=16,l2=2,lr=0.0567_2025-03-21_17-07-18/checkpoint_000000 Trial train_cifar_f231b_00004 completed after 1 iterations at 2025-03-21 17:09:05. Total running time: 1min 46s Trial train_cifar_f231b_00005 finished iteration 1 at 2025-03-21 17:09:05. Total running time: 1min 47s +------------------------------------------------------------+ | Trial train_cifar_f231b_00005 result | +------------------------------------------------------------+ | checkpoint_dir_name checkpoint_000000 | | time_this_iter_s 99.2248 | | time_total_s 99.2248 | | training_iteration 1 | | accuracy 0.3781 | | loss 1.66134 | +------------------------------------------------------------+ Trial train_cifar_f231b_00005 saved a checkpoint for iteration 1 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-03-21_17-07-18/train_cifar_f231b_00005_5_batch_size=4,l1=8,l2=64,lr=0.0004_2025-03-21_17-07-18/checkpoint_000000 Trial train_cifar_f231b_00006 finished iteration 2 at 2025-03-21 17:09:11. Total running time: 1min 52s +------------------------------------------------------------+ | Trial train_cifar_f231b_00006 result | +------------------------------------------------------------+ | checkpoint_dir_name checkpoint_000001 | | time_this_iter_s 51.50313 | | time_total_s 106.10257 | | training_iteration 2 | | accuracy 0.1792 | | loss 2.27292 | +------------------------------------------------------------+ Trial train_cifar_f231b_00006 saved a checkpoint for iteration 2 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-03-21_17-07-18/train_cifar_f231b_00006_6_batch_size=8,l1=16,l2=4,lr=0.0001_2025-03-21_17-07-18/checkpoint_000001 (func pid=4479) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2025-03-21_17-07-18/train_cifar_f231b_00006_6_batch_size=8,l1=16,l2=4,lr=0.0001_2025-03-21_17-07-18/checkpoint_000001) [repeated 3x across cluster] (func pid=4475) [1, 12000] loss: 0.386 [repeated 2x across cluster] Trial train_cifar_f231b_00007 finished iteration 2 at 2025-03-21 17:09:15. Total running time: 1min 56s +------------------------------------------------------------+ | Trial train_cifar_f231b_00007 result | +------------------------------------------------------------+ | checkpoint_dir_name checkpoint_000001 | | time_this_iter_s 53.38768 | | time_total_s 110.20689 | | training_iteration 2 | | accuracy 0.5198 | | loss 1.34171 | +------------------------------------------------------------+ Trial train_cifar_f231b_00007 saved a checkpoint for iteration 2 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-03-21_17-07-18/train_cifar_f231b_00007_7_batch_size=8,l1=256,l2=256,lr=0.0048_2025-03-21_17-07-18/checkpoint_000001 Trial train_cifar_f231b_00008 finished iteration 1 at 2025-03-21 17:09:18. Total running time: 1min 59s +------------------------------------------------------------+ | Trial train_cifar_f231b_00008 result | +------------------------------------------------------------+ | checkpoint_dir_name checkpoint_000000 | | time_this_iter_s 56.68488 | | time_total_s 56.68488 | | training_iteration 1 | | accuracy 0.2221 | | loss 2.11022 | +------------------------------------------------------------+ Trial train_cifar_f231b_00008 saved a checkpoint for iteration 1 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-03-21_17-07-18/train_cifar_f231b_00008_8_batch_size=8,l1=128,l2=256,lr=0.0306_2025-03-21_17-07-18/checkpoint_000000 (func pid=4476) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2025-03-21_17-07-18/train_cifar_f231b_00008_8_batch_size=8,l1=128,l2=256,lr=0.0306_2025-03-21_17-07-18/checkpoint_000000) [repeated 2x across cluster] Trial status: 7 RUNNING | 3 TERMINATED Current time: 2025-03-21 17:09:19. Total running time: 2min 0s Logical resource usage: 14.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:M60) +------------------------------------------------------------------------------------------------------------------------------------+ | Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy | +------------------------------------------------------------------------------------------------------------------------------------+ | train_cifar_f231b_00000 RUNNING 16 1 0.00213327 2 | | train_cifar_f231b_00002 RUNNING 256 64 0.0113784 2 | | train_cifar_f231b_00005 RUNNING 8 64 0.000353097 4 1 99.2248 1.66134 0.3781 | | train_cifar_f231b_00006 RUNNING 16 4 0.000147684 8 2 106.103 2.27292 0.1792 | | train_cifar_f231b_00007 RUNNING 256 256 0.00477469 8 2 110.207 1.34171 0.5198 | | train_cifar_f231b_00008 RUNNING 128 256 0.0306227 8 1 56.6849 2.11022 0.2221 | | train_cifar_f231b_00009 RUNNING 2 16 0.0286986 2 | | train_cifar_f231b_00001 TERMINATED 1 2 0.013416 4 1 98.711 2.30777 0.1028 | | train_cifar_f231b_00003 TERMINATED 64 256 0.0274071 8 1 57.0853 2.42043 0.148 | | train_cifar_f231b_00004 TERMINATED 16 2 0.056666 4 1 100.222 2.31545 0.0973 | +------------------------------------------------------------------------------------------------------------------------------------+ (func pid=4474) [1, 2000] loss: 2.340 [repeated 2x across cluster] (func pid=4479) [3, 2000] loss: 2.259 [repeated 2x across cluster] (func pid=4484) [3, 2000] loss: 1.232 [repeated 3x across cluster] (func pid=4473) [1, 18000] loss: 0.256 [repeated 4x across cluster] (func pid=4474) [1, 6000] loss: 0.778 [repeated 3x across cluster] Trial status: 7 RUNNING | 3 TERMINATED Current time: 2025-03-21 17:09:49. Total running time: 2min 30s Logical resource usage: 14.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:M60) +------------------------------------------------------------------------------------------------------------------------------------+ | Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy | +------------------------------------------------------------------------------------------------------------------------------------+ | train_cifar_f231b_00000 RUNNING 16 1 0.00213327 2 | | train_cifar_f231b_00002 RUNNING 256 64 0.0113784 2 | | train_cifar_f231b_00005 RUNNING 8 64 0.000353097 4 1 99.2248 1.66134 0.3781 | | train_cifar_f231b_00006 RUNNING 16 4 0.000147684 8 2 106.103 2.27292 0.1792 | | train_cifar_f231b_00007 RUNNING 256 256 0.00477469 8 2 110.207 1.34171 0.5198 | | train_cifar_f231b_00008 RUNNING 128 256 0.0306227 8 1 56.6849 2.11022 0.2221 | | train_cifar_f231b_00009 RUNNING 2 16 0.0286986 2 | | train_cifar_f231b_00001 TERMINATED 1 2 0.013416 4 1 98.711 2.30777 0.1028 | | train_cifar_f231b_00003 TERMINATED 64 256 0.0274071 8 1 57.0853 2.42043 0.148 | | train_cifar_f231b_00004 TERMINATED 16 2 0.056666 4 1 100.222 2.31545 0.0973 | +------------------------------------------------------------------------------------------------------------------------------------+ (func pid=4476) [2, 4000] loss: 1.055 [repeated 3x across cluster] Trial train_cifar_f231b_00006 finished iteration 3 at 2025-03-21 17:09:55. Total running time: 2min 37s +------------------------------------------------------------+ | Trial train_cifar_f231b_00006 result | +------------------------------------------------------------+ | checkpoint_dir_name checkpoint_000002 | | time_this_iter_s 44.68938 | | time_total_s 150.79195 | | training_iteration 3 | | accuracy 0.2333 | | loss 2.16889 | +------------------------------------------------------------+ Trial train_cifar_f231b_00006 saved a checkpoint for iteration 3 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-03-21_17-07-18/train_cifar_f231b_00006_6_batch_size=8,l1=16,l2=4,lr=0.0001_2025-03-21_17-07-18/checkpoint_000002 (func pid=4479) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2025-03-21_17-07-18/train_cifar_f231b_00006_6_batch_size=8,l1=16,l2=4,lr=0.0001_2025-03-21_17-07-18/checkpoint_000002) (func pid=4474) [1, 8000] loss: 0.583 [repeated 2x across cluster] Trial train_cifar_f231b_00007 finished iteration 3 at 2025-03-21 17:10:02. Total running time: 2min 43s +------------------------------------------------------------+ | Trial train_cifar_f231b_00007 result | +------------------------------------------------------------+ | checkpoint_dir_name checkpoint_000002 | | time_this_iter_s 47.10437 | | time_total_s 157.31126 | | training_iteration 3 | | accuracy 0.5505 | | loss 1.27905 | +------------------------------------------------------------+ Trial train_cifar_f231b_00007 saved a checkpoint for iteration 3 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-03-21_17-07-18/train_cifar_f231b_00007_7_batch_size=8,l1=256,l2=256,lr=0.0048_2025-03-21_17-07-18/checkpoint_000002 (func pid=4484) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2025-03-21_17-07-18/train_cifar_f231b_00007_7_batch_size=8,l1=256,l2=256,lr=0.0048_2025-03-21_17-07-18/checkpoint_000002) Trial train_cifar_f231b_00008 finished iteration 2 at 2025-03-21 17:10:09. Total running time: 2min 50s +------------------------------------------------------------+ | Trial train_cifar_f231b_00008 result | +------------------------------------------------------------+ | checkpoint_dir_name checkpoint_000001 | | time_this_iter_s 50.41293 | | time_total_s 107.09781 | | training_iteration 2 | | accuracy 0.222 | | loss 2.08701 | +------------------------------------------------------------+ Trial train_cifar_f231b_00008 saved a checkpoint for iteration 2 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-03-21_17-07-18/train_cifar_f231b_00008_8_batch_size=8,l1=128,l2=256,lr=0.0306_2025-03-21_17-07-18/checkpoint_000001 Trial train_cifar_f231b_00008 completed after 2 iterations at 2025-03-21 17:10:09. Total running time: 2min 50s (func pid=4476) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2025-03-21_17-07-18/train_cifar_f231b_00008_8_batch_size=8,l1=128,l2=256,lr=0.0306_2025-03-21_17-07-18/checkpoint_000001) (func pid=4479) [4, 2000] loss: 2.127 [repeated 3x across cluster] Trial train_cifar_f231b_00000 finished iteration 1 at 2025-03-21 17:10:14. Total running time: 2min 55s +------------------------------------------------------------+ | Trial train_cifar_f231b_00000 result | +------------------------------------------------------------+ | checkpoint_dir_name checkpoint_000000 | | time_this_iter_s 169.32698 | | time_total_s 169.32698 | | training_iteration 1 | | accuracy 0.0966 | | loss 2.30395 | +------------------------------------------------------------+ Trial train_cifar_f231b_00000 saved a checkpoint for iteration 1 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-03-21_17-07-18/train_cifar_f231b_00000_0_batch_size=2,l1=16,l2=1,lr=0.0021_2025-03-21_17-07-18/checkpoint_000000 Trial train_cifar_f231b_00000 completed after 1 iterations at 2025-03-21 17:10:14. Total running time: 2min 55s (func pid=4473) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2025-03-21_17-07-18/train_cifar_f231b_00000_0_batch_size=2,l1=16,l2=1,lr=0.0021_2025-03-21_17-07-18/checkpoint_000000) (func pid=4475) [1, 20000] loss: 0.232 [repeated 3x across cluster] Trial status: 5 TERMINATED | 5 RUNNING Current time: 2025-03-21 17:10:19. Total running time: 3min 0s Logical resource usage: 10.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:M60) +------------------------------------------------------------------------------------------------------------------------------------+ | Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy | +------------------------------------------------------------------------------------------------------------------------------------+ | train_cifar_f231b_00002 RUNNING 256 64 0.0113784 2 | | train_cifar_f231b_00005 RUNNING 8 64 0.000353097 4 1 99.2248 1.66134 0.3781 | | train_cifar_f231b_00006 RUNNING 16 4 0.000147684 8 3 150.792 2.16889 0.2333 | | train_cifar_f231b_00007 RUNNING 256 256 0.00477469 8 3 157.311 1.27905 0.5505 | | train_cifar_f231b_00009 RUNNING 2 16 0.0286986 2 | | train_cifar_f231b_00000 TERMINATED 16 1 0.00213327 2 1 169.327 2.30395 0.0966 | | train_cifar_f231b_00001 TERMINATED 1 2 0.013416 4 1 98.711 2.30777 0.1028 | | train_cifar_f231b_00003 TERMINATED 64 256 0.0274071 8 1 57.0853 2.42043 0.148 | | train_cifar_f231b_00004 TERMINATED 16 2 0.056666 4 1 100.222 2.31545 0.0973 | | train_cifar_f231b_00008 TERMINATED 128 256 0.0306227 8 2 107.098 2.08701 0.222 | +------------------------------------------------------------------------------------------------------------------------------------+ (func pid=4479) [4, 4000] loss: 1.011 [repeated 2x across cluster] Trial train_cifar_f231b_00005 finished iteration 2 at 2025-03-21 17:10:24. Total running time: 3min 5s +------------------------------------------------------------+ | Trial train_cifar_f231b_00005 result | +------------------------------------------------------------+ | checkpoint_dir_name checkpoint_000001 | | time_this_iter_s 78.22336 | | time_total_s 177.44816 | | training_iteration 2 | | accuracy 0.4652 | | loss 1.45765 | +------------------------------------------------------------+ Trial train_cifar_f231b_00005 saved a checkpoint for iteration 2 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-03-21_17-07-18/train_cifar_f231b_00005_5_batch_size=4,l1=8,l2=64,lr=0.0004_2025-03-21_17-07-18/checkpoint_000001 (func pid=4478) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2025-03-21_17-07-18/train_cifar_f231b_00005_5_batch_size=4,l1=8,l2=64,lr=0.0004_2025-03-21_17-07-18/checkpoint_000001) (func pid=4484) [4, 4000] loss: 0.581 [repeated 2x across cluster] Trial train_cifar_f231b_00002 finished iteration 1 at 2025-03-21 17:10:33. Total running time: 3min 14s +------------------------------------------------------------+ | Trial train_cifar_f231b_00002 result | +------------------------------------------------------------+ | checkpoint_dir_name checkpoint_000000 | | time_this_iter_s 188.7492 | | time_total_s 188.7492 | | training_iteration 1 | | accuracy 0.1005 | | loss 2.32115 | +------------------------------------------------------------+ Trial train_cifar_f231b_00002 saved a checkpoint for iteration 1 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-03-21_17-07-18/train_cifar_f231b_00002_2_batch_size=2,l1=256,l2=64,lr=0.0114_2025-03-21_17-07-18/checkpoint_000000 Trial train_cifar_f231b_00002 completed after 1 iterations at 2025-03-21 17:10:33. Total running time: 3min 14s (func pid=4475) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2025-03-21_17-07-18/train_cifar_f231b_00002_2_batch_size=2,l1=256,l2=64,lr=0.0114_2025-03-21_17-07-18/checkpoint_000000) Trial train_cifar_f231b_00006 finished iteration 4 at 2025-03-21 17:10:35. Total running time: 3min 16s +------------------------------------------------------------+ | Trial train_cifar_f231b_00006 result | +------------------------------------------------------------+ | checkpoint_dir_name checkpoint_000003 | | time_this_iter_s 39.3127 | | time_total_s 190.10464 | | training_iteration 4 | | accuracy 0.2828 | | loss 1.90732 | +------------------------------------------------------------+ Trial train_cifar_f231b_00006 saved a checkpoint for iteration 4 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-03-21_17-07-18/train_cifar_f231b_00006_6_batch_size=8,l1=16,l2=4,lr=0.0001_2025-03-21_17-07-18/checkpoint_000003 Trial train_cifar_f231b_00007 finished iteration 4 at 2025-03-21 17:10:43. Total running time: 3min 24s +------------------------------------------------------------+ | Trial train_cifar_f231b_00007 result | +------------------------------------------------------------+ | checkpoint_dir_name checkpoint_000003 | | time_this_iter_s 40.59188 | | time_total_s 197.90314 | | training_iteration 4 | | accuracy 0.5597 | | loss 1.28452 | +------------------------------------------------------------+ Trial train_cifar_f231b_00007 saved a checkpoint for iteration 4 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-03-21_17-07-18/train_cifar_f231b_00007_7_batch_size=8,l1=256,l2=256,lr=0.0048_2025-03-21_17-07-18/checkpoint_000003 (func pid=4484) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2025-03-21_17-07-18/train_cifar_f231b_00007_7_batch_size=8,l1=256,l2=256,lr=0.0048_2025-03-21_17-07-18/checkpoint_000003) [repeated 2x across cluster] (func pid=4474) [1, 16000] loss: 0.292 [repeated 3x across cluster] Trial status: 6 TERMINATED | 4 RUNNING Current time: 2025-03-21 17:10:49. Total running time: 3min 30s Logical resource usage: 8.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:M60) +------------------------------------------------------------------------------------------------------------------------------------+ | Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy | +------------------------------------------------------------------------------------------------------------------------------------+ | train_cifar_f231b_00005 RUNNING 8 64 0.000353097 4 2 177.448 1.45765 0.4652 | | train_cifar_f231b_00006 RUNNING 16 4 0.000147684 8 4 190.105 1.90732 0.2828 | | train_cifar_f231b_00007 RUNNING 256 256 0.00477469 8 4 197.903 1.28452 0.5597 | | train_cifar_f231b_00009 RUNNING 2 16 0.0286986 2 | | train_cifar_f231b_00000 TERMINATED 16 1 0.00213327 2 1 169.327 2.30395 0.0966 | | train_cifar_f231b_00001 TERMINATED 1 2 0.013416 4 1 98.711 2.30777 0.1028 | | train_cifar_f231b_00002 TERMINATED 256 64 0.0113784 2 1 188.749 2.32115 0.1005 | | train_cifar_f231b_00003 TERMINATED 64 256 0.0274071 8 1 57.0853 2.42043 0.148 | | train_cifar_f231b_00004 TERMINATED 16 2 0.056666 4 1 100.222 2.31545 0.0973 | | train_cifar_f231b_00008 TERMINATED 128 256 0.0306227 8 2 107.098 2.08701 0.222 | +------------------------------------------------------------------------------------------------------------------------------------+ (func pid=4474) [1, 18000] loss: 0.259 [repeated 3x across cluster] (func pid=4474) [1, 20000] loss: 0.233 [repeated 4x across cluster] Trial train_cifar_f231b_00006 finished iteration 5 at 2025-03-21 17:11:09. Total running time: 3min 50s +------------------------------------------------------------+ | Trial train_cifar_f231b_00006 result | +------------------------------------------------------------+ | checkpoint_dir_name checkpoint_000004 | | time_this_iter_s 34.28823 | | time_total_s 224.39288 | | training_iteration 5 | | accuracy 0.3211 | | loss 1.78305 | +------------------------------------------------------------+ Trial train_cifar_f231b_00006 saved a checkpoint for iteration 5 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-03-21_17-07-18/train_cifar_f231b_00006_6_batch_size=8,l1=16,l2=4,lr=0.0001_2025-03-21_17-07-18/checkpoint_000004 (func pid=4479) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2025-03-21_17-07-18/train_cifar_f231b_00006_6_batch_size=8,l1=16,l2=4,lr=0.0001_2025-03-21_17-07-18/checkpoint_000004) Trial status: 6 TERMINATED | 4 RUNNING Current time: 2025-03-21 17:11:19. Total running time: 4min 0s Logical resource usage: 8.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:M60) +------------------------------------------------------------------------------------------------------------------------------------+ | Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy | +------------------------------------------------------------------------------------------------------------------------------------+ | train_cifar_f231b_00005 RUNNING 8 64 0.000353097 4 2 177.448 1.45765 0.4652 | | train_cifar_f231b_00006 RUNNING 16 4 0.000147684 8 5 224.393 1.78305 0.3211 | | train_cifar_f231b_00007 RUNNING 256 256 0.00477469 8 4 197.903 1.28452 0.5597 | | train_cifar_f231b_00009 RUNNING 2 16 0.0286986 2 | | train_cifar_f231b_00000 TERMINATED 16 1 0.00213327 2 1 169.327 2.30395 0.0966 | | train_cifar_f231b_00001 TERMINATED 1 2 0.013416 4 1 98.711 2.30777 0.1028 | | train_cifar_f231b_00002 TERMINATED 256 64 0.0113784 2 1 188.749 2.32115 0.1005 | | train_cifar_f231b_00003 TERMINATED 64 256 0.0274071 8 1 57.0853 2.42043 0.148 | | train_cifar_f231b_00004 TERMINATED 16 2 0.056666 4 1 100.222 2.31545 0.0973 | | train_cifar_f231b_00008 TERMINATED 128 256 0.0306227 8 2 107.098 2.08701 0.222 | +------------------------------------------------------------------------------------------------------------------------------------+ (func pid=4478) [3, 10000] loss: 0.274 [repeated 3x across cluster] Trial train_cifar_f231b_00007 finished iteration 5 at 2025-03-21 17:11:20. Total running time: 4min 1s +------------------------------------------------------------+ | Trial train_cifar_f231b_00007 result | +------------------------------------------------------------+ | checkpoint_dir_name checkpoint_000004 | | time_this_iter_s 37.09912 | | time_total_s 235.00226 | | training_iteration 5 | | accuracy 0.5809 | | loss 1.2468 | +------------------------------------------------------------+ Trial train_cifar_f231b_00007 saved a checkpoint for iteration 5 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-03-21_17-07-18/train_cifar_f231b_00007_7_batch_size=8,l1=256,l2=256,lr=0.0048_2025-03-21_17-07-18/checkpoint_000004 (func pid=4484) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2025-03-21_17-07-18/train_cifar_f231b_00007_7_batch_size=8,l1=256,l2=256,lr=0.0048_2025-03-21_17-07-18/checkpoint_000004) Trial train_cifar_f231b_00009 finished iteration 1 at 2025-03-21 17:11:22. Total running time: 4min 3s +------------------------------------------------------------+ | Trial train_cifar_f231b_00009 result | +------------------------------------------------------------+ | checkpoint_dir_name checkpoint_000000 | | time_this_iter_s 138.41704 | | time_total_s 138.41704 | | training_iteration 1 | | accuracy 0.097 | | loss 2.35534 | +------------------------------------------------------------+ Trial train_cifar_f231b_00009 saved a checkpoint for iteration 1 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-03-21_17-07-18/train_cifar_f231b_00009_9_batch_size=2,l1=2,l2=16,lr=0.0287_2025-03-21_17-07-18/checkpoint_000000 Trial train_cifar_f231b_00009 completed after 1 iterations at 2025-03-21 17:11:22. Total running time: 4min 3s Trial train_cifar_f231b_00005 finished iteration 3 at 2025-03-21 17:11:28. Total running time: 4min 9s +------------------------------------------------------------+ | Trial train_cifar_f231b_00005 result | +------------------------------------------------------------+ | checkpoint_dir_name checkpoint_000002 | | time_this_iter_s 63.89062 | | time_total_s 241.33878 | | training_iteration 3 | | accuracy 0.5056 | | loss 1.35293 | +------------------------------------------------------------+ Trial train_cifar_f231b_00005 saved a checkpoint for iteration 3 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-03-21_17-07-18/train_cifar_f231b_00005_5_batch_size=4,l1=8,l2=64,lr=0.0004_2025-03-21_17-07-18/checkpoint_000002 (func pid=4478) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2025-03-21_17-07-18/train_cifar_f231b_00005_5_batch_size=4,l1=8,l2=64,lr=0.0004_2025-03-21_17-07-18/checkpoint_000002) [repeated 2x across cluster] (func pid=4484) [6, 2000] loss: 1.006 [repeated 2x across cluster] (func pid=4478) [4, 2000] loss: 1.340 [repeated 2x across cluster] Trial train_cifar_f231b_00006 finished iteration 6 at 2025-03-21 17:11:42. Total running time: 4min 23s +------------------------------------------------------------+ | Trial train_cifar_f231b_00006 result | +------------------------------------------------------------+ | checkpoint_dir_name checkpoint_000005 | | time_this_iter_s 32.73869 | | time_total_s 257.13156 | | training_iteration 6 | | accuracy 0.3501 | | loss 1.73277 | +------------------------------------------------------------+ Trial train_cifar_f231b_00006 saved a checkpoint for iteration 6 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-03-21_17-07-18/train_cifar_f231b_00006_6_batch_size=8,l1=16,l2=4,lr=0.0001_2025-03-21_17-07-18/checkpoint_000005 (func pid=4479) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2025-03-21_17-07-18/train_cifar_f231b_00006_6_batch_size=8,l1=16,l2=4,lr=0.0001_2025-03-21_17-07-18/checkpoint_000005) (func pid=4484) [6, 4000] loss: 0.533 (func pid=4478) [4, 4000] loss: 0.670 Trial status: 7 TERMINATED | 3 RUNNING Current time: 2025-03-21 17:11:49. Total running time: 4min 30s Logical resource usage: 6.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:M60) +------------------------------------------------------------------------------------------------------------------------------------+ | Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy | +------------------------------------------------------------------------------------------------------------------------------------+ | train_cifar_f231b_00005 RUNNING 8 64 0.000353097 4 3 241.339 1.35293 0.5056 | | train_cifar_f231b_00006 RUNNING 16 4 0.000147684 8 6 257.132 1.73277 0.3501 | | train_cifar_f231b_00007 RUNNING 256 256 0.00477469 8 5 235.002 1.2468 0.5809 | | train_cifar_f231b_00000 TERMINATED 16 1 0.00213327 2 1 169.327 2.30395 0.0966 | | train_cifar_f231b_00001 TERMINATED 1 2 0.013416 4 1 98.711 2.30777 0.1028 | | train_cifar_f231b_00002 TERMINATED 256 64 0.0113784 2 1 188.749 2.32115 0.1005 | | train_cifar_f231b_00003 TERMINATED 64 256 0.0274071 8 1 57.0853 2.42043 0.148 | | train_cifar_f231b_00004 TERMINATED 16 2 0.056666 4 1 100.222 2.31545 0.0973 | | train_cifar_f231b_00008 TERMINATED 128 256 0.0306227 8 2 107.098 2.08701 0.222 | | train_cifar_f231b_00009 TERMINATED 2 16 0.0286986 2 1 138.417 2.35534 0.097 | +------------------------------------------------------------------------------------------------------------------------------------+ (func pid=4479) [7, 2000] loss: 1.740 Trial train_cifar_f231b_00007 finished iteration 6 at 2025-03-21 17:11:54. Total running time: 4min 35s +------------------------------------------------------------+ | Trial train_cifar_f231b_00007 result | +------------------------------------------------------------+ | checkpoint_dir_name checkpoint_000005 | | time_this_iter_s 33.64137 | | time_total_s 268.64364 | | training_iteration 6 | | accuracy 0.566 | | loss 1.29 | +------------------------------------------------------------+ Trial train_cifar_f231b_00007 saved a checkpoint for iteration 6 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-03-21_17-07-18/train_cifar_f231b_00007_7_batch_size=8,l1=256,l2=256,lr=0.0048_2025-03-21_17-07-18/checkpoint_000005 (func pid=4484) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2025-03-21_17-07-18/train_cifar_f231b_00007_7_batch_size=8,l1=256,l2=256,lr=0.0048_2025-03-21_17-07-18/checkpoint_000005) (func pid=4478) [4, 6000] loss: 0.445 (func pid=4479) [7, 4000] loss: 0.862 (func pid=4484) [7, 2000] loss: 0.975 Trial train_cifar_f231b_00006 finished iteration 7 at 2025-03-21 17:12:13. Total running time: 4min 54s +------------------------------------------------------------+ | Trial train_cifar_f231b_00006 result | +------------------------------------------------------------+ | checkpoint_dir_name checkpoint_000006 | | time_this_iter_s 31.24474 | | time_total_s 288.3763 | | training_iteration 7 | | accuracy 0.3668 | | loss 1.68538 | +------------------------------------------------------------+ Trial train_cifar_f231b_00006 saved a checkpoint for iteration 7 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-03-21_17-07-18/train_cifar_f231b_00006_6_batch_size=8,l1=16,l2=4,lr=0.0001_2025-03-21_17-07-18/checkpoint_000006 (func pid=4479) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2025-03-21_17-07-18/train_cifar_f231b_00006_6_batch_size=8,l1=16,l2=4,lr=0.0001_2025-03-21_17-07-18/checkpoint_000006) (func pid=4484) [7, 4000] loss: 0.513 [repeated 2x across cluster] Trial status: 7 TERMINATED | 3 RUNNING Current time: 2025-03-21 17:12:19. Total running time: 5min 0s Logical resource usage: 6.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:M60) +------------------------------------------------------------------------------------------------------------------------------------+ | Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy | +------------------------------------------------------------------------------------------------------------------------------------+ | train_cifar_f231b_00005 RUNNING 8 64 0.000353097 4 3 241.339 1.35293 0.5056 | | train_cifar_f231b_00006 RUNNING 16 4 0.000147684 8 7 288.376 1.68538 0.3668 | | train_cifar_f231b_00007 RUNNING 256 256 0.00477469 8 6 268.644 1.29 0.566 | | train_cifar_f231b_00000 TERMINATED 16 1 0.00213327 2 1 169.327 2.30395 0.0966 | | train_cifar_f231b_00001 TERMINATED 1 2 0.013416 4 1 98.711 2.30777 0.1028 | | train_cifar_f231b_00002 TERMINATED 256 64 0.0113784 2 1 188.749 2.32115 0.1005 | | train_cifar_f231b_00003 TERMINATED 64 256 0.0274071 8 1 57.0853 2.42043 0.148 | | train_cifar_f231b_00004 TERMINATED 16 2 0.056666 4 1 100.222 2.31545 0.0973 | | train_cifar_f231b_00008 TERMINATED 128 256 0.0306227 8 2 107.098 2.08701 0.222 | | train_cifar_f231b_00009 TERMINATED 2 16 0.0286986 2 1 138.417 2.35534 0.097 | +------------------------------------------------------------------------------------------------------------------------------------+ (func pid=4479) [8, 2000] loss: 1.693 [repeated 2x across cluster] Trial train_cifar_f231b_00005 finished iteration 4 at 2025-03-21 17:12:25. Total running time: 5min 6s +------------------------------------------------------------+ | Trial train_cifar_f231b_00005 result | +------------------------------------------------------------+ | checkpoint_dir_name checkpoint_000003 | | time_this_iter_s 57.59381 | | time_total_s 298.93259 | | training_iteration 4 | | accuracy 0.5282 | | loss 1.31436 | +------------------------------------------------------------+ Trial train_cifar_f231b_00005 saved a checkpoint for iteration 4 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-03-21_17-07-18/train_cifar_f231b_00005_5_batch_size=4,l1=8,l2=64,lr=0.0004_2025-03-21_17-07-18/checkpoint_000003 (func pid=4478) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2025-03-21_17-07-18/train_cifar_f231b_00005_5_batch_size=4,l1=8,l2=64,lr=0.0004_2025-03-21_17-07-18/checkpoint_000003) Trial train_cifar_f231b_00007 finished iteration 7 at 2025-03-21 17:12:27. Total running time: 5min 8s +------------------------------------------------------------+ | Trial train_cifar_f231b_00007 result | +------------------------------------------------------------+ | checkpoint_dir_name checkpoint_000006 | | time_this_iter_s 33.39238 | | time_total_s 302.03601 | | training_iteration 7 | | accuracy 0.5472 | | loss 1.36817 | +------------------------------------------------------------+ Trial train_cifar_f231b_00007 saved a checkpoint for iteration 7 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-03-21_17-07-18/train_cifar_f231b_00007_7_batch_size=8,l1=256,l2=256,lr=0.0048_2025-03-21_17-07-18/checkpoint_000006 (func pid=4479) [8, 4000] loss: 0.832 (func pid=4478) [5, 2000] loss: 1.246 Trial train_cifar_f231b_00006 finished iteration 8 at 2025-03-21 17:12:45. Total running time: 5min 26s +------------------------------------------------------------+ | Trial train_cifar_f231b_00006 result | +------------------------------------------------------------+ | checkpoint_dir_name checkpoint_000007 | | time_this_iter_s 31.61772 | | time_total_s 319.99402 | | training_iteration 8 | | accuracy 0.3779 | | loss 1.63442 | +------------------------------------------------------------+ Trial train_cifar_f231b_00006 saved a checkpoint for iteration 8 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-03-21_17-07-18/train_cifar_f231b_00006_6_batch_size=8,l1=16,l2=4,lr=0.0001_2025-03-21_17-07-18/checkpoint_000007 (func pid=4479) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2025-03-21_17-07-18/train_cifar_f231b_00006_6_batch_size=8,l1=16,l2=4,lr=0.0001_2025-03-21_17-07-18/checkpoint_000007) [repeated 2x across cluster] (func pid=4478) [5, 4000] loss: 0.635 [repeated 2x across cluster] Trial status: 7 TERMINATED | 3 RUNNING Current time: 2025-03-21 17:12:49. Total running time: 5min 30s Logical resource usage: 6.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:M60) +------------------------------------------------------------------------------------------------------------------------------------+ | Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy | +------------------------------------------------------------------------------------------------------------------------------------+ | train_cifar_f231b_00005 RUNNING 8 64 0.000353097 4 4 298.933 1.31436 0.5282 | | train_cifar_f231b_00006 RUNNING 16 4 0.000147684 8 8 319.994 1.63442 0.3779 | | train_cifar_f231b_00007 RUNNING 256 256 0.00477469 8 7 302.036 1.36817 0.5472 | | train_cifar_f231b_00000 TERMINATED 16 1 0.00213327 2 1 169.327 2.30395 0.0966 | | train_cifar_f231b_00001 TERMINATED 1 2 0.013416 4 1 98.711 2.30777 0.1028 | | train_cifar_f231b_00002 TERMINATED 256 64 0.0113784 2 1 188.749 2.32115 0.1005 | | train_cifar_f231b_00003 TERMINATED 64 256 0.0274071 8 1 57.0853 2.42043 0.148 | | train_cifar_f231b_00004 TERMINATED 16 2 0.056666 4 1 100.222 2.31545 0.0973 | | train_cifar_f231b_00008 TERMINATED 128 256 0.0306227 8 2 107.098 2.08701 0.222 | | train_cifar_f231b_00009 TERMINATED 2 16 0.0286986 2 1 138.417 2.35534 0.097 | +------------------------------------------------------------------------------------------------------------------------------------+ (func pid=4484) [8, 4000] loss: 0.503 (func pid=4478) [5, 6000] loss: 0.419 Trial train_cifar_f231b_00007 finished iteration 8 at 2025-03-21 17:13:01. Total running time: 5min 42s +------------------------------------------------------------+ | Trial train_cifar_f231b_00007 result | +------------------------------------------------------------+ | checkpoint_dir_name checkpoint_000007 | | time_this_iter_s 33.5588 | | time_total_s 335.59482 | | training_iteration 8 | | accuracy 0.5762 | | loss 1.30593 | +------------------------------------------------------------+ Trial train_cifar_f231b_00007 saved a checkpoint for iteration 8 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-03-21_17-07-18/train_cifar_f231b_00007_7_batch_size=8,l1=256,l2=256,lr=0.0048_2025-03-21_17-07-18/checkpoint_000007 (func pid=4484) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2025-03-21_17-07-18/train_cifar_f231b_00007_7_batch_size=8,l1=256,l2=256,lr=0.0048_2025-03-21_17-07-18/checkpoint_000007) (func pid=4478) [5, 8000] loss: 0.313 [repeated 2x across cluster] (func pid=4484) [9, 2000] loss: 0.902 [repeated 2x across cluster] Trial train_cifar_f231b_00006 finished iteration 9 at 2025-03-21 17:13:16. Total running time: 5min 57s +------------------------------------------------------------+ | Trial train_cifar_f231b_00006 result | +------------------------------------------------------------+ | checkpoint_dir_name checkpoint_000008 | | time_this_iter_s 31.17297 | | time_total_s 351.16699 | | training_iteration 9 | | accuracy 0.3993 | | loss 1.59557 | +------------------------------------------------------------+ Trial train_cifar_f231b_00006 saved a checkpoint for iteration 9 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-03-21_17-07-18/train_cifar_f231b_00006_6_batch_size=8,l1=16,l2=4,lr=0.0001_2025-03-21_17-07-18/checkpoint_000008 (func pid=4479) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2025-03-21_17-07-18/train_cifar_f231b_00006_6_batch_size=8,l1=16,l2=4,lr=0.0001_2025-03-21_17-07-18/checkpoint_000008) Trial status: 7 TERMINATED | 3 RUNNING Current time: 2025-03-21 17:13:19. Total running time: 6min 0s Logical resource usage: 6.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:M60) +------------------------------------------------------------------------------------------------------------------------------------+ | Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy | +------------------------------------------------------------------------------------------------------------------------------------+ | train_cifar_f231b_00005 RUNNING 8 64 0.000353097 4 4 298.933 1.31436 0.5282 | | train_cifar_f231b_00006 RUNNING 16 4 0.000147684 8 9 351.167 1.59557 0.3993 | | train_cifar_f231b_00007 RUNNING 256 256 0.00477469 8 8 335.595 1.30593 0.5762 | | train_cifar_f231b_00000 TERMINATED 16 1 0.00213327 2 1 169.327 2.30395 0.0966 | | train_cifar_f231b_00001 TERMINATED 1 2 0.013416 4 1 98.711 2.30777 0.1028 | | train_cifar_f231b_00002 TERMINATED 256 64 0.0113784 2 1 188.749 2.32115 0.1005 | | train_cifar_f231b_00003 TERMINATED 64 256 0.0274071 8 1 57.0853 2.42043 0.148 | | train_cifar_f231b_00004 TERMINATED 16 2 0.056666 4 1 100.222 2.31545 0.0973 | | train_cifar_f231b_00008 TERMINATED 128 256 0.0306227 8 2 107.098 2.08701 0.222 | | train_cifar_f231b_00009 TERMINATED 2 16 0.0286986 2 1 138.417 2.35534 0.097 | +------------------------------------------------------------------------------------------------------------------------------------+ Trial train_cifar_f231b_00005 finished iteration 5 at 2025-03-21 17:13:23. Total running time: 6min 4s +------------------------------------------------------------+ | Trial train_cifar_f231b_00005 result | +------------------------------------------------------------+ | checkpoint_dir_name checkpoint_000004 | | time_this_iter_s 57.86861 | | time_total_s 356.80121 | | training_iteration 5 | | accuracy 0.5115 | | loss 1.35296 | +------------------------------------------------------------+ Trial train_cifar_f231b_00005 saved a checkpoint for iteration 5 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-03-21_17-07-18/train_cifar_f231b_00005_5_batch_size=4,l1=8,l2=64,lr=0.0004_2025-03-21_17-07-18/checkpoint_000004 (func pid=4478) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2025-03-21_17-07-18/train_cifar_f231b_00005_5_batch_size=4,l1=8,l2=64,lr=0.0004_2025-03-21_17-07-18/checkpoint_000004) (func pid=4484) [9, 4000] loss: 0.490 [repeated 2x across cluster] (func pid=4478) [6, 2000] loss: 1.210 [repeated 2x across cluster] Trial train_cifar_f231b_00007 finished iteration 9 at 2025-03-21 17:13:34. Total running time: 6min 15s +------------------------------------------------------------+ | Trial train_cifar_f231b_00007 result | +------------------------------------------------------------+ | checkpoint_dir_name checkpoint_000008 | | time_this_iter_s 33.64809 | | time_total_s 369.24291 | | training_iteration 9 | | accuracy 0.5229 | | loss 1.55028 | +------------------------------------------------------------+ Trial train_cifar_f231b_00007 saved a checkpoint for iteration 9 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-03-21_17-07-18/train_cifar_f231b_00007_7_batch_size=8,l1=256,l2=256,lr=0.0048_2025-03-21_17-07-18/checkpoint_000008 (func pid=4484) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2025-03-21_17-07-18/train_cifar_f231b_00007_7_batch_size=8,l1=256,l2=256,lr=0.0048_2025-03-21_17-07-18/checkpoint_000008) (func pid=4478) [6, 4000] loss: 0.604 [repeated 2x across cluster] Trial train_cifar_f231b_00006 finished iteration 10 at 2025-03-21 17:13:48. Total running time: 6min 29s +------------------------------------------------------------+ | Trial train_cifar_f231b_00006 result | +------------------------------------------------------------+ | checkpoint_dir_name checkpoint_000009 | | time_this_iter_s 31.90389 | | time_total_s 383.07089 | | training_iteration 10 | | accuracy 0.3991 | | loss 1.57063 | +------------------------------------------------------------+ (func pid=4479) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2025-03-21_17-07-18/train_cifar_f231b_00006_6_batch_size=8,l1=16,l2=4,lr=0.0001_2025-03-21_17-07-18/checkpoint_000009) Trial train_cifar_f231b_00006 saved a checkpoint for iteration 10 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-03-21_17-07-18/train_cifar_f231b_00006_6_batch_size=8,l1=16,l2=4,lr=0.0001_2025-03-21_17-07-18/checkpoint_000009 Trial train_cifar_f231b_00006 completed after 10 iterations at 2025-03-21 17:13:48. Total running time: 6min 29s Trial status: 8 TERMINATED | 2 RUNNING Current time: 2025-03-21 17:13:49. Total running time: 6min 30s Logical resource usage: 4.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:M60) +------------------------------------------------------------------------------------------------------------------------------------+ | Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy | +------------------------------------------------------------------------------------------------------------------------------------+ | train_cifar_f231b_00005 RUNNING 8 64 0.000353097 4 5 356.801 1.35296 0.5115 | | train_cifar_f231b_00007 RUNNING 256 256 0.00477469 8 9 369.243 1.55028 0.5229 | | train_cifar_f231b_00000 TERMINATED 16 1 0.00213327 2 1 169.327 2.30395 0.0966 | | train_cifar_f231b_00001 TERMINATED 1 2 0.013416 4 1 98.711 2.30777 0.1028 | | train_cifar_f231b_00002 TERMINATED 256 64 0.0113784 2 1 188.749 2.32115 0.1005 | | train_cifar_f231b_00003 TERMINATED 64 256 0.0274071 8 1 57.0853 2.42043 0.148 | | train_cifar_f231b_00004 TERMINATED 16 2 0.056666 4 1 100.222 2.31545 0.0973 | | train_cifar_f231b_00006 TERMINATED 16 4 0.000147684 8 10 383.071 1.57063 0.3991 | | train_cifar_f231b_00008 TERMINATED 128 256 0.0306227 8 2 107.098 2.08701 0.222 | | train_cifar_f231b_00009 TERMINATED 2 16 0.0286986 2 1 138.417 2.35534 0.097 | +------------------------------------------------------------------------------------------------------------------------------------+ (func pid=4478) [6, 6000] loss: 0.403 [repeated 2x across cluster] (func pid=4478) [6, 8000] loss: 0.299 [repeated 2x across cluster] Trial train_cifar_f231b_00007 finished iteration 10 at 2025-03-21 17:14:05. Total running time: 6min 46s +------------------------------------------------------------+ | Trial train_cifar_f231b_00007 result | +------------------------------------------------------------+ | checkpoint_dir_name checkpoint_000009 | | time_this_iter_s 30.66806 | | time_total_s 399.91097 | | training_iteration 10 | | accuracy 0.5601 | | loss 1.47924 | +------------------------------------------------------------+ Trial train_cifar_f231b_00007 saved a checkpoint for iteration 10 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-03-21_17-07-18/train_cifar_f231b_00007_7_batch_size=8,l1=256,l2=256,lr=0.0048_2025-03-21_17-07-18/checkpoint_000009 Trial train_cifar_f231b_00007 completed after 10 iterations at 2025-03-21 17:14:05. Total running time: 6min 46s (func pid=4484) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2025-03-21_17-07-18/train_cifar_f231b_00007_7_batch_size=8,l1=256,l2=256,lr=0.0048_2025-03-21_17-07-18/checkpoint_000009) (func pid=4478) [6, 10000] loss: 0.241 Trial train_cifar_f231b_00005 finished iteration 6 at 2025-03-21 17:14:16. Total running time: 6min 57s +------------------------------------------------------------+ | Trial train_cifar_f231b_00005 result | +------------------------------------------------------------+ | checkpoint_dir_name checkpoint_000005 | | time_this_iter_s 53.00542 | | time_total_s 409.80662 | | training_iteration 6 | | accuracy 0.5515 | | loss 1.25844 | +------------------------------------------------------------+ Trial train_cifar_f231b_00005 saved a checkpoint for iteration 6 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-03-21_17-07-18/train_cifar_f231b_00005_5_batch_size=4,l1=8,l2=64,lr=0.0004_2025-03-21_17-07-18/checkpoint_000005 (func pid=4478) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2025-03-21_17-07-18/train_cifar_f231b_00005_5_batch_size=4,l1=8,l2=64,lr=0.0004_2025-03-21_17-07-18/checkpoint_000005) Trial status: 9 TERMINATED | 1 RUNNING Current time: 2025-03-21 17:14:19. Total running time: 7min 0s Logical resource usage: 2.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:M60) +------------------------------------------------------------------------------------------------------------------------------------+ | Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy | +------------------------------------------------------------------------------------------------------------------------------------+ | train_cifar_f231b_00005 RUNNING 8 64 0.000353097 4 6 409.807 1.25844 0.5515 | | train_cifar_f231b_00000 TERMINATED 16 1 0.00213327 2 1 169.327 2.30395 0.0966 | | train_cifar_f231b_00001 TERMINATED 1 2 0.013416 4 1 98.711 2.30777 0.1028 | | train_cifar_f231b_00002 TERMINATED 256 64 0.0113784 2 1 188.749 2.32115 0.1005 | | train_cifar_f231b_00003 TERMINATED 64 256 0.0274071 8 1 57.0853 2.42043 0.148 | | train_cifar_f231b_00004 TERMINATED 16 2 0.056666 4 1 100.222 2.31545 0.0973 | | train_cifar_f231b_00006 TERMINATED 16 4 0.000147684 8 10 383.071 1.57063 0.3991 | | train_cifar_f231b_00007 TERMINATED 256 256 0.00477469 8 10 399.911 1.47924 0.5601 | | train_cifar_f231b_00008 TERMINATED 128 256 0.0306227 8 2 107.098 2.08701 0.222 | | train_cifar_f231b_00009 TERMINATED 2 16 0.0286986 2 1 138.417 2.35534 0.097 | +------------------------------------------------------------------------------------------------------------------------------------+ (func pid=4478) [7, 2000] loss: 1.172 (func pid=4478) [7, 4000] loss: 0.591 (func pid=4478) [7, 6000] loss: 0.387 (func pid=4478) [7, 8000] loss: 0.293 Trial status: 9 TERMINATED | 1 RUNNING Current time: 2025-03-21 17:14:49. Total running time: 7min 30s Logical resource usage: 2.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:M60) +------------------------------------------------------------------------------------------------------------------------------------+ | Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy | +------------------------------------------------------------------------------------------------------------------------------------+ | train_cifar_f231b_00005 RUNNING 8 64 0.000353097 4 6 409.807 1.25844 0.5515 | | train_cifar_f231b_00000 TERMINATED 16 1 0.00213327 2 1 169.327 2.30395 0.0966 | | train_cifar_f231b_00001 TERMINATED 1 2 0.013416 4 1 98.711 2.30777 0.1028 | | train_cifar_f231b_00002 TERMINATED 256 64 0.0113784 2 1 188.749 2.32115 0.1005 | | train_cifar_f231b_00003 TERMINATED 64 256 0.0274071 8 1 57.0853 2.42043 0.148 | | train_cifar_f231b_00004 TERMINATED 16 2 0.056666 4 1 100.222 2.31545 0.0973 | | train_cifar_f231b_00006 TERMINATED 16 4 0.000147684 8 10 383.071 1.57063 0.3991 | | train_cifar_f231b_00007 TERMINATED 256 256 0.00477469 8 10 399.911 1.47924 0.5601 | | train_cifar_f231b_00008 TERMINATED 128 256 0.0306227 8 2 107.098 2.08701 0.222 | | train_cifar_f231b_00009 TERMINATED 2 16 0.0286986 2 1 138.417 2.35534 0.097 | +------------------------------------------------------------------------------------------------------------------------------------+ (func pid=4478) [7, 10000] loss: 0.232 Trial train_cifar_f231b_00005 finished iteration 7 at 2025-03-21 17:15:03. Total running time: 7min 44s +------------------------------------------------------------+ | Trial train_cifar_f231b_00005 result | +------------------------------------------------------------+ | checkpoint_dir_name checkpoint_000006 | | time_this_iter_s 47.00625 | | time_total_s 456.81288 | | training_iteration 7 | | accuracy 0.5814 | | loss 1.19347 | +------------------------------------------------------------+ Trial train_cifar_f231b_00005 saved a checkpoint for iteration 7 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-03-21_17-07-18/train_cifar_f231b_00005_5_batch_size=4,l1=8,l2=64,lr=0.0004_2025-03-21_17-07-18/checkpoint_000006 (func pid=4478) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2025-03-21_17-07-18/train_cifar_f231b_00005_5_batch_size=4,l1=8,l2=64,lr=0.0004_2025-03-21_17-07-18/checkpoint_000006) (func pid=4478) [8, 2000] loss: 1.121 Trial status: 9 TERMINATED | 1 RUNNING Current time: 2025-03-21 17:15:19. Total running time: 8min 0s Logical resource usage: 2.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:M60) +------------------------------------------------------------------------------------------------------------------------------------+ | Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy | +------------------------------------------------------------------------------------------------------------------------------------+ | train_cifar_f231b_00005 RUNNING 8 64 0.000353097 4 7 456.813 1.19347 0.5814 | | train_cifar_f231b_00000 TERMINATED 16 1 0.00213327 2 1 169.327 2.30395 0.0966 | | train_cifar_f231b_00001 TERMINATED 1 2 0.013416 4 1 98.711 2.30777 0.1028 | | train_cifar_f231b_00002 TERMINATED 256 64 0.0113784 2 1 188.749 2.32115 0.1005 | | train_cifar_f231b_00003 TERMINATED 64 256 0.0274071 8 1 57.0853 2.42043 0.148 | | train_cifar_f231b_00004 TERMINATED 16 2 0.056666 4 1 100.222 2.31545 0.0973 | | train_cifar_f231b_00006 TERMINATED 16 4 0.000147684 8 10 383.071 1.57063 0.3991 | | train_cifar_f231b_00007 TERMINATED 256 256 0.00477469 8 10 399.911 1.47924 0.5601 | | train_cifar_f231b_00008 TERMINATED 128 256 0.0306227 8 2 107.098 2.08701 0.222 | | train_cifar_f231b_00009 TERMINATED 2 16 0.0286986 2 1 138.417 2.35534 0.097 | +------------------------------------------------------------------------------------------------------------------------------------+ (func pid=4478) [8, 4000] loss: 0.567 (func pid=4478) [8, 6000] loss: 0.380 (func pid=4478) [8, 8000] loss: 0.291 (func pid=4478) [8, 10000] loss: 0.232 Trial status: 9 TERMINATED | 1 RUNNING Current time: 2025-03-21 17:15:49. Total running time: 8min 31s Logical resource usage: 2.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:M60) +------------------------------------------------------------------------------------------------------------------------------------+ | Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy | +------------------------------------------------------------------------------------------------------------------------------------+ | train_cifar_f231b_00005 RUNNING 8 64 0.000353097 4 7 456.813 1.19347 0.5814 | | train_cifar_f231b_00000 TERMINATED 16 1 0.00213327 2 1 169.327 2.30395 0.0966 | | train_cifar_f231b_00001 TERMINATED 1 2 0.013416 4 1 98.711 2.30777 0.1028 | | train_cifar_f231b_00002 TERMINATED 256 64 0.0113784 2 1 188.749 2.32115 0.1005 | | train_cifar_f231b_00003 TERMINATED 64 256 0.0274071 8 1 57.0853 2.42043 0.148 | | train_cifar_f231b_00004 TERMINATED 16 2 0.056666 4 1 100.222 2.31545 0.0973 | | train_cifar_f231b_00006 TERMINATED 16 4 0.000147684 8 10 383.071 1.57063 0.3991 | | train_cifar_f231b_00007 TERMINATED 256 256 0.00477469 8 10 399.911 1.47924 0.5601 | | train_cifar_f231b_00008 TERMINATED 128 256 0.0306227 8 2 107.098 2.08701 0.222 | | train_cifar_f231b_00009 TERMINATED 2 16 0.0286986 2 1 138.417 2.35534 0.097 | +------------------------------------------------------------------------------------------------------------------------------------+ Trial train_cifar_f231b_00005 finished iteration 8 at 2025-03-21 17:15:50. Total running time: 8min 31s +------------------------------------------------------------+ | Trial train_cifar_f231b_00005 result | +------------------------------------------------------------+ | checkpoint_dir_name checkpoint_000007 | | time_this_iter_s 46.86667 | | time_total_s 503.67955 | | training_iteration 8 | | accuracy 0.5926 | | loss 1.15038 | +------------------------------------------------------------+ Trial train_cifar_f231b_00005 saved a checkpoint for iteration 8 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-03-21_17-07-18/train_cifar_f231b_00005_5_batch_size=4,l1=8,l2=64,lr=0.0004_2025-03-21_17-07-18/checkpoint_000007 (func pid=4478) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2025-03-21_17-07-18/train_cifar_f231b_00005_5_batch_size=4,l1=8,l2=64,lr=0.0004_2025-03-21_17-07-18/checkpoint_000007) (func pid=4478) [9, 2000] loss: 1.123 (func pid=4478) [9, 4000] loss: 0.561 (func pid=4478) [9, 6000] loss: 0.375 Trial status: 9 TERMINATED | 1 RUNNING Current time: 2025-03-21 17:16:19. Total running time: 9min 1s Logical resource usage: 2.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:M60) +------------------------------------------------------------------------------------------------------------------------------------+ | Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy | +------------------------------------------------------------------------------------------------------------------------------------+ | train_cifar_f231b_00005 RUNNING 8 64 0.000353097 4 8 503.68 1.15038 0.5926 | | train_cifar_f231b_00000 TERMINATED 16 1 0.00213327 2 1 169.327 2.30395 0.0966 | | train_cifar_f231b_00001 TERMINATED 1 2 0.013416 4 1 98.711 2.30777 0.1028 | | train_cifar_f231b_00002 TERMINATED 256 64 0.0113784 2 1 188.749 2.32115 0.1005 | | train_cifar_f231b_00003 TERMINATED 64 256 0.0274071 8 1 57.0853 2.42043 0.148 | | train_cifar_f231b_00004 TERMINATED 16 2 0.056666 4 1 100.222 2.31545 0.0973 | | train_cifar_f231b_00006 TERMINATED 16 4 0.000147684 8 10 383.071 1.57063 0.3991 | | train_cifar_f231b_00007 TERMINATED 256 256 0.00477469 8 10 399.911 1.47924 0.5601 | | train_cifar_f231b_00008 TERMINATED 128 256 0.0306227 8 2 107.098 2.08701 0.222 | | train_cifar_f231b_00009 TERMINATED 2 16 0.0286986 2 1 138.417 2.35534 0.097 | +------------------------------------------------------------------------------------------------------------------------------------+ (func pid=4478) [9, 8000] loss: 0.280 (func pid=4478) [9, 10000] loss: 0.223 Trial train_cifar_f231b_00005 finished iteration 9 at 2025-03-21 17:16:37. Total running time: 9min 18s +------------------------------------------------------------+ | Trial train_cifar_f231b_00005 result | +------------------------------------------------------------+ | checkpoint_dir_name checkpoint_000008 | | time_this_iter_s 47.23348 | | time_total_s 550.91303 | | training_iteration 9 | | accuracy 0.5744 | | loss 1.21333 | +------------------------------------------------------------+ Trial train_cifar_f231b_00005 saved a checkpoint for iteration 9 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-03-21_17-07-18/train_cifar_f231b_00005_5_batch_size=4,l1=8,l2=64,lr=0.0004_2025-03-21_17-07-18/checkpoint_000008 (func pid=4478) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2025-03-21_17-07-18/train_cifar_f231b_00005_5_batch_size=4,l1=8,l2=64,lr=0.0004_2025-03-21_17-07-18/checkpoint_000008) (func pid=4478) [10, 2000] loss: 1.108 Trial status: 9 TERMINATED | 1 RUNNING Current time: 2025-03-21 17:16:50. Total running time: 9min 31s Logical resource usage: 2.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:M60) +------------------------------------------------------------------------------------------------------------------------------------+ | Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy | +------------------------------------------------------------------------------------------------------------------------------------+ | train_cifar_f231b_00005 RUNNING 8 64 0.000353097 4 9 550.913 1.21333 0.5744 | | train_cifar_f231b_00000 TERMINATED 16 1 0.00213327 2 1 169.327 2.30395 0.0966 | | train_cifar_f231b_00001 TERMINATED 1 2 0.013416 4 1 98.711 2.30777 0.1028 | | train_cifar_f231b_00002 TERMINATED 256 64 0.0113784 2 1 188.749 2.32115 0.1005 | | train_cifar_f231b_00003 TERMINATED 64 256 0.0274071 8 1 57.0853 2.42043 0.148 | | train_cifar_f231b_00004 TERMINATED 16 2 0.056666 4 1 100.222 2.31545 0.0973 | | train_cifar_f231b_00006 TERMINATED 16 4 0.000147684 8 10 383.071 1.57063 0.3991 | | train_cifar_f231b_00007 TERMINATED 256 256 0.00477469 8 10 399.911 1.47924 0.5601 | | train_cifar_f231b_00008 TERMINATED 128 256 0.0306227 8 2 107.098 2.08701 0.222 | | train_cifar_f231b_00009 TERMINATED 2 16 0.0286986 2 1 138.417 2.35534 0.097 | +------------------------------------------------------------------------------------------------------------------------------------+ (func pid=4478) [10, 4000] loss: 0.551 (func pid=4478) [10, 6000] loss: 0.365 (func pid=4478) [10, 8000] loss: 0.277 (func pid=4478) [10, 10000] loss: 0.222 Trial status: 9 TERMINATED | 1 RUNNING Current time: 2025-03-21 17:17:20. Total running time: 10min 1s Logical resource usage: 2.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:M60) +------------------------------------------------------------------------------------------------------------------------------------+ | Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy | +------------------------------------------------------------------------------------------------------------------------------------+ | train_cifar_f231b_00005 RUNNING 8 64 0.000353097 4 9 550.913 1.21333 0.5744 | | train_cifar_f231b_00000 TERMINATED 16 1 0.00213327 2 1 169.327 2.30395 0.0966 | | train_cifar_f231b_00001 TERMINATED 1 2 0.013416 4 1 98.711 2.30777 0.1028 | | train_cifar_f231b_00002 TERMINATED 256 64 0.0113784 2 1 188.749 2.32115 0.1005 | | train_cifar_f231b_00003 TERMINATED 64 256 0.0274071 8 1 57.0853 2.42043 0.148 | | train_cifar_f231b_00004 TERMINATED 16 2 0.056666 4 1 100.222 2.31545 0.0973 | | train_cifar_f231b_00006 TERMINATED 16 4 0.000147684 8 10 383.071 1.57063 0.3991 | | train_cifar_f231b_00007 TERMINATED 256 256 0.00477469 8 10 399.911 1.47924 0.5601 | | train_cifar_f231b_00008 TERMINATED 128 256 0.0306227 8 2 107.098 2.08701 0.222 | | train_cifar_f231b_00009 TERMINATED 2 16 0.0286986 2 1 138.417 2.35534 0.097 | +------------------------------------------------------------------------------------------------------------------------------------+ Trial train_cifar_f231b_00005 finished iteration 10 at 2025-03-21 17:17:25. Total running time: 10min 6s +------------------------------------------------------------+ | Trial train_cifar_f231b_00005 result | +------------------------------------------------------------+ | checkpoint_dir_name checkpoint_000009 | | time_this_iter_s 47.74513 | | time_total_s 598.65816 | | training_iteration 10 | | accuracy 0.5787 | | loss 1.1842 | +------------------------------------------------------------+ Trial train_cifar_f231b_00005 saved a checkpoint for iteration 10 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-03-21_17-07-18/train_cifar_f231b_00005_5_batch_size=4,l1=8,l2=64,lr=0.0004_2025-03-21_17-07-18/checkpoint_000009 Trial train_cifar_f231b_00005 completed after 10 iterations at 2025-03-21 17:17:25. Total running time: 10min 6s Trial status: 10 TERMINATED Current time: 2025-03-21 17:17:25. Total running time: 10min 6s Logical resource usage: 2.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:M60) +------------------------------------------------------------------------------------------------------------------------------------+ | Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy | +------------------------------------------------------------------------------------------------------------------------------------+ | train_cifar_f231b_00000 TERMINATED 16 1 0.00213327 2 1 169.327 2.30395 0.0966 | | train_cifar_f231b_00001 TERMINATED 1 2 0.013416 4 1 98.711 2.30777 0.1028 | | train_cifar_f231b_00002 TERMINATED 256 64 0.0113784 2 1 188.749 2.32115 0.1005 | | train_cifar_f231b_00003 TERMINATED 64 256 0.0274071 8 1 57.0853 2.42043 0.148 | | train_cifar_f231b_00004 TERMINATED 16 2 0.056666 4 1 100.222 2.31545 0.0973 | | train_cifar_f231b_00005 TERMINATED 8 64 0.000353097 4 10 598.658 1.1842 0.5787 | | train_cifar_f231b_00006 TERMINATED 16 4 0.000147684 8 10 383.071 1.57063 0.3991 | | train_cifar_f231b_00007 TERMINATED 256 256 0.00477469 8 10 399.911 1.47924 0.5601 | | train_cifar_f231b_00008 TERMINATED 128 256 0.0306227 8 2 107.098 2.08701 0.222 | | train_cifar_f231b_00009 TERMINATED 2 16 0.0286986 2 1 138.417 2.35534 0.097 | +------------------------------------------------------------------------------------------------------------------------------------+ Best trial config: {'l1': 8, 'l2': 64, 'lr': 0.0003530972286268149, 'batch_size': 4} Best trial final validation loss: 1.1841994988113642 Best trial final validation accuracy: 0.5787 (func pid=4478) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2025-03-21_17-07-18/train_cifar_f231b_00005_5_batch_size=4,l1=8,l2=64,lr=0.0004_2025-03-21_17-07-18/checkpoint_000009) Best trial test set accuracy: 0.5926 If you run the code, an example output could look like this: Number of trials: /10 TERMINATED +-----+--------------+------+------+-------------+--------+---------+------------+ ... batch_size l1 l2 lr iter loss accuracy -----+--------------+------+------+-------------+--------+---------+------------ ... .000668163 .31479 .0977 ... .0331514 .31605 .0983 ... .000150295 .30755 .1023 ... .0128248 .66912 .4391 ... .00464561 .7316 .3463 ... .00031556 .19409 .1736 ... .00574329 .85679 .3368 ... .00325652 .30272 .0984 ... .000342987 .76044 .292 ... .003734 .53101 .4761 +-----+--------------+------+------+-------------+--------+---------+------------+ Best trial config: : , : , : .0037339984519545164, : Best trial final validation loss: .5310075663924216 Best trial final validation accuracy: .4761 Best trial accuracy: .4737 Most trials have been stopped early in order to avoid wasting resources. The best performing trial achieved a validation accuracy of about 47%, which could be confirmed on the test set. So thatâ€™s it! You can now tune the parameters of your PyTorch models."
    },
    {
        "link": "https://debuggercafe.com/hyperparameter-tuning-with-pytorch-and-ray-tune",
        "document": "In this tutorial, you will learn how to use Ray Tune for Hyperparameter Tuning in PyTorch. Finding the right hyperparameters is quite important to build a very good model for solving the deep learning problem we have at hand. In most situations, experience in training deep learning models can play a crucial role in choosing the right hyperparameters. But there will be a few situations where we need to employ some extra tools. Ray Tune is one such tool that we can use to find the best hyperparameters for our deep learning models in PyTorch. We will be exploring Ray Tune in depth in this tutorial, and writing the code to tune the hyperparameters of a PyTorch model.\n\nIf you are new to hyperparameter tuning or hyperparameter search in deep learning, you may find the following tutorials helpful.\nâ€¢ An Introduction to Hyperparameter Tuning in Deep Learning.\n\nIn this tutorial, we will go one step further for hyperparameter tuning in deep learning. We will use Ray Tune which happens to be one of the best tools for this.\n\nLetâ€™s check out the points that we will cover in this tutorial:\nâ€¢ We will start with a short introduction to Ray Tune. In that we will cover:\nâ€¢ What are the obvious disadvantages of using Skorch for hyperparamter tuning/search that we faced in the last tutorial?\nâ€¢ The solutions that Ray Tune provides to overcome the disadvantage of Skorch.\nâ€¢ Then we will explore the dataset in short that we will use in this tutorial (It is the same dataset as in the last two tutorials).\nâ€¢ Next, we will move over to the coding part of the tutorial. We will try to get into as much depth of the code as possible.\nâ€¢ After the experiment, we will analyze the results along with visualizing the TensorBoard logs.\nâ€¢ We will discuss a few possible next steps to take to learn even more about the working of Ray Tune. You will also get access to a bonus Kaggle notebook which use a different dataset.\nâ€¢ Finally, we will end with a short conclusion to the post.\n\nRay Tune is part of the Ray (Ray Core) project. Ray provides an API for building distributed applications distributed.\n\nBut we are most interested in Ray Tune which is a Python library for scalable hyperparameter tuning.\n\nAlthough we will be using Ray Tune for hyperparameter tuning with PyTorch here, it is not limited to only PyTorch. In fact, the following points from the official website summarize its wide range of capabilities quite well.\n\nAs we can see, it has support for multiple deep learning frameworks, automatic checkpointing and TensorBoard logging, and even different algorithms for hyperparameter search. You will get to experience a lot of these in this tutorial.\n\nIn the last tutorial, we did a hyperparameter search using the Skorch library. Although it was quite helpful, there were a few disadvantages.\nâ€¢ We could not use GPU while carrying out the hyperparameter search. Or at least, it is not just as easy to use a GPU with Skorch and PyTorch for hyperparameter search and tuning. And we know how crucial it is to speed up computations in deep learning with the use of a GPU.\nâ€¢ And even more so when we want to run a number of different searches to solve a problem. If you remember or go through the previous tutorial, then you will know that we were not able to run the search by training on the entire dataset because it was very time-consuming. We were only running the search on a few batches of data.\nâ€¢ We performed Grid Search which by now we know is not the best method. Random Search is much better than Grid Search for Hyperparameter Tuning.\n\nThat changes with this tutorial. We will use Ray Tune along with PyTorch for hyperparameter tuning. The integration between the two is quite good. The following are a few of the advantageous points that we will experience:\nâ€¢ Will be able to use a GPU while searching for the best hyperparameters.\nâ€¢ This means that we will also be able to run the search on the entire dataset.\nâ€¢ Ray Tune does automatic checkpointing and TensorBoard logging. We need not save the chechkpoints or the accuracy and loss plots manually as we did with Skorch.\nâ€¢ Ray Tune is even capable of running multiple search experiments on a single GPU if the GPU memory allows it.\nâ€¢ And we will be performing Random Search instead of Grid Search using Ray Tune.\n\nThe above are really some very compelling reasons to learn and try out Ray Tune. Before using it, letâ€™s install it first.\n\nRay has integration with a few other dependencies as well. But we need to install Ray with Tune. In your Anaconda or Python virtual environment of choice, execute the following command.\n\nLet the installation complete and you are good to go.\n\nOther Libraries that We Needâ€¦\n\nThis tutorial also needs PyTorch. All the code has been developed, run, and tested with PyTorch 1.10 (latest at the time of writing this). To install PyTorch in your system head over to the official site and choose the build according to your environment.\n\nWe will use the same dataset as we did in the last two tutorials.\n\nThat is the Natural Images from Kaggle with 8 classes: airplane, car, cat, dog, flower, fruit, motorbike, person. It has a total of 6899 images.\n\nThe main reason is that in each post we are trying to improve upon the methods of the previous post. And unless we use the same dataset we will not be able to compare the results.\n\nTo avoid the process of dataset selection being monotonous, you will also get access to a Kaggle notebook at the end of the tutorial that uses a different dataset with PyTorch and Ray Tune for hyperparameter tuning.\n\nFor now, you can download the dataset from here.\n\nLetâ€™s check out the directory structure for this tutorial.\nâ€¢ The folder contains the dataset directory.\nâ€¢ The folder will contain all the results from the hyperparameter search. This includes the checkpoints for different runs, the best hyperparameter, and even the TensorBoard logs.\nâ€¢ Finally, we five Python files in the folder. We will get into the details of these in the coding section of the tutorial.\n\nDownloading the zip file for this tutorial will give you access to the source code and the directory structure. You just need to download the dataset and set it up as needed.\n\nFrom this section onward, we will start with the coding part of the tutorial. As there are 5 Python files, we will tackle them in the following order:\n\nWe will try to keep the code as modular as possible. So, if you would like to edit the code in the future, you can do it easily.\n\nThe configuration file will hold the training parameters, constants for the dataset preparation, scheduler settings for Ray Tune, and search settings for Ray Tune.\n\nThe following code block contains the content that will go into file.\n\nHere, we have just one import, that is the module.\n\nFor the model training parameters we have:\nâ€¢ Number of epochs to train for equal, to 50.\nâ€¢ The root path to the data directory. You may notice that it is enclosed within the function. Without this, I was facing, , although there is nothing wrong with the path or the data folder. This must have something to do with the multi-processing that Ray Tune uses. Still, I am not quite sure but this solves the issue. They use the same technique in one of the official tutorials, so, we can safely use this.\nâ€¢ We are using 4 processes for the data preparation which will fasten up the process of and transforms in PyTorch.\nâ€¢ Just like the previous tutorials, we use 10% of the data for validation.\nâ€¢ The images are resized to 224Ã—224 dimensions.\n\nThen we have the settings for the Ray Tune which stands for . This is one of the easiest scheduling techniques to start with for hyperparameter tuning in Ray Tune. Letâ€™s take a look at the setting (these are the parameters for the scheduler). Note that the actual parameter names are different and these are just constant names we are defining corresponding to those settings that we can import. You will see the actual parameter names in where we define the scheduler.\nâ€¢ : Even though we have as 50, the scheduler can choose to stop any experiment that is within the , which is 50 as well. This means that an experiment might run for 20 epochs, or 30 epochs, or even the complete 50 epochs. And if the search experiment with the corresponding hyperparameters are not going well, then the scheduler will terminate the experiment.\nâ€¢ : Here the value is 1. This will ensure that even if the search with a particular set of hyperparameters is not going well, do not terminate the experiment if at least number of epochs have not passed for that experiment. So, when an experiment is not going well, and it has completed at least one epoch, then the scheduler will terminate it. We will see few such cases when running the experiment.\n\nNext, we have the settings for the hyperparameter search in Ray Tune. Note that the actual parameter names are different and these are just constant names we are defining corresponding to those settings that we can import. You will see the actual parameter names in where we define the method.\nâ€¢ : Number of processors to use for each search. If you have access to a multi-core processor, you can set this to one for each experiment. And potentially, Ray Tune will be able to run multiple search experiments at a time.\nâ€¢ : This is the number of GPUs to use for each search experiment. And it is an interestng one actually. You can also give a fractional number like 0.5 to this setting. This will actually divide you entire GPU memory in half and try to fit two search experiments within each half. So, if you have 10 GB of GPU memory, a value of 0.5, will try to alloctate around 5 GB for two experiments simulataneously. This will speed up the search process by a lot. But there is a catch to this. You need to ensure that according to the batch size and model parameters, all experiments will fit within the 5 GB of memory, else that particular search will error out.\n\nIf you have doubts about any of the above Ray Tune settings, do not worry. Things will become clear in .\n\nThe dataset preparation code is almost similar to the previous tutorial.\n\nThe code here will go into the file.\n\nStarting with the imports and the training and validation transforms.\n\nThis part is exactly the same as in the previous tutorial. We are not using any augmentation. We are just resizing the image according to the resize configuration that will be passed to the respective functions when creating the datasets.\n\nNext, the functions to create the datasets and data loaders.\n\nThe function prepares the training and validation dataset. It also returns the class names at the end.\n\nWe will be calling the function from the executable script ( ) while providing all the required arguments. This function calls the function and passes the required arguments to it. The data loader preparation function returns the training and validation data loaders along with the class names.\n\nWe have two functions for the training utils. Those are the training and validation functions. We will keep them separate from the executable script so that the code remains as clean and modular as possible. Any changes to these functions should not affect the other parts of the code. These should calculate the loss and accuracy for each epoch and return them only.\n\nThe code will go into the file.\n\nThe above two are very general training and validation functions for image classification. Also, these two are exactly the same as we had in the previous tutorial. Both of them calculate the loss and accuracy for each epoch and return them.\n\nWe just need to keep in mind that the training function does the backpropagation and parameter update which we do not need in the validation function.\n\nThere is no change to the neural network model as well compared to the previous tutorial. Letâ€™s write the code for that in .\n\nWe have the same class. The model has two searchable parameters:\nâ€¢ : The output channels of the first convolutional layer. After that, the output channels keep on doubling\nâ€¢ : The output features of the first fully connected layer. Then the second fully connected layer halves the output features.\n\nOne other common way to describe the searchable parameters while building a neural network model is to completely define the convolutional layers manually. Then keep the number of output features of the fully connected layers as the hyperparameters. But we are following a bit of a different approach here.\n\nNow, we will write the code for the final executable script.\n\nHere, all the code will go into the file. This will contain:\nâ€¢ A function that will prepare the data loaders and run the training and validation loops for the required number of epochs.\nâ€¢ A function that will set the Ray Tuneâ€™s search algorithm and scheduler and start the hyperparameter search.\nâ€¢ Lines 1 to 5 contain the imports from our own modules and classes.\nâ€¢ From line 9, we have the and imports:\nâ€¢ The is the one that will output the required metrics on the terminal after each epoch.\nâ€¢ And is used to set the scheduler and start the hyperparameter search. You will get to know the details as we code further.\nâ€¢ Then we have the imports for the modules.\n\nNext, we will define the function. This prepares the data loaders, and execute the and functions for the required number of epochs. After each epoch, it will pass down the validation loss and accuracy to the .\n\nThe following code block contains the entire function.\n\nLetâ€™s check out the important bits of the above code block:\nâ€¢ On line 20, we get the data loaders.\n\nNote that the batch size is according to the dictionary. We will get to see the configuration settings a bit later when we define the function. For now, letâ€™s keep in mind that the dictionary holds the values for , , , and learning rate as hyperparameters.\nâ€¢ On line 27, we initialize the model where , , set the output channels and output features for neural network model.\nâ€¢ Then, we have chosen the SGD optimizer, where the learning rate is again one of the configuration hyperparameter settings.\nâ€¢ Next, we start the training loop from line 39. A few things to note here:\nâ€¢ After one training and validation epoch completes (after line 47), we have the context. This saves the model checkpoint for that epoch. We can control how many models from each search is saved to disk. Surely, we do not want each epochâ€™s model. This we will see a bit later.\nâ€¢ Finally, on line 55, we report back the validation loss and validation accuracy to the .\n\nThis is the last function that we need and will contain the code specific to Ray Tune and hyperparameter search.\n\nFirst, letâ€™s write the entire function, then get into its explanation.\nâ€¢ On line 60, we define the dictionary that we saw in the previous code block. And we can observe our four hyperparameters.\nâ€¢ For and : It will take values between 16 and 128. We use the function where Ray Tune will sample values from a list containing the values .\nâ€¢ For , it can be any value between 0.0001 and 0.1.\nâ€¢ Finally, the batch size is going to be one of the values from . As we are direcly providing a list here, so, we use .\nâ€¢ Next, we define the on line 70.\nâ€¢ It will monitor the metric and stop any bad performing search experiment to save resources and give chance to the next hyperaparameter search.\nâ€¢ We use the from our file to define the parameter. Any experiment will not go further than these number of epochs. So, even if we define the as 100 in , the scheduler will stop every search exerperiment after 50 epochs (the current value for ).\nâ€¢ Any bad performing search will only be stopped after at least number of epochs. Currently, its 1.\nâ€¢ On line 78, we define the . The defines the metrics to show on the terminal after each epoch.\nâ€¢ We start the search on line 81 by executing .\nâ€¢ The first argument is the function which we defined earlier. This is the function that carries out the trianing and validation loop for each epoch.\nâ€¢ Then we have . It is a dictionary stating the number of CPUs and GPUs to use for each trial. We have already defined the numbers in .\nâ€¢ Then the argument takes the dictionary.\nâ€¢ The defines the path to the directory where all the search trial results will be saved.\nâ€¢ Then we have the which defines the number of checkpoints to save. By default, it will save all the checkpoints, which we obviously donâ€™t want. For us, itâ€™s 1. This means that it will only save one checkpoint from each search trial based on the minimum loss value of the epoch number. This attribute is checked by where we tell it to monitor the validation loss of each epoch and save the model from epoch which has the minimum loss.\nâ€¢ Finally, the is the instance of the .\n\nAfter all the search trials/experiments are complete we print the best trialâ€™s loss and accuracy.\n\nThatâ€™s all the code we need for now. We can start executing the code. If you wish, you may go over the code once more to understand it even better.\n\nWe are all set to execute the script.\n\nNote: It might take some time to complete the entire execution. We are running 20 different searches here. The complete execution can take anywhere between 45 minutes to 2 hours if you are on a modern and powerful GPU. The entire run time will also depend upon the hyperparameters that are sampled, as each sampling will be random. The modelâ€™s parameters are the ones that will affect the run time the most. On an RTX 3080, it will take somewhere around 45 minutes to complete the entire run.\n\nOpen your command line/terminal inside the directory, and execute the following command.\n\nYou should see a similar output to the following.\n\nIn the reporter above, we can see a few experiments are in RUNNING state, a few are in PENDING state, and a few are in the TERMINATED state. Eventually, all will be TERMINATED as in the last reporter.\n\nThis is the best trial configuration from the entire search:\n\nAnd we have a validation loss of 0.185 and a validation accuracy of 95.065. Both are respectively lower and higher than we got in the case of searching with Skorch in the last tutorial (around 0.213 and 94 %).\nâ€¢ We have successfully beat the previous method with Random Hyperparameter Search.\nâ€¢ We were able to use a GPU and train on the entire dataset which directly provided us with the best model at the end.\n\nRay Tune saves the TensorBoard logs automatically. Letâ€™s take a look at the loss and accuracy graphs.\n\nWe can clearly see the searches that were terminated before 50 epochs by the scheduler.\n\nNext, you can try your own experiments on different datasets. Maybe use the image resizing as one of the hyperparameters as well.\n\nYou may also take a look at this Kaggle Notebook. Here, after training and validation, we also carry out testing on a held-out test set. We use a Blood Cell Images dataset here which has more images compared to the one used in this tutorial. Hopefully, this will even expand your learning experience. Do let us know in the comment section of your experience with different experiments.\n\nWe carried out Random Hyperparameter Search and Tuning using Ray Tune and PyTorch in this tutorial. You saw how to create an entire pipeline for hyperparameter search using Ray Tune, how to use GPUs, and even visualized the proper logs of the searches. I hope that this was a good learning experience for you.\n\nIf you have any doubts, thoughts, or suggestions, please leave them in the comment section. I will surely address them.\n\nYou can contact me using the Contact section. You can also find me on LinkedIn, and Twitter."
    },
    {
        "link": "https://restack.io/p/hyperparameter-tuning-answer-lstm-pytorch-cat-ai",
        "document": "Explore techniques for hyperparameter tuning in LSTM models using PyTorch to enhance performance and accuracy."
    },
    {
        "link": "https://crosstab.io/articles/time-series-pytorch-lstm",
        "document": "Most intros to LSTM models use natural language processing as the motivating application, but LSTMs can be a good option for multivariable time series regression and classification as well. Hereâ€™s how to structure the data and model to make it work."
    },
    {
        "link": "https://medium.com/geekculture/10-hyperparameters-to-keep-an-eye-on-for-your-lstm-model-and-other-tips-f0ff5b63fcd4",
        "document": "Deep Learning has proved to be a fast evolving subset of Machine Learning. It aims to identify patterns and make real world predictions by mimicking the human brain. Models based on such kinds of neural network topology has applications in virtually every industry. The most important step among all the (integral) steps is perhaps the training of such a model so that it is capable of making robust predictions in any new testing data. It is thus pertinent to choose a modelâ€™s hyperparameters (parameters whose values are used to control the learning process) in such a way that training is effective in terms of both time and fit (whether the model â€œknowsâ€ the training data too well, or too poor; to constrict any form of overfitting or underfitting).\n\nThis article talks about LSTM in particular, a unique kind of recurrent neural network (RNN) capable of learning all the long term dependencies in the dataset. Recurrent neural networks are a class of neural networks which deal with temporal data. Long short-term memory (LSTM) has a similar control flow as a recurrent neural network in the sense that it processes the data while passing on information as it propagates forward. The actual difference lies in the operations within the cells of the long short-term memory network. These operations allow the LSTM to keep or forget information. LSTMs enable backpropagation of the error through time and layers hence helping preserve them. An LSTM (Long short-term memory) model is an artificial recurrent neural network (RNN) architecture which has feedback connections, making it able to not only process single data points, but also entire sequences of data. This article address all such hypermeters for an LSTM model necessary to improve the performance and what values are used as best practice.\n\nBefore we get into the tuning of the most relevant hyperparameters for LSTM, it is worth noting that there are ways to let your system find the hyperparameters for you by using optimizations tools. These methods are useful to bypass more manual processes in identifying good hyperparameters and tuning them. In Python, some such tools are:\n\nIt should be kept in mind that many such hyperparameters are volatile, in the sense that different values (or even same values and different runs) may yield different results. So make sure you always compare models and performance by tweaking these hyperparameters to get the optimum results.\n\nThe layers between the input and output layers are called hidden layers. This fundamental concept is what makes deep learning networks being termed as a â€œblack boxâ€, often being criticized for not being transparent and their predictions not being traceable by humans. There is no final number on how many nodes (hidden neurons) or hidden layers one should use, so depending on the individual problem (believe it or not) a trial and error approach will give the best results.\n\nAs a general rule of thumb, one hidden layer will work with most simple problems and two layers with reasonably complex ones. Also, while many nodes (with regularization techniques) within a layer can increase accuracy, fewer number of nodes may cause underfitting.\n\nA dense layer is the most frequently used layer which is basically a layer where each neuron receives input from all neurons in the previous layer â€” thus, â€œdensely connectedâ€. Dense layers improve overall accuracy and 5â€“10 units or nodes per layer is a good base. So the output shape of the final dense layer will be affected by the number of neuron / units specified.\n\nEvery LSTM layer should be accompanied by a dropout layer. Such a layer helps avoid overfitting in training by bypassing randomly selected neurons, thereby reducing the sensitivity to specific weights of the individual neurons. While dropout layers can be used with input layers, they shouldnâ€™t be used with output layers as that may mess up the output from the model and the calculation of error. While adding more complexity may risk overfitting (by increasing nodes in dense layers or adding more number of dense layers and have poor validation accuracy), this can be addressed by adding dropout.\n\nA good starting point is 20% but the dropout value should be kept small (up to 50%). The 20% value is widely accepted as the best compromise between preventing model overfitting and retaining model accuracy.\n\nIdeally, it is better to employ different weight initialization schemes according to what activation function is used. However, more commonly a uniform distribution is used while choose initial weight values. It is not possible to set all weights to 0.0 as the asymmetry in the error gradient is brought out by the optimization algorithm; to begin searching effectively. Different set of weights results in different starting points of the optimization process, potentially leading to different final sets with different performance characteristics. Weights should finally be initialized randomly to small numbers (an expectation of the stochastic optimization algorithm, otherwise known as stochastic gradient descent) to harness randomness in the search process.\n\nThe weight decay can be added in the weight update rule that makes the weights decay to zero exponentially, if no other weight update is scheduled. After each update, the weights are multiplied by a factor slightly less than 1, thereby preventing them from growing to huge. This specifies regularization in the network.\n\nThe default value of 0.97 should be enough to start off.\n\nActivation functions are what defines the output of a node as either being ON or OFF. These functions are used to introduce non-linearity to models, allowing deep learning models to learn non-linear prediction boundaries. Technically, activation functions can be included in the dense layers but splitting them into them into different layers makes it possible to retrieve the reduced output of the density layer.\n\nAgain, choice of activation layer depends on the application, however, the rectifier activation function is most popular. Specific situations entail specific functions. For example, sigmoid activation is used in the output layer for binary predictions and softmax is used to make multi-class predictions (softmax gives your ability the ability to interpret the outputs as probabilities.\n\nMethod: The process is to create user defined functions and have it return the output associated with any specific activation function. For example, here is a sigmoid activation function:\n\nSigmoid (log-sigmoid) and hyperbolic tangent are some of the more popular activation functions adopted in LSTM blocks.\n\nThis hyperparameter defines how quickly the network updates its parameters. Setting a higher learning rate accelerates the learning but the model may not converge (a state during training where the loss settles to within an error range around the final value), or even diverge. Conversely, a lower rate will slow down the learning drastically as steps towards the minimum of loss function will be tiny, but will allow the model to converge smoothly.\n\nUsually a decaying learning rate is preferred and this hyperparameter is used in the training phase and has a small positive value, mostly between 0.0 and 0.1.\n\nThe momentum hyperparameter has been researched into to integrate with RNN and LSTM. Momentum is a unique hyperparameter which allows the accumulation of the gradients of the past steps to determine the direction to go with, instead of using the gradient of only the current step to guide the search.\n\nTypically, the value is between 0.5 to 0.9.\n\nThis hyperparameters sets how many complete iterations of the dataset is to be run. While theoretically, this number can be set to an integer value between one and infinity, this should be increased until the validation accuracy starts to decrease even though training accuracy increases (and hence risking overfitting).\n\nA pro move is to employ the early stopping method to first specify a large number of training epochs and stop training once the model performance stops improving by a pre-set threshold on the validation dataset.\n\nThis hyperparameter defines the number of samples to work on before the internal parameters of the model are updated. Large sizes make large gradient steps compared to smaller ones for the same number of samples â€œseenâ€.\n\nWidely accepted, a good default value for batch size is 32. For experimentation, you can try multiples of 32, such as 64, 128 and 256.\n\nApart from tuning the hyperparameters, here are some tips to for training your LSTM or RNN model.\n\nÂ· Adaptive learning rate: To better handle the complex training dynamics of recurrent neural networks (that a plain gradient descent may not address), adaptive optimizers such as Adam is recommended.\n\nÂ· Gradient clipping: Spikes in gradient can mess up parameters during training. This can be prevented by first plotting the gradient norm (to see its usual range) and then scaling down those gradients that exceeds this range.\n\nÂ· Normalizing the loss: Adding the loss terms along the sequence and then dividing them by the maximum sequence length. This will average out the loss across the batch and in turn make it easier to reuse the hyperparameters between experiments.\n\nÂ· Truncated backpropagation: Any form of recurrent network may struggle with learning long sequences due to vanishing and noisy gradients. Even though LSTM specifically designed to address the vanishing gradient problem, it is worth noting how some professionals recommend training on overlapping chunks of around 200 steps instead, gradually increasing the chunk length during training.\n\nÂ· Gated Recurrent unit: GRU is an alternative cell design that uses fewer parameters and computes faster compared to LSTM.\n\nÂ· Layer normalization: Another way to speed up learning and improve final performance is by adding layer normalization to all the linear mappings of the recurrent network.\n\nÂ· Feed-forward layers: It is possible to enable your model project the data into a space with simpler temporal dynamics by pre-processing the input with feed-forward layers. This helps increase the performance.\n\nÂ· Learned initial state: Large loss terms are caused in the first few time steps as a result of initializing the hidden state as zeroes thereby rendering the model to focus less on the actual sequence. Training the initial state as a variable can improve performance.\n\nÂ· Bias due to forget gate: Recurrent networks can take a while to learn to remember information from the last time step. This can be improved by initializing the bias for LSTMâ€™s forget gate to 1, enabling it to remember more by default. Similarly, for GRUs, the bias needs to be initialized to -1.\n\nÂ· Regularization: Regularization methods such as dropout are well known to address model overfitting.\n\nOpen source libraries such as Keras has freed us from writing complex codes to make complex deep learning algorithms and every day more research is being conducted to make modelling more robust. While these tips on how to use hyperparameters in your LSTM model may be useful, you still will have to make some choices along the way like choosing the right activation function. It is important to remember that not all results tell an unbiased story. For example, the smallest improvements in loss can end up making a big difference in the perceived quality of the model. If the training loss does not improve multiple epochs, it is better to just stop the training. Otherwise the evaluation loss will start increasing. In the end, best results come by evaluating outcomes after testing various configurations.\n\n5. A comparative performance analysis of different activation functions in LSTM networks for classification. (https://link.springer.com/article/10.1007/s00521-017-3210-6#:~:text=The%20most%20popular%20activation%20functions,functions%20have%20been%20successfully%20applied.)"
    }
]