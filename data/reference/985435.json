[
    {
        "link": "https://docs.aiogram.dev",
        "document": "# Bot token can be obtained via https://t.me/BotFather # All handlers should be attached to the Router (or Dispatcher) # Most event objects have aliases for API methods that can be called in events' context # For example if you want to answer to incoming message you can use `message.answer(...)` alias # and the target chat will be passed to :ref:`aiogram.methods.send_message.SendMessage` Handler will forward receive a message back to the sender By default, message handler will handle all message types (like a text, photo, sticker etc.) # But not all the types is supported to be copied so need to handle it # Initialize Bot instance with default bot properties which will be passed to all API calls"
    },
    {
        "link": "https://docs.aiogram.dev/en/latest/dispatcher/finite_state_machine/index.html",
        "document": "An FSM is defined by a list of its states, its initial state, and the inputs that trigger each transition.\n\nIt is an abstract machine that can be in exactly one of a finite number of states at any given time. The FSM can change from one state to another in response to some inputs; the change from one state to another is called a transition.\n\nA finite-state machine (FSM) or finite-state automaton (FSA, plural: automata), finite automaton, or simply a state machine, is a mathematical model of computation.\n\nNot all functionality of the bot can be implemented as single handler, for example you will need to collect some data from user in separated steps you will need to use FSM.\n\nLet‚Äôs see how to do that step-by-step\n\nBefore handle any states you will need to specify what kind of states you want to handle And then write handler for each state separately from the start of dialog Here is dialog can be started only via command , so lets handle it and make transition user to state \"Hi there! What's your name?\" After that you will need to save some data to the storage and make transition to next step. Did you like to write bots?\" At the next steps user can make different answers, it can be , or any other Handle and soon we need to handle state What programming language did you use for it?\" And handle any other answers All possible cases of step was covered, let‚Äôs implement finally step \"Python, you say? That's the language that makes my circuits light up! üòâ\" \"you like to write bots with \"you don't like to write bots, so sad...\" And now you have covered all steps from the image, but you can make possibility to cancel conversation, lets do that via command or text Allow user to cancel any action\n\n\"Hi there! What's your name?\" Allow user to cancel any action Did you like to write bots?\" What programming language did you use for it?\" \"Python, you say? That's the language that makes my circuits light up! üòâ\" \"you like to write bots with \"you don't like to write bots, so sad...\" # Initialize Bot instance with default bot properties which will be passed to all API calls"
    },
    {
        "link": "https://stackoverflow.com/questions/73783887/finite-state-machine-in-aiogram-2-for-telegram-bots",
        "document": "There is the following situation, in my bot there is a registration of a new user after the /start command, and after it a person can press the /next command and start searching for an interlocutor. And everything will be ok. But if I reload the bot, the user will not be able to get to the same State as after registration.\n\nIs it possible to somehow set the initial state in aiogram ?\n\nAnd yeah, is there a default state in aiogram states?"
    },
    {
        "link": "https://docs.aiogram.dev/en/v2.25.1",
        "document": "aiogram is a pretty simple and fully asynchronous framework for Telegram Bot API written in Python 3.7 with asyncio and aiohttp. It helps you to make your bots faster and simpler.\n‚Ä¢ None Can reply into webhook. (In other words make requests in response to updates)"
    },
    {
        "link": "https://hostman.com/tutorials/how-to-create-and-set-up-a-telegram-chatbot",
        "document": "Database Connection in Python, Go, and JavaScript\n\nDatabases are an essential part of almost any project today. Database interactions are especially familiar to system and database administrators, DevOps/SRE professionals, and software developers. While administrators typically deploy one or multiple database instances and configure the necessary connection parameters for applications, developers need to connect directly to the database within their code. This article explores how to connect to databases using different programming languages. Prerequisites We will provide examples for connecting to MySQL, PostgreSQL, Redis, MongoDB, and ClickHouse databases using Python, Go, and JavaScript. To follow this guide, you will need: A database deployed on a server or in the cloud. Installed environments for Python, Go, and JavaScript, depending on your application programming language. Additionally for Python: pip installed. Additionally for JavaScript: Node.js and npm installed. Database Connection in Python MySQL and Python For connecting to MySQL databases, we can use a Python driver called MySQL Connector. Install the driver using pip: pip install mysql-connector-python Initialize a new connection: Import the mysql.connector library and the Error class to handle specific connection errors. Create a function named create_connection, passing the database address (host), user name (user), and user password (password). To establish the connection, define a class called create_connection that receives the variable names containing the database connection details. import mysql.connector from mysql.connector import Error def create_connection(host_name, user_name, user_password): connection = None try: connection = mysql.connector.connect( host=\"91.206.179.29\", user=\"gen_user\", password=\"m-EE6Wm}z@wCKe\" ) print(\"Successfully connected to MySQL Server!\") except Error as e: print(f\"The error '{e}' occurred\") return connection def execute_query(connection, query): cursor = connection.cursor() try: cursor.execute(query) connection.commit() print(\"Query executed successfully\") except Error as e: print(f\"The error '{e}' occurred\") connection = create_connection(\"91.206.179.29\", \"gen_user\", \"m-EE6Wm}z@wCKe\") Run the script. If everything works correctly, you will see the \"Successfully connected to MySQL Server!\" message. If any errors occur, the console will display error code and description. Create a new table: Connect to the database using the connection.database class, specifying the name of the database. Note that the database should already exist. To create a table, initialize a variable create_table_query containing the SQL CREATE TABLE query. For data insertion, initialize another variable insert_data_query with the SQL INSERT INTO query. To execute each query, use the execute_query class, which takes the database connection string and the variable containing the SQL query. connection.database = 'test_db' create_table_query = \"\"\" CREATE TABLE IF NOT EXISTS users ( id INT AUTO_INCREMENT PRIMARY KEY, name VARCHAR(100) NOT NULL, age INT NOT NULL ) \"\"\" execute_query(connection, create_table_query) insert_data_query = \"\"\" INSERT INTO users (name, age) VALUES ('Alice', 30), ('Bob', 25) \"\"\" execute_query(connection, insert_data_query) if connection.is_connected(): connection.close() print(\"Connection closed\") Run the script. PostgreSQL and Python Python offers several plugins for connecting to PostgreSQL, but the most popular one is psycopg2, which we will use here. Psycopg2 is one of the most frequently used Python plugins for PostgreSQL connections. One of its key advantages is its support for multithreading which allows you to maintain the database connection across multiple threads. Install psycopg2 using pip (if not already installed): pip install psycopg2-binary Connect to PostgreSQL. Import the Python psycopg2 package and create a function create_new_conn, using the try block. Establish the connection with the psycopg2.connect function, which requires the database name, user name, password, and database address as input. To initialize the connection, use the create_new_conn() function. Here‚Äôs the full code example for connecting to a database: import psycopg2 from psycopg2 import OperationalError def create_new_conn(): conn_to_postgres = None while not conn_to_postgres: try: conn_to_postgres = psycopg2.connect( default_db=\"default_db\", default_user=\"gen_user\", password_for_default_user=\"PasswordForDefautUser9893#\", db_address=\"91.206.179.128\" ) print(\"The connection to PostgreSQL has been successfully established!\") except OperationalError as e: print(e) return conn_to_postgres conn_to_postgres = create_new_conn() Run the script: python3 connect_to_postgres.py If successful, you will see the \"The connection to PostgreSQL has been successfully established!\" message. . Next, create a table named books, which will have three columns. Use the cursor class for SQL expressions, such as creating database objects. If the query involves adding or modifying data, you must call the conn_to_postgres.commit() function afterward to apply the changes. import psycopg2 from psycopg2 import OperationalError def create_new_conn(): conn_to_postgres = None while not conn_to_postgres: try: conn_to_postgres = psycopg2.connect( default_db=\"default_db\", default_user=\"gen_user\", password_for_default_user=\"PasswordForDefautUser9893#\", db_address=\"91.206.179.128\" ) except OperationalError as e: print(e) return conn_to_postgres conn_to_postgres = create_new_conn() cursor = conn_to_postgres.cursor() cursor.execute(\"\"\" CREATE TABLE books ( book_id INT PRIMARY KEY NOT NULL, book_name VARCHAR(255) NOT NULL, book_author VARCHAR(255) NOT NULL ) \"\"\") conn_to_postgres.commit() print(\"Table Created successfully\") Run the script: python3 create_table.py Now, let‚Äôs run INSERT INTO to add a new line: cursor.execute(\"\"\" INSERT INTO books (book_id,book_name,book_author) VALUES (1, 'Long Walk to Freedom', 'Nelson_Mandela') \"\"\") The full code is below: import psycopg2 from psycopg2 import OperationalError def create_new_conn(): conn_to_postgres = None while not conn_to_postgres: try: conn_to_postgres = psycopg2.connect( default_db=\"default_db\", default_user=\"gen_user\", password_for_default_user=\"PasswordForDefautUser9893#\", db_address=\"91.206.179.128\" ) except OperationalError as e: print(e) return conn_to_postgres conn_to_postgres = create_new_conn() cursor = conn_to_postgres.cursor() cursor.execute(\"\"\" INSERT INTO books (book_id,book_name,book_author) VALUES (1, 'Long Walk to Freedom', 'Nelson_Mandela') \"\"\") conn_to_postgres.commit() conn_to_postgres.close() print(\"Data inserted successfully\") Run the script: python3 insert-data.py Redis and Python Redis belongs to the class of NoSQL databases, where data is stored in memory rather than on hard drives. It uses a key-value format for data storage. Redis has a wide range of applications, from data storage and caching to serving as a message broker. We will use the redis-py (or simply redis) library for connecting to Redis. Install the Redis library using pip: pip install redis Connecting to a Redis instance: Use a try block structure for connection, specifying the function redis.StrictRedis where you provide the Redis address, port, and user password. import redis try: connect_to_redis_server = redis.StrictRedis( redis_db_host=91.206.179.128, redis_db_port=6379, redis_user_password='PasswordForRedis6379') print connect_to_redis_server connect_to_redis_server.ping() print 'Successfully connected to Redis Server!' except Exception as ex: print 'Error:', ex exit('Failed to connect to Redis server.') Run the script: python3 connect_to_redis.py If successful, you will see a message like \"Successfully connected to Redis Server!\". Unlike relational databases, Redis stores data in a key-value format. The key uniquely identifies the corresponding value. Use the set method to create a new record. The example below creates a record with the key City and the value Berlin: print('Create new record:', connect_to_redis_server.set(\"City\", \"Berlin\")) Use the get method to retrieve the value associated with a key: print('Print record using record key:', connect_to_redis_server.get(\"City\")) Use the delete method to remove a record by its key: print('Delete record with key:', connect_to_redis_server.delete(\"City\")) The complete code fragment is below. import redis try: connect_to_redis_server = redis.StrictRedis( redis_db_host=91.206.179.128, redis_db_port=6379, redis_user_password='PasswordForRedis6379') print ('New record created:', connect_to_redis_server.set(\"City\", \"Berlin\")) print ('Print created record using record key', connect_to_redis_server.get(\"City\")) print ('Delete created record with key :', connect_to_redis_server.delete(\"City\")) except Exception as ex: print ('Error:', ex) MongoDB and Python MongoDB is another widely used NoSQL database that belongs to the document-oriented category. Data is organized as JSON-like documents. To connect to a MongoDB database with Python, the recommended library is PyMongo, which provides a synchronous API. Install the PyMongo plugin: pip3 install pymongo Connect to MongoDB server using the following Python code. Import the pymongo module and use the MongoClient class to specify the database server address. To establish a connection to the MongoDB server, use a try block for error handling: import pymongo connect_to_mongo = pymongo.MongoClient(\"mongodb://91.206.179.29:27017/\") first_db = connect_to_mongo[\"mongo-db1\"] try: first_db.command(\"serverStatus\") except Exception as e: print(e) else: print(\"Successfully connected to MongoDB Server!\") connect_to_mongo.close() Run: python3 connect_mongodb.py If the connection is successfully established, the script will return the message: \"Successfully connected to MongoDB Server!\" Add data to MongoDB. To add data, you need to create a dictionary. Let's create a dictionary named record1, containing three keys: record1 = { \"name\": \"Alex\", \"age\": 25, \"location\": \"London\" } To insert the dictionary data, use the insert_one method in MongoDB. insertrecord = collection1.insert_one(record1) import pymongo connect_to_mongo = pymongo.MongoClient(\"mongodb://91.206.179.29:27017/\") db1 = connect_to_mongo[\"newdb\"] collection1 = db1[\"userdata\"] record1 = { \"name\": \"Alex\", \"age\": 25, \"location\": \"London\" } insertrecord = collection1.insert_one(record1) print(insertrecord) Run the script: python3 connect_mongodb.py ClickHouse and Python ClickHouse is a columnar NoSQL database where data is stored in columns rather than rows. It is widely used for handling analytical queries. Install the ClickHouse driver for Python. There is a dedicated plugin for ClickHouse called clickhouse-driver. Install the driver using the pip package manager: pip install clickhouse-driver Connect to ClickHouse. To initialize a connection with ClickHouse, you need to import the Client class from the clickhouse_driver library. To execute SQL queries, use the client.execute function. You also need to specify the engine. For more details on supported engines in ClickHouse, you can refer to the official documentation. We'll use the default engine, MergeTree. Next, create a new table called users and insert two columns with data. To list the data to be added to the table, use the tuple data type. After executing the necessary queries, make sure to close the connection to the database using the client.disconnect() method. The final code will look like this: from clickhouse_driver import Client client = Client(host=91.206.179.128', user='root', password='P@$$w0rd123', port=9000) client.execute(''' CREATE TABLE IF NOT EXISTS Users ( id UInt32, name String, ) ENGINE = MergeTree() ORDER BY id ''') data = [ (1, 'Alice'), (2, 'Mary') ] client.execute('INSERT INTO Users (id, name) VALUES', data) result = client.execute('SELECT * FROM Users') for row in result: print(row) client.disconnect() Database Connection in Go Go is one of the youngest programming languages, developed in 2009 by Google. It is widely used in developing microservice architectures and network utilities. For example, services like Docker and Kubernetes are written in Go. Go supports integrating all popular databases, including PostgreSQL, Redis, MongoDB, MySQL, ClickHouse, etc. MySQL and Go For working with the MySQL databases in Go, use the go-sql-driver/mysql driver. Create a new directory for storing project files and navigate into it: mkdir mysql-connect && cd mysql-connect Create a go.mod file to store the dependencies: go mod init golang-connect-mysql Download the MySQL driver using the go get command: go get -u github.com/go-sql-driver/mysql Create a new file named main.go. Specify the database connection details in the dsn variable: package main import ( \"database/sql\" \"fmt\" \"log\" _ \"github.com/go-sql-driver/mysql\" ) func main() { dsn := \"root:password@tcp(localhost:3306)/testdb\" db, err := sql.Open(\"mysql\", dsn) if err != nil { log.Fatal(err) } defer db.Close() if err := db.Ping(); err != nil { log.Fatal(err) } fmt.Println(\"Successfully connected to the database!\") query := \"INSERT INTO users (name, age) VALUES (?, ?)\" result, err := db.Exec(query, \"Alex\", 25) if err != nil { log.Fatal(err) } lastInsertID, err := result.LastInsertId() if err != nil { log.Fatal(err) } fmt.Printf(\"Inserted data with ID: %d\n\n\", lastInsertID) } PostgreSQL and Go To connect to PostgreSQL, use the pq driver. Before installing the driver, let's prepare our environment. Create a new directory for storing the project files and navigate into it: mkdir postgres-connect && cd postgres-connect Since we will be working with dependencies, we need to create a go.mod file to store them: go mod init golang-connect-postgres Download the pq driver using the go get command: go get github.com/lib/pq Create a new file named main.go. In addition to importing the pq library, it is necessary to add the database/sql library as Go does not come with official database drivers by default. The database/sql library consists of general, independent interfaces for working with databases. It is also important to note the underscore (empty identifier) when importing the pq module: _ \"github.com/lib/pq\" The empty identifier is used to avoid the \"unused import\" error, as in this case, we only need the driver to be registered in database/sql. The fmt package is required to output data to the standard output stream, for example, to the console. To open a connection to the database, the sql.Open function is used, which takes the connection string (connStr) and the driver name (postgres). The connection string specifies the username, database name, password, and host address: package main import ( \"database/sql\" \"fmt\" \"log\" _ \"github.com/lib/pq\" ) func main() { connStr := \"user=golang dbname=db_for_golang password=Golanguserfordb0206$ host=47.45.249.146 sslmode=disable\" db, err := sql.Open(\"postgres\", connStr) if err != nil { log.Fatal(err) } defer db.Close() err = db.Ping() if err != nil { log.Fatal(err) } fmt.Println(\"Successfully connected to PostgreSQL!\") } Compile and run: go run main.go If everything works correctly, the terminal will display the message Successfully connected to PostgreSQL! Now, let's look at an example of how to insert data into a table. First, we need to create a table in the database. When using Hostman cloud databases, you can copy the PostgreSQL connection string displayed in the \"Connections\" section of the Hostman web interface. Make sure that the postgresql-client utility is installed on your device beforehand. Enter the psql shell and connect to the previously created database: \\c db_for_golang Create a table named Cities with three fields ‚Äî city_id, city_name, and city_population: CREATE TABLE Cities ( city_id INT PRIMARY KEY, city_name VARCHAR(45) NOT NULL, city_population INT NOT NULL); Grant full privileges to the created table for the user: GRANT ALL PRIVILEGES ON TABLE cities TO golang; The function db.Prepare is used to prepare data. It specifies the query for insertion in advance. To insert data, use the function stmt.Exec. In Go, it's common to use plain SQL without using the ORM (Object-Relational Mapping) approach. stmt, err := db.Prepare(\"INSERT INTO Cities(city_id, city_name, city_population) VALUES($1, $2, $3)\") if err != nil { log.Fatal(err) } defer stmt.Close() _, err = stmt.Exec(1, \"Toronto\", 279435) if err != nil { log.Fatal(err) } fmt.Println(\"Data inserted successfully!\") } If all works correctly, you will see: Data inserted successfully! Redis and Go To connect to Redis, you need to use the go-redis driver. –°reate a new directory: mkdir connect-to-redis && cd connect-to-redis Prepare the dependency file: go mod init golang-connect-redis And optimize them: go mod tidy Download the go-redis module: go get github.com/go-redis/redis/v8 To connect to Redis, use the redis.Options function to specify the address and port of the Redis server. Since Redis does not use authentication by default, you can leave the Password field empty and use the default database (database 0): package main import ( \"context\" \"fmt\" \"log\" \"github.com/go-redis/redis/v8\" ) func main() { rdb := redis.NewClient(&redis.Options{ Addr: \"91.206.179.128:6379\", Password: \"\", DB: 0, }) ctx := context.Background() _, err := rdb.Ping(ctx).Result() if err != nil { log.Fatalf(\"Couldn't connect to Redis: %v\", err) } fmt.Println(\"Successfully connected to Redis!\") } You should see the message ¬´Successfully connected to Redis!¬ª MongoDB and Go To work with MongoDB, we'll use the mongo driver. Create a new directory to store the project structure: mkdir connect-to-mongodb && cd connect-to-mongodb Initialize the dependency file: go mod init golang-connect-mongodb Download the mongo library: go get go.mongodb.org/mongo-driver/mongo Connect to MongoDB using the options.Client().ApplyURI method. It takes a connection string such as mongodb://91.206.179.29:27017, where 91.206.179.29 is the MongoDB server address and 27017 is the port for connecting to MongoDB. The options.Client().ApplyURI string is used only for specifying connection data. To check the connection status, you can use another function, client.Ping, which shows the success or failure of the connection: package main import ( \"context\" \"fmt\" \"log\" \"time\" \"go.mongodb.org/mongo-driver/mongo\" \"go.mongodb.org/mongo-driver/mongo/options\" ) func main() { clientOptions := options.Client().ApplyURI(\"mongodb://91.206.179.29:27017\") client, err := mongo.Connect(context.TODO(), clientOptions) if err != nil { log.Fatalf(\"Couldn't connect to MongoDB server: %v\", err) } fmt.Println(\"successfully connected to MongoDB!\") ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second) defer cancel() err = client.Ping(ctx, nil) if err != nil { log.Fatalf(\"Could not ping MongoDB server: %v\", err) } fmt.Println(\"Ping MongoDB server successfully!\") } You should see the message: successfully connected to MongoDB!Ping MongoDB server successfully MongoDB uses collections to store data. You can create collections using the .Collection function. Below, we will create a database called first-database and a collection called first-collection. The collection will have a new document, containing three keys: user-name, user-age, and user-email. collection := client.Database(\"first-database\").Collection(\"first-collection\") document := map[string]interface{}{ \"user-name\": \"Alice\", \"user-age\": 25, \"user-email\": \"alice@corporate.com\", } insertResult, err := collection.InsertOne(ctx, document) if err != nil { log.Fatalf(\"Couldn't insert new document: %v\", err) } fmt.Printf(\"Inserted new document with ID: %v\n\n\", insertResult.InsertedID) if err := client.Disconnect(ctx); err != nil { log.Fatalf(\"Could not disconnect from MongoDB: %v\", err) } fmt.Println(\"Disconnected from MongoDB!\") } If successful, you will see the Inserted new document message with the document ID. ClickHouse and Go To work with ClickHouse, use the clickhouse-go driver. Create a new directory to store the project files and navigate to it: clickhouse-connect && cd clickhouse-connect Create a go.mod file to store the dependencies: go mod init golang-connect-clickhouse Download the Clickhouse driver using the command: go get github.com/ClickHouse/clickhouse-go/v2 Create a new file named main.go, where you will specify the connection data to ClickHouse. package main import ( \"database/sql\" \"log\" \"github.com/ClickHouse/clickhouse-go/v2\" ) func main() { dsn := \"tcp://localhost:9000?username=user1&password=PasswordForuser175465&database=new_db\" db, err := sql.Open(\"clickhouse\", dsn) if err != nil { log.Fatal(err) } defer db.Close() if err := db.Ping(); err != nil { log.Fatal(err) } log.Println(\"Connected to ClickHouse!\") } Database Connection in JavaScript In JavaScript, all connections to external services are made using the Node.js platform. Make sure that you have Node.js and the npm package manager installed on your device. MySQL and JavaScript To work with MySQL, use the mysql2 driver. Create a directory where we will store the project files: mkdir js-mysql-connect && cd js-mysql-connect Initialize the project: npm init -y Install the mysql2 library: npm install mysql2 Use the following code to connect to MySQL: const mysql = require('mysql2'); const connection_to_mysql = mysql.createConnection({ host: 'localhost', user: 'root', password: 'PasswordForRoot74463', database: db1, }); connection_to_mysql.connect((err) => { if (err) { console.error('Error connecting to MySQL:', err.message); return; } console.log('Successfully connected to MySQL Server!'); connection_to_mysql.end((endErr) => { if (endErr) { console.error('Error closing the connection_to_mysql:', endErr.message); } else { console.log('Connection closed.'); } }); }); PostgreSQL and JavaScript Connecting to PostgreSQL is done using the pg library. Create a directory where we will store the project files: mkdir js-postgres-connect && cd js-postgres-connect Initialize the project: npm init -y Install the pg library: npm install pg To connect to PostgreSQL, first import the pg library. Then, create a constant where you specify variables for the database address, username, password, database name, and port. Use the new pg.Client class to pass the connection data. We will create a table called cities and add two records into it. To do this, we will use the queryDatabase function, which contains the SQL queries. const pg = require('pg'); const config = { postgresql_server_host: '91.206.179.29', postgresql_user: 'gen_user', postgresql_user_password: 'PasswordForGenUser56467$', postgresql_database_name: 'default_db', postgresql_database_port: 5432, }; const client = new pg.Client(config); client.connect(err => { if (err) throw err; else { queryDatabase(); } }); function queryDatabase() { const query = ` DROP TABLE IF EXISTS cities; CREATE TABLE cities (id serial PRIMARY KEY, name VARCHAR(80), population INTEGER); INSERT INTO cities (name, population) VALUES ('Berlin', 3645000); INSERT INTO cities (name, population) VALUES ('Paris', 2161000); `; client .query(query) .then(() => { console.log('Table created successfully!'); client.end(console.log('Closed client connection')); }) .catch(err => console.log(err)) .then(() => { console.log('Finished execution, exiting now'); process.exit(); }); } Use this command to run the code: node connect-to-postgres.js Redis and JavaScript To work with Redis, use the ioredis library. Create a directory to store the project files: mkdir js-redis-connect && cd js-redis-connect Initialize the project: npm init -y Install the ioredis library: npm install ioredis To connect to Redis, import the ioredis library. Then create a constant named redis and specify the Redis server address. Inserting data, i.e., creating key-value objects, is done using an asynchronous function named setData, which takes two values ‚Äî key and value, corresponding to the data format of the Redis system. const Redis = require('ioredis'); const redis = new Redis({ host: '91.206.179.29', port: 6379, password: 'UY+p8e?Kxmqqfa', }); async function setData(key, value) { try { await redis.set(key, value); console.log('Data successfully set'); } catch (error) { console.error('Error setting data:', error); } } async function getData(key) { try { const value = await redis.get(key); console.log('Data retrieved'); return value; } catch (error) { console.error('Error getting data:', error); } } (async () => { await redis.select(1); await setData('user', 'alex'); await getData('user'); redis.disconnect(); })(); Run: node connect-to-redis.js MongoDB and JavaScript To work with MongoDB, use the mongodb driver. Create a directory for storing the project files: mkdir js-mongodb-connect && cd js-mongodb-connect Initialize the project: npm init -y Install the mongodb library: npm install mongodb To connect to MongoDB, import the mongodb library. Specify the database address in the constant uri and pass the address into the MongoClient class. const { MongoClient } = require('mongodb'); const uri = \"mongodb://91.206.179.29:27017\"; const client = new MongoClient(uri, { useNewUrlParser: true, useUnifiedTopology: true }); async function connectToDatabase() { try { await client.connect(); console.log(\"Successfully connected to MongoDB!\"); const database = client.db(\"myDatabase\"); const collection = database.collection(\"myCollection\"); const documents = await collection.find({}).toArray(); console.log(\"Documents found:\", documents); } catch (error) { console.error(\"Error connecting to MongoDB:\", error); } finally { await client.close(); console.log(\"Connection closed.\"); } } connectToDatabase(); ClickHouse and JavaScript To work with ClickHouse, use the clickhouse/client driver. Create a directory where we will store the project files: mkdir js-clickhouse-connect && cd js-clickhouse-connect Initialize the project: npm init -y Install the @clickhouse/client library: npm install @clickhouse/client To connect to ClickHouse, use the code below where we set the connection details and execute a simple SQL query that will return the first 10 records from the system table named system.tables: const { ClickHouse } = require('@clickhouse/client'); const client = new ClickHouse({ host: 'http://localhost:8123', username: 'default', password: 'PasswordforDefaultUser45435', database: 'default', }); async function connectAndQuery() { try { console.log('Successfully connected to ClickHouse Server!'); const rows = await client.query({ query: 'SELECT * FROM system.tables LIMIT 10', format: 'JSON', }).then((result) => result.json()); console.log('Query results:', rows); } catch (error) { console.error('Error Successfully connected to ClickHouse Server! or running the query:', error); } finally { console.log('Done.'); } } connectAndQuery(); Conclusion In today's article, we thoroughly explored how to connect to PostgreSQL, Redis, MongoDB, MySQL, and ClickHouse databases using Python, Go, and JavaScript. These languages can be used to create both web applications and microservices that utilize databases in their operation."
    },
    {
        "link": "https://medium.com/@euricopaes/store-realtime-data-into-sqlite3-using-python-asyncio-and-binance-websocket-c2ea8d3f11f8",
        "document": "In this article, we‚Äôre going to go through how you can store real-time data into your Sqlite3 Database with the use of Python, asyncio and aiosqlite libraries and Binance WebSockets.\n\nWe‚Äôre using a direct WebSocket connection to Binance, we will request the aggregated trades for Bitcoin USDT from Binance. The endpoint is:\n\nWe will use the aggregated trade stream, because there‚Äôll be fewer updates and therefore it‚Äôs easier to process the data. So we‚Äôre taking in this data in real-time from a WebSocket, and we‚Äôre saving that into a SQLite database.\n\nYou‚Äôll want to install a asynchronous version of the SQLite module, so pip install aiosqlite.\n\nWe‚Äôll get started and import some modules. We‚Äôll start with asyncio, which is going to be important for allowing us to use the WebSocket.\n\nWe‚Äôll also import sys, sqlite3, this is the built-in SQLite Python module, and we‚Äôll also import our asynchronous SQLite module, as well as JSON, because the type of data that we get back from the Binance WebSocket is in JSON format.\n\nSo connection is equal to sqlite3.connect, and then we just insert the name of the database we want. That‚Äôs just going to be a path to the file. So the dot here just stands for the current directory.\n\nIf that doesn‚Äôt work, you can always type in a full path here to the database directory that you want to save it in. Once we‚Äôve got that going, we can create a cursor here.\n\nSo cursor is equal to conn.cursor. This is going to allow us to execute our commands.\n\nSo the first command that I‚Äôm going to execute here in our cursor, is I‚Äôm going to tell it if we have any trades already stored into the trades table. If yes then I want it to just delete the trades table and start fresh.\n\nThis means that that file that we have on disk doesn‚Äôt get too big.\n\nSo we‚Äôll just get rid of this table if it happens to exist, and now that we‚Äôve gotten rid of it, we want to recreate it. So cursor.execute, we want to make our new trades table here. We‚Äôll do the three quotes here, so that we can have this over multiple lines.\n\nWhat we are going to do is, save the unique ID, that‚Äôll be really helpful in making sure that we actually get every single trade, because they‚Äôre ID‚Äôd sequentially, and so if we have 100 trades, and they‚Äôre all sequentially labeled, we know that we‚Äôve actually got all of the trades, and then we haven‚Äôt lost any of them.\n\nWe‚Äôll save the price, because we need that for our ticker. We‚Äôll save the quantity, that could be useful if we wanted to do some metrics around how much volume was traded in the last amount of time, and we‚Äôll also save the time here.\n\nSo that‚Äôs the schema for our table, and that‚Äôs what it‚Äôs going to look like on disk.\n\nWe are also going to create an index here.\n\nWhat this is going to do is it‚Äôs going to create an index on this time column that we have. It‚Äôs going to be called index time. That allows us to query much faster on this column.\n\nAfter that, we‚Äôll do cursor.commit, make sure everything‚Äôs committed to the database, everything‚Äôs finalized, and then we‚Äôll do connection.close here.\n\nAt this point if you don‚Äôt want to use an SQL lite database like the one we‚Äôre using here, because it‚Äôs not suitable for your purposes, you can follow pretty much the exact steps I‚Äôm showing here with something like a timescale DB or a Postgres DB. Either one of those systems will work just fine for what we‚Äôre doing here.\n\nIt‚Äôll be pretty much exactly the same commands. The reason why we‚Äôre using SQL lite is that you can easily create and delete full databases just using Python.\n\nWhereas something like Postgres is a little bit harder to set up and requires specific instructions for each operating system and setup. So that‚Äôs why we‚Äôre only covering SQL lite. If you have a significantly higher volume of data, or you need to do more queries per second, then I‚Äôd highly recommend setting up something like a timescale DB.\n\nNow we actually need to populate it with data. And the way we‚Äôre going to do that is using async io. We‚Äôre going to create a asynchronous function.\n\nAnd we‚Äôll call it save_data. And it‚Äôs going to take a URL. And that URL is going to be the web socket connection.\n\nThen let‚Äôs open a connection to that web socket inside this function,we‚Äôll use the context manager that comes with the WebSockets library.\n\nSo this context manager gives us a really easy way to receive information from our WebSocket.\n\nWe then create a variable called data, that will receive the data from the Websocket. We will use await keyword before websocket.recv() in order to let Python know that it can go and do other tasks while it‚Äôs waiting for a message to be received, which includes things like saving down data to the database.\n\nTo run our codewith async IO function like this, the easiest way to do it, if you‚Äôve just got one to run, is to do asyncio.run() , insert the name of the functions and any parameters (I‚Äôll pass in the URL here.)\n\nWe‚Äôll let that run and we should get at least one message here.\n\nThis is one individual Bitcoin USDT trade that‚Äôs happened. And then the WebSocket closes and the function stops running.\n\nWe only awaited the reception of just one piece of data here from the WebSocket. If we want to get all of the trades, we‚Äôll have to repeatedly do this. Now, an easy way to do that in Python is just to add a while true here.\n\nSo essentially forever, we‚Äôre going to stay in this loop. We‚Äôll want to wait any incoming data. And then after we get that data, we want to print it out.\n\nSo let‚Äôs see what happens here.\n\nThere we go. You can see we‚Äôve got pretty much like hundreds and hundreds of trades coming in here.\n\nSo now we‚Äôve got the data, we want to save it to our database here.\n\nSo let‚Äôs have a check on the type of data that we‚Äôre getting here. So print type data. I‚Äôll just put a break here so it‚Äôll get one piece of data and then it‚Äôll break.\n\nYou can see that this whole thing is actually a string apparently. So we need to convert that into a dictionary. It very much looks like a Python dictionary, but it has a string currently.\n\nSo let‚Äôs do something like data is equal to json.loads() and then pass in data, that‚Äôll convert it back into a dictionary here.\n\nSo if I let it run again, we can see this is now a dictionary and therefore we can key in with any of these particular variables here.\n\nNow we‚Äôre going to create a list called buffer and it‚Äôs going to be an empty list. And the reason I‚Äôm doing that is because I want to save to our SQLite database multiple trades at the same time.\n\nWe don‚Äôt want to be opening and closing a connection with the database that many times a minute. That‚Äôs a lot of overhead and it‚Äôs likely to cause problems with the database, especially with SQLite. It does not handle concurrency quite as well as timescale or Postgres.\n\nAnd so to reduce the amount of times we‚Äôre opening and closing that connection every time we receive a trade, I‚Äôm going to add it to this buffer. And then when we have more than 10 trades saved up (for example), we then save them all at once. That way we‚Äôre only opening the connection a few hundred times a minute which is still a lot but it‚Äôs a lot better than a few thousand times a minute.\n\nOn this trades buffer I‚Äôm going to append the data that we want to keep.\n\nSo what information do I want to save in this list?\n\nWe want the id , the time of the trade, the price and the quantity.\n\nSo that‚Äôs all I‚Äôm doing here. We‚Äôre taking the id (‚Äòa‚Äô,) the time of the trade (‚ÄòT‚Äô), the quantity (‚Äòq‚Äô) and finally the price (‚Äòp‚Äô) and I‚Äôm appending that as a tuple into this buffer.\n\nSo that‚Äôs great. We‚Äôve got our buffer here. And then if the number of trades in the trades buffer is greater than 10, well now want to get down to business and we want to actually save this down to our database.\n\nThe way we do that is with yet another context manager here.\n\nWe will use async with aiosqlite.connect then the name of the database.\n\nAgain this context manager, much like the one with the web socket over here, is going to handle connecting, disconnecting, making sure nothing goes horribly wrong. It just saves us having to manually open and close these connections and handle a lot of exceptions ourselves. Now saving this information is honestly pretty simple.\n\nAgain we will use await . ‚ÄúThe await keyword is used to signify that the coroutine should pause execution until the awaited task completes. During this pause, the event loop can switch to executing other tasks efficiently using system resources. Once the awaited task is complete, the coroutine resumes from where it left off.‚Äù\n\nWe‚Äôre going to be inserting multiple rows at once.\n\nThe query is going to be insert into the name of your table (trades).\n\nThen we write the name of the columns in the order that we want. The order will be: id, time, quantity, and price.\n\nAfter the columns names we will tell SQLite here you want to insert the VALUES for those columns using a question mark here. 4 question marks for 4 columns.\n\nClose out this triple string here and pass in the buffer. So that‚Äôs how it‚Äôs going to extract each one of these items from these tuples and insert them.\n\nAfter that we‚Äôll await a db.commit. So we‚Äôll make sure that all of this information is being committed to the database.\n\nAnd that‚Äôs us pretty much done here. I will wipe the trades buffer so we reset that back to an empty list after we‚Äôve done the commit here.\n\nNow you are able to store real time market data into a sqlite database and further use it for your projects.\n\nHere is a link to get the full code."
    },
    {
        "link": "https://en.ittrip.xyz/python/async-sqlite-python",
        "document": "Python‚Äôs simplicity and readability have made it a preferred language for both beginners and seasoned developers, particularly for database operations. SQLite, a lightweight yet powerful database engine, is commonly integrated with Python for local data storage and manipulation. However, as applications grow in complexity, the need to manage database operations without blocking the main thread becomes evident. This is where asynchronous programming steps in to enhance performance and user experience. In this article, we delve deep into the realm of Python‚Äôs asynchronous capabilities with SQLite, providing you with the knowledge to implement non-blocking database interactions in your next Python project.\n\nAsynchronous operations are essential in modern application development, enabling the program to continue running while waiting for IO-bound tasks, such as database operations, to complete. This non-blocking behavior is particularly crucial for applications that require high concurrency or need to maintain responsiveness under load.\n\nPython‚Äôs asynchronous programming capabilities are built on the foundations of coroutines, event loops, and the ‚Äòasync‚Äô and ‚Äòawait‚Äô syntax introduced in Python 3.5. These features allow developers to write code that can perform multiple operations in the background while the main program thread executes other tasks.\n\nThe Role of Asyncio in Asynchronous Operations\n\nAsyncio is Python‚Äôs standard library for writing concurrent code using the async/await syntax. It provides the framework for creating and managing event loops, which are critical for handling asynchronous tasks.\n\nThe integration of SQLite with asyncio in Python applications involves several steps and considerations to ensure smooth and efficient operation.\n\nBefore diving into SQLite operations, it is important to set up an environment that supports asynchronous execution. This often involves creating an event loop and establishing a database connection using an asynchronous SQLite library.\n\nNot all SQLite libraries support asynchronous operations. Libraries like `aiosqlite` are designed to work seamlessly with Python‚Äôs async/await features. When selecting a library, ensure it is compatible with asyncio and provides non-blocking database operations.\n\nOnce you have an asynchronous SQLite library in place, you can start executing database operations without blocking the main thread. This includes creating tables, inserting data, and querying the database asynchronously.\n\nCRUD operations‚ÄîCreate, Read, Update, Delete‚Äîcan be performed using the async/await syntax. Here‚Äôs an example of how to perform an asynchronous query:\n\nBest Practices for Asynchronous SQLite in Python\n\nTo effectively use SQLite in an asynchronous context, follow these best practices:\n\n‚Äì Use connection pools to manage database connections efficiently.\n\n ‚Äì Handle exceptions properly to avoid crashing your application.\n\n ‚Äì Test asynchronous operations thoroughly to ensure they work as expected.\n\nResource management is critical in asynchronous programming. Ensure that all database connections are properly opened and closed, and that resources are cleaned up after use.\n\nContext managers are a great way to manage resources with minimal code. They automatically close connections and clean up resources, even if an error occurs during an operation.\n\nIncorporating asynchronous SQLite operations into a Python project can significantly improve its performance and responsiveness. By understanding the principles of asynchronous programming and following best practices, you can unlock the full potential of your applications. Remember to select the right tools and libraries, manage your resources carefully, and test your code to ensure reliable and efficient database interactions."
    },
    {
        "link": "https://blog.amber.org/blog/2022/03/python-library-aiosql",
        "document": "While it‚Äôs de rigueur in most languages to use an object-relational mapper (ORM) to translate between your internal representation (typically object-oriented) and the relational model, I find that you end up with a bit of the ‚Äúworst of both worlds‚Äù in the end. This article isn‚Äôt attempting to dig into all the challenges with ORMs. Instead, I want to look at a Python library for an alternative pattern.\n\nIn the olden times, when I was still writing Clojure code like the cool kids, I was a big fan of Kris Jenkins‚Äô library. The basic idea was that you wrote SQL in SQL to query and manipulate your RDBMS, and then you would get back very basic Clojure types to work with. A while back, though, I stumbled over Will Vaughn‚Äôs , which does something of the same but for Python.\n\nWhile can operate in either syncronous or asynchronous code, it‚Äôs much easier to explain without the bits of Python async detritus in the middle. For this exploration, we‚Äôre going to just use a simple SQLite database as the backend. I‚Äôm also going to skip over whatever machinations you use to install 3rd party packages. My personal preference is for PDM .\n\nFirst, we need to import a few things:\n\nThen, we can build a bunch of queries from SQL. This is where differentiates itself from the rest of the world. You write these in normal SQL, with access to all the crazy capabilities innate in the language. For now, we‚Äôll just use a single triple-quoted string, but you can load from file(s) just as easily:\n\nNow we have an object, , that contains a set of functions we can use. The string tells the library what dialect you‚Äôre going to be using. Currently, it suppoprts a couple sync and async libraries for SQLite and PostgreSQL, but really, what other database do you need?\n\nBut let‚Äôs talk about a few things that might look a bit different from just ‚Äúpure‚Äù SQL. First, we use SQL comments ( ) to annotate our queries. This isn‚Äôt just good practice, it‚Äôs necessary for the library to parse them into functions. Specifically, it looks at the comment to identify the function. This is anything that‚Äôs a valid Python identifier . But wait, you might be saying, there‚Äôs some crazy characters there at the end on some of them. These are query type annotations:\n‚Ä¢ : This is a script, and it really only has a sense of ‚Äúdone‚Äù. This is typically used for things like DDL.\n‚Ä¢ : The statement is DDL/DML, and will make changes, but won‚Äôt return any results.\n‚Ä¢ : The statement will return a single value. This means that what you‚Äôll get back will be a single row and it won‚Äôt be contained in Python list.\n‚Ä¢ : Now we‚Äôre getting a bit Perly with our syntax, but in short it is used when you are doing an insert/update and want. The details are a bit more complicated, but basically it lets you return things from a query, like the primary key/row id of the inserted row, or something from a clause in PostgreSQL for example.\n\nMoving on, we also will need a SQLite database for it to work on:\n\nSo, now we have two pieces: a set of queries, and a connection handle to a database. Let‚Äôs get started. First, let‚Äôs create the test database schema:\n\nNow the table exists, and we can insert some sample data into it:\n\nNow, we have a little sample data. We can see it here:\n\nYou can see that what‚Äôs returned from the database is just basic Python data structures. A tuple for each row, and a all of the rows wrapped in a list. You can, however, leverage a capability in the driver to get you something a bit smarter:\n\nNow we can be a little more elegant when we‚Äôre asking the returned rows:\n\nObviously, in most cases, you‚Äôre writing SQL that takes input. So, in order to get input, you need to insert variables. You can see those in and in the form of . These get passed in as arguments to the query function call. For example:\n\nSo far, I think these are all super simple, but what if you want to do something a bit more complicated with SQL? Well, let‚Äôs take the chinook database and play with it. Let‚Äôs say we want to see who all the music listeners of a specific genre are. This is the SQL we might use:\n\nWe can simply replace the with , and then:\n\nNow, I realize that for many developers, SQL isn‚Äôt the most comfortable syntax, but for more complex database schemas, and especially for legacy databases that were designed for a totally different model, being able to work and access 100% of the richness of SQL is exceptionally powerful. It may not be the perfect answer, but it‚Äôs one to keep in your toolbelt.\n\nFinally, if you want a bit more complicated version, you can take a look at this gist I wrote a while back. It combined not just , but also the brilliant module and :"
    },
    {
        "link": "https://sqldocs.org/sqlite-database/aiosqlite-python",
        "document": "SQLite is one of the most widely used database engines in the world. Its lightweight, file-based architecture makes it perfect for embedded databases in desktop and mobile applications. However, Python‚Äôs standard SQLite library blocks the event loop while executing queries, hurting performance in asynchronous programs.\n\nEnter AioSQLite ‚Äì an elegant asynchronous library interacting with SQLite from Python‚Äôs asyncio event loop without blocking. Let‚Äôs learn how AioSQLite makes SQLite fully non-blocking and explore some real-world use cases through examples.\n\nAioSQLite is an asynchronous, non-blocking SQLite driver for Python. It allows you to access SQLite databases from asyncio coroutines without blocking the main event loop thread.\n\nHere are some key things to know about AioSQLite:\n‚Ä¢ It provides an async version of the SQLite Python API, replicating nearly all the features of\n‚Ä¢ Uses a background thread per connection to talk to SQLite without blocking asyncio event loop\n‚Ä¢ Lets you access SQLite using async/await instead of callbacks\n\nInstalling AioSQLite is as simple as:\n\nThat‚Äôs it! Now, let‚Äôs look at how to use it.\n\nThe API for AioSQLite mirrors , but with async methods instead of synchronous ones.\n\nFor example, here is how to connect to an SQLite database file, insert data, and query it:\n\nThere are a few key things to notice here:\n‚Ä¢ Executing SQL happens via , using\n‚Ä¢ Getting query results uses an async cursor that returns rows via\n\nThis allows the database code to run non-blockingly within an asyncio event loop.\n\nA key feature of AioSQLite is the use of async context managers to manage connections and cursors automatically:\n\nThis takes care of opening and closing connections for you.\n\nAioSQLite replicates most other advanced features of too:\n\nThe API supports nearly everything you can do in standard SQLite.\n\nHere is an example of using AioSQLite with a real-world data analytics database containing user analytics events:\n\nThis allows storing analytic event data from users in SQLite by leveraging AioSQLite‚Äôs non-blocking API!\n\nHere are some of the major benefits you get from using AioSQLite:\n‚Ä¢ Avoid callback hell by using instead\n‚Ä¢ Replicates nearly all functionality of module\n‚Ä¢ Enables real-time analytics, streaming, and more use cases by removing SQLite blocking behavior\n\nIf you need to use SQLite from an asyncio app, AioSQLite is currently the best way to go about it while avoiding bottlenecks.\n\nAs you can see, AioSQLite retains nearly all the underlying SQLite functionality while adding async capabilities.\n\nAioSQLite brings the simplicity and ubiquity of SQLite to the world of asyncio and Python async programming. With its async API using , context managers, non-blocking behavior, and Python 3.7+ support, AioSQLite is the go-to choice for accessing SQLite from an asyncio application.\n\nWhether you need real-time analytics, streaming processing, or just a simple offline data store, AioSQLite takes the blocking factor out of SQLite. This lets you build highly concurrent database-backed programs without the headaches of callback hell or stalling the event loop.\n\nIf you found this useful, be sure to check out the AioSQLite documentation to learn more!"
    },
    {
        "link": "https://stackoverflow.com/questions/73416313/whats-making-this-code-lock-my-aiosqlite-database",
        "document": "My discord.py bot handles data inputs (invites, bans, warns, etc.) and inserts them into an aiosqlite database. An error is raised:\n\nMultiple commands work in unison to handle warnings and such. When I invoke my warning system, it locks my database:"
    }
]