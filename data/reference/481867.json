[
    {
        "link": "https://stackoverflow.com/questions/60950050/how-to-add-multiple-applications-to-different-ports-using-nginx",
        "document": "I want to deploy a web app (React) and a mobile app (Ionic React) on the same domain with different ports (using nginx). When I run both applications should start running on different ports.\n\nI'm currently running my web app (localhost:80) and rest api (localhost:80/api) with success.\n\nBut I 'cant figure out how to connect the mobile application to port 81 with nginx. I'm kinda new to this.\n\nMy file currently looks like this:\n\nWhen I request and everything works.\n\nMy browser doesn't find anything when I request"
    },
    {
        "link": "https://medium.com/@mrsid96/deploy-multiple-vue-applications-under-a-single-domain-with-nginx-8c66f8281269",
        "document": "We will be seeing how we deploy multiple VueJs applications under a single domain/ hostname. For simplicity, we will be taking here two VueJs applications, vueApp1, and vueApp2. Then we have a single domain, having the URL like ‚Äòhttps://my-awesome-url.com‚Äô. Our end goal will be something like achieving these\n\nFor both the Vue applications, we need to configure two things, firstly the ‚Äòvue.config.js‚Äô and secondly the ‚Äòvue-router‚Äô.\n\nLet‚Äôs being from the vueApp1\n\nIn your project root directory, check for ‚Äòvue.config.js‚Äô. If the same file doesn‚Äôt exist, we can manually create one. Now let us add the below in the same file\n\nSo, now we are mentioning webpack, to use the ‚ÄòpublicPath‚Äô which again acts as a virtual directory, while building all the assets. The value of publicPath is the path, where you want to render your applications. Here, for our case, it‚Äôs app1.\n\nNow, let‚Äôs tell the vue-router to use the ‚Äòbase‚Äô path for all the routes to render. To that, navigate where you have added your vue-router, then add the ‚Äòbase‚Äô parameter as per the below configuration\n\nAwesome, we are done with vueApp1. Now you can repeat the same with vueApp2, and once done, let‚Äôs take our leap towards the Nginx configuration\n\nIn our Nginx configuration, we will mention our location blocks for both vue applications. We will be using alias under each location block. For reference, below is a sample configuration.\n\nNow as we are done with everything, restart your Nginx and give it a try. You will be able to access both the Vue applications under https://my-awesome-url.com/app1 and https://my-awesome-url.com/app2 respectively. In case you faced any issues, drop them down.\n\nAppreciated this article?\n\nWhy not give me some claps üòâ"
    },
    {
        "link": "https://stackoverflow.com/questions/50810022/cant-get-two-single-page-applications-to-run-together-on-one-server-using-nginx",
        "document": "UPDATE: I have edited this post to provide a clearer understanding of the issue. The previous post has been overwritten by this update.\n\nWe have two single page applications that need to be accessible through the same domain and ports but at different locations.\n\nApplication1 is a public user facing application that should be loaded when visiting https://example.com.\n\nApplication2 is a public admin facing application that will require authentication and should be loaded instead of application1 if they visit https://example.com/admin.\n\nCurrently I have no problem loading the first application, however, I have tried all sorts of combinations with my nginx conf file to get the second application to load when visiting https://example.com/admin without success.\n\nIt is always loading the application1 app instead.\n\nThis is the example.com.conf file. I have tried all sorts of combinations but this is me trying to keep it very simple.\n\nI have spent days on this, can't believe something that should be so simple has been holding me up this long."
    },
    {
        "link": "https://cli.vuejs.org/guide/deployment",
        "document": "If you are using Vue CLI along with a backend framework that handles static assets as part of its deployment, all you need to do is make sure Vue CLI generates the built files in the correct location, and then follow the deployment instruction of your backend framework.\n\nIf you are developing your frontend app separately from your backend - i.e. your backend exposes an API for your frontend to talk to, then your frontend is essentially a purely static app. You can deploy the built content in the directory to any static file server, but make sure to set the correct publicPath.\n\nThe directory is meant to be served by an HTTP server (unless you've configured to be a relative value), so it will not work if you open directly over protocol. The easiest way to preview your production build locally is using a Node.js static file server, for example serve:\n\nIf you are using Vue Router in mode, a simple static file server will fail. For example, if you used Vue Router with a route for , the dev server has been configured to respond to properly, but a simple static server serving a production build will respond with a 404 instead.\n\nTo fix that, you will need to configure your production server to fallback to for any requests that do not match a static file. The Vue Router docs provide configuration instructions for common server setups.\n\nIf your static frontend is deployed to a different domain from your backend API, you will need to properly configure CORS.\n\nIf you are using the PWA plugin, your app must be served over HTTPS so that Service Worker can be properly registered.\n‚Ä¢ None If you are deploying to or to a custom domain, you can omit as it defaults to . If you are deploying to , (i.e. your repository is at ), set to . For example, if your repo name is \"my-project\", your should look like this:\n‚Ä¢ None Inside your project, create with the following content (with highlighted lines uncommented appropriately) and run it to deploy:\n‚Ä¢ None Set correct in as explained above.\n‚Ä¢ None Grant the Travis job access to your repository: ( is the personal access token from step 3.)\n‚Ä¢ None Create a file in the root of your project.\n‚Ä¢ None Push the file to your repository to trigger the first build.\n\nAs described by GitLab Pages documentation, everything happens with a file placed in the root of your repository. This working example will get you started:\n\nTypically, your static website will be hosted on https://yourUserName.gitlab.io/yourProjectName, so you will also want to create an initial file to update the value to match your project name (the environment variable contains this value):\n\nPlease read through the docs on GitLab Pages domains for more info about the URL where your project website will be hosted. Be aware you can also use a custom domain.\n\nCommit both the and files before pushing to your repository. A GitLab CI pipeline will be triggered: when successful, visit your project's to see your website link, and click on it.\n‚Ä¢ None On Netlify, setup up a new project from GitHub with the following settings:\n\nIn order to receive direct hits using on Vue Router, you need to redirect all traffic to the file.\n\nCreate a file called in the root of your repository with the following content:\n\nCreate a file called under with the following content:\n\nIf you are using @vue/cli-plugin-pwa make sure to exclude the file from being cached by the service worker. To do so, add the following to your :\n\nCheckout workboxOptions and exclude for more.\n\nRender offers free static site hosting with fully managed SSL, a global CDN and continuous auto deploys from GitHub.\n‚Ä¢ None Create a new Static Site on Render, and give Render‚Äôs GitHub app permission to access your Vue repo.\n‚Ä¢ None Use the following values during creation:\n\nThat‚Äôs it! Your app will be live on your Render URL as soon as the build finishes.\n\nIn order to receive direct hits using history mode on Vue Router, you need to add the following rewrite rule in the tab for your site.\n\nLearn more about setting up redirects, rewrites and custom domains on Render.\n\nCreate a new Firebase project on your Firebase console. Please refer to this documentation on how to setup your project.\n\nMake sure you have installed firebase-tools globally:\n\nFrom the root of your project, initialize using the command:\n\nFirebase will ask some questions on how to setup your project.\n‚Ä¢ Choose which Firebase CLI features you want to setup your project. Make sure to select .\n‚Ä¢ Select the default Firebase project for your project.\n‚Ä¢ Set your directory to (or where your build's output is) which will be uploaded to Firebase Hosting.\n‚Ä¢ Select to configure your project as a single-page app. This will create an and on your folder and configure your information.\n\nTo deploy your project on Firebase Hosting, run the command:\n\nIf you want other Firebase CLI features you use on your project to be deployed, run without the option.\n\nYou can now access your project on or .\n\nPlease refer to the Firebase Documentation for more details.\n\nVercel is a cloud platform that enables developers to host Jamstack websites and web services that deploy instantly, scale automatically, and requires no supervision, all with zero configuration. They provide a global edge network, SSL encryption, asset compression, cache invalidation, and more.\n\nTo deploy your Vue project with a Vercel for Git Integration, make sure it has been pushed to a Git repository.\n\nImport the project into Vercel using the Import Flow. During the import, you will find all relevant options preconfigured for you with the ability to change as needed.\n\nAfter your project has been imported, all subsequent pushes to branches will generate Preview Deployments, and all changes made to the Production Branch (commonly \"master\" or \"main\") will result in a Production Deployment.\n\nOnce deployed, you will get a URL to see your app live, such as the following: https://vue-example-tawny.vercel.app/.\n\nIf you want to use a Custom Domain with your Vercel deployment, you can Add or Transfer in your domain via your Vercel account Domain settings.\n\nTo add your domain to your project, navigate to your Project from the Vercel Dashboard. Once you have selected your project, click on the \"Settings\" tab, then select the Domains menu item. From your projects Domain page, enter the domain you wish to add to your project.\n\nOnce the domain has been added, you will be presented with different methods for configuring it.\n\nYou can deploy a fresh Vue project, with a Git repository set up for you, with the following Deploy Button:\n\nMore info: Getting started with SPAs on Heroku\n\nTo deploy with Surge the steps are very straightforward.\n\nFirst, you would need to build your project by running . And if you haven't installed Surge's command line tool, you can simply do so by running the command:\n\nThen cd into the folder of your project and then run and follow the screen prompt. It will ask you to set up email and password if it is the first time you are using Surge. Confirm the project folder and type in your preferred domain and watch your project being deployed such as below.\n\nVerify your project is successfully published by Surge by visiting , vola! For more setup details such as custom domains, you can visit Surge's help page.\n‚Ä¢ None As described in the Bitbucket documentation you need to create a repository named exactly .\n‚Ä¢ None It is possible to publish to a subfolder of the main repository, for instance if you want to have multiple websites. In that case, set correct in . If you are deploying to , you can omit as it defaults to . If you are deploying to , set to . In this case, the directory structure of the repository should reflect the url structure, for instance, the repository should have a directory.\n‚Ä¢ None Inside your project, create with the following content and run it to deploy:\n\nDeploy your application using nginx inside of a docker container.\n‚Ä¢ None Create a file in the root of your project.\n‚Ä¢ None Create a file in the root of your project Setting up the file prevents and any intermediate build artifacts from being copied to the image which can cause issues during building.\n‚Ä¢ None Create a file in the root of your project is an HTTP(s) server that will run in your docker container. It uses a configuration file to determine how to serve content/which ports to listen on/etc. See the nginx configuration documentation for an example of all of the possible configuration options. The following is a simple configuration that serves your vue project on port . The root is served for / errors which allows us to use based routing.\n‚Ä¢ None This build is based on the official image so log redirection has already been set up and self daemonizing has been turned off. Some other default settings have been setup to improve running nginx in a docker container. See the nginx docker repo for more info."
    },
    {
        "link": "https://reddit.com/r/vuejs/comments/160d60b/couldnt_serve_two_vue_applications_in_a_single",
        "document": "I'm trying to set up an Nginx Docker container that serves multiple Vue.js applications. One of these applications is a login app, and the other is the main app. I want to route requests to the /login endpoint to the login app and all other requests to the main app.\n\nI have tried various Nginx configurations, but the login app is not working as expected. When accessing the /login endpoint, it's not rendering correctly, and resources such as JavaScript files are not loading properly.\n\nBoth Apps in the Same Container:\n\nBoth the login app and the main app are in the same Docker container.\n\nBoth apps are built and located in /usr/share/nginx/html and /usr/share/nginx/html/login, respectively.\n\nThe Vue Router in the main app uses hash mode.\n\nWhat could be causing the login app to not render correctly when accessed through the /login endpoint?\n\nTried with url rewriting approach by adding this line in the /login location block."
    },
    {
        "link": "https://docs.nginx.com/nginx/admin-guide/web-server/reverse-proxy",
        "document": "This article describes the basic configuration of a proxy server. You will learn how to pass a request from NGINX to proxied servers over different protocols, modify client request headers that are sent to the proxied server, and configure buffering of responses coming from the proxied servers.\n\nProxying is typically used to distribute the load among several servers, seamlessly show content from different websites, or pass requests for processing to application servers over protocols other than HTTP.\n\nWhen NGINX proxies a request, it sends the request to a specified proxied server, fetches the response, and sends it back to the client. It is possible to proxy requests to an HTTP server (another NGINX server or any other server) or a non-HTTP server (which can run an application developed with a specific framework, such as PHP or Python) using a specified protocol. Supported protocols include FastCGI, uwsgi, SCGI, and memcached.\n\nTo pass a request to an HTTP proxied server, the proxy_pass directive is specified inside a location. For example:\n\nThis example configuration results in passing all requests processed in this location to the proxied server at the specified address. This address can be specified as a domain name or an IP address. The address may also include a port:\n\nNote that in the first example above, the address of the proxied server is followed by a URI, . If the URI is specified along with the address, it replaces the part of the request URI that matches the location parameter. For example, here the request with the URI will be proxied to . If the address is specified without a URI, or it is not possible to determine the part of URI to be replaced, the full request URI is passed (possibly, modified).\n\nTo pass a request to a non-HTTP proxied server, the appropriate directive should be used:\n\nNote that in these cases, the rules for specifying addresses may be different. You may also need to pass additional parameters to the server (see the reference documentation for more detail).\n\nThe proxy_pass directive can also point to a named group of servers. In this case, requests are distributed among the servers in the group according to the specified method.\n\nBy default, NGINX redefines two header fields in proxied requests, ‚ÄúHost‚Äù and ‚ÄúConnection‚Äù, and eliminates the header fields whose values are empty strings. ‚ÄúHost‚Äù is set to the variable, and ‚ÄúConnection‚Äù is set to .\n\nTo change these setting, as well as modify other header fields, use the proxy_set_header directive. This directive can be specified in a location or higher. It can also be specified in a particular server context or in the http block. For example:\n\nIn this configuration the ‚ÄúHost‚Äù field is set to the $host variable.\n\nTo prevent a header field from being passed to the proxied server, set it to an empty string as follows:\n\nBy default NGINX buffers responses from proxied servers. A response is stored in the internal buffers and is not sent to the client until the whole response is received. Buffering helps to optimize performance with slow clients, which can waste proxied server time if the response is passed from NGINX to the client synchronously. However, when buffering is enabled NGINX allows the proxied server to process responses quickly, while NGINX stores the responses for as much time as the clients need to download them.\n\nThe directive that is responsible for enabling and disabling buffering is proxy_buffering. By default it is set to and buffering is enabled.\n\nThe proxy_buffers directive controls the size and the number of buffers allocated for a request. The first part of the response from a proxied server is stored in a separate buffer, the size of which is set with the proxy_buffer_size directive. This part usually contains a comparatively small response header and can be made smaller than the buffers for the rest of the response.\n\nIn the following example, the default number of buffers is increased and the size of the buffer for the first portion of the response is made smaller than the default.\n\nIf buffering is disabled, the response is sent to the client synchronously while it is receiving it from the proxied server. This behavior may be desirable for fast interactive clients that need to start receiving the response as soon as possible.\n\nTo disable buffering in a specific location, place the proxy_buffering directive in the location with the parameter, as follows:\n\nIn this case NGINX uses only the buffer configured by proxy_buffer_size to store the current part of a response.\n\nA common use of a reverse proxy is to provide load balancing. Learn how to improve power, performance, and focus on your apps with rapid deployment in the free Five Reasons to Choose a Software Load Balancer ebook.\n\nIf your proxy server has several network interfaces, sometimes you might need to choose a particular source IP address for connecting to a proxied server or an upstream. This may be useful if a proxied server behind NGINX is configured to accept connections from particular IP networks or IP address ranges.\n\nSpecify the proxy_bind directive and the IP address of the necessary network interface:\n\nThe IP address can be also specified with a variable. For example, the variable passes the IP address of the network interface that accepted the request:"
    },
    {
        "link": "http://nginx.org/en/docs/beginners_guide.html",
        "document": "This guide gives a basic introduction to nginx and describes some simple tasks that can be done with it. It is supposed that nginx is already installed on the reader‚Äôs machine. If it is not, see the Installing nginx page. This guide describes how to start and stop nginx, and reload its configuration, explains the structure of the configuration file and describes how to set up nginx to serve out static content, how to configure nginx as a proxy server, and how to connect it with a FastCGI application.\n\nnginx has one master process and several worker processes. The main purpose of the master process is to read and evaluate configuration, and maintain worker processes. Worker processes do actual processing of requests. nginx employs event-based model and OS-dependent mechanisms to efficiently distribute requests among worker processes. The number of worker processes is defined in the configuration file and may be fixed for a given configuration or automatically adjusted to the number of available CPU cores (see worker_processes).\n\nThe way nginx and its modules work is determined in the configuration file. By default, the configuration file is named and placed in the directory , , or .\n\nChanges made in the configuration file will not be applied until the command to reload configuration is sent to nginx or it is restarted. To reload configuration, execute:\n\nOnce the master process receives the signal to reload configuration, it checks the syntax validity of the new configuration file and tries to apply the configuration provided in it. If this is a success, the master process starts new worker processes and sends messages to old worker processes, requesting them to shut down. Otherwise, the master process rolls back the changes and continues to work with the old configuration. Old worker processes, receiving a command to shut down, stop accepting new connections and continue to service current requests until all such requests are serviced. After that, the old worker processes exit.\n\nA signal may also be sent to nginx processes with the help of Unix tools such as the utility. In this case a signal is sent directly to a process with a given process ID. The process ID of the nginx master process is written, by default, to the in the directory or . For example, if the master process ID is 1628, to send the QUIT signal resulting in nginx‚Äôs graceful shutdown, execute:\n\nnginx consists of modules which are controlled by directives specified in the configuration file. Directives are divided into simple directives and block directives. A simple directive consists of the name and parameters separated by spaces and ends with a semicolon ( ). A block directive has the same structure as a simple directive, but instead of the semicolon it ends with a set of additional instructions surrounded by braces ( and ). If a block directive can have other directives inside braces, it is called a context (examples: events, http, server, and location).\n\nDirectives placed in the configuration file outside of any contexts are considered to be in the main context. The and directives reside in the context, in , and in .\n\nAn important web server task is serving out files (such as images or static HTML pages). You will implement an example where, depending on the request, files will be served from different local directories: (which may contain HTML files) and (containing images). This will require editing of the configuration file and setting up of a server block inside the http block with two location blocks.\n\nThis block specifies the ‚Äú ‚Äù prefix compared with the URI from the request. For matching requests, the URI will be added to the path specified in the root directive, that is, to , to form the path to the requested file on the local file system. If there are several matching blocks nginx selects the one with the longest prefix. The block above provides the shortest prefix, of length one, and so only if all other blocks fail to provide a match, this block will be used.\n\nThis is already a working configuration of a server that listens on the standard port 80 and is accessible on the local machine at . In response to requests with URIs starting with , the server will send files from the directory. For example, in response to the request nginx will send the file. If such file does not exist, nginx will send a response indicating the 404 error. Requests with URIs not starting with will be mapped onto the directory. For example, in response to the request nginx will send the file.\n\nOne of the frequent uses of nginx is setting it up as a proxy server, which means a server that receives requests, passes them to the proxied servers, retrieves responses from them, and sends them to the clients.\n\nWe will configure a basic proxy server, which serves requests of images with files from the local directory and sends all other requests to a proxied server. In this example, both servers will be defined on a single nginx instance.\n\nThis will be a simple server that listens on the port 8080 (previously, the directive has not been specified since the standard port 80 was used) and maps all requests to the directory on the local file system. Create this directory and put the file into it. Note that the directive is placed in the context. Such directive is used when the block selected for serving a request does not include its own directive.\n\nNext, use the server configuration from the previous section and modify it to make it a proxy server configuration. In the first block, put the proxy_pass directive with the protocol, name and port of the proxied server specified in the parameter (in our case, it is ):\n\nThe most basic nginx configuration to work with a FastCGI server includes using the fastcgi_pass directive instead of the directive, and fastcgi_param directives to set parameters passed to a FastCGI server. Suppose the FastCGI server is accessible on . Taking the proxy configuration from the previous section as a basis, replace the directive with the directive and change the parameter to . In PHP, the parameter is used for determining the script name, and the parameter is used to pass request parameters. The resulting configuration would be:"
    },
    {
        "link": "https://nginx.org/en/docs",
        "document": ""
    },
    {
        "link": "https://docs.nginx.com/nginx/admin-guide/web-server/web-server",
        "document": "Configuring NGINX and NGINX Plus as a Web Server\n\nThis article explains how to configure NGINX Open Source and F5 NGINX Plus as a web server.\n\nNote: The information in this article applies to both NGINX Open Source and NGINX Plus. For ease of reading, the remainder of the article refers to NGINX Plus only.\n\nAt a high level, configuring NGINX Plus as a web server is a matter of defining which URLs it handles and how it processes HTTP requests for resources at those URLs. At a lower level, the configuration defines a set of virtual servers that control the processing of requests for particular domains or IP addresses. For more information about configuration files, refer to Creating NGINX and NGINX Plus Configuration Files.\n\nEach virtual server for HTTP traffic defines special configuration instances called locations that control processing of specific sets of URIs. Each location defines its own scenario of what happens to requests that are mapped to this location. NGINX Plus provides full control over this process. Each location can proxy the request or return a file. In addition, the URI can be modified, so that the request is redirected to another location or virtual server. Also, a specific error code can be returned and you can configure a specific page to correspond to each error code.\n\nThe NGINX Plus configuration file must include at least one server directive to define a virtual server. When NGINX Plus processes a request, it first selects the virtual server that will serve the request.\n\nA virtual server is defined by a directive in the context, for example:\n\nIt is possible to add multiple directives into the context to define multiple virtual servers.\n\nThe configuration block usually includes a listen directive to specify the IP address and port (or Unix domain socket and path) on which the server listens for requests. Both IPv4 and IPv6 addresses are accepted; enclose IPv6 addresses in square brackets.\n\nThe example below shows configuration of a server that listens on IP address 127.0.0.1 and port 8080:\n\nIf a port is omitted, the standard port is used. Likewise, if an address is omitted, the server listens on all addresses. If the directive is not included at all, the ‚Äústandard‚Äù port is and the ‚Äúdefault‚Äù port is , depending on superuser privileges.\n\nIf there are several servers that match the IP address and port of the request, NGINX Plus tests the request‚Äôs header field against the server_name directives in the blocks. The parameter to can be a full (exact) name, a wildcard, or a regular expression. A wildcard is a character string that includes the asterisk ( ) at its beginning, end, or both; the asterisk matches any sequence of characters. NGINX Plus uses the Perl syntax for regular expressions; precede them with the tilde ( ). This example illustrates an exact name.\n\nIf several names match the header, NGINX Plus selects one by searching for names in the following order and using the first match it finds:\n‚Ä¢ Longest wildcard starting with an asterisk, such as\n‚Ä¢ Longest wildcard ending with an asterisk, such as\n‚Ä¢ First matching regular expression (in order of appearance in the configuration file)\n\nIf the header field does not match a server name, NGINX Plus routes the request to the default server for the port on which the request arrived. The default server is the first one listed in the nginx.conf file, unless you include the parameter to the directive to explicitly designate a server as the default.\n\nNGINX Plus can send traffic to different proxies or serve different files based on the request URIs. These blocks are defined using the location directive placed within a directive.\n\nFor example, you can define three blocks to instruct the virtual server to send some requests to one proxied server, send other requests to a different proxied server, and serve the rest of the requests by delivering files from the local file system.\n\nNGINX Plus tests request URIs against the parameters of all directives and applies the directives defined in the matching location. Inside each block, it is usually possible (with a few exceptions) to place even more directives to further refine the processing for specific groups of requests.\n\nNote: In this guide, the word location refers to a single location context.\n\nThere are two types of parameter to the directive: prefix strings (pathnames) and regular expressions. For a request URI to match a prefix string, it must start with the prefix string.\n\nThe following sample location with a pathname parameter matches request URIs that begin with /some/path/, such as /some/path/document.html. (It does not match /my-site/some/path because /some/path does not occur at the start of that URI.)\n\nA regular expression is preceded with the tilde ( ) for case-sensitive matching, or the tilde-asterisk ( ) for case-insensitive matching. The following example matches URIs that include the string .html or .htm in any position.\n\nTo find the location that best matches a URI, NGINX Plus first compares the URI to the locations with a prefix string. It then searches the locations with a regular expression.\n\nHigher priority is given to regular expressions, unless the modifier is used. Among the prefix strings NGINX Plus selects the most specific one (that is, the longest and most complete string). The exact logic for selecting a location to process a request is given below:\n‚Ä¢ Test the URI against all prefix strings.\n‚Ä¢ The (equals sign) modifier defines an exact match of the URI and a prefix string. If the exact match is found, the search stops.\n‚Ä¢ If the (caret-tilde) modifier prepends the longest matching prefix string, the regular expressions are not checked.\n‚Ä¢ Stop processing when the first matching regular expression is found and use the corresponding location.\n‚Ä¢ If no regular expression matches, use the location corresponding to the stored prefix string.\n\nA typical use case for the modifier is requests for / (forward slash). If requests for / are frequent, specifying as the parameter to the directive speeds up processing, because the search for matches stops after the first comparison.\n\nA context can contain directives that define how to resolve a request ‚Äì either serve a static file or pass the request to a proxied server. In the following example, requests that match the first context are served files from the /data directory and the requests that match the second are passed to the proxied server that hosts content for the <www.example.com> domain.\n\nThe root directive specifies the file system path in which to search for the static files to serve. The request URI associated with the location is appended to the path to obtain the full name of the static file to serve. In the example above, in response to a request for /images/example.png, NGINX Plus delivers the file /data/images/example.png.\n\nThe proxy_pass directive passes the request to the proxied server accessed with the configured URL. The response from the proxied server is then passed back to the client. In the example above, all requests with URIs that do not start with /images/ are be passed to the proxied server.\n\nYou can use variables in the configuration file to have NGINX Plus process requests differently depending on defined circumstances. Variables are named values that are calculated at runtime and are used as parameters to directives. A variable is denoted by the (dollar) sign at the beginning of its name. Variables define information based upon NGINX‚Äôs state, such as the properties of the request being currently processed.\n\nThere are a number of predefined variables, such as the core HTTP variables, and you can define custom variables using the set, map, and geo directives. Most variables are computed at runtime and contain information related to a specific request. For example, contains the client IP address and holds the current URI value.\n\nSome website URIs require immediate return of a response with a specific error or redirect code, for example when a page has been moved temporarily or permanently. The easiest way to do this is to use the return directive. For example:\n\nThe first parameter of is a response code. The optional second parameter can be the URL of a redirect (for codes , , , and ) or the text to return in the response body. For example:\n\nThe directive can be included in both the and contexts.\n\nA request URI can be modified multiple times during request processing through the use of the rewrite directive, which has one optional and two required parameters. The first (required) parameter is the regular expression that the request URI must match. The second parameter is the URI to substitute for the matching URI. The optional third parameter is a flag that can halt processing of further directives or send a redirect (code or ). For example:\n\nAs this example shows, the second parameter captures though matching of regular expressions.\n\nYou can include multiple directives in both the and contexts. NGINX Plus executes the directives one-by-one in the order they occur. The directives in a context are executed once when that context is selected.\n\nAfter NGINX processes a set of rewriting instructions, it selects a context according to the new URI. If the selected location contains directives, they are executed in turn. If the URI matches any of those, a search for the new location starts after all defined directives are processed.\n\nThe following example shows directives in combination with a directive.\n\nThis example configuration distinguishes between two sets of URIs. URIs such as /download/some/media/file are changed to /download/some/mp3/file.mp3. Because of the flag, the subsequent directives (the second and the directive) are skipped but NGINX Plus continues processing the request, which now has a different URI. Similarly, URIs such as /download/some/audio/file are replaced with /download/some/mp3/file.ra. If a URI doesn‚Äôt match either directive, NGINX Plus returns the error code to the client.\n\nThere are two parameters that interrupt processing of directives:\n‚Ä¢ ‚Äì Stops execution of the directives in the current or context, but NGINX Plus searches for locations that match the rewritten URI, and any directives in the new location are applied (meaning the URI can be changed again).\n‚Ä¢ ‚Äì Like the break directive, stops processing of directives in the current context and cancels the search for locations that match the new URI. The directives in the new location are not executed.\n\nSometimes you need to rewrite or change the content in an HTTP response, substituting one string for another. You can use the sub_filter directive to define the rewrite to apply. The directive supports variables and chains of substitutions, making more complex changes possible.\n\nFor example, you can change absolute links that refer to a server other than the proxy:\n\nAnother example changes the scheme from to and replaces the address with the hostname from the request header field. The sub_filter_once directive tells NGINX to apply sub_filter directives consecutively within a location:\n\nNote that the part of the response already modified with the is not replaced again if another match occurs.\n\nWith the error_page directive, you can configure NGINX Plus to return a custom page along with an error code, substitute a different error code in the response, or redirect the browser to a different URI. In the following example, the directive specifies the page (/404.html) to return with the error code.\n\nNote that this directive does not mean that the error is returned immediately (the directive does that), but simply specifies how to treat errors when they occur. The error code can come from a proxied server or occur during processing by NGINX Plus (for example, the results when NGINX Plus can‚Äôt find the file requested by the client).\n\nIn the following example, when NGINX Plus cannot find a page, it substitutes code for code , and redirects the client to http:/example.com/new/path.html. This configuration is useful when clients are still trying to access a page at its old URI. The code informs the browser that the page has moved permanently, and it needs to replace the old address with the new one automatically upon return.\n\nThe following configuration is an example of passing a request to the back end when a file is not found. Because there is no status code specified after the equals sign in the directive, the response to the client has the status code returned by the proxied server (not necessarily ).\n\nThe directive instructs NGINX Plus to make an internal redirect when a file is not found. The variable in the final parameter to the directive holds the URI of the current request, which gets passed in the redirect.\n\nFor example, if /images/some/file is not found, it is replaced with /fetch/images/some/file and a new search for a location starts. As a result, the request ends up in the second context and is proxied to ‚Äúhttp://backend/‚Äù.\n\nThe open_file_cache_errors directive prevents writing an error message if a file is not found. This is not necessary here since missing files are correctly handled."
    },
    {
        "link": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/9/html/deploying_web_servers_and_reverse_proxies/setting-up-and-configuring-nginx_deploying-web-servers-and-reverse-proxies",
        "document": "NGINX is a high performance and modular server that you can use, for example, as a:\n\nThis section describes how to NGINX in these scenarios.\n\nIn Red Hat Enterprise Linux 9,different versions to NGINX are provided by Application Streams. By using the default configuration, NGINX runs as a web server on port and provides content from the directory.\n‚Ä¢ The host is subscribed to the Red Hat Customer Portal.\n‚Ä¢ The service is enabled and started.\n‚Ä¢ None\n‚Ä¢ None To install NGINX 1.20 as the initial version of this Application Stream from an RPM package: If you have previously enabled an NGINX module stream, this command installs the NGINX version from the enabled stream.\n‚Ä¢ None To install an alternate later version of NGINX from a module stream:\n‚Ä¢ None Open the ports on which NGINX should provide its service in the firewall. For example, to open the default ports for HTTP (port 80) and HTTPS (port 443) in , enter:\n‚Ä¢ None Enable the service to start automatically when the system boots:\n‚Ä¢ None If you do not want to use the default configuration, skip this step, and configure NGINX accordingly before you start the service.\n‚Ä¢ None Use the utility to verify that the package is installed.\n‚Ä¢ None In case of the NGINX 1.20 RPM package:\n‚Ä¢ None Ensure that the ports on which NGINX should provide its service are opened in the firewalld:\n‚Ä¢ None Verify that the service is enabled:\n\n2.2. Configuring NGINX as a web server that provides different content for different domains By default, NGINX acts as a web server that provides the same content to clients for all domain names associated with the IP addresses of the server. This procedure explains how to configure NGINX:\n‚Ä¢ To serve requests to the domain with content from the directory\n‚Ä¢ To serve requests to the domain with content from the directory\n‚Ä¢ To serve all other requests, for example, to the IP address of the server or to other domains associated with the IP address of the server, with content from the directory\n‚Ä¢ None Clients and the web server resolve the and domain to the IP address of the web server. Note that you must manually add these entries to your DNS server.\n‚Ä¢ None\n‚Ä¢ None By default, the file already contains a catch-all configuration. If you have deleted this part from the configuration, re-add the following block to the block in the file: These settings configure the following:\n‚Ä¢ The directive define which IP address and ports the service listens. In this case, NGINX listens on port on both all IPv4 and IPv6 addresses. The parameter indicates that NGINX uses this block as the default for requests matching the IP addresses and ports.\n‚Ä¢ The parameter defines the host names for which this block is responsible. Setting to configures NGINX to accept any host name for this block.\n‚Ä¢ The directive sets the path to the web content for this block.\n‚Ä¢ None Append a similar block for the domain to the block:\n‚Ä¢ The directive defines a separate access log file for this domain.\n‚Ä¢ The directive defines a separate error log file for this domain.\n‚Ä¢ None Append a similar block for the domain to the block:\n‚Ä¢ None Create the root directories for both domains:\n‚Ä¢ None Set the context on both root directories: These commands set the context on the and directories. Note that you must install the package to run the commands.\n‚Ä¢ None Create the log directories for both domains:\n‚Ä¢ None Create a different example file in each virtual host‚Äôs document root:\n‚Ä¢ Use a browser and connect to . The web server shows the example content from the file.\n‚Ä¢ Use a browser and connect to . The web server shows the example content from the file.\n‚Ä¢ Use a browser and connect to . The web server shows the example content from the file.\n\nYou can enable TLS encryption on an NGINX web server for the domain.\n‚Ä¢ None The private key is stored in the file. For details about creating a private key and certificate signing request (CSR), as well as how to request a certificate from a certificate authority (CA), see your CA‚Äôs documentation.\n‚Ä¢ The TLS certificate is stored in the file. If you use a different path, adapt the corresponding steps of the procedure.\n‚Ä¢ The CA certificate has been appended to the TLS certificate file of the server.\n‚Ä¢ Clients and the web server resolve the host name of the server to the IP address of the web server.\n‚Ä¢ Port is open in the local firewall.\n‚Ä¢ If the server runs RHEL 9.2 or later and the FIPS mode is enabled, clients must either support the Extended Master Secret (EMS) extension or use TLS 1.3. TLS 1.2 connections without EMS fail. For more information, see the Red Hat Knowledgebase solution TLS extension \"Extended Master Secret\" enforced.\n‚Ä¢ None Edit the file, and add the following block to the block in the configuration:\n‚Ä¢ None Optional: Starting with RHEL 9.3, you can use the directive to configure an external program that is called at start for each encrypted private key. Add one of the following lines to the file:\n‚Ä¢ None To call an external program for each encrypted private key file, enter: NGINX calls this program with the following two arguments:\n‚Ä¢ The server name specified in the setting.\n‚Ä¢ One of the following algorithms: , , , , or if a cryptographic algorithm cannot be recognized.\n‚Ä¢ None If you want to manually enter a passphrase for each encrypted private key file, enter: This is the default behavior if is not configured. The service fails to start if you use this method but have at least one private key protected by a passphrase. In this case, use one of the other methods.\n‚Ä¢ None If you want to prompt for the passphrase for each encrypted private key when you start the service by using the utility, enter:\n‚Ä¢ None For security reasons, configure that only the user can access the private key file: If the private key was accessed by unauthorized users, revoke the certificate, create a new private key, and request a new certificate. Otherwise, the TLS connection is no longer secure.\n‚Ä¢ Use a browser and connect to\n\nYou can use the NGINX reverse proxy feature to load-balance traffic. This procedure describes how to configure NGINX as an HTTP load balancer that sends requests to different servers, based on which of them has the least number of active connections. If both servers are not available, the procedure also defines a third host for fallback reasons.\n‚Ä¢ NGINX is installed as described in Installing and preparing NGINX.\n‚Ä¢ None Edit the file and add the following settings: The directive in the host group named defines that NGINX sends requests to or , depending on which host has the least number of active connections. NGINX uses only as a backup in case that the other two hosts are not available. With the directive set to , NGINX acts as a reverse proxy and uses the host group to distribute requests based on the settings of this group. Instead of the load balancing method, you can specify:\n‚Ä¢ No method to use round robin and distribute requests evenly across servers.\n‚Ä¢ to send requests from one client address to the same server based on a hash calculated from the first three octets of the IPv4 address or the whole IPv6 address of the client.\n‚Ä¢ to determine the server based on a user-defined key, which can be a string, a variable, or a combination of both. The parameter configures that NGINX distributes requests across all servers based on the user-defined hashed key value."
    }
]