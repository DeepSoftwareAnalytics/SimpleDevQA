[
    {
        "link": "https://tensorflow.org/api_docs/python/tf/keras/layers/LSTM",
        "document": "Used in the notebooks\n\nBased on available runtime hardware and constraints, this layer will choose different implementations (cuDNN-based or backend-native) to maximize the performance. If a GPU is available and all the arguments to the layer meet the requirement of the cuDNN kernel (see below for details), the layer will use a fast cuDNN implementation when using the TensorFlow backend. The requirements to use the cuDNN implementation are:\nâ€¢ Inputs, if use masking, are strictly right-padded.\nâ€¢ Eager execution is enabled in the outermost context.\n\nThis method is the reverse of , capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by )."
    },
    {
        "link": "https://keras.io/api/layers/recurrent_layers/lstm",
        "document": "Based on available runtime hardware and constraints, this layer will choose different implementations (cuDNN-based or backend-native) to maximize the performance. If a GPU is available and all the arguments to the layer meet the requirement of the cuDNN kernel (see below for details), the layer will use a fast cuDNN implementation when using the TensorFlow backend. The requirements to use the cuDNN implementation are:\nâ€¢ Inputs, if use masking, are strictly right-padded.\nâ€¢ Eager execution is enabled in the outermost context.\nâ€¢ activation: Activation function to use. Default: hyperbolic tangent ( ). If you pass , no activation is applied (ie. \"linear\" activation: ).\nâ€¢ recurrent_activation: Activation function to use for the recurrent step. Default: sigmoid ( ). If you pass , no activation is applied (ie. \"linear\" activation: ).\nâ€¢ use_bias: Boolean, (default ), whether the layer should use a bias vector.\nâ€¢ kernel_initializer: Initializer for the weights matrix, used for the linear transformation of the inputs. Default: .\nâ€¢ recurrent_initializer: Initializer for the weights matrix, used for the linear transformation of the recurrent state. Default: .\nâ€¢ unit_forget_bias: Boolean (default ). If , add 1 to the bias of the forget gate at initialization. Setting it to will also force . This is recommended in Jozefowicz et al.\nâ€¢ activity_regularizer: Regularizer function applied to the output of the layer (its \"activation\"). Default: .\nâ€¢ dropout: Float between 0 and 1. Fraction of the units to drop for the linear transformation of the inputs. Default: 0.\nâ€¢ recurrent_dropout: Float between 0 and 1. Fraction of the units to drop for the linear transformation of the recurrent state. Default: 0.\nâ€¢ return_sequences: Boolean. Whether to return the last output in the output sequence, or the full sequence. Default: .\nâ€¢ return_state: Boolean. Whether to return the last state in addition to the output. Default: .\nâ€¢ go_backwards: Boolean (default: ). If , process the input sequence backwards and return the reversed sequence.\nâ€¢ stateful: Boolean (default: ). If , the last state for each sample at index i in a batch will be used as initial state for the sample of index i in the following batch.\nâ€¢ unroll: Boolean (default False). If , the network will be unrolled, else a symbolic loop will be used. Unrolling can speed-up a RNN, although it tends to be more memory-intensive. Unrolling is only suitable for short sequences.\nâ€¢ use_cudnn: Whether to use a cuDNN-backed implementation. will attempt to use cuDNN when feasible, and will fallback to the default implementation if not.\nâ€¢ mask: Binary tensor of shape indicating whether a given timestep should be masked (optional). An individual entry indicates that the corresponding timestep should be utilized, while a entry indicates that the corresponding timestep should be ignored. Defaults to .\nâ€¢ training: Python boolean indicating whether the layer should behave in training mode or in inference mode. This argument is passed to the cell when calling it. This is only relevant if or is used (optional). Defaults to .\nâ€¢ initial_state: List of initial state tensors to be passed to the first call of the cell (optional, causes creation of zero-filled initial state tensors). Defaults to ."
    },
    {
        "link": "https://github.com/christianversloot/machine-learning-articles/blob/main/build-an-lstm-model-with-tensorflow-and-keras.md",
        "document": "Long Short-Term Memory (LSTM) based neural networks have played an important role in the field of Natural Language Processing. In addition, they have been used widely for sequence modeling. The reason why LSTMs have been used widely for this is because the model connects back to itself during a forward pass of your samples, and thus benefits from context generated by previous predictions when prediction for any new sample.\n\nIn this article, we're going to take a look at how we can build an LSTM model with TensorFlow and Keras. For doing so, we're first going to take a brief look at what LSTMs are and how they work. Don't worry, we won't cover this in much detail, because we already did so in another article. It is necessary though to understand what is happening before we actually get to work. That's how you build intuition for the models you'll use for Machine Learning tasks.\n\nOnce we know about LSTMs, we're going to take a look at how we can build one with TensorFlow. More specifically, we're going to use , or TensorFlow's tightly coupled (or frankly, embedded) version of Keras for the job. First of all, we're going to see how LSTMs are represented as . We'll then move on and actually build the model. With step-by-step explanations, you will understand what is going on at each line and build an understanding of LSTM models in code.\n\nThe code example below gives you a working LSTM based model with TensorFlow 2.x and Keras. If you want to understand it in more detail, make sure to read the rest of the article below.\n\nBefore we will actually write any code, it's important to understand what is happening inside an LSTM. First of all, we must say that an LSTM is an improvement upon what is known as a vanilla or traditional Recurrent Neural Network, or RNN. Such networks look as follows:\n\nA fully recurrent network. Created by fdeloche at Wikipedia, licensed as CC BY-SA 4.0. No changes were made.\n\nIn a vanilla RNN, an input value ( ) is passed through the model, which has a hidden or learned state at that point in time. The model produces the output which is in the target representation. Using this way of working, we can convert inputs in English into outputs in German, to give just an example. Vanilla RNNs are therefore widely used as sequence-to-sequence models.\n\nHowever, we can do the same with classic neural networks. Their benefit compared to classic MLPs is that they pass the output back to themselves, so that it can be used during the next pass. This provides the neural network with context with respect to previous inputs (which in semantically confusing tasks like translation can sometimes be really important). Classic RNNs are therefore nothing more than a fully-connected network that passes neural outputs back to the neurons.\n\nSo far, so good. RNNs really boosted the state-of-the-art back in the days. But well, there's a problem. It emerges when you want to train classic Recurrent Neural Networks. If you apply backpropagation to training a regular neural network, errors are computed backwards, so that the gradient update becomes known that can be applied by the optimizer. Recurrent backpropagation is something that is however not so easy or available, so another approach had to be taken. Effectively, this involved unfolding the network, effectively making copies of the network (with exactly the same initialization) and improving upon them. This way, we can compute gradients more easily, and chain them together. It allowed for the training of RNNs.\n\nBut chaining gradients together effectively means that you have to apply multiplications. And here's the catch: classic RNNs were combined with activation functions like Sigmoid and Tanh, but primarily Sigmoid. As the output of the derivative of these functions is almost always < 1.0, you get a severe case of vanishing gradients. Classic RNNs could therefore not be used when sequences got long; they simply got stuck or trained very slowly.\n\nEnter LSTMs. These Long Short-Term Memory networks effectively split up the output and memory. In so-called memory cells, they allow all functionality to happen, the prediction to be generated, and memory to be updated. Visually, this looks as follows:\n\nLet's take a brief look at all the components in a bit more detail:\nâ€¢ All functionality is embedded into a memory cell, visualized above with the rounded border.\nâ€¢ The and variables represent the outputs of the memory cell at respectively and . In plain English: the output of the previous cell into the current cell, and the output of the current cell to the next one.\nâ€¢ The and variables represent the memory itself, at the known time steps. As you can see, memory has been cut away from the output variable, being an entity on its own.\nâ€¢ We have three so-called gates, represented by the three blocks of elements within the cell:\nâ€¢ On the left, we see a forget gate. It takes the previous output and current input and by means of Sigmoid activation computes what can be forgotten and hence removed from memory related to current and previous input. By multiplying this with the memory, the removal is performed.\nâ€¢ In the middle, we see an input gate. It takes the previous output and current input and applies both a Sigmoid and Tanh activation. The Sigmoid activation effectively learns what must be kept from the inputs, whereas the Tanh normalizes the values into the range , stabilizing the training process. As you can see, the results are first multiplied (to ensure that normalization occurs) after which it is added into memory.\nâ€¢ On the right, we see an output gate. It takes a normalized value for memory through Tanh and a Sigmoid activated value for the previous output and current input, effectively learning what must be predicted for the current input value. This value is then output, and the memory and output values are also passed to the next cell.\n\nThe benefit of LSTMs with respect to simple RNNs lies in the fact that memory has been separated from the actual output mechanisms. As you can see, all vanishing gradient-causing mechanisms lie within the cell. In inter-cell communication, the only elements that are encountered during gradient computation are multiplication (x) and addition (+). These are linear operations, and by consequence the LSTM can ensure that gradients between cells are always 1.0. Hence, with LSTMs, the vanishing gradients problem is resolved.\n\nThis makes them a lot faster than vanilla RNNs.\n\nNow that we understand how LSTMs work in theory, let's take a look at constructing them in TensorFlow and Keras. Of course, we must take a look at how they are represented first. In TensorFlow and Keras, this happens through the class, and it is described as:\n\nIndeed, that's the LSTM we want, although it might not have all the gates yet - gates were changed in another paper that was a follow-up to the Hochreiter paper. Nevertheless, understanding the LSTM with all the gates is a good idea, because that's what most of them look like today.\n\nIn code, it looks as follows:\n\nThese are the attributes that can be configured:\nâ€¢ With units, we can define the dimensionality of the output space, as we are used to e.g. with Dense layers.\nâ€¢ The activation attribute defines the activation function that will be used. By default, it is the Tanh function.\nâ€¢ With recurrent_activation, you can define the activation function for the recurrent functionality.\nâ€¢ The use_bias attribute can be used to configure whether bias must be used to steer the model as well.\nâ€¢ The initializers can be used to initialize the weights of the kernels and recurrent segment, as well as the biases.\nâ€¢ The unit_forget_bias represents the bias value (+1) at the forget gate. This is recommended in a follow-up study to the original LSTM paper.\nâ€¢ The regularizers and constraints allow you to constrain the training process, possibly blocking vanishing and exploding gradients, and keeping the model at adequate complexity.\nâ€¢ Dropout can be added to avoid overfitting, to both the cell itself as well as the recurrent segment.\nâ€¢ With return_sequences, you can indicate whether you want only the prediction for the current input as the output, or that with all the previous predictions appended.\nâ€¢ With return_state, you can indicate whether you also want to have state returned besides the outputs.\nâ€¢ With go_backwards, you can indicate whether you want to have the sequence returned in reverse order.\nâ€¢ If you set stateful to True, the recurrent segment will work on a batch level rather than model level.\nâ€¢ Structure of your input (timesteps, batch, features or batch, timesteps, features) can be switched with time_major.\nâ€¢ With unroll, you can still unroll the network at training. If set to False, a symbolic loop will be used.\nâ€¢ Additional arguments can be passed with **kwargs.\n\nNow that we understand how LSTMs work and how they are represented within TensorFlow, it's time to actually build one with Python, TensorFlow and its Keras APIs. We'll walk you through the process with step-by-step examples. The process is composed of the following steps:\nâ€¢ Importing the Keras functionality that we need into the Python script.\nâ€¢ Listing the configuration for our LSTM model and preparing for training.\nâ€¢ Loading and preparing a dataset; we'll use the IMDB dataset today.\n\nOpen up a code editor and create a file, e.g. called , and let's go!\n\nLet's specify the model imports first:\nâ€¢ We'll need TensorFlow so we import it as .\nâ€¢ From the TensorFlow Keras Datasets, we import the one.\nâ€¢ We'll need word embeddings ( ), MLP layers ( ) and LSTM layers ( ), so we import them as well.\nâ€¢ Our loss function will be binary cross entropy.\nâ€¢ As we'll stack all layers on top of each other with , we need (the Keras Sequential API) for constructing our variable in the first place.\nâ€¢ For optimization we use an extension of classic gradient descent called Adam.\nâ€¢ Finally, we need to import . We're going to use the IMDB dataset which has sequences of reviews. While we'll specify a maximum length, this can mean that shorter sequences are present as well; these are not cutoff and therefore have different sizes than our desired one (i.e. the maximum length). We'll have to pad them with zeroes in order to make them of equal length.\n\nThe next step is specifying the model configuration. While strictly not necessary (we can also specify them hardcoded), I always think it's a good idea to group them together. This way, you can easily see how your model is configured, without having to take a look through all the aspects.\n\nBelow, we can see that our model will be trained with a batch size of 128, using binary crossentropy loss and Adam optimization, and only for five epochs (we only have to show you that it works). 20% of our training data will be used for validation purposes, and the output will be verbose, with verbosity mode set to 1 out of 0, 1 and 2. Our learned word embedding will have 15 hidden dimensions and each sequence passed through the model is 300 characters at max. Our vocabulary will contain 5000 words at max.\n\nYou might now also want to disable Eager Execution in TensorFlow. While it doesn't work for all, some people report that the training process speeds up after using it. However, it's not necessary to do so - simply test how it behaves on your machine:\n\nOnce this is complete, we can load and prepare the data. To make things easier, Keras comes with a standard set of datasets, of which the IMDB dataset can be used for sentiment analysis (essentially text classification with two classes). Using , we can load the data.\n\nOnce the data has been loaded, we apply . This ensures that sentences shorter than the maximum sentence length are brought to equal length by applying padding with, in this case, zeroes, because that often corresponds with the padding character.\n\nWe can then define the Keras model. As we are using the Sequential API, we can initialize the variable with . The first layer is an layer, which learns a word embedding that in our case has a dimensionality of 15. This is followed by an layer providing the recurrent segment (with default activation enabled), and a layer that has one output - through Sigmoid a number between 0 and 1, representing an orientation towards a class.\n\nThe model can then be compiled. This initializes the model that has so far been a skeleton, a foundation, but no actual model yet. We do so by specifying the optimizer, the loss function, and the additional metrics that we had specified before.\n\nThis is also a good place to generate a summary of what the model looks like.\n\nThen, we can instruct TensorFlow to start the training process.\n\nThe pairs passed to the model are the padded inputs and their corresponding class labels. Training happens with the batch size, number of epochs, verbosity mode and validation split that were also defined in the configuration section above.\n\nWe cannot evaluate the model on the same dataset that was used for training it. We fortunately have testing data available through the train/test split performed in the section, and can use built-in evaluation facilities to evaluate the model. We then print the test results on screen.\n\nIf you want to get the full model code just at once, e.g. for copy-and-run, here you go:\n\nTime to run the model! Open up a terminal where at least TensorFlow and Python have been installed, and run the model - .\n\nYou should see that the model starts training after e.g. a few seconds. If you have the IMDB dataset not downloaded to your machine, it will be downloaded first.\n\nEventually, you'll approximately see an 87.1% accuracy on the evaluation set:\n\nIf you face speed issues with training the TensorFlow LSTM on your GPU, you might decide to temporarily disable its access to your GPUs by adding the following before :\n\nLong Short-Term Memory Networks (LSTMs) are a type of recurrent neural network that can be used in Natural Language Processing, time series and other sequence modeling tasks. In this article, we covered their usage within TensorFlow and Keras in a step-by-step fashion.\n\nWe first briefly looked at LSTMs in general. What are they? What can they be used for? How do they improve compared to previous RNN based approaches? This analysis gives you the necessary context in order to understand what is going on within your code.\n\nWe then looked at how LSTMs are represented in TensorFlow and Keras. We saw that there is a separate layer that can be configured with a wide variety of attributes. In the article, we looked at the meaning for each attribute and saw how everything interrelates. Once understanding this, we moved on to actually implementing the model with TensorFlow. In a step-by-step phased approach, we explained in detail why we made certain choices, allowing you to see exactly how the model was constructed.\n\nAfter training on the IMDB dataset, we saw that the model achieves an accuracy of approximately 87.1% on the evaluation set.\n\nI hope that you have learned something from this article. If you did, please feel free to drop a message, as I'd love to hear from you ðŸ’¬ Please do the same if you have any questions, or click the Ask Questions button to the right. Thank you for reading MachineCurve today and happy engineering! ðŸ˜Ž\n\nKeras Team. (n.d.). Keras documentation: The sequential class. Keras: the Python deep learning API. https://keras.io/api/models/sequential/"
    },
    {
        "link": "https://stackoverflow.com/questions/64040973/keras-lstm-input-output-shape",
        "document": "takes as input shape and returns . Your target output is however encoded as .\n\nFrom the dimensions, I assume you want to map each sequence to a non-sequence, i.e. you can do:\n\nThis will return the final hidden state, i.e. a shape of which matches your target output."
    },
    {
        "link": "https://tensorflow.org/guide/keras/sequential_model",
        "document": "Save and categorize content based on your preferences.\n\nStay organized with collections Save and categorize content based on your preferences.\n\nWhen to use a Sequential model\n\nA model is appropriate for a plain stack of layers where each layer has exactly one input tensor and one output tensor.\n\nis equivalent to this function:\n\nA Sequential model is not appropriate when:\nâ€¢ Your model has multiple inputs or multiple outputs\nâ€¢ Any of your layers has multiple inputs or multiple outputs\nâ€¢ You need to do layer sharing\n\nYou can create a Sequential model by passing a list of layers to the Sequential constructor:\n\nIts layers are accessible via the attribute:\n\nYou can also create a Sequential model incrementally via the method:\n\nNote that there's also a corresponding method to remove layers: a Sequential model behaves very much like a list of layers.\n\nAlso note that the Sequential constructor accepts a argument, just like any layer or model in Keras. This is useful to annotate TensorBoard graphs with semantically meaningful names.\n\nSpecifying the input shape in advance\n\nGenerally, all layers in Keras need to know the shape of their inputs in order to be able to create their weights. So when you create a layer like this, initially, it has no weights:\n\nIt creates its weights the first time it is called on an input, since the shape of the weights depends on the shape of the inputs:\n\nNaturally, this also applies to Sequential models. When you instantiate a Sequential model without an input shape, it isn't \"built\": it has no weights (and calling results in an error stating just this). The weights are created when the model first sees some input data:\n\nOnce a model is \"built\", you can call its method to display its contents:\n\nHowever, it can be very useful when building a Sequential model incrementally to be able to display the summary of the model so far, including the current output shape. In this case, you should start your model by passing an object to your model, so that it knows its input shape from the start:\n\nNote that the object is not displayed as part of , since it isn't a layer:\n\nA simple alternative is to just pass an argument to your first layer:\n\nModels built with a predefined input shape like this always have weights (even before seeing any data) and always have a defined output shape.\n\nIn general, it's a recommended best practice to always specify the input shape of a Sequential model in advance if you know what it is.\n\nWhen building a new Sequential architecture, it's useful to incrementally stack layers with and frequently print model summaries. For instance, this enables you to monitor how a stack of and layers is downsampling image feature maps:\n\nWhat to do once you have a model\n\nOnce your model architecture is ready, you will want to:\nâ€¢ Train your model, evaluate it, and run inference. See our guide to training & evaluation with the built-in loops\nâ€¢ Save your model to disk and restore it. See our guide to serialization & saving.\nâ€¢ Speed up model training by leveraging multiple GPUs. See our guide to multi-GPU and distributed training.\n\nOnce a Sequential model has been built, it behaves like a Functional API model. This means that every layer has an and attribute. These attributes can be used to do neat things, like quickly creating a model that extracts the outputs of all intermediate layers in a Sequential model:\n\nHere's a similar example that only extract features from one layer:\n\nTransfer learning consists of freezing the bottom layers in a model and only training the top layers. If you aren't familiar with it, make sure to read our guide to transfer learning.\n\nHere are two common transfer learning blueprint involving Sequential models.\n\nFirst, let's say that you have a Sequential model, and you want to freeze all layers except the last one. In this case, you would simply iterate over and set on each layer, except the last one. Like this:\n\nAnother common blueprint is to use a Sequential model to stack a pre-trained model and some freshly initialized classification layers. Like this:\n\nIf you do transfer learning, you will probably find yourself frequently using these two patterns.\n\nThat's about all you need to know about Sequential models!\n\nTo find out more about building models in Keras, see:\nâ€¢ Guide to making new Layers & Models via subclassing"
    },
    {
        "link": "https://stackoverflow.com/questions/33612935/large-pandas-dataframe-parallel-processing",
        "document": "The entire DataFrame needs to be pickled and unpickled for each process created by joblib. In practice, this is very slow and also requires many times the memory of each.\n\nOne solution is to store your data in HDF ( ) using the table format. You can then use to select subsets of data for further processing. In practice this will be too slow for interactive use. It is also very complex, and your workers will need to store their work so that it can be consolidated in the final step.\n\nAn alternative would be to explore with . This would require the use of NumPy arrays not Pandas objects, so it also has some complexity costs.\n\nIn the long run, dask is hoped to bring parallel execution to Pandas, but this is not something to expect soon."
    },
    {
        "link": "https://stackoverflow.com/questions/71312750/joblib-memory-and-pandas-dataframe-inputs",
        "document": "I've been finding that results in unreliable caching when using dataframes as inputs to the decorated functions. Playing around, I found that results in inconsistent hashes, at least in some cases. If I understand correctly, is used by , so this is probably the source of the problem.\n\nProblems seem to occur when new columns are added to dataframes followed by a copy, or when a dataframe is saved and loaded from disk. The following example compares the inconsistent hash output when applied to dataframes, or the consistent results when applied to the equivalent numpy data.\n\nThe \"Equivalent Numpy Hashes\" is the behavior I'd like. I'm guessing the problem is due to some kind of complex internal metadata that DataFrames utililize. Is there any canonical way to use on pandas DataFrames so it will cache based upon the data values only?\n\nA \"good enough\" workaround would be if there is a way a user can tell to utilize something like my function above for specific arguments."
    },
    {
        "link": "https://github.com/scikit-learn/scikit-learn/issues/27037",
        "document": "I have a configured to output pandas dataframes using the api. When is called with data of type , it throws an error as shown below.\n\nI found that this issue does not occur when the line in the MWE below is commented out.\n\nOn further digging and looking at the traceback, this bug occurs because the argument in the line of the traceback is actually the bound method of list object X (which was passed as input to ), instead of a pandas .\n\nno error is thrown when is called."
    },
    {
        "link": "https://coiled.io/blog/sklearn-joblib-dask",
        "document": "You can train scikit-learn models in parallel using the scikit-learn joblib interface. This allows scikit-learn to take full advantage of the multiple cores in your machine (or, spoiler alert, on your cluster) and speed up training.\n\nUsing the Dask joblib backend, you can maximize parallelism by scaling your scikit-learn model training out to a remote cluster.\n\nThis works for all scikit-learn models that expose the n_jobs keyword, like random forests, linear regressions, and other machine learning algorithms. You can also use the joblib library for distributed hyperparameter tuning with grid search cross-validation.\n\nWhen training machine learning models, you can run into 2 types of scalability issues: your model size may get too large, or your data size may start to cause issues (or even worse: both!). You can typically recognize a scalability problem related to your model size by long training times: the computation will complete (eventually), but itâ€™s becoming a bottleneck in your data processing pipeline. This means youâ€™re in the top-left corner of the diagram below:\n\nUsing the scikit-learn joblib integration, you can address this problem by processing your scikit-learn models in parallel, distributed over the multiple cores in your machine.\n\nNote that this is only a good solution if your training data and test data fit in memory comfortably. If this is not the case (and youâ€™re in the â€œI give up!â€ quadrant of large models and large datasets), see the â€œDataset Too Large?â€ section below to learn how Dask-ML or XGBoost-on-Dask can help you out.\n\n\n\nLetâ€™s see this in action with an scikit-learn algorithm that is embarrassingly parallel: a random forest model. A random forest model fits a number of decision tree classifiers on sub-samples of the training data , then combines the results from the various decision trees to make a prediction. Because each decision tree is an independent process, this model can easily be trained in parallel.\n\nFirst, we'll create a new virtual environment for this work. (If you're following along, you can also use pip or any other package manager, or your own existing environment.)\n\nWeâ€™ll import the necessary libraries and create a synthetic classification dataset.\n\nWe can then use a joblib context manager to train this classifier in parallel, specifying the number of cores we want to use with the n_jobs argument.\n\nTo save users from having to use context managers every time they want to train a model in parallel, scikit-learn's developers exposed the n_jobs argument as part of the instantiating call:\n\nThe n_jobs keyword communicates to the joblib backend, so you can directly call clf.fit(X, y) without wrapping it in a context manager. This is the recommended approach for using joblib to train sklearn models in parallel locally.\n\nRunning this locally with n_jobs = -1 on a MacBook Pro with 8 cores and 16GB of RAM takes just under 3 minutes.\n\nThe context manager syntax can still come in handy when your model size increases beyond the capabilities of your local machine, or if you want to train your model even faster. In those situations, you could consider scaling out to remote clusters on the cloud. This can be especially useful if youâ€™re running heavy grid search cross-validation, or other forms of hyperparameter tuning.\n\nYou can use the Dask joblib backend to delegate the distributed training of your model to a Dask cluster of virtual machines in the cloud.\n\nWe first need to install some additional packages:\n\nNext, weâ€™ll launch a Dask cluster of 20 machines in the cloud.\n\nWhen you create the cluster, Coiled will automatically replicate your local sklearn-example environment in your cluster (see Package Sync).\n\nWeâ€™ll then connect Dask to our remote cluster. This will ensure that all future Dask computations are routed to the remote cluster instead of to our local cores.\n\nNow use the joblib context manager to specify the Dask backend. This will delegate all model training to the workers in your remote cluster:\n\nModel training is now occurring in parallel on your remote Dask cluster.\n\nThis runs in about a minute and a half on my machineâ€”thatâ€™s a 2x speed-up. You could make this run even faster by adding more workers to your cluster.\n\nYou can also use your Dask cluster to run an extensive hyperparameter grid search that would be too heavy to run on your local machine:\n\nBefore we execute this compute-heavy Grid Search, letâ€™s just scale up our cluster to 100 workers to speed up training and ensure we donâ€™t run into memory overloads:\n\nNow letâ€™s execute the grid search in parallel:\n\nIf this is your first time working in a distributed computing system or with remote clusters, you may want to consider reading The Beginnerâ€™s Guide to Distributed Computing.\n\nIs your dataset becoming too large, for example because you have acquired new data? In that case, you may be experiencing two scalability issues at once: both your model and your dataset are becoming too large. You typically notice an issue with your dataset size by pandas throwing a MemoryError when trying to run a computation over the entire dataset.\n\nDask exists precisely to solve this problem. Using Dask, you can scale your existing Python data processing workflows to larger-than-memory datasets. You can use Dask DataFrames or Dask Arrays to process data that is too large for pandas or NumPy to handle. If youâ€™re new to Dask, check out this Introduction to Dask.\n\nDask also scales machine learning for larger-than-memory datasets. Use dask-ml or the distributed Dask backend to XGBoost to train machine learning models on data that doesn't fit into memory. Dask-ML is a library that contains parallel implementation of many scikit-learn models.\n\nThe code snippet below demonstrates training a Logistic Regression model on larger-than-memory data in parallel.\n\nDask also integrates natively with XGBoost to train XGBoost models in parallel:\nâ€¢ You can use the scikit-learn's joblib integration to distribute certain scikit-learn tasks over all the cores in your machine for a faster runtime.\nâ€¢ You can connect joblib to the Dask backend to scale out to a remote cluster for even faster processing times.\nâ€¢ You can use XGBoost-on-Dask and/or dask-ml for distributed machine learning training on datasets that donâ€™t fit into local memory.\n\nIf youâ€™d like to scale your Dask work to the cloud, Coiled provides quick and on-demand Dask clusters along with tools to manage environments, teams, and costs. Click below to learn more!"
    },
    {
        "link": "https://medium.com/@erkansirin/deploying-scikit-learn-models-on-spark-a-comprehensive-guide-58cfd5559a8f",
        "document": "Machine learning models form the basis of data analysis and predictions in many fields today. Scikit-learn, on the other hand, has been the most common library in ML studies with its simple interface and wide range of algorithms in studies carried out with Python since the very beginning. However, working on large data sets with scikit-learn may pose some challenges in terms of performance and scalability. At this point, Apache Spark, famous for its success on big data, comes into play. Sparkâ€™s success in big data depends on its distributed operation and tremendous scalability. Spark is as successful at distributed data processing as Scikit-lean is at ML development.\n\nWe can also do the machine learning studies with Pandas and Scikit-learn duo with Spark. Because Spark also has an ML library and is inspired by scikit-learn. However, due to the difficulty of writing many ML algorithms as distributed, the Spark ML richness is less than scikit-learn.\n\nThe problem this article is trying to solve is this: I developed the model with Pandas and Scikit-learn duo, but the environment in which the model will be used is a big data environment and Spark is there. Can I use this sklearn model on the Spark data frame? Yes, you can, and in a distributed manner. With Sparkâ€™s broadcast feature, the model is distributed to all executors and each piece of data (partition) goes through the model for prediction in parallel. Therefore, the situation here is about making predictions with the model rather than using the advantages of Spark in model training. Below we see the figure that will help us understand.\n\nThe sklearn logo in Figure-1 represents the sklearn model sent to each executor via spark broadcast. It makes the prediction simultaneously with many parts on 3 executors.\n\nIn this blog post, we will present an example of deploying scikit-learn models on Spark. In this example, we will use Sparkâ€™s broadcast feature to enable the model to be used for prediction on distributed datasets in parallel. These examples and practical tips will help you easily deploy your scikit-learn models with Spark.\n\nWe will use the adult dataset. It is a classification problem that aims to predict whose income is greater or lower than 50K USD. The dataset also has numeric and categorical type columns. Categorical data requires special preprocessing for model development as well as prediction.\n\nScikit-learn pipelines are a powerful tool for streamlining machine-learning workflows. They allow you to chain together multiple steps involved in data preprocessing and model training into a single, cohesive unit. Hereâ€™s a quick breakdown:\n\nEfficiency: Combines all steps into one process, reducing code and potential errors.\n\nReadability: Makes your code more organized and easier to understand.\n\nReproducibility: Ensures consistency in how data is processed for training and prediction.\n\nGridSearchCV compatibility: Allows joint parameter selection for all steps in the pipeline.\n\nEstimators: These are the processing steps, like data scaling or feature selection.\n\nTransformers: These modify the data without creating a new copy, often used for feature engineering.\n\nPredictor: This is the final step, typically a machine learning model used for making predictions.\n\nA pipeline consists of a sequence of estimators and transformers, followed by a single predictor at the end.\n\nAll steps except the final predictor must be transformers (methods that transform data).\n\nOverall, scikit-learn pipelines offer a structured way to handle the entire machine-learning process, from data preparation to model building. This makes your code more efficient, maintainable, and reliable.\n\nNow we have trained pipeline including encode, scaler and estimator.\n\nSince this is imbalanced data letâ€™s use ROC rather than accuracy.\n\nLetâ€™s create raw data for prediction apart from the original dataset.\n\nSave the model to the disk\n\nYou donâ€™t have to do this but usually we do, furthermore version it. Now letâ€™s save the model and then load it again.\n\nNow we will read the data to be predicted with spark. First create spark session and context.\n\nRead data from the GitHub repo and see the first five rows. For simplicity we will not use all columns but two categorical, 6 numeric, and target columns.\n\nThis function will get a spark dataframe including input columns, scikit-learn model, and spark context then will return a column that has predictions.\n\nHere udf decorator is used. For more information please review this post.\n\nNow prediction by calling the function.\n\nPredictions are now part of the Spark DataFrame.\n\nIf you want to improve yourself for model deployment and more, MLOps Videocamp (Turkish) is for you.\n\nThatâ€™s all for this article. Goodbye until next timeâ€¦"
    }
]