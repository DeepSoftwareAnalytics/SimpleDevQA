[
    {
        "link": "https://docs.python.org/3/tutorial/errors.html",
        "document": "Until now error messages haven‚Äôt been more than mentioned, but if you have tried out the examples you have probably seen some. There are (at least) two distinguishable kinds of errors: syntax errors and exceptions.\n\nSyntax errors, also known as parsing errors, are perhaps the most common kind of complaint you get while you are still learning Python: The parser repeats the offending line and displays little arrows pointing at the place where the error was detected. Note that this is not always the place that needs to be fixed. In the example, the error is detected at the function , since a colon ( ) is missing just before it. The file name ( in our example) and line number are printed so you know where to look in case the input came from a file.\n\nEven if a statement or expression is syntactically correct, it may cause an error when an attempt is made to execute it. Errors detected during execution are called exceptions and are not unconditionally fatal: you will soon learn how to handle them in Python programs. Most exceptions are not handled by programs, however, and result in error messages as shown here: File , line , in : File , line , in : name 'spam' is not defined File , line , in : can only concatenate str (not \"int\") to str The last line of the error message indicates what happened. Exceptions come in different types, and the type is printed as part of the message: the types in the example are , and . The string printed as the exception type is the name of the built-in exception that occurred. This is true for all built-in exceptions, but need not be true for user-defined exceptions (although it is a useful convention). Standard exception names are built-in identifiers (not reserved keywords). The rest of the line provides detail based on the type of exception and what caused it. The preceding part of the error message shows the context where the exception occurred, in the form of a stack traceback. In general it contains a stack traceback listing source lines; however, it will not display lines read from standard input. Built-in Exceptions lists the built-in exceptions and their meanings.\n\nIt is possible to write programs that handle selected exceptions. Look at the following example, which asks the user for input until a valid integer has been entered, but allows the user to interrupt the program (using - or whatever the operating system supports); note that a user-generated interruption is signalled by raising the exception. \"Oops! That was no valid number. Try again...\" The statement works as follows.\n‚Ä¢ None First, the try clause (the statement(s) between the and keywords) is executed.\n‚Ä¢ None If no exception occurs, the except clause is skipped and execution of the statement is finished.\n‚Ä¢ None If an exception occurs during execution of the clause, the rest of the clause is skipped. Then, if its type matches the exception named after the keyword, the except clause is executed, and then execution continues after the try/except block.\n‚Ä¢ None If an exception occurs which does not match the exception named in the except clause, it is passed on to outer statements; if no handler is found, it is an unhandled exception and execution stops with an error message. A statement may have more than one except clause, to specify handlers for different exceptions. At most one handler will be executed. Handlers only handle exceptions that occur in the corresponding try clause, not in other handlers of the same statement. An except clause may name multiple exceptions as a parenthesized tuple, for example: A class in an clause matches exceptions which are instances of the class itself or one of its derived classes (but not the other way around ‚Äî an except clause listing a derived class does not match instances of its base classes). For example, the following code will print B, C, D in that order: Note that if the except clauses were reversed (with first), it would have printed B, B, B ‚Äî the first matching except clause is triggered. When an exception occurs, it may have associated values, also known as the exception‚Äôs arguments. The presence and types of the arguments depend on the exception type. The except clause may specify a variable after the exception name. The variable is bound to the exception instance which typically has an attribute that stores the arguments. For convenience, builtin exception types define to print all the arguments without explicitly accessing . # __str__ allows args to be printed directly, # but may be overridden in exception subclasses The exception‚Äôs output is printed as the last part (‚Äòdetail‚Äô) of the message for unhandled exceptions. is the common base class of all exceptions. One of its subclasses, , is the base class of all the non-fatal exceptions. Exceptions which are not subclasses of are not typically handled, because they are used to indicate that the program should terminate. They include which is raised by and which is raised when a user wishes to interrupt the program. can be used as a wildcard that catches (almost) everything. However, it is good practice to be as specific as possible with the types of exceptions that we intend to handle, and to allow any unexpected exceptions to propagate on. The most common pattern for handling is to print or log the exception and then re-raise it (allowing a caller to handle the exception as well): \"Could not convert data to an integer.\" The ‚Ä¶ statement has an optional else clause, which, when present, must follow all except clauses. It is useful for code that must be executed if the try clause does not raise an exception. For example: The use of the clause is better than adding additional code to the clause because it avoids accidentally catching an exception that wasn‚Äôt raised by the code being protected by the ‚Ä¶ statement. Exception handlers do not handle only exceptions that occur immediately in the try clause, but also those that occur inside functions that are called (even indirectly) in the try clause. For example:\n\nThe statement allows the programmer to force a specified exception to occur. For example: The sole argument to indicates the exception to be raised. This must be either an exception instance or an exception class (a class that derives from , such as or one of its subclasses). If an exception class is passed, it will be implicitly instantiated by calling its constructor with no arguments: If you need to determine whether an exception was raised but don‚Äôt intend to handle it, a simpler form of the statement allows you to re-raise the exception:\n\nIf an unhandled exception occurs inside an section, it will have the exception being handled attached to it and included in the error message: File , line , in : [Errno 2] No such file or directory: 'database.sqlite' During handling of the above exception, another exception occurred: File , line , in : To indicate that an exception is a direct consequence of another, the statement allows an optional clause: # exc must be exception instance or None. This can be useful when you are transforming exceptions. For example: File , line , in File , line , in The above exception was the direct cause of the following exception: File , line , in : It also allows disabling automatic exception chaining using the idiom: For more information about chaining mechanics, see Built-in Exceptions.\n\nThe statement has another optional clause which is intended to define clean-up actions that must be executed under all circumstances. For example: If a clause is present, the clause will execute as the last task before the statement completes. The clause runs whether or not the statement produces an exception. The following points discuss more complex cases when an exception occurs:\n‚Ä¢ None If an exception occurs during execution of the clause, the exception may be handled by an clause. If the exception is not handled by an clause, the exception is re-raised after the clause has been executed.\n‚Ä¢ None An exception could occur during execution of an or clause. Again, the exception is re-raised after the clause has been executed.\n‚Ä¢ None If the clause executes a , or statement, exceptions are not re-raised.\n‚Ä¢ None If the statement reaches a , or statement, the clause will execute just prior to the , or statement‚Äôs execution.\n‚Ä¢ None If a clause includes a statement, the returned value will be the one from the clause‚Äôs statement, not the value from the clause‚Äôs statement. As you can see, the clause is executed in any event. The raised by dividing two strings is not handled by the clause and therefore re-raised after the clause has been executed. In real world applications, the clause is useful for releasing external resources (such as files or network connections), regardless of whether the use of the resource was successful.\n\nSome objects define standard clean-up actions to be undertaken when the object is no longer needed, regardless of whether or not the operation using the object succeeded or failed. Look at the following example, which tries to open a file and print its contents to the screen. The problem with this code is that it leaves the file open for an indeterminate amount of time after this part of the code has finished executing. This is not an issue in simple scripts, but can be a problem for larger applications. The statement allows objects like files to be used in a way that ensures they are always cleaned up promptly and correctly. After the statement is executed, the file f is always closed, even if a problem was encountered while processing the lines. Objects which, like files, provide predefined clean-up actions will indicate this in their documentation.\n\nThere are situations where it is necessary to report several exceptions that have occurred. This is often the case in concurrency frameworks, when several tasks may have failed in parallel, but there are also other use cases where it is desirable to continue execution and collect multiple errors rather than raise the first exception. The builtin wraps a list of exception instances so that they can be raised together. It is an exception itself, so it can be caught like any other exception. By using instead of , we can selectively handle only the exceptions in the group that match a certain type. In the following example, which shows a nested exception group, each clause extracts from the group exceptions of a certain type while letting all other exceptions propagate to other clauses and eventually to be reraised. Note that the exceptions nested in an exception group must be instances, not types. This is because in practice the exceptions would typically be ones that have already been raised and caught by the program, along the following pattern:\n\nWhen an exception is created in order to be raised, it is usually initialized with information that describes the error that has occurred. There are cases where it is useful to add information after the exception was caught. For this purpose, exceptions have a method that accepts a string and adds it to the exception‚Äôs notes list. The standard traceback rendering includes all notes, in the order they were added, after the exception. For example, when collecting exceptions into an exception group, we may want to add context information for the individual errors. In the following each exception in the group has a note indicating when this error has occurred. | ExceptionGroup: We have some problems (3 sub-exceptions)"
    },
    {
        "link": "https://docs.python.org/3/library/exceptions.html",
        "document": "In Python, all exceptions must be instances of a class that derives from . In a statement with an clause that mentions a particular class, that clause also handles any exception classes derived from that class (but not exception classes from which it is derived). Two exception classes that are not related via subclassing are never equivalent, even if they have the same name.\n\nThe built-in exceptions listed in this chapter can be generated by the interpreter or built-in functions. Except where mentioned, they have an ‚Äúassociated value‚Äù indicating the detailed cause of the error. This may be a string or a tuple of several items of information (e.g., an error code and a string explaining the code). The associated value is usually passed as arguments to the exception class‚Äôs constructor.\n\nUser code can raise built-in exceptions. This can be used to test an exception handler or to report an error condition ‚Äújust like‚Äù the situation in which the interpreter raises the same exception; but beware that there is nothing to prevent user code from raising an inappropriate error.\n\nThe built-in exception classes can be subclassed to define new exceptions; programmers are encouraged to derive new exceptions from the class or one of its subclasses, and not from . More information on defining exceptions is available in the Python Tutorial under User-defined Exceptions.\n\nThree attributes on exception objects provide information about the context in which the exception was raised: When raising a new exception while another exception is already being handled, the new exception‚Äôs attribute is automatically set to the handled exception. An exception may be handled when an or clause, or a statement, is used. This implicit exception context can be supplemented with an explicit cause by using with : The expression following must be an exception or . It will be set as on the raised exception. Setting also implicitly sets the attribute to , so that using effectively replaces the old exception with the new one for display purposes (e.g. converting to ), while leaving the old exception available in for introspection when debugging. The default traceback display code shows these chained exceptions in addition to the traceback for the exception itself. An explicitly chained exception in is always shown when present. An implicitly chained exception in is shown only if is and is false. In either case, the exception itself is always shown after any chained exceptions so that the final line of the traceback always shows the last exception that was raised.\n\nThe following are used when it is necessary to raise multiple unrelated exceptions. They are part of the exception hierarchy so they can be handled with like all other exceptions. In addition, they are recognised by , which matches their subgroups based on the types of the contained exceptions. Both of these exception types wrap the exceptions in the sequence . The parameter must be a string. The difference between the two classes is that extends and it can wrap any exception, while extends and it can only wrap subclasses of . This design is so that catches an but not . The constructor returns an rather than a if all contained exceptions are instances, so it can be used to make the selection automatic. The constructor, on the other hand, raises a if any contained exception is not an subclass. The argument to the constructor. This is a read-only attribute. A tuple of the exceptions in the sequence given to the constructor. This is a read-only attribute. Returns an exception group that contains only the exceptions from the current group that match condition, or if the result is empty. The condition can be an exception type or tuple of exception types, in which case each exception is checked for a match using the same check that is used in an clause. The condition can also be a callable (other than a type object) that accepts an exception as its single argument and returns true for the exceptions that should be in the subgroup. The nesting structure of the current exception is preserved in the result, as are the values of its , , , and fields. Empty nested groups are omitted from the result. The condition is checked for all exceptions in the nested exception group, including the top-level and any nested exception groups. If the condition is true for such an exception group, it is included in the result in full. Added in version 3.13: can be any callable which is not a type object. Like , but returns the pair where is and is the remaining non-matching part. Returns an exception group with the same , but which wraps the exceptions in . This method is used by and , which are used in various contexts to break up an exception group. A subclass needs to override it in order to make and return instances of the subclass rather than . and copy the , , and fields from the original exception group to the one returned by , so these fields do not need to be updated by . Note that defines , so subclasses that need a different constructor signature need to override that rather than . For example, the following defines an exception group subclass which accepts an exit_code and and constructs the group‚Äôs message from it. Like , any subclass of which is also a subclass of can only wrap instances of ."
    },
    {
        "link": "https://digitalocean.com/community/tutorials/python-valueerror-exception-handling-examples",
        "document": "Python ValueError is raised when a function receives an argument of the correct type but an inappropriate value. Also, the situation should not be described by a more precise exception such as IndexError.\n\nYou will get ValueError with mathematical operations, such as square root of a negative number.\n\nHere is a simple example to handle ValueError exception using try-except block.\n\nHere is the output of the program with different types of input.\n\nOur program can raise ValueError in int() and math.sqrt() functions. So, we can create a nested try-except block to handle both of them. Here is the updated snippet to take care of all the ValueError scenarios.\n\nHere is a simple example where we are raising ValueError for input argument of correct type but inappropriate value."
    },
    {
        "link": "https://stackoverflow.com/questions/2052390/manually-raising-throwing-an-exception-in-python",
        "document": "Use the most specific Exception constructor that semantically fits your issue.\n\nBe specific in your message, e.g.:\n\nAvoid raising a generic . To catch it, you'll have to catch all other more specific exceptions that subclass it.\n\nAnd more specific catches won't catch the general exception:\n\nInstead, use the most specific Exception constructor that semantically fits your issue.\n\nwhich also handily allows an arbitrary number of arguments to be passed to the constructor:\n\nThese arguments are accessed by the attribute on the object. For example:\n\nIn Python 2.5, an actual attribute was added to in favor of encouraging users to subclass Exceptions and stop using , but the introduction of and the original deprecation of args has been retracted.\n\nWhen inside an except clause, you might want to, for example, log that a specific type of error happened, and then re-raise. The best way to do this while preserving the stack trace is to use a bare raise statement. For example:\n\nDon't modify your errors... but if you insist.\n\nYou can preserve the stacktrace (and error value) with , but this is way more error prone and has compatibility problems between Python 2 and 3, prefer to use a bare to re-raise.\n\nTo explain - the returns the type, value, and traceback.\n\nThis is the syntax in Python 2 - note this is not compatible with Python 3:\n\nIf you want to, you can modify what happens with your new raise - e.g. setting new for the instance:\n\nAnd we have preserved the whole traceback while modifying the args. Note that this is not a best practice and it is invalid syntax in Python 3 (making keeping compatibility much harder to work around).\n\nAgain: avoid manually manipulating tracebacks. It's less efficient and more error prone. And if you're using threading and you may even get the wrong traceback (especially if you're using exception handling for control flow - which I'd personally tend to avoid.)\n\nIn Python 3, you can chain Exceptions, which preserve tracebacks:\n‚Ä¢ this does allow changing the error type raised, and\n‚Ä¢ this is not compatible with Python 2.\n\nThese can easily hide and even get into production code. You want to raise an exception, and doing them will raise an exception, but not the one intended!\n\nValid in Python 2, but not in Python 3 is the following:\n\nOnly valid in much older versions of Python (2.4 and lower), you may still see people raising strings:\n\nIn all modern versions, this will actually raise a , because you're not raising a type. If you're not checking for the right exception and don't have a reviewer that's aware of the issue, it could get into production.\n\nI raise Exceptions to warn consumers of my API if they're using it incorrectly:\n\nCreate your own error types when apropos\n\nYou can create your own error types, if you want to indicate something specific is wrong with your application, just subclass the appropriate point in the exception hierarchy:"
    },
    {
        "link": "https://docs.python.org/3/c-api/exceptions.html",
        "document": "The functions described in this chapter will let you handle and raise Python exceptions. It is important to understand some of the basics of Python exception handling. It works somewhat like the POSIX variable: there is a global indicator (per thread) of the last error that occurred. Most C API functions don‚Äôt clear this on success, but will set it to indicate the cause of the error on failure. Most C API functions also return an error indicator, usually if they are supposed to return a pointer, or if they return an integer (exception: the functions return for success and for failure).\n\nConcretely, the error indicator consists of three object pointers: the exception‚Äôs type, the exception‚Äôs value, and the traceback object. Any of those pointers can be if non-set (although some combinations are forbidden, for example you can‚Äôt have a non- traceback if the exception type is ).\n\nWhen a function must fail because some function it called failed, it generally doesn‚Äôt set the error indicator; the function it called already set it. It is responsible for either handling the error and clearing the exception or returning after cleaning up any resources it holds (such as object references or memory allocations); it should not continue normally if it is not prepared to handle the error. If returning due to an error, it is important to indicate to the caller that an error has been set. If the error is not handled or carefully propagated, additional calls into the Python/C API may not behave as intended and may fail in mysterious ways.\n\nUse these functions to issue warnings from C code. They mirror similar functions exported by the Python module. They normally print a warning message to sys.stderr; however, it is also possible that the user has specified that warnings are to be turned into errors, and in that case they will raise an exception. It is also possible that the functions raise an exception because of a problem with the warning machinery. The return value is if no exception is raised, or if an exception is raised. (It is not possible to determine whether a warning message is actually printed, nor what the reason is for the exception; this is intentional.) If an exception is raised, the caller should do its normal exception handling (for example, owned references and return an error value). Part of the . Issue a warning message. The category argument is a warning category (see below) or ; the message argument is a UTF-8 encoded string. stack_level is a positive number giving a number of stack frames; the warning will be issued from the currently executing line of code in that stack frame. A stack_level of 1 is the function calling , 2 is the function above that, and so forth. Warning categories must be subclasses of ; is a subclass of ; the default warning category is . The standard Python warning categories are available as global variables whose names are enumerated at Standard Warning Categories. For information about warning control, see the documentation for the module and the option in the command line documentation. There is no C API for warning control. Issue a warning message with explicit control over all warning attributes. This is a straightforward wrapper around the Python function ; see there for more information. The module and registry arguments may be set to to get the default effect described there. Part of the . Similar to except that message and module are UTF-8 encoded strings, and filename is decoded from the filesystem encoding and error handler. Part of the . Function similar to , but use to format the warning message. format is an ASCII-encoded string. Part of the since version 3.6. Function similar to , but category is and it passes source to .\n\nReturn value: Borrowed reference. Part of the . Test whether the error indicator is set. If set, return the exception type (the first argument to the last call to one of the functions or to ). If not set, return . You do not own a reference to the return value, so you do not need to it. The caller must hold the GIL. Do not compare the return value to a specific exception; use instead, shown below. (The comparison could easily fail since the exception may be an instance instead of a class, in the case of a class exception, or it may be a subclass of the expected exception.) Part of the . Equivalent to . This should only be called when an exception is actually set; a memory access violation will occur if no exception has been raised. Part of the . Return true if the given exception matches the exception type in exc. If exc is a class object, this also returns true when given is an instance of a subclass. If exc is a tuple, all exception types in the tuple (and recursively in subtuples) are searched for a match. Return value: New reference. Part of the since version 3.12. Return the exception currently being raised, clearing the error indicator at the same time. Return if the error indicator is not set. This function is used by code that needs to catch exceptions, or code that needs to save and restore the error indicator temporarily. /* ... code that might produce other errors ... */ , to save the exception currently being handled. Part of the since version 3.12. Set exc as the exception currently being raised, clearing the existing exception if one is set. This call steals a reference to exc, which must be a valid exception. Part of the . Deprecated since version 3.12: Use instead. Retrieve the error indicator into three variables whose addresses are passed. If the error indicator is not set, set all three variables to . If it is set, it will be cleared and you own a reference to each object retrieved. The value and traceback object may be even when the type object is not. This function is normally only used by legacy code that needs to catch exceptions or save and restore the error indicator temporarily. /* ... code that might produce other errors ... */ Part of the . Deprecated since version 3.12: Use instead. Set the error indicator from the three objects, type, value, and traceback, clearing the existing exception if one is set. If the objects are , the error indicator is cleared. Do not pass a type and non- value or traceback. The exception type should be a class. Do not pass an invalid exception type or value. (Violating these rules will cause subtle problems later.) This call takes away a reference to each object: you must own a reference to each object before the call and after the call you no longer own these references. (If you don‚Äôt understand this, don‚Äôt use this function. I warned you.) This function is normally only used by legacy code that needs to save and restore the error indicator temporarily. Use to save the current error indicator. Part of the . Deprecated since version 3.12: Use instead, to avoid any possible de-normalization. Under certain circumstances, the values returned by below can be ‚Äúunnormalized‚Äù, meaning that is a class object but is not an instance of the same class. This function can be used to instantiate the class in that case. If the values are already normalized, nothing happens. The delayed normalization is implemented to improve performance. This function does not implicitly set the attribute on the exception value. If setting the traceback appropriately is desired, the following additional snippet is needed: Part of the since version 3.11. Retrieve the active exception instance, as would be returned by . This refers to an exception that was already caught, not to an exception that was freshly raised. Returns a new reference to the exception or . Does not modify the interpreter‚Äôs exception state. This function is not normally used by code that wants to handle exceptions. Rather, it can be used when code needs to save and restore the exception state temporarily. Use to restore or clear the exception state. Part of the since version 3.11. Set the active exception, as known from . This refers to an exception that was already caught, not to an exception that was freshly raised. To clear the exception state, pass . This function is not normally used by code that wants to handle exceptions. Rather, it can be used when code needs to save and restore the exception state temporarily. Use to get the exception state. Part of the since version 3.7. Retrieve the old-style representation of the exception info, as known from . This refers to an exception that was already caught, not to an exception that was freshly raised. Returns new references for the three objects, any of which may be . Does not modify the exception info state. This function is kept for backwards compatibility. Prefer using . This function is not normally used by code that wants to handle exceptions. Rather, it can be used when code needs to save and restore the exception state temporarily. Use to restore or clear the exception state. Part of the since version 3.7. Set the exception info, as known from . This refers to an exception that was already caught, not to an exception that was freshly raised. This function steals the references of the arguments. To clear the exception state, pass for all three arguments. This function is kept for backwards compatibility. Prefer using . This function is not normally used by code that wants to handle exceptions. Rather, it can be used when code needs to save and restore the exception state temporarily. Use to read the exception state. Changed in version 3.11: The and arguments are no longer used and can be NULL. The interpreter now derives them from the exception instance (the argument). The function still steals references of all three arguments.\n\nPart of the . If the function is called from the main thread and under the main Python interpreter, it checks whether a signal has been sent to the processes and if so, invokes the corresponding signal handler. If the module is supported, this can invoke a signal handler written in Python. The function attempts to handle all pending signals, and then returns . However, if a Python signal handler raises an exception, the error indicator is set and the function returns immediately (such that other pending signals may not have been handled yet: they will be on the next invocation). If the function is called from a non-main thread, or under a non-main Python interpreter, it does nothing and returns . This function can be called by long-running C code that wants to be interruptible by user requests (such as by pressing Ctrl-C). The default Python signal handler for raises the exception. Part of the . Simulate the effect of a signal arriving. This is equivalent to . This function is async-signal-safe. It can be called without the GIL and from a C signal handler. Part of the since version 3.10. Simulate the effect of a signal arriving. The next time is called, the Python signal handler for the given signal number will be called. This function can be called by C code that sets up its own signal handling and wants Python signal handlers to be invoked as expected when an interruption is requested (for example when the user presses Ctrl-C to interrupt an operation). If the given signal isn‚Äôt handled by Python (it was set to or ), it will be ignored. If signum is outside of the allowed range of signal numbers, is returned. Otherwise, is returned. The error indicator is never changed by this function. This function is async-signal-safe. It can be called without the GIL and from a C signal handler. This utility function specifies a file descriptor to which the signal number is written as a single byte whenever a signal is received. fd must be non-blocking. It returns the previous such file descriptor. The value disables the feature; this is the initial state. This is equivalent to in Python, but without any error checking. fd should be a valid file descriptor. The function should only be called from the main thread. Changed in version 3.5: On Windows, the function now also supports socket handles.\n\nThese two functions provide a way to perform safe recursive calls at the C level, both in the core and in extension modules. They are needed if the recursive code does not necessarily invoke Python code (which tracks its recursion depth automatically). They are also not needed for tp_call implementations because the call protocol takes care of recursion handling. Part of the since version 3.9. Marks a point where a recursive C-level call is about to be performed. If is defined, this function checks if the OS stack overflowed using . If this is the case, it sets a and returns a nonzero value. The function then checks if the recursion limit is reached. If this is the case, a is set and a nonzero value is returned. Otherwise, zero is returned. where should be a UTF-8 encoded string such as to be concatenated to the message caused by the recursion depth limit. Changed in version 3.9: This function is now also available in the limited API. Part of the since version 3.9. Ends a . Must be called once for each successful invocation of . Changed in version 3.9: This function is now also available in the limited API. Properly implementing for container types requires special recursion handling. In addition to protecting the stack, also needs to track objects to prevent cycles. The following two functions facilitate this functionality. Effectively, these are the C equivalent to . Part of the . Called at the beginning of the implementation to detect cycles. If the object has already been processed, the function returns a positive integer. In that case the implementation should return a string object indicating a cycle. As examples, objects return and objects return . The function will return a negative integer if the recursion limit is reached. In that case the implementation should typically return . Otherwise, the function returns zero and the implementation can continue normally. Part of the . Ends a . Must be called once for each invocation of that returns zero."
    },
    {
        "link": "https://geeksforgeeks.org/how-to-calculate-jaccard-similarity-in-python",
        "document": "In Data Science, Similarity measurements between the two sets are a crucial task. Jaccard Similarity is one of the widely used techniques for similarity measurements in machine learning, natural language processing and recommendation systems. This article explains what Jaccard similarity is, why it is important, and how to compute it with Python.\n\nJaccard Similarity also known as Jaccard index, is a statistic to measure the similarity between two data sets. It is measured as the size of the intersection of two sets divided by the size of their union.\n\nFor example: Given two sets A and B, their Jaccard Similarity is provided by,\n‚Ä¢ None is the cardinality (size) of the intersection of sets A and B.\n‚Ä¢ None is the cardinality (size) of the union of sets A and B.\n\nJaccard Similarity is also known as the Jaccard index or Jaccard coefficient, its values lie between 0 and 1. where 0 means no similarity and the values get closer to 1 means increasing similarity 1 means the same datasets.\n\nThe Jaccard similarity can be used to compare the similarity of two sets of words, which are frequently represented as sets of unique terms.\n\nThe Jaccard similarity is especially effective when the order of items is irrelevant and only the presence or absence of elements is examined. It is extensively used in:\n‚Ä¢ None Text Analysis: Jaccard similarity can be used in natural language processing to compare texts, text samples, or even individual words.\n‚Ä¢ None Recommendation Systems: Jaccard similarity can help in finding similar items or products based on user behavior.\n‚Ä¢ None Data Deduplication: Jaccard similarity can be used to find duplicate or near-duplicate records in a dataset.\n‚Ä¢ None Social Network Analysis: Jaccard similarity can be used in social networks to detect similarities between user profiles or groups.\n‚Ä¢ None Genomics: Jaccard similarity is employed to compare gene sets in biological studies.\n\nThe Jaccard distance is a measure of how different two sets are i.e Unlike the Jaccard coefficient, which determines the similarity of two sets. The Jaccard distance is computed by subtracting the Jaccard coefficient from one, or by dividing the difference in the sizes of the union and the intersection of two sets by the size of the union.\n‚Ä¢ None is the cardinality (size) of the intersection of sets A and B.\n‚Ä¢ None is the cardinality (size) of the union of sets A and B.\n‚Ä¢ None represents the cardinality (size) of symmetric difference of sets (A) and (B), containing elements that are in either set but not in their intersection.\n\nThe Jaccard distance is often used to calculate a nxn matrix For clustering and multidimensional scaling of n sample sets. This distance is a collection metric for all finite sets.\n\nSuppose two persons, A and B, went shopping in a department store, and there are five items. Let A = {1, 1,1, 0,1} and B = {1, 1, 0, 0, 1} sets represent items they picked (1) or not (0). Then ‚ÄòJaccard score‚Äô will represent the similar items they bought, and Jaccard Distance measure of dissimilarity and is calculated as 1 minus the Jaccard similarity score:\n\nThe Jaccard similarity coefficient is a useful tool to check the similarity of sets, with applications ranging from text analysis to recommendation systems to data deduplication. You may quickly compute Jaccard similarity to improve your data analysis and decision-making processes by learning the formula and employing Python‚Äôs capabilities."
    },
    {
        "link": "https://stackoverflow.com/questions/50770858/python-how-to-compute-jaccard-similarity-more-quickly",
        "document": "There are about 98,000 sentences (length from 5 - 100 words) in and about 1000 sentences (length from 5 - 100 words) in . For each sentence in I want to find if it's plagiarized from a sentence in . If the sentence is plagiarized, I should return the id in lst_train or else null.\n\nNow I want to compute the jaccard similarity of each sentence in relative to each sentence in . Here's my code, b.JaccardSim computes two sentences' jaccard similarity:\n\nBut I found that each time of computing one sentence with each sentence in lst_train is more than 1 minutes. Since there are about 1000 sentences, it may take about 1000 minutes to finish it. It is too long.\n\nDo you guys know how to make the computing speed faster or better method to solve the issue to detect the sentence is plagiarized from a sentence in lst_train?"
    },
    {
        "link": "https://koshurai.medium.com/exploring-jaccard-similarity-a-powerful-tool-for-similarity-analysis-in-python-6767ed21377a",
        "document": "In the realm of data analysis and machine learning, similarity measures play a crucial role in comparing and understanding the relationships between datasets. One such measure that stands out for its simplicity and effectiveness is the Jaccard similarity coefficient. In this article, we‚Äôll delve into what Jaccard similarity is, how it works, and how it can be implemented using Python. Whether you‚Äôre a beginner or an experienced data scientist, understanding Jaccard similarity can significantly enhance your analytical capabilities.\n\nWhat is Jaccard Similarity? The Jaccard similarity coefficient, named after the French mathematician Paul Jaccard, is a statistic used for comparing the similarity and diversity of sample sets. It measures the similarity between two sets by calculating the ratio of the intersection of the sets to the union of the sets. In simpler terms, it quantifies the overlap between two datasets, disregarding the order and duplicates of elements.\n\nHow Does it Work?\n\nTo calculate the Jaccard similarity between two sets A and B, we divide the size of their intersection by the size of their union:\n\nThe resulting coefficient ranges from 0 to 1, with 0 indicating no overlap between the sets and 1 indicating complete similarity.\n\nApplications of Jaccard Similarity: Jaccard similarity finds applications across various domains, including:\n‚Ä¢ Social Network Analysis: Assessing the similarity of user profiles or networks.\n\nIn conclusion, Jaccard similarity is a versatile and powerful tool for comparing datasets and identifying similarities. By understanding its principles and leveraging Python libraries, data scientists can gain valuable insights into their data and develop more accurate models and analyses. Incorporating Jaccard similarity into your toolkit opens up a world of possibilities for exploring and understanding complex datasets.\n\nIf you found this article insightful, consider following me for more informative content on data science and machine learning. Let‚Äôs explore the fascinating world of data together!"
    },
    {
        "link": "https://newscatcherapi.com/blog/ultimate-guide-to-text-similarity-with-python",
        "document": "Learn the different similarity measures and text embedding techniques. Play around with code examples and develop a general intuition.\n\nIn this article, you will learn about different similarity metrics and text embedding techniques. By the end, you'll have a good grasp of when to use what metrics and embedding techniques. You‚Äôll also get to play around with them to help establish a general intuition.\n\nYou can find the accompanying web app here.\n\nSimilarity is the distance between two vectors where the vector dimensions represent the features of two objects. In simple terms, similarity is the measure of how different or alike two data objects are. If the distance is small, the objects are said to have a high degree of similarity and vice versa. Generally, it is measured in the range 0 to 1. This score in the range of [0, 1] is called the similarity score.\n\nAn important point to remember about similarity is that it‚Äôs subjective and highly dependent on the domain and use case. For example, two cars can be similar because of simple things like the manufacturing company, color, price range, or technical details like fuel type, wheelbase, horsepower. So, special care should be taken when calculating similarity across features that are unrelated to each other or not relevant to the problem.\n\nAs simple as the idea may be, similarity forms the basis of many machine learning techniques. For instance, the K-Nearest-Neighbors classifier uses similarity to classify new data objects, similarly, K-means clustering utilizes similarity measures to assign data points to appropriate clusters. Even recommendation engines use neighborhood-based collaborative filtering methods which use similarity to identify a user‚Äôs neighbors.\n\nThe use of similarity measures is quite prominent in the field of natural language processing. Everything from information retrieval systems, search engines, paraphrase detection to text classification, automated document linking, spell correction makes use of similarity measures.\n\nTake a look at the following sentences:\n\nAs humans, it is very obvious to us that the two sentences mean the same thing despite being written in completely different formats. But how do we make an algorithm come to that same conclusion?\n\nThe first part of this problem is representation. How do we represent the text? We could leave the text as it is or convert it into feature vectors using a suitable text embedding technique. Once we have the text representation, we can compute the similarity score using one of the many distance/similarity measures.\n\nLet‚Äôs dive deeper into the two aspects of the problem, starting with the similarity measures.\n\nJaccard index, also known as Jaccard similarity coefficient, treats the data objects like sets. It is defined as the size of the intersection of two sets divided by the size of the union. Let‚Äôs continue with our previous example:\n\nSentence 2: There is nothing in the bottle.\n\nTo calculate the similarity using Jaccard similarity, we will first perform text normalization to reduce words their roots/lemmas. There are no words to reduce in the case of our example sentences, so we can move on to the next part. Drawing a Venn diagram of the sentences we get:\n\nSize of the intersection of the two sets: 3\n\nSize of the union of the two sets: 1+3+3 = 7\n\nUsing the Jaccard index, we get a similarity score of 3/7 = 0.42\n\nTesting the function for our example sentences\n\nEuclidean distance, or L2 norm, is the most commonly used form of the Minkowski distance. Generally speaking, when people talk about distance, they refer to Euclidean distance. It uses the Pythagoras theorem to calculate the distance between two points as indicated in the figure below:\n\nThe larger the distance d between two vectors, the lower the similarity score and vice versa.\n\nLet‚Äôs compute the similarity between our example statements using Euclidean distance:\n\nTo compute the Euclidean distance we need vectors, so we‚Äôll use spaCy‚Äôs in-built Word2Vec model to create text embeddings. (We‚Äôll learn more about this later in the article)\n\nOkay, so we have the Euclidean distance of 1.86, but what does that mean? See, the problem with using distance is that it‚Äôs hard to make sense if there is nothing to compare to. The distances can vary from 0 to infinity, we need to use some way to normalize them to the range of 0 to 1.\n\nAlthough we have our typical normalization formula that uses mean and standard deviation, it is sensitive to outliers. That means if there are a few extremely large distances, every other distance will become smaller as a consequence of the normalization operation. So the best option here is to use something like the Euler‚Äôs constant as follows:\n\nCosine Similarity computes the similarity of two vectors as the cosine of the angle between two vectors. It determines whether two vectors are pointing in roughly the same direction. So if the angle between the vectors is 0 degrees, then the cosine similarity is 1.\n\nIt is given as:\n\nWhere ||v|| represents the length of the vector v, ùúÉ denotes the angle between v and w, and ‚Äò.‚Äô denotes the dot product operator.\n\nWhat Metric To Use?\n\nJaccard similarity takes into account only the set of unique words for each text document. This makes it the likely candidate for assessing the similarity of documents when repetition is not an issue. A prime example of such an application is comparing product descriptions. For instance, if a term like ‚ÄúHD‚Äù or ‚Äúthermal efficiency‚Äù is used multiple times in one description and just once in another, the Euclidean distance and cosine similarity would drop. On the other hand, if the total number of unique words stays the same, the Jaccard similarity will remain unchanged.\n\nBoth Euclidean and cosine similarity metrics drop if an additional ‚Äòempty‚Äô is added to our first example sentence:\n\nThat being said, Jaccard similarity is rarely used when working with text data as it does not work with text embeddings. This means that is limited to assessing the lexical similarity of text, i.e., how similar documents are on a word level.\n\nAs far as cosine and Euclidean metrics are concerned, the differentiating factor between the two is that cosine similarity is not affected by the magnitude/length of the feature vectors. Let‚Äôs say we are creating a topic tagging algorithm. If a word (e.g. senate) occurs more frequently in document 1 than it does in document 2, we could assume that document 1 is more related to the topic of Politics. However, it could also be the case that we are working with news articles of different lengths. Then, the word ‚Äòsenate‚Äô probably occurred more in document 1 simply because it was way longer. As we saw earlier when the word ‚Äòempty‚Äô was repeated, cosine similarity is less sensitive to a difference in lengths.\n\nIn addition to that, Euclidean distance doesn‚Äôt work well with the sparse vectors of text embeddings. So cosine similarity is generally preferred over Euclidean distance when working with text data. The only length-sensitive text similarity use case that comes to mind is plagiarism detection.\n\nHumans can easily understand and derive meaning from words, but computers don‚Äôt have this natural neuro-linguistic ability. To make words machine-understandable we need to encode them into a numeric form, so the computer can apply mathematical formulas and operations to make sense of them. Even beyond the task of text similarity, representing documents in the form of numbers and vectors is an active area of study.\n\nSimply put, word embedding is the vector representation of a word. They aim to capture the meaning, context, and semantic relationships of the words. A lot of the word embeddings are created based on the notion of the ‚Äúdistributional hypothesis‚Äù introduced by Zellig Harris: words that are used close to one another typically have the same meaning.\n\nThe most straightforward way to numerically represent words is through the one-hot encoding method. The idea is simple, create a vector with the size of the total number of unique words in the corpora. Each unique word has a unique feature and will be represented by a 1 with 0s everywhere else.\n\nDocuments contain large chunks of text with the possibility of repetition. Simply marking the presence or absence of words leads to loss of information. In the \"bag of words\" representation (also called count vectorizing), each word is represented by its count instead of 1. Regardless of that, both these approaches create huge, sparse vectors that capture absolutely no relational information.\n\nThe scikit-learn module implements this method, let‚Äôs use it to calculate the similarity of the following news headlines:\n\nTo make for better output, let‚Äôs create a function that creates a heatmap of the similarity scores.\n\nNow that we have our data and helper function, we can test countvectorizer\n\nTF-IDF vectors are an extension of the one-hot encoding model. Instead of considering the frequency of words in one document, the frequency of words across the whole corpus is taken into account. The big idea is that words that occur a lot everywhere carry very little meaning or significance. For instance, trivial words like ‚Äúand‚Äù, ‚Äúor‚Äù, ‚Äúis‚Äù don‚Äôt carry as much significance as nouns and proper nouns that occur less frequently.\n\nMathematically, Term Frequency (TF) is the number of times a word appears in a document divided by the total number of words in the document. And Inverse Document Frequency (IDF) = log(N/n) where N is the total number of documents and n is the number of documents a term has appeared in. The TF-IDF value for a word is the product of the term frequency and the inverse document frequency.\n\nAlthough TF-IDF vectors offer a slight improvement over simple count vectorizing, they still have very high dimensionality and don‚Äôt capture semantic relationships.\n\nScikit-learn also offers a `TfidfVectorizer` class for creating TF-IDF vectors from the text.\n\nWord2Vec is a predictive method for forming word embeddings. Unlike the previous methods that need to be ‚Äútrained‚Äù on the working corpus, Word2Vec is a pre-trained two-layer neural network. It takes as input the text corpus and outputs a set of feature vectors that represent words in that corpus. It uses one of two neural network-based methods:\n\nContinuous Bag Of Words takes the context of each word as the input and tries to predict the word corresponding to the context. Here, context simply means the surrounding words.\n\nFor example, consider the greeting: ‚ÄúHave a nice day‚Äù\n\nLet‚Äôs say we use the word ‚Äònice‚Äô as the input to the neural network and we are trying to predict the word ‚Äòday‚Äô. We will use the one-hot encoding of the input word ‚Äònice‚Äô, then measure and optimize for the output error of the target word ‚Äòday‚Äô. In this process of trying to predict the target word, this shallow network learns its vector representation.\n\nJust like how this model used a single context word to predict the target, it can be extended to use multiple context words to do the same:\n\nSo CBOW generates word representations based on the context words, but there‚Äôs another way to do the same. We can use the target word, i.e., the word we want to generate the representation for, to predict the context.\n\nThat‚Äôs what Skip-gram does. In the process of predicting the context words, the Skip-gram model learns the vector representation of the target word. representations are generated using the context words.\n\nWhen To Use What?\n\nIntuitively, the CBOW task is much simpler as it is using multiple inputs to predict one target while Skip-gram relies on one-word inputs. This is reflected in the faster convergence time of CBOW, in the original paper the authors wrote that CBOW took hours to train, while Skip-gram took 3 days.\n\nContinuing on the same train of thought, CBOW is better at learning syntactic relationships between words while skip-gram is better at understanding the semantic relationships. In practical terms, this means that for a word like ‚Äòdog‚Äô, CBOW would return morphologically similar words like plurals like ‚Äòdogs‚Äô. On the other hand, Skip-gram would consider morphologically different but semantically similar words like ‚Äòcat‚Äô or ‚Äòhamster‚Äô.\n\nAnd lastly, as Skip-gram relies on single-word input it is less sensitive to overfitting frequent words. Because even if some words appear more times during the training they considered one at a time. CBOW is prone to overfit frequent words because they can appear several times in the same set of context words. This characteristic also allows Skip-gram to be more efficient in terms amount of documents required to achieve good performance.\n\nTLDR: Skip-gram works better when working with a small amount of data, focuses on semantic similarity of words, and represents rare words well. On the other hand, CBOW is faster, focuses more on the morphological similarity of words, and needs more data to achieve similar performance.\n\nWord2Vec is used in spaCy to create word vectors, one important thing to note: In order to create word vectors we need larger spaCy models. For example, For example, the medium or large English model, but not the small one. So if we want to use vectors, we will go with a model that ends in 'md' or 'lg'. More details about this can be found here.\n\nInstall spaCy and download one of the larger models:\n\nCreate a pipeline object and use it to create to the Docs for the headlines:\n\nWe can look up the embedding vector for the `Doc` or individual tokens using the `.vector` attribute. Let‚Äôs see what the headline embeddings look like.\n\nThe result is a 300-dimensional vector of the first headline. We can use these vectors to calculate the cosine similarity of the headlines. spaCy `doc` object have their own `similarity` method that calculates the cosine similarity.\n\nSo far all the text embeddings we have covered learn a global word embedding. They first build a vocabulary of all the unique words in the document, then learn similar representations for words that appear together frequently. The issue with such word representations is that the words‚Äô contextual meaning is ignored. In practice, this approach to word representation does not address polysemy or the coexistence of many possible meanings for a given word or phrase. For instance, consider the following statement :\n\n‚ÄúAfter stealing gold from the bank vault, the bank robber was seen fishing on the river bank.‚Äù\n\nTraditional word embedding techniques will only learn one representation for the word ‚Äòbank‚Äô. But ‚Äòbank‚Äô has two different meanings in the sentence and needs to have two different representations in the embedding space. Contextual embedding methods like BERT and ELMo learn sequence-level semantics by considering the sequence of all words in the document. As a result, these techniques learn different representations for polysemous words like ‚Äòbank‚Äô in the above example, based on their context.\n\nELMo computes the embeddings from the internal states of a two-layer bidirectional Language Model (LM), thus the name ‚ÄúELMo‚Äù: Embeddings from Language Models. It assigns each word a representation that is a function of the entire corpus of sentences. ELMo embeddings are a function of all of the internal layers of the biLM. Different layers encode different kinds of information for the same word. For example, the lower levels work well for Part-Of-Speech tagging, while the higher levels are better at dealing with polysemous words.\n\nConcatenating the activations of all layers allows ELMo to combine a wide range of word representations that perform better on downstream tasks. In addition to that, ELMo works on the character level instead of words. This enables it to take advantage of sub-word units to derive meaningful embeddings for even out-of-vocabulary words.\n\nThis means that the way ELMo is used is quite different compared to traditional embedding methods. Instead of having a dictionary of words and their corresponding vectors, ELMo creates embeddings on the fly.\n\nThere are many implementations of ELMo, we‚Äôll be trying out the simpe-elmo module. We‚Äôll also need to download a pre-trained model to create the embeddings\n\nNow, let's create an ElmoModel instance and load the pre-trained model we just downloaded.\n\nThe Tensor‚Äôs second dimension of 92 corresponds to the 92 characters in the sentence. To get the word embeddings we can average the embeddings of the characters for each word. Our main concern is ELMo‚Äôs ability to extract contextual information so let‚Äôs focus only on the three instances of the word ‚Äúbank‚Äù:\n\nWe can now evaluate how similar the three instances are:\n\nSo far we have discussed how word embeddings represent the meaning of the words in a text document. But sometimes we need to go a step further and encode the meaning of the whole sentence to readily understand the context in which the words are used. This sentence representation is important for many downstream tasks. It enables us to understand the meaning of the sentence without calculating the individual embeddings of the words. It also allows us to make comparisons on the sentence level.\n\nUsing simple mathematical manipulation, it is possible to adapt sentence embeddings for tasks such as semantic search, clustering, intent detection, paraphrase detection. In addition to that, cross-lingual sentence embedding models can be used for parallel text mining or translation pair detection. For example, the TAUS Data Marketplace uses a data cleaning technique that uses sentence embeddings to compute the semantic similarity between parallel segments of text in different languages to assess translation quality.\n\nA straightforward approach for creating sentence embeddings is to use a word embedding model to encode all words of the given sentence and take the average of all the word vectors. While this provides a strong baseline, it falls short of capturing information related to word order and other aspects of overall sentence semantics.\n\nThe Doc2Vec model (or Paragraph Vector) extends the idea of the Word2Vec algorithm. The algorithm follows the assumption that a word‚Äôs meaning is given by the words that appear close by. Similar to Word2Vec, Doc2Vec has two variants.\n\nEach word and sentence of the training corpus are one-hot encoded and stored in matrices D and W, respectively. The training process involves passing a sliding window over the sentence, trying to predict the next word based on the previous words and the sentence vector (or Paragraph Matrix in the figure above). This prediction of the next word is done by concatenating the sentence and word vectors and passing the result into a softmax layer. The sentence vectors change with sentences, while the word vectors remain the same. Both are updated during training.\n\nThe inference process also involves the same sliding window approach. The difference is that all the vectors of the models are fixed except the sentence vector. After all the predictions of the next word are computed for a sentence, the sentence embedding is the resultant sentence vector.\n\nThe DBOW model ignores the word order and has a simpler architecture. Each sentence in the training corpus is converted into a one-hot representation. During training, a random sentence is selected from the corpus, and from the sentence, a random number of words. The model tries to predict these words using only the sentence ID, and the sentence vector is updated(Paragraph ID and Paragraph Matrix in the figure).\n\nDuring inference, a new sentence ID is trained with random words from the sentence. The sentence vector is updated in each step, and the resulting sentence vector is the embedding for that sentence.\n\nWhat Variant To Use?\n\nThe DM model takes into account the word order, the DBOW model doesn‚Äôt. Also, the DBOW model doesn‚Äôt use word vectors so the semantics of the words are not preserved. As a result, it‚Äôs harder for it to detect similarities between words. And because of its simpler architecture, the DBOW model requires more training to obtain accurate vectors. The main drawback of the DM model is the time and the resources needed to generate an embedding, which is higher than with the DBOW model.\n\nWhat approach produces better Sentence Embeddings? In the original paper, the authors state that the DM model is ‚Äúconsistently better than‚Äù DBOW. However, later studies showed that the DBOW approach is better for most tasks. For that reason, the implementation in the Gensim of Doc2Vec uses the DBOW approach as the default algorithm.\n\nInstall Gensim, get the ‚Äútext8‚Äù dataset to train the Doc2Vec model.\n\nTag the text data, then use it to build the model vocabulary and train the model.\n\nUse the model to get the sentence embeddings of the headlines and calculate the cosine similarity between them.\n\nMuch like ELMo, there are a few bidirectional LSTM based sentence encoders (InferSent, etc) but LSTMs have certain problems. Firstly, they use a hidden vector to represent the memory at the current state of the input. But for long input sequences such as sentences, a single vector isn't enough to provide all the information needed to predict the next state correctly. This bottleneck of the size of the hidden vector makes LSTM based methods more susceptible to mistakes, as in practice it can only hold information from a limited number of steps back. The Attention mechanism in transformers doesn‚Äôt have this bottleneck issue as it has access to all the previous hidden states for making predictions.\n\nAnother problem with LSTMs is the time to train. As the output is always dependent on the previous input, the training is done sequentially. This makes parallelization harder and results in longer training times. The Transformer architecture parallelized the use of the Attention mechanism in a neural network allowing for lower training time.\n\nTransformer-based general language understanding models perform much better than all their predecessors. When BERT was introduced, it achieved state-of-the-art results in a wide range of tasks such as question answering or language inference with minor tweaks in its architecture. That being said, it has a massive computational overhead. For example, finding the most similar pair of sentences in a collection of 10,000 requires about 50 million inference computations (~65 hours). The structure of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering.\n\nSentence-BERT (SBERT) is a modified BERT network that uses siamese and triplet network structures to derive semantically meaningful sentence embeddings. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT.\n\nWe‚Äôll try out the RoBERTa based models implemented in the sentence-transformer module.\n\nUse it to calculate the headline embeddings and their pairwise similarity.\n\nGoogle‚Äôs Universal Sentence Encoder(USE) leverages a one-to-many multi-tasking learning framework to learn a universal sentence embedding by switching between several tasks. The 6 tasks, skip-thoughts prediction of the next/previous sentence, neural machine translation, constituency parsing, and natural language inference, share the same sentence embedding. This reduces training time greatly while preserving the performance on a variety of transfer tasks.\n\nOne version of the Universal Sentence Encoder model uses a deep average network (DAN) encoder, while another version uses a Transformer.\n\nThe more complicated Transformer architecture performs better than the simpler DAN model on a variety of sentiment and similarity classification tasks. The compute time for the Transformer version increases noticeably as sentence length increases. On the other hand, the compute time for the DAN model stays nearly constant as sentence length increases.\n\nWhat Embedding To Use?\n\nAs a general rule of thumb, traditional embeddings techniques like Word2Vec and Doc2Vec offer good results when the task only requires the global meaning of the text. This is reflected in their outperformance of state-of-the-art deep learning techniques on tasks such as semantic text similarity or paraphrase detection. On the other hand, when the task needs somethings more specific than just the global meaning, take for example sentiment analysis or sequence labeling, more complex contextual methods perform better.\n\nSo always start with a simple and fast method as the baseline before moving on to more complex methods if required."
    },
    {
        "link": "https://medium.com/@mayurdhvajsinhjadeja/jaccard-similarity-34e2c15fb524",
        "document": "Jaccard Similarity is a measure of similarity between two asymmetric binary vectors or we can say a way to find the similarity between two sets. It is a common proximity measurement used to compute the similarity of two items, such as two text documents. The index ranges from 0 to 1. Range closer to 1 means more similarity in two sets of data.\n\nIt is denoted by J and it is also referred as Jaccard Index, Jaccard Coefficient, Jaccard Dissimilarity, and Jaccard Distance. It is frequently used in Data Science and Machine Learning such as Text Mining, E-Commerce, Recommendation System, etc.\n\nIt is calculated by the formula:\n\nJaccard Similarity = (number of observations in both sets) / (number in either set)\n\nIf two datasets share exact same members then their Jaccard Similarity Index will be 1 and if there are no common members then Jaccard Similarity index will be 0.\n\nJaccard Similarity will tell us that how much features are similar to each other in the dataset."
    }
]