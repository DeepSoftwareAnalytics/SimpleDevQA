[
    {
        "link": "https://huggingface.co/docs/transformers/en/model_doc/gpt2",
        "document": "and get access to the augmented documentation experience\n\nOpenAI GPT-2 model was proposed in Language Models are Unsupervised Multitask Learners by Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei and Ilya Sutskever from OpenAI. It‚Äôs a causal (unidirectional) transformer pretrained using language modeling on a very large corpus of ~40 GB of text data.\n\nThe abstract from the paper is the following:\n\nGPT-2 is a large transformer-based language model with 1.5 billion parameters, trained on a dataset[1] of 8 million web pages. GPT-2 is trained with a simple objective: predict the next word, given all of the previous words within some text. The diversity of the dataset causes this simple goal to contain naturally occurring demonstrations of many tasks across diverse domains. GPT-2 is a direct scale-up of GPT, with more than 10X the parameters and trained on more than 10X the amount of data.\n\nWrite With Transformer is a webapp created and hosted by Hugging Face showcasing the generative capabilities of several models. GPT-2 is one of them and is available in five different sizes: small, medium, large, xl and a distilled version of the small checkpoint: distilgpt-2.\n\nThis model was contributed by thomwolf. The original code can be found here.\n‚Ä¢ GPT-2 is a model with absolute position embeddings so it‚Äôs usually advised to pad the inputs on the right rather than the left.\n‚Ä¢ GPT-2 was trained with a causal language modeling (CLM) objective and is therefore powerful at predicting the next token in a sequence. Leveraging this feature allows GPT-2 to generate syntactically coherent text as it can be observed in the run_generation.py example script.\n‚Ä¢ The model can take the past_key_values (for PyTorch) or past (for TF) as input, which is the previously computed key/value attention pairs. Using this (past_key_values or past) value prevents the model from re-computing pre-computed values in the context of text generation. For PyTorch, see past_key_values argument of the GPT2Model.forward() method, or for TF the past argument of the TFGPT2Model.call() method for more information on its usage.\n‚Ä¢ Enabling the scale_attn_by_inverse_layer_idx and reorder_and_upcast_attn flags will apply the training stability improvements from Mistral (for PyTorch only).\n\nThe method can be used to generate text using GPT2 model.\n\nFlash Attention 2 is a faster, optimized version of the attention scores computation which relies on kernels.\n\nFirst, check whether your hardware is compatible with Flash Attention 2. The latest list of compatible hardware can be found in the official documentation. If your hardware is not compatible with Flash Attention 2, you can still benefit from attention kernel optimisations through Better Transformer support covered above.\n\nNext, install the latest version of Flash Attention 2:\n\nTo load a model using Flash Attention 2, we can pass the argument to . We‚Äôll also load the model in half-precision (e.g. ), since it results in almost no degradation to audio quality but significantly lower memory usage and faster inference:\n\nBelow is an expected speedup diagram that compares pure inference time between the native implementation in transformers using checkpoint and the Flash Attention 2 version of the model using a sequence length of 512.\n\nPyTorch includes a native scaled dot-product attention (SDPA) operator as part of . This function encompasses several implementations that can be applied depending on the inputs and the hardware in use. See the official documentation or the GPU Inference page for more information.\n\nSDPA is used by default for when an implementation is available, but you may also set in to explicitly request SDPA to be used.\n\nFor the best speedups, we recommend loading the model in half-precision (e.g. or ).\n\nOn a local benchmark (rtx3080ti-16GB, PyTorch 2.2.1, OS Ubuntu 22.04) using with gpt2-large, we saw the following speedups during training and inference.\n\nA list of official Hugging Face and community (indicated by üåé) resources to help you get started with GPT2. If you‚Äôre interested in submitting a resource to be included here, please feel free to open a Pull Request and we‚Äôll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n‚Ä¢ A blog on how to Finetune a non-English GPT-2 Model with Hugging Face.\n‚Ä¢ A blog on How to generate text: using different decoding methods for language generation with Transformers with GPT-2.\n‚Ä¢ A blog on Faster Text Generation with TensorFlow and XLA with GPT-2.\n‚Ä¢ A blog on How to train a Language Model with Megatron-LM with a GPT-2 model.\n‚Ä¢ A notebook on how to finetune GPT2 to generate lyrics in the style of your favorite artist. üåé\n‚Ä¢ A notebook on how to finetune GPT2 to generate tweets in the style of your favorite Twitter user. üåé\n‚Ä¢ Causal language modeling chapter of the ü§ó Hugging Face Course.\n‚Ä¢ GPT2LMHeadModel is supported by this causal language modeling example script, text generation example script, and notebook.\n‚Ä¢ TFGPT2LMHeadModel is supported by this causal language modeling example script and notebook.\n‚Ä¢ FlaxGPT2LMHeadModel is supported by this causal language modeling example script and notebook."
    },
    {
        "link": "https://huggingface.co/docs/transformers/main/en/model_doc/gpt2",
        "document": "and get access to the augmented documentation experience\n\nOpenAI GPT-2 model was proposed in Language Models are Unsupervised Multitask Learners by Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei and Ilya Sutskever from OpenAI. It‚Äôs a causal (unidirectional) transformer pretrained using language modeling on a very large corpus of ~40 GB of text data.\n\nThe abstract from the paper is the following:\n\nGPT-2 is a large transformer-based language model with 1.5 billion parameters, trained on a dataset[1] of 8 million web pages. GPT-2 is trained with a simple objective: predict the next word, given all of the previous words within some text. The diversity of the dataset causes this simple goal to contain naturally occurring demonstrations of many tasks across diverse domains. GPT-2 is a direct scale-up of GPT, with more than 10X the parameters and trained on more than 10X the amount of data.\n\nWrite With Transformer is a webapp created and hosted by Hugging Face showcasing the generative capabilities of several models. GPT-2 is one of them and is available in five different sizes: small, medium, large, xl and a distilled version of the small checkpoint: distilgpt-2.\n\nThis model was contributed by thomwolf. The original code can be found here.\n‚Ä¢ GPT-2 is a model with absolute position embeddings so it‚Äôs usually advised to pad the inputs on the right rather than the left.\n‚Ä¢ GPT-2 was trained with a causal language modeling (CLM) objective and is therefore powerful at predicting the next token in a sequence. Leveraging this feature allows GPT-2 to generate syntactically coherent text as it can be observed in the run_generation.py example script.\n‚Ä¢ The model can take the past_key_values (for PyTorch) or past (for TF) as input, which is the previously computed key/value attention pairs. Using this (past_key_values or past) value prevents the model from re-computing pre-computed values in the context of text generation. For PyTorch, see past_key_values argument of the GPT2Model.forward() method, or for TF the past argument of the TFGPT2Model.call() method for more information on its usage.\n‚Ä¢ Enabling the scale_attn_by_inverse_layer_idx and reorder_and_upcast_attn flags will apply the training stability improvements from Mistral (for PyTorch only).\n\nThe method can be used to generate text using GPT2 model.\n\nFlash Attention 2 is a faster, optimized version of the attention scores computation which relies on kernels.\n\nFirst, check whether your hardware is compatible with Flash Attention 2. The latest list of compatible hardware can be found in the official documentation. If your hardware is not compatible with Flash Attention 2, you can still benefit from attention kernel optimisations through Better Transformer support covered above.\n\nNext, install the latest version of Flash Attention 2:\n\nTo load a model using Flash Attention 2, we can pass the argument to . We‚Äôll also load the model in half-precision (e.g. ), since it results in almost no degradation to audio quality but significantly lower memory usage and faster inference:\n\nBelow is an expected speedup diagram that compares pure inference time between the native implementation in transformers using checkpoint and the Flash Attention 2 version of the model using a sequence length of 512.\n\nPyTorch includes a native scaled dot-product attention (SDPA) operator as part of . This function encompasses several implementations that can be applied depending on the inputs and the hardware in use. See the official documentation or the GPU Inference page for more information.\n\nSDPA is used by default for when an implementation is available, but you may also set in to explicitly request SDPA to be used.\n\nFor the best speedups, we recommend loading the model in half-precision (e.g. or ).\n\nOn a local benchmark (rtx3080ti-16GB, PyTorch 2.2.1, OS Ubuntu 22.04) using with gpt2-large, we saw the following speedups during training and inference.\n\nA list of official Hugging Face and community (indicated by üåé) resources to help you get started with GPT2. If you‚Äôre interested in submitting a resource to be included here, please feel free to open a Pull Request and we‚Äôll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n‚Ä¢ A blog on how to Finetune a non-English GPT-2 Model with Hugging Face.\n‚Ä¢ A blog on How to generate text: using different decoding methods for language generation with Transformers with GPT-2.\n‚Ä¢ A blog on Faster Text Generation with TensorFlow and XLA with GPT-2.\n‚Ä¢ A blog on How to train a Language Model with Megatron-LM with a GPT-2 model.\n‚Ä¢ A notebook on how to finetune GPT2 to generate lyrics in the style of your favorite artist. üåé\n‚Ä¢ A notebook on how to finetune GPT2 to generate tweets in the style of your favorite Twitter user. üåé\n‚Ä¢ Causal language modeling chapter of the ü§ó Hugging Face Course.\n‚Ä¢ GPT2LMHeadModel is supported by this causal language modeling example script, text generation example script, and notebook.\n‚Ä¢ TFGPT2LMHeadModel is supported by this causal language modeling example script and notebook.\n‚Ä¢ FlaxGPT2LMHeadModel is supported by this causal language modeling example script and notebook."
    },
    {
        "link": "https://huggingface.co/openai-community/gpt2",
        "document": "Test the whole generation capabilities here: https://transformer.huggingface.co/doc/gpt2-large\n\nPretrained model on English language using a causal language modeling (CLM) objective. It was introduced in this paper and first released at this page.\n\nDisclaimer: The team releasing GPT-2 also wrote a model card for their model. Content from this model card has been written by the Hugging Face team to complete the information they provided and give specific examples of bias.\n\nGPT-2 is a transformers model pretrained on a very large corpus of English data in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it was trained to guess the next word in sentences.\n\nMore precisely, inputs are sequences of continuous text of a certain length and the targets are the same sequence, shifted one token (word or piece of word) to the right. The model uses internally a mask-mechanism to make sure the predictions for the token only uses the inputs from to but not the future tokens.\n\nThis way, the model learns an inner representation of the English language that can then be used to extract features useful for downstream tasks. The model is best at what it was pretrained for however, which is generating texts from a prompt.\n\nThis is the smallest version of GPT-2, with 124M parameters.\n\nYou can use the raw model for text generation or fine-tune it to a downstream task. See the model hub to look for fine-tuned versions on a task that interests you.\n\nHow to use\n\nYou can use this model directly with a pipeline for text generation. Since the generation relies on some randomness, we set a seed for reproducibility:\n\nHere is how to use this model to get the features of a given text in PyTorch:\n\nThe training data used for this model has not been released as a dataset one can browse. We know it contains a lot of unfiltered content from the internet, which is far from neutral. As the openAI team themselves point out in their model card:\n\nBecause large-scale language models like GPT-2 do not distinguish fact from fiction, we don‚Äôt support use-cases that require the generated text to be true. Additionally, language models like GPT-2 reflect the biases inherent to the systems they were trained on, so we do not recommend that they be deployed into systems that interact with humans > unless the deployers first carry out a study of biases relevant to the intended use-case. We found no statistically significant difference in gender, race, and religious bias probes between 774M and 1.5B, implying all versions of GPT-2 should be approached with similar levels of caution around use cases that are sensitive to biases around human attributes.\n\nHere's an example of how the model can have biased predictions:\n\nThis bias will also affect all fine-tuned versions of this model.\n\nThe OpenAI team wanted to train this model on a corpus as large as possible. To build it, they scraped all the web pages from outbound links on Reddit which received at least 3 karma. Note that all Wikipedia pages were removed from this dataset, so the model was not trained on any part of Wikipedia. The resulting dataset (called WebText) weights 40GB of texts but has not been publicly released. You can find a list of the top 1,000 domains present in WebText here.\n\nThe texts are tokenized using a byte-level version of Byte Pair Encoding (BPE) (for unicode characters) and a vocabulary size of 50,257. The inputs are sequences of 1024 consecutive tokens.\n\nThe larger model was trained on 256 cloud TPU v3 cores. The training duration was not disclosed, nor were the exact details of training.\n\nThe model achieves the following results without any fine-tuning (zero-shot):"
    },
    {
        "link": "https://huggingface.co/docs/transformers/en/index",
        "document": "and get access to the augmented documentation experience\n\nü§ó Transformers provides APIs and tools to easily download and train state-of-the-art pretrained models. Using pretrained models can reduce your compute costs, carbon footprint, and save you the time and resources required to train a model from scratch. These models support common tasks in different modalities, such as:\n\nüìù Natural Language Processing: text classification, named entity recognition, question answering, language modeling, code generation, summarization, translation, multiple choice, and text generation.\n\n üñºÔ∏è Computer Vision: image classification, object detection, and segmentation.\n\n üó£Ô∏è Audio: automatic speech recognition and audio classification.\n\n üêô Multimodal: table question answering, optical character recognition, information extraction from scanned documents, video classification, and visual question answering.\n\nü§ó Transformers support framework interoperability between PyTorch, TensorFlow, and JAX. This provides the flexibility to use a different framework at each stage of a model‚Äôs life; train a model in three lines of code in one framework, and load it for inference in another. Models can also be exported to a format like ONNX and TorchScript for deployment in production environments.\n\nJoin the growing community on the Hub, forum, or Discord today!\n\nIf you are looking for custom support from the Hugging Face team\n\nThe documentation is organized into five sections:\n‚Ä¢ None GET STARTED provides a quick tour of the library and installation instructions to get up and running.\n‚Ä¢ None TUTORIALS are a great place to start if you‚Äôre a beginner. This section will help you gain the basic skills you need to start using the library.\n‚Ä¢ None HOW-TO GUIDES show you how to achieve a specific goal, like finetuning a pretrained model for language modeling or how to write and share a custom model.\n‚Ä¢ None CONCEPTUAL GUIDES offers more discussion and explanation of the underlying concepts and ideas behind models, tasks, and the design philosophy of ü§ó Transformers.\n‚Ä¢ \n‚Ä¢ MAIN CLASSES details the most important classes like configuration, model, tokenizer, and pipeline.\n‚Ä¢ MODELS details the classes and functions related to each model implemented in the library.\n\nThe table below represents the current support in the library for each of those models, whether they have a Python tokenizer (called ‚Äúslow‚Äù). A ‚Äúfast‚Äù tokenizer backed by the ü§ó Tokenizers library, whether they have support in Jax (via Flax), PyTorch, and/or TensorFlow."
    },
    {
        "link": "https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py",
        "document": ""
    },
    {
        "link": "https://huggingface.co/docs/transformers/en/installation",
        "document": "and get access to the augmented documentation experience\n\nInstall ü§ó Transformers for whichever deep learning library you‚Äôre working with, setup your cache, and optionally configure ü§ó Transformers to run offline.\n\nü§ó Transformers is tested on Python 3.6+, PyTorch 1.1.0+, TensorFlow 2.0+, and Flax. Follow the installation instructions below for the deep learning library you are using:\n\nYou should install ü§ó Transformers in a virtual environment. If you‚Äôre unfamiliar with Python virtual environments, take a look at this guide. A virtual environment makes it easier to manage different projects, and avoid compatibility issues between dependencies.\n\nCreate a virtual environment with uv (refer to Installation for installation instructions), a fast Rust-based Python package and project manager.\n\nNow you‚Äôre ready to install ü§ó Transformers with pip or uv.\n\nFor GPU acceleration, install the appropriate CUDA drivers for PyTorch and TensorFlow(https://www.tensorflow.org/install/pip).\n\nRun the command below to check if your system detects an NVIDIA GPU.\n\nFor CPU-support only, you can conveniently install ü§ó Transformers and a deep learning library in one line. For example, install ü§ó Transformers and PyTorch with:\n\nFinally, check if ü§ó Transformers has been properly installed by running the following command. It will download a pretrained model:\n\nThen print out the label and score:\n\nInstall ü§ó Transformers from source with the following command:\n\nThis command installs the bleeding edge version rather than the latest version. The version is useful for staying up-to-date with the latest developments. For instance, if a bug has been fixed since the last official release but a new release hasn‚Äôt been rolled out yet. However, this means the version may not always be stable. We strive to keep the version operational, and most issues are usually resolved within a few hours or a day. If you run into a problem, please open an Issue so we can fix it even sooner!\n\nCheck if ü§ó Transformers has been properly installed by running the following command:\n\nYou will need an editable install if you‚Äôd like to:\n‚Ä¢ Use the version of the source code.\n‚Ä¢ Contribute to ü§ó Transformers and need to test changes in the code.\n\nClone the repository and install ü§ó Transformers with the following commands:\n\nThese commands will link the folder you cloned the repository to and your Python library paths. Python will now look inside the folder you cloned to in addition to the normal library paths. For example, if your Python packages are typically installed in , Python will also search the folder you cloned to: .\n\nNow you can easily update your clone to the latest version of ü§ó Transformers with the following command:\n\nYour Python environment will find the version of ü§ó Transformers on the next run.\n\nPretrained models are downloaded and locally cached at: . This is the default directory given by the shell environment variable . On Windows, the default directory is given by . You can change the shell environment variables shown below - in order of priority - to specify a different cache directory:\n\nRun ü§ó Transformers in a firewalled or offline environment with locally cached files by setting the environment variable .\n\nThis script should run without hanging or waiting to timeout because it won‚Äôt attempt to download the model from the Hub.\n\nYou can also bypass loading a model from the Hub from each from_pretrained() call with the parameter. When set to , only local files are loaded:\n\nFetch models and tokenizers to use offline\n\nAnother option for using ü§ó Transformers offline is to download the files ahead of time, and then point to their local path when you need to use them offline. There are three ways to do this:\n‚Ä¢ None Download a file through the user interface on the Model Hub by clicking on the ‚Üì icon.\n‚Ä¢ None Use the PreTrainedModel.from_pretrained() and PreTrainedModel.save_pretrained() workflow:\n‚Ä¢ None Download your files ahead of time with PreTrainedModel.from_pretrained():\n‚Ä¢ None Save your files to a specified directory with PreTrainedModel.save_pretrained():\n‚Ä¢ None Now when you‚Äôre offline, reload your files with PreTrainedModel.from_pretrained() from the specified directory:\n‚Ä¢ \n‚Ä¢ None Install the library in your virtual environment:\n‚Ä¢ None Use the function to download a file to a specific path. For example, the following command downloads the file from the T0 model to your desired path:\n\nOnce your file is downloaded and locally cached, specify it‚Äôs local path to load and use it:\n\nSee below for some of the more common installation issues and how to resolve them.\n\nEnsure you are using Python 3.9 or later. Run the command below to check your Python version.\n\nInstall all required dependencies by running the following command. Ensure you‚Äôre in the project directory before executing the command.\n\nIf you encounter issues on Windows, you may need to activate Developer Mode. Navigate to Windows Settings > For Developers > Developer Mode.\n\nAlternatively, create and activate a virtual environment as shown below."
    },
    {
        "link": "https://huggingface.co/docs/transformers/en/add_new_model",
        "document": "How to add a model to ü§ó Transformers?\n\nThe ü§ó Transformers library is often able to offer new models thanks to community contributors. But this can be a challenging project and requires an in-depth knowledge of the ü§ó Transformers library and the model to implement. At Hugging Face, we‚Äôre trying to empower more of the community to actively add models and we‚Äôve put together this guide to walk you through the process of adding a PyTorch model (make sure you have PyTorch installed).\n\nAlong the way, you‚Äôll:\n‚Ä¢ get insights into open-source best practices\n‚Ä¢ understand the design principles behind one of the most popular deep learning libraries\n‚Ä¢ learn how to integrate Python utilities like , , and to ensure clean and readable code\n\nA Hugging Face team member will be available to help you along the way so you‚Äôll never be alone. ü§ó ‚ù§Ô∏è\n\nTo get started, open a New model addition issue for the model you want to see in ü§ó Transformers. If you‚Äôre not especially picky about contributing a specific model, you can filter by the New model label to see if there are any unclaimed model requests and work on it.\n\nOnce you‚Äôve opened a new model request, the first step is to get familiar with ü§ó Transformers if you aren‚Äôt already!\n\nFirst, you should get a general overview of ü§ó Transformers. ü§ó Transformers is a very opinionated library, so there is a chance that you don‚Äôt agree with some of the library‚Äôs philosophies or design choices. From our experience, however, we found that the fundamental design choices and philosophies of the library are crucial to efficiently scale ü§ó Transformers while keeping maintenance costs at a reasonable level.\n\nA good first starting point to better understand the library is to read the documentation of our philosophy. As a result of our way of working, there are some choices that we try to apply to all models:\n‚Ä¢ Duplicating code is not always bad if it strongly improves the readability or accessibility of a model\n‚Ä¢ Model files are as self-contained as possible so that when you read the code of a specific model, you ideally only have to look into the respective file.\n\nIn our opinion, the library‚Äôs code is not just a means to provide a product, e.g. the ability to use BERT for inference, but also as the very product that we want to improve. Hence, when adding a model, the user is not only the person who will use your model, but also everybody who will read, try to understand, and possibly tweak your code.\n\nWith this in mind, let‚Äôs go a bit deeper into the general library design.\n\nTo successfully add a model, it is important to understand the interaction between your model and its config, PreTrainedModel, and PretrainedConfig. For exemplary purposes, we will call the model to be added to ü§ó Transformers .\n\nAs you can see, we do make use of inheritance in ü§ó Transformers, but we keep the level of abstraction to an absolute minimum. There are never more than two levels of abstraction for any model in the library. inherits from which in turn inherits from PreTrainedModel and that‚Äôs it. As a general rule, we want to make sure that a new model only depends on PreTrainedModel. The important functionalities that are automatically provided to every new model are from_pretrained() and save_pretrained(), which are used for serialization and deserialization. All of the other important functionalities, such as should be completely defined in the new script. Next, we want to make sure that a model with a specific head layer, such as does not inherit from , but rather uses as a component that can be called in its forward pass to keep the level of abstraction low. Every new model requires a configuration class, called . This configuration is always stored as an attribute in PreTrainedModel, and thus can be accessed via the attribute for all classes inheriting from :\n\nSimilar to the model, the configuration inherits basic serialization and deserialization functionalities from PretrainedConfig. Note that the configuration and the model are always serialized into two different formats - the model to a pytorch_model.bin file and the configuration to a config.json file. Calling the model‚Äôs save_pretrained() will automatically call the config‚Äôs save_pretrained(), so that both model and configuration are saved.\n\nWhen coding your new model, keep in mind that Transformers is an opinionated library and we have a few quirks of our own regarding how code should be written :-)\n‚Ä¢ The forward pass of your model should be fully written in the modeling file while being fully independent of other models in the library. If you want to reuse a block from another model, copy the code and paste it with a comment on top (see here for a good example and there for more documentation on Copied from).\n‚Ä¢ The code should be fully understandable, even by a non-native English speaker. This means you should pick descriptive variable names and avoid abbreviations. As an example, is preferred to . One-letter variable names are strongly discouraged unless it‚Äôs an index in a for loop.\n‚Ä¢ More generally we prefer longer explicit code to short magical one.\n‚Ä¢ Avoid subclassing in PyTorch but subclass and write the forward pass, so that anyone using your code can quickly debug it by adding print statements or breaking points.\n‚Ä¢ Your function signature should be type-annotated. For the rest, good variable names are way more readable and understandable than type annotations.\n\nNot quite ready yet :-( This section will be added soon!\n\nEveryone has different preferences of how to port a model so it can be very helpful for you to take a look at summaries of how other contributors ported models to Hugging Face. Here is a list of community blog posts on how to port a model:\n\nFrom experience, we can tell you that the most important things to keep in mind when adding a model are:\n‚Ä¢ Don‚Äôt reinvent the wheel! Most parts of the code you will add for the new ü§ó Transformers model already exist somewhere in ü§ó Transformers. Take some time to find similar, already existing models and tokenizers you can copy from. grep and rg are your friends. Note that it might very well happen that your model‚Äôs tokenizer is based on one model implementation, and your model‚Äôs modeling code on another one. E.g. FSMT‚Äôs modeling code is based on BART, while FSMT‚Äôs tokenizer code is based on XLM.\n‚Ä¢ It‚Äôs more of an engineering challenge than a scientific challenge. You should spend more time creating an efficient debugging environment rather than trying to understand all theoretical aspects of the model in the paper.\n‚Ä¢ Ask for help, when you‚Äôre stuck! Models are the core component of ü§ó Transformers so we at Hugging Face are more than happy to help you at every step to add your model. Don‚Äôt hesitate to ask if you notice you are not making progress.\n\nIn the following, we try to give you a general recipe that we found most useful when porting a model to ü§ó Transformers.\n\nThe following list is a summary of everything that has to be done to add a model and can be used by you as a To-Do List:\n\n‚òê (Optional) Understood the model‚Äôs theoretical aspects\n\n ‚òê Prepared ü§ó Transformers dev environment\n\n ‚òê Set up debugging environment of the original repository\n\n ‚òê Created script that successfully runs the pass using the original repository and checkpoint\n\n ‚òê Successfully added the model skeleton to ü§ó Transformers\n\n ‚òê Successfully converted original checkpoint to ü§ó Transformers checkpoint\n\n ‚òê Successfully ran pass in ü§ó Transformers that gives identical output to original checkpoint\n\n ‚òê Finished model tests in ü§ó Transformers\n\n ‚òê Successfully added tokenizer in ü§ó Transformers\n\n ‚òê Run end-to-end integration tests\n\n ‚òê Finished docs\n\n ‚òê Uploaded model weights to the Hub\n\n ‚òê Submitted the pull request\n\n ‚òê (Optional) Added a demo notebook\n\nTo begin with, we usually recommend starting by getting a good theoretical understanding of . However, if you prefer to understand the theoretical aspects of the model on-the-job, then it is totally fine to directly dive into the ‚Äôs code-base. This option might suit you better if your engineering skills are better than your theoretical skill, if you have trouble understanding ‚Äôs paper, or if you just enjoy programming much more than reading scientific papers.\n\nYou should take some time to read BrandNewBert‚Äôs paper, if such descriptive work exists. There might be large sections of the paper that are difficult to understand. If this is the case, this is fine - don‚Äôt worry! The goal is not to get a deep theoretical understanding of the paper, but to extract the necessary information required to effectively re-implement the model in ü§ó Transformers. That being said, you don‚Äôt have to spend too much time on the theoretical aspects, but rather focus on the practical ones, namely:\n‚Ä¢ What type of model is brand_new_bert? BERT-like encoder-only model? GPT2-like decoder-only model? BART-like encoder-decoder model? Look at the model_summary if you‚Äôre not familiar with the differences between those.\n‚Ä¢ What are the applications of brand_new_bert? Text classification? Text generation? Seq2Seq tasks, e.g., summarization?\n‚Ä¢ What is the novel feature of the model that makes it different from BERT/GPT-2/BART?\n‚Ä¢ Which of the already existing ü§ó Transformers models is most similar to brand_new_bert?\n‚Ä¢ What type of tokenizer is used? A sentencepiece tokenizer? Word piece tokenizer? Is it the same tokenizer as used for BERT or BART?\n\nAfter you feel like you have gotten a good overview of the architecture of the model, you might want to write to the Hugging Face team with any questions you might have. This might include questions regarding the model‚Äôs architecture, its attention layer, etc. We will be more than happy to help you.\n‚Ä¢ None Fork the repository by clicking on the ‚ÄòFork‚Äô button on the repository‚Äôs page. This creates a copy of the code under your GitHub user account.\n‚Ä¢ None Clone your fork to your local disk, and add the base repository as a remote:\n‚Ä¢ None Set up a development environment, for instance by running the following command: Depending on your OS, and since the number of optional dependencies of Transformers is growing, you might get a failure with this command. If that‚Äôs the case make sure to install the Deep Learning framework you are working with (PyTorch, TensorFlow and/or Flax) then do: which should be enough for most use cases. You can then return to the parent directory\n‚Ä¢ None We recommend adding the PyTorch version of brand_new_bert to Transformers. To install PyTorch, please follow the instructions on https://pytorch.org/get-started/locally/. Note: You don‚Äôt need to have CUDA installed. Making the new model work on CPU is sufficient.\n‚Ä¢ None To port brand_new_bert, you will also need access to its original repository:\n\nNow you have set up a development environment to port brand_new_bert to ü§ó Transformers.\n\nAt first, you will work on the original brand_new_bert repository. Often, the original implementation is very ‚Äúresearchy‚Äù. Meaning that documentation might be lacking and the code can be difficult to understand. But this should be exactly your motivation to reimplement brand_new_bert. At Hugging Face, one of our main goals is to make people stand on the shoulders of giants which translates here very well into taking a working model and rewriting it to make it as accessible, user-friendly, and beautiful as possible. This is the number-one motivation to re-implement models into ü§ó Transformers - trying to make complex new NLP technology accessible to everybody.\n\nYou should start thereby by diving into the original repository.\n\nSuccessfully running the official pretrained model in the original repository is often the most difficult step. From our experience, it is very important to spend some time getting familiar with the original code-base. You need to figure out the following:\n‚Ä¢ Where to find the pretrained weights?\n‚Ä¢ How to load the pretrained weights into the corresponding model?\n‚Ä¢ How to run the tokenizer independently from the model?\n‚Ä¢ Trace one forward pass so that you know which classes and functions are required for a simple forward pass. Usually, you only have to reimplement those functions.\n‚Ä¢ Be able to locate the important components of the model: Where is the model‚Äôs class? Are there model sub-classes, e.g. EncoderModel, DecoderModel? Where is the self-attention layer? Are there multiple different attention layers, e.g. self-attention, cross-attention‚Ä¶?\n‚Ä¢ How can you debug the model in the original environment of the repo? Do you have to add print statements, can you work with an interactive debugger like ipdb, or should you use an efficient IDE to debug the model, like PyCharm?\n\nIt is very important that before you start the porting process, you can efficiently debug code in the original repository! Also, remember that you are working with an open-source library, so do not hesitate to open an issue, or even a pull request in the original repository. The maintainers of this repository are most likely very happy about someone looking into their code!\n\nAt this point, it is really up to you which debugging environment and strategy you prefer to use to debug the original model. We strongly advise against setting up a costly GPU environment, but simply work on a CPU both when starting to dive into the original repository and also when starting to write the ü§ó Transformers implementation of the model. Only at the very end, when the model has already been successfully ported to ü§ó Transformers, one should verify that the model also works as expected on GPU.\n\nIn general, there are two possible debugging environments for running the original model\n\nJupyter notebooks have the advantage that they allow for cell-by-cell execution which can be helpful to better split logical components from one another and to have faster debugging cycles as intermediate results can be stored. Also, notebooks are often easier to share with other contributors, which might be very helpful if you want to ask the Hugging Face team for help. If you are familiar with Jupyter notebooks, we strongly recommend you work with them.\n\nThe obvious disadvantage of Jupyter notebooks is that if you are not used to working with them you will have to spend some time adjusting to the new programming environment and you might not be able to use your known debugging tools anymore, like .\n\nFor each code-base, a good first step is always to load a small pretrained checkpoint and to be able to reproduce a single forward pass using a dummy integer vector of input IDs as an input. Such a script could look like this (in pseudocode):\n\nNext, regarding the debugging strategy, there are generally a few from which to choose from:\n‚Ä¢ Decompose the original model into many small testable components and run a forward pass on each of those for verification\n‚Ä¢ Decompose the original model only into the original tokenizer and the original model, run a forward pass on those, and use intermediate print statements or breakpoints for verification\n\nAgain, it is up to you which strategy to choose. Often, one or the other is advantageous depending on the original code base.\n\nIf the original code-base allows you to decompose the model into smaller sub-components, e.g. if the original code-base can easily be run in eager mode, it is usually worth the effort to do so. There are some important advantages to taking the more difficult road in the beginning:\n‚Ä¢ at a later stage when comparing the original model to the Hugging Face implementation, you can verify automatically for each component individually that the corresponding component of the ü§ó Transformers implementation matches instead of relying on visual comparison via print statements\n‚Ä¢ it can give you some rope to decompose the big problem of porting a model into smaller problems of just porting individual components and thus structure your work better\n‚Ä¢ separating the model into logical meaningful components will help you to get a better overview of the model‚Äôs design and thus to better understand the model\n‚Ä¢ at a later stage those component-by-component tests help you to ensure that no regression occurs as you continue changing your code\n\nLysandre‚Äôs integration checks for ELECTRA gives a nice example of how this can be done.\n\nHowever, if the original code-base is very complex or only allows intermediate components to be run in a compiled mode, it might be too time-consuming or even impossible to separate the model into smaller testable sub-components. A good example is T5‚Äôs MeshTensorFlow library which is very complex and does not offer a simple way to decompose the model into its sub-components. For such libraries, one often relies on verifying print statements.\n\nNo matter which strategy you choose, the recommended procedure is often the same that you should start to debug the starting layers first and the ending layers last.\n\nIt is recommended that you retrieve the output, either by print statements or sub-component functions, of the following layers in the following order:\n‚Ä¢ Retrieve the input IDs passed to the model\n‚Ä¢ Retrieve the input of the first Transformer layer\n‚Ä¢ Retrieve the output of the first Transformer layer\n‚Ä¢ Retrieve the output of the following n - 1 Transformer layers\n‚Ä¢ Retrieve the output of the whole BrandNewBert Model\n\nInput IDs should thereby consists of an array of integers, e.g.\n\nThe outputs of the following layers often consist of multi-dimensional float arrays and can look like this:\n\nWe expect that every model added to ü§ó Transformers passes a couple of integration tests, meaning that the original model and the reimplemented version in ü§ó Transformers have to give the exact same output up to a precision of 0.001! Since it is normal that the exact same model written in different libraries can give a slightly different output depending on the library framework, we accept an error tolerance of 1e-3 (0.001). It is not enough if the model gives nearly the same output, they have to be almost identical. Therefore, you will certainly compare the intermediate outputs of the ü§ó Transformers version multiple times against the intermediate outputs of the original implementation of brand_new_bert in which case an efficient debugging environment of the original repository is absolutely important. Here is some advice to make your debugging environment as efficient as possible.\n‚Ä¢ Find the best way of debugging intermediate results. Is the original repository written in PyTorch? Then you should probably take the time to write a longer script that decomposes the original model into smaller sub-components to retrieve intermediate values. Is the original repository written in Tensorflow 1? Then you might have to rely on TensorFlow print operations like tf.print to output intermediate values. Is the original repository written in Jax? Then make sure that the model is not jitted when running the forward pass, e.g. check-out this link.\n‚Ä¢ Use the smallest pretrained checkpoint you can find. The smaller the checkpoint, the faster your debug cycle becomes. It is not efficient if your pretrained model is so big that your forward pass takes more than 10 seconds. In case only very large checkpoints are available, it might make more sense to create a dummy model in the new environment with randomly initialized weights and save those weights for comparison with the ü§ó Transformers version of your model\n‚Ä¢ Make sure you are using the easiest way of calling a forward pass in the original repository. Ideally, you want to find the function in the original repository that only calls a single forward pass, i.e. that is often called , , or . You don‚Äôt want to debug a function that calls multiple times, e.g. to generate text, like , .\n‚Ä¢ Try to separate the tokenization from the model‚Äôs forward pass. If the original repository shows examples where you have to input a string, then try to find out where in the forward call the string input is changed to input ids and start from this point. This might mean that you have to possibly write a small script yourself or change the original code so that you can directly input the ids instead of an input string.\n‚Ä¢ Make sure that the model in your debugging setup is not in training mode, which often causes the model to yield random outputs due to multiple dropout layers in the model. Make sure that the forward pass in your debugging environment is deterministic so that the dropout layers are not used. Or use transformers.utils.set_seed if the old and new implementations are in the same framework.\n\nThe following section gives you more specific details/tips on how you can do this for brand_new_bert.\n\nNext, you can finally start adding new code to ü§ó Transformers. Go into the clone of your ü§ó Transformers‚Äô fork:\n\nIn the special case that you are adding a model whose architecture exactly matches the model architecture of an existing model you only have to add a conversion script as described in this section. In this case, you can just re-use the whole model architecture of the already existing model.\n\nOtherwise, let‚Äôs start generating a new model. We recommend using the following script to add a model starting from an existing model:\n\nYou will be prompted with a questionnaire to fill in the basic information of your model.\n\nBefore starting to adapt the automatically generated code, now is the time to open a ‚ÄúWork in progress (WIP)‚Äù pull request, e.g. ‚Äú[WIP] Add brand_new_bert‚Äù, in ü§ó Transformers so that you and the Hugging Face team can work side-by-side on integrating the model into ü§ó Transformers.\n\nYou should do the following:\n‚Ä¢ None Create a branch with a descriptive name from your main branch\n‚Ä¢ None Push the changes to your account using:\n‚Ä¢ None Once you are satisfied, go to the webpage of your fork on GitHub. Click on ‚ÄúPull request‚Äù. Make sure to add the GitHub handle of some members of the Hugging Face team as reviewers, so that the Hugging Face team gets notified for future changes.\n‚Ä¢ None Change the PR into a draft by clicking on ‚ÄúConvert to draft‚Äù on the right of the GitHub pull request web page.\n\nIn the following, whenever you have made some progress, don‚Äôt forget to commit your work and push it to your account so that it shows in the pull request. Additionally, you should make sure to update your work with the current main from time to time by doing:\n\nIn general, all questions you might have regarding the model or your implementation should be asked in your PR and discussed/solved in the PR. This way, the Hugging Face team will always be notified when you are committing new code or if you have a question. It is often very helpful to point the Hugging Face team to your added code so that the Hugging Face team can efficiently understand your problem or question.\n\nTo do so, you can go to the ‚ÄúFiles changed‚Äù tab where you see all of your changes, go to a line regarding which you want to ask a question, and click on the ‚Äú+‚Äù symbol to add a comment. Whenever a question or problem has been solved, you can click on the ‚ÄúResolve‚Äù button of the created comment.\n\nIn the same way, the Hugging Face team will open comments when reviewing your code. We recommend asking most questions on GitHub on your PR. For some very general questions that are not very useful for the public, feel free to ping the Hugging Face team by Slack or email.\n\nAt first, we will focus only on the model itself and not care about the tokenizer. All the relevant code should be found in the generated files and .\n\nNow you can finally start coding :). The generated code in will either have the same architecture as BERT if it‚Äôs an encoder-only model or BART if it‚Äôs an encoder-decoder model. At this point, you should remind yourself what you‚Äôve learned in the beginning about the theoretical aspects of the model: How is the model different from BERT or BART?‚Äù. Implement those changes which often means changing the self-attention layer, the order of the normalization layer, etc‚Ä¶ Again, it is often useful to look at the similar architecture of already existing models in Transformers to get a better feeling of how your model should be implemented.\n\nNote that at this point, you don‚Äôt have to be very sure that your code is fully correct or clean. Rather, it is advised to add a first unclean, copy-pasted version of the original code to until you feel like all the necessary code is added. From our experience, it is much more efficient to quickly add a first version of the required code and improve/correct the code iteratively with the conversion script as described in the next section. The only thing that has to work at this point is that you can instantiate the ü§ó Transformers implementation of brand_new_bert, i.e. the following command should work:\n\nThe above command will create a model according to the default parameters as defined in with random weights, thus making sure that the methods of all components works.\n\nNote that all random initialization should happen in the method of your class. It should initialize all leaf modules depending on the variables of the config. Here is an example with the BERT method:\n\nYou can have some more custom schemes if you need a special initialization for some modules. For instance, in , the last two linear layers need to have the initialization of the regular PyTorch but all the other ones should use an initialization as above. This is coded like this:\n\nThe flag is internally used to make sure we only initialize a submodule once. By setting it to for and , we make sure the custom initialization we did is not overridden later on, the function won‚Äôt be applied to them.\n\nNext, you should write a conversion script that lets you convert the checkpoint you used to debug brand_new_bert in the original repository to a checkpoint compatible with your just created ü§ó Transformers implementation of brand_new_bert. It is not advised to write the conversion script from scratch, but rather to look through already existing conversion scripts in ü§ó Transformers for one that has been used to convert a similar model that was written in the same framework as brand_new_bert. Usually, it is enough to copy an already existing conversion script and slightly adapt it for your use case. Don‚Äôt hesitate to ask the Hugging Face team to point you to a similar already existing conversion script for your model.\n‚Ä¢ If you are porting a model from TensorFlow to PyTorch, a good starting point might be BERT‚Äôs conversion script here\n‚Ä¢ If you are porting a model from PyTorch to PyTorch, a good starting point might be BART‚Äôs conversion script here\n\nIn the following, we‚Äôll quickly explain how PyTorch models store layer weights and define layer names. In PyTorch, the name of a layer is defined by the name of the class attribute you give the layer. Let‚Äôs define a dummy model in PyTorch, called as follows:\n\nNow we can create an instance of this model definition which will fill all weights: , , with random weights. We can print the model to see its architecture\n\nThis will print out the following:\n\nWe can see that the layer names are defined by the name of the class attribute in PyTorch. You can print out the weight values of a specific layer:\n\nto see that the weights were randomly initialized\n\nIn the conversion script, you should fill those randomly initialized weights with the exact weights of the corresponding layer in the checkpoint. E.g.\n\nWhile doing so, you must verify that each randomly initialized weight of your PyTorch model and its corresponding pretrained checkpoint weight exactly match in both shape and name. To do so, it is necessary to add assert statements for the shape and print out the names of the checkpoints weights. E.g. you should add statements like:\n\nBesides, you should also print out the names of both weights to make sure they match, e.g.\n\nIf either the shape or the name doesn‚Äôt match, you probably assigned the wrong checkpoint weight to a randomly initialized layer of the ü§ó Transformers implementation.\n\nAn incorrect shape is most likely due to an incorrect setting of the config parameters in that do not exactly match those that were used for the checkpoint you want to convert. However, it could also be that PyTorch‚Äôs implementation of a layer requires the weight to be transposed beforehand.\n\nFinally, you should also check that all required weights are initialized and print out all checkpoint weights that were not used for initialization to make sure the model is correctly converted. It is completely normal, that the conversion trials fail with either a wrong shape statement or a wrong name assignment. This is most likely because either you used incorrect parameters in , have a wrong architecture in the ü§ó Transformers implementation, you have a bug in the functions of one of the components of the ü§ó Transformers implementation or you need to transpose one of the checkpoint weights.\n\nThis step should be iterated with the previous step until all weights of the checkpoint are correctly loaded in the Transformers model. Having correctly loaded the checkpoint into the ü§ó Transformers implementation, you can then save the model under a folder of your choice that should then contain both a file and a file:\n\nHaving managed to correctly load the pretrained weights into the ü§ó Transformers implementation, you should now make sure that the forward pass is correctly implemented. In Get familiar with the original repository, you have already created a script that runs a forward pass of the model using the original repository. Now you should write an analogous script using the ü§ó Transformers implementation instead of the original one. It should look as follows:\n\nIt is very likely that the ü§ó Transformers implementation and the original model implementation don‚Äôt give the exact same output the very first time or that the forward pass throws an error. Don‚Äôt be disappointed - it‚Äôs expected! First, you should make sure that the forward pass doesn‚Äôt throw any errors. It often happens that the wrong dimensions are used leading to a Dimensionality mismatch error or that the wrong data type object is used, e.g. instead of . Don‚Äôt hesitate to ask the Hugging Face team for help, if you don‚Äôt manage to solve certain errors.\n\nThe final part to make sure the ü§ó Transformers implementation works correctly is to ensure that the outputs are equivalent to a precision of . First, you should ensure that the output shapes are identical, i.e. should yield the same value for the script of the ü§ó Transformers implementation and the original implementation. Next, you should make sure that the output values are identical as well. This one of the most difficult parts of adding a new model. Common mistakes why the outputs are not identical are:\n‚Ä¢ Some layers were not added, i.e. an activation layer was not added, or the residual connection was forgotten\n‚Ä¢ The word embedding matrix was not tied\n‚Ä¢ The wrong positional embeddings are used because the original implementation uses on offset\n‚Ä¢ Dropout is applied during the forward pass. To fix this make sure model.training is False and that no dropout layer is falsely activated during the forward pass, i.e. pass self.training to PyTorch‚Äôs functional dropout\n\nThe best way to fix the problem is usually to look at the forward pass of the original implementation and the ü§ó Transformers implementation side-by-side and check if there are any differences. Ideally, you should debug/print out intermediate outputs of both implementations of the forward pass to find the exact position in the network where the ü§ó Transformers implementation shows a different output than the original implementation. First, make sure that the hard-coded in both scripts are identical. Next, verify that the outputs of the first transformation of the (usually the word embeddings) are identical. And then work your way up to the very last layer of the network. At some point, you will notice a difference between the two implementations, which should point you to the bug in the ü§ó Transformers implementation. From our experience, a simple and efficient way is to add many print statements in both the original implementation and ü§ó Transformers implementation, at the same positions in the network respectively, and to successively remove print statements showing the same values for intermediate presentations.\n\nWhen you‚Äôre confident that both implementations yield the same output, verify the outputs with , you‚Äôre done with the most difficult part! Congratulations - the work left to be done should be a cakewalk üòä.\n\nAt this point, you have successfully added a new model. However, it is very much possible that the model does not yet fully comply with the required design. To make sure, the implementation is fully compatible with ü§ó Transformers, all common tests should pass. The Cookiecutter should have automatically added a test file for your model, probably under the same . Run this test file to verify that all common tests pass:\n\nHaving fixed all common tests, it is now crucial to ensure that all the nice work you have done is well tested, so that\n‚Ä¢ a) The community can easily understand your work by looking at specific tests of brand_new_bert\n‚Ä¢ b) Future changes to your model will not break any important feature of the model.\n\nAt first, integration tests should be added. Those integration tests essentially do the same as the debugging scripts you used earlier to implement the model to ü§ó Transformers. A template of those model tests has already added by the Cookiecutter, called and only has to be filled out by you. To ensure that those tests are passing, run\n\nSecond, all features that are special to brand_new_bert should be tested additionally in a separate test under / . This part is often forgotten but is extremely useful in two ways:\n‚Ä¢ It helps to transfer the knowledge you have acquired during the model addition to the community by showing how the special features of brand_new_bert should work.\n‚Ä¢ Future contributors can quickly test changes to the model by running those special tests.\n\nNext, we should add the tokenizer of brand_new_bert. Usually, the tokenizer is equivalent to or very similar to an already existing tokenizer of ü§ó Transformers.\n\nIt is very important to find/extract the original tokenizer file and to manage to load this file into the ü§ó Transformers‚Äô implementation of the tokenizer.\n\nTo ensure that the tokenizer works correctly, it is recommended to first create a script in the original repository that inputs a string and returns the . It could look similar to this (in pseudo-code):\n\nYou might have to take a deeper look again into the original repository to find the correct tokenizer function or you might even have to do changes to your clone of the original repository to only output the . Having written a functional tokenization script that uses the original repository, an analogous script for ü§ó Transformers should be created. It should look similar to this:\n\nWhen both yield the same values, as a final step a tokenizer test file should also be added.\n\nAnalogous to the modeling test files of brand_new_bert, the tokenization test files of brand_new_bert should contain a couple of hard-coded integration tests.\n\nHaving added the tokenizer, you should also add a couple of end-to-end integration tests using both the model and the tokenizer to in ü§ó Transformers. Such a test should show on a meaningful text-to-text sample that the ü§ó Transformers implementation works as expected. A meaningful text-to-text sample can include e.g. a source-to-target-translation pair, an article-to-summary pair, a question-to-answer pair, etc‚Ä¶ If none of the ported checkpoints has been fine-tuned on a downstream task it is enough to simply rely on the model tests. In a final step to ensure that the model is fully functional, it is advised that you also run all tests on GPU. It can happen that you forgot to add some statements to internal tensors of the model, which in such a test would show in an error. In case you have no access to a GPU, the Hugging Face team can take care of running those tests for you.\n\nNow, all the necessary functionality for brand_new_bert is added - you‚Äôre almost done! The only thing left to add is a nice docstring and a doc page. The Cookiecutter should have added a template file called that you should fill out. Users of your model will usually first look at this page before using your model. Hence, the documentation must be understandable and concise. It is very useful for the community to add some Tips to show how the model should be used. Don‚Äôt hesitate to ping the Hugging Face team regarding the docstrings.\n\nNext, make sure that the docstring added to is correct and included all necessary inputs and outputs. We have a detailed guide about writing documentation and our docstring format here. It is always good to remind oneself that documentation should be treated at least as carefully as the code in ü§ó Transformers since the documentation is usually the first contact point of the community with the model.\n\nGreat, now you have added all the necessary code for brand_new_bert. At this point, you should correct some potential incorrect code style by running:\n\nand verify that your coding style passes the quality check:\n\nThere are a couple of other very strict design tests in ü§ó Transformers that might still be failing, which shows up in the tests of your pull request. This is often because of some missing information in the docstring or some incorrect naming. The Hugging Face team will surely help you if you‚Äôre stuck here.\n\nLastly, it is always a good idea to refactor one‚Äôs code after having ensured that the code works correctly. With all tests passing, now it‚Äôs a good time to go over the added code again and do some refactoring.\n\nYou have now finished the coding part, congratulation! üéâ You are Awesome! üòé\n\n12. Upload the models to the model hub\n\nIn this final part, you should convert and upload all checkpoints to the model hub and add a model card for each uploaded model checkpoint. You can get familiar with the hub functionalities by reading our Model sharing and uploading Page. You should work alongside the Hugging Face team here to decide on a fitting name for each checkpoint and to get the required access rights to be able to upload the model under the author‚Äôs organization of brand_new_bert. The method, present in all models in , is a quick and efficient way to push your checkpoint to the hub. A little snippet is pasted below:\n\nIt is worth spending some time to create fitting model cards for each checkpoint. The model cards should highlight the specific characteristics of this particular checkpoint, e.g. On which dataset was the checkpoint pretrained/fine-tuned on? On what down-stream task should the model be used? And also include some code on how to correctly use the model.\n\nIt is very helpful to add a notebook that showcases in-detail how brand_new_bert can be used for inference and/or fine-tuned on a downstream task. This is not mandatory to merge your PR, but very useful for the community.\n\nYou‚Äôre done programming now and can move to the last step, which is getting your PR merged into main. Usually, the Hugging Face team should have helped you already at this point, but it is worth taking some time to give your finished PR a nice description and eventually add comments to your code, if you want to point out certain design choices to your reviewer.\n\nNow, it‚Äôs time to get some credit from the community for your work! Having completed a model addition is a major contribution to Transformers and the whole NLP community. Your code and the ported pre-trained models will certainly be used by hundreds and possibly even thousands of developers and researchers. You should be proud of your work and share your achievements with the community.\n\nYou have made another model that is super easy to access for everyone in the community! ü§Ø\n\nModel additions and their timeline: when is a model added to transformers?\n\nWe aim for to have support for new model architectures and checkpoints as early as possible: availability can range from day-0 (and hour-0) releases for some models, to a few days/weeks for others.\n\nThe availability of this is usually up to the model contributors, as well as how excited the community is for the architecture.\n\nWe can split the model architecture possibilities in four sections:\n\nLet‚Äôs dive into each of these and see how we (the transformers team) can help you contribute your architecture and get your architecture to be very easily used by all members of the community.\n\nFor a day-0 integration to work, we‚Äôll usually want to work hand-in-hand with you directly. In order to keep your architecture private until your checkpoints and release are ready, we‚Äôll work together in a private fork of transformers.\n\nIf you plan on having a transformers-first release, this is a great option: we run CI ahead of time, ensure the documentation is clear, and we aim to optimize your model as much as possible (providing quantization, optimizing it with Flash-Attention/SDPA, optimizing the KV cache, etc).\n\nWe can also lend you a hand in adding the model, reviewing it early, and help you make sure the API works as expected!\n\nIf this is the path you wish to go with, we ask for you to reach out in advance, especially if the architecture is particularly novel (at least a few days, but a few weeks will enable the absolute best integration). In order to reach out, please contact transformers@huggingface.co ü§ó.\n\nA same-week integration usually happens when model authors do not reach out; but we see significant community requests.\n\nIn order to specify you‚Äôd like for us to integrate a specific model, we‚Äôll redirect you to our issue tracker where you can request a specific model.\n\nThe more activity on the issue, the faster/more likely we are to integrate the model!\n\nA post-release integration usually happens when there has not been sufficient activity/requests to warrant a same-week integration, or that we lack the sufficient bandwidth to integrate it.\n\nWe very gladly welcome community contributions in those instances; more than half of the library was contributed by contributors external to Hugging Face. If this is something that is interesting to you, we recommend that you look at our open issues tagged with ‚ÄúNew model‚Äù.\n\nWe recommend you try your hand at a heavily requested model as this will multiply the impact of your contribution. We‚Äôll be there to help you in case that‚Äôs your first contribution ü§ó.\n\nFinally, transformers has a ‚Äúremote-code‚Äù possibility, in which contributions are not made within the toolkit, but on the Hub. This can be particularly interesting for groups that are using as a backbone for their project, but don‚Äôt have the bandwidth to contribute the model to transformers directly.\n\nIn case the model is very successful, then we‚Äôll very likely end up integrating it in at the end - as this provides better documentation, CI, maintenance, and optimizations - but this remains a great way to make your model accessible day-0 with minimal friction.\n\nThis guide is a great starting point for a Hub-first release: Custom models"
    },
    {
        "link": "https://medium.com/analytics-vidhya/installing-the-requirements-for-gpt-2-tensorflow-pytorch-and-transformers-9105348f10f4",
        "document": "‚ÄúI am become TensorFlow, the destroyer of environments.‚Äù\n\nHello fellow data enthusiasts. I‚Äôm currently working a fun AI writing project that will use GPT-2. I had a lot of heartache over installing the requirements to do this and got lost in an abundance of information that did not seem to help. So I thought I would share my success story in case it could help someone else also going through this struggle fest.\n\nI am assuming you‚Äôve already completed step 0 of having anaconda installed and in my case I am using gitbash for my command line interface. I am also running on Windows 10 and this may change things up if you are on a different system, especially if you intend to install pytorch. I‚Äôll show how it is done in step 3. Okay disclaimer over. Lets roll.\n‚Ä¢ Go to your home directory in GIT BASH using which stands for ‚Äòchange directory‚Äô.\n‚Ä¢ Then go to Anaconda environments using if you installed your anaconda in your home directory. If it‚Äôs not there you can find where you installed anaconda using\n‚Ä¢ Create a new environment. I‚Äôm calling my new environment gpt_env but you can substitute that with whatever you would like. IMPORTANT: USE PYTHON 3.7 or earlier model. 3.9 which is as of now the most up to date version of python will not work effectively with TensorFlow. 3.8 will work with TensorFlow 2 but if you intend to train gpt-2 you will need an older TensorFlow and an older python version. \n\n\n\nActivate the new environment by using conda activate. \n\n \n\nYou should be able to determine that we are in the new environment in your CLI. My environment appears before the username.\n\nI recommend installing TensorFlow first, because it has a lot of dependencies that can be messed up if the installation process is in the wrong order. I learned this the hard way, so you don‚Äôt have to‚Ä¶ hopefully.\n\nIt is highly important to note that TensorFlow 2 (the current version of TF as of this post) does not allow for training/fine-tuning of gpt2 without some creative modifications. If you intend to fine-tune gpt2 I recommend installing TensorFlow version 1.15. More details are available here : Install TensorFlow with pip. As I mentioned above TensorFlow 2 works on Python 8 and above, while TensorFlow 1.15 works on Python 7 and below.\n\nNext I installed PyTorch. PyTorch is great with gpt-2 for text generation which is why I‚Äôm using it. You will need to go to Start Locally | PyTorch and list your installation specifications.\n\nNext I installed Transformers from huggingface.co. Here is the installation guide : Installation ‚Äî transformers 4.3.0 documentation (huggingface.co).\n\nI found several methods of doing this. For my simple use case I decided to install gpt-2 on my system. It is available on the python project index here : pygpt-2-simple ¬∑ PyPI. It can be installed by entering this in the CLI.\n\nStep 5: Run Jupyter Lab, and use GPT-2 to Take Over the World!\n\nNow you are off to the fun part! Well I hope this works for you. If you have any issues comment below and let me know! \n\nFor a test run you can follow along with this video : Language Generation with OpenAI‚Äôs GPT-2 in Python from a fellow named James Briggs. It is pretty sweet what GPT-2 can do! Good Luck!"
    },
    {
        "link": "https://pytorch.org/hub/huggingface_pytorch-transformers",
        "document": "PyTorch-Transformers (formerly known as ) is a library of state-of-the-art pre-trained models for Natural Language Processing (NLP).\n\nThe library currently contains PyTorch implementations, pre-trained model weights, usage scripts and conversion utilities for the following models:\n‚Ä¢ BERT (from Google) released with the paper BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova.\n‚Ä¢ GPT (from OpenAI) released with the paper Improving Language Understanding by Generative Pre-Training by Alec Radford, Karthik Narasimhan, Tim Salimans and Ilya Sutskever.\n‚Ä¢ GPT-2 (from OpenAI) released with the paper Language Models are Unsupervised Multitask Learners by Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei** and Ilya Sutskever**.\n‚Ä¢ Transformer-XL (from Google/CMU) released with the paper Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context by Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov.\n‚Ä¢ XLNet (from Google/CMU) released with the paper ‚ÄãXLNet: Generalized Autoregressive Pretraining for Language Understanding by Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le.\n‚Ä¢ XLM (from Facebook) released together with the paper Cross-lingual Language Model Pretraining by Guillaume Lample and Alexis Conneau.\n‚Ä¢ RoBERTa (from Facebook), released together with the paper a Robustly Optimized BERT Pretraining Approach by Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov.\n‚Ä¢ DistilBERT (from HuggingFace), released together with the blogpost Smaller, faster, cheaper, lighter: Introducing DistilBERT, a distilled version of BERT by Victor Sanh, Lysandre Debut and Thomas Wolf.\n\nThe components available here are based on the and classes of the library.\n\nUnlike most other PyTorch Hub models, BERT requires a few additional Python packages to be installed.\n\nThe available methods are the following:\n‚Ä¢ : returns a configuration item corresponding to the specified model or pth.\n‚Ä¢ : returns a tokenizer corresponding to the specified model or path\n‚Ä¢ : returns a model corresponding to the specified model or path\n‚Ä¢ : returns a model with a language modeling head corresponding to the specified model or path\n‚Ä¢ : returns a model with a sequence classifier corresponding to the specified model or path\n‚Ä¢ : returns a model with a question answering head corresponding to the specified model or path\n\nAll these methods share the following argument: , which is a string identifying a pre-trained model or path from which an instance will be returned. There are several checkpoints available for each model, which are detailed below:\n\nThe available models are listed on the transformers documentation, models page.\n\nHere are a few examples detailing the usage of each available method.\n\nThe tokenizer object allows the conversion from character strings to tokens understood by the different models. Each model has its own tokenizer, and some tokenizing methods are different across tokenizers. The complete documentation can be found here.\n\nThe model object is a model instance inheriting from a . Each model is accompanied by their saving/loading methods, either from a local file or directory, or from a pre-trained configuration (see previously described ). Each model works differently, a complete overview of the different models can be found in the documentation.\n\nThe configuration is optional. The configuration object holds information concerning the model, such as the number of heads/layers, if the model should output attentions or hidden states, or if it should be adapted for TorchScript. Many parameters are available, some specific to each model. The complete documentation can be found here.\n\nHere is an example on how to tokenize the input text to be fed as input to a BERT model, and then get the hidden states computed by such a model or predict masked tokens using language modeling BERT model.\n\nUsing to encode the input sentence in a sequence of last layer hidden-states\n\nUsing to predict a masked token with BERT\n\nUsing to do question answering with BERT\n\n# The format is paragraph first and then question # Or get the total loss which is the sum of the CrossEntropy loss for the start and end token positions (set model to train mode before if used for training)\n\nUsing to do paraphrase classification with BERT"
    },
    {
        "link": "https://erickleppen.medium.com/run-open-source-large-language-models-locally-using-hugging-face-transformers-617d358462d2",
        "document": "Why Go Open-source When GPT-4 Exists?\n\nAlthough close, open-source large language models (LLMs) can‚Äôt yet match the power and accuracy of the closed-source, commercially available apps like GPT-4 and Bard (Gemini). Even though they are less powerful, there are several reasons why you might want to run open-source LLMs locally:\n‚Ä¢ Local models are free to use on your own machine while commercial models like GPT-4 are often pay-to-use APIs.\n‚Ä¢ Local models can be used without an internet connection while GPT-4 requires you to be connected.\n‚Ä¢ Local models offer you privacy while GPT-4 can monitor your usage and use your data for training.\n‚Ä¢ Local models can produce uncensored content while closed-source models are censored and use guardrails to prevent the model from producing certain content.\n\nThe censorship on closed-source LLMs has become a hotly discussed topic. For example, if you ask Chat-GPT, ‚ÄúWhat are some deadly poisons?‚Äù it will respond by saying it cannot provide information on harmful activities. When I ask my locally running LLM‚Ä¶"
    }
]